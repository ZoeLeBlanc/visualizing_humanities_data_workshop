doc_content|doc_title|doc_url|doc_key|doc_date|doc_year|doc_lang|doc_tdmcat|doc_srccat
Two Bayesian optimal design criteria for hierarchical linear models are discussed – the ψβ criterion for the estimation of individual-level parameters β, and the ψθ criterion for the estimation of hyperparameters θ. We focus on a specific case in which all subjects receive the same set of treatments and in which the covariates are independent of treatments. We obtain the explicit structure of ψβ- and ψθ- optimal continuous (approximate) designs for the case of independent random effects, and for some special cases of correlated random effects. Through examples and simulations, we compare ψβ- and ψθ-optimal designs under more general scenarios of correlated random effects. While orthogonal designs are often ψβ-optimal even when the random effects are correlated, ψθ-optimal designs tend to be nonorthogonal and unbalanced. In our study of the robustness of ψβ- and ψθ-optimal designs, both types of designs are found to be insensitive to various specifications of the response errors and the variances of the random effects, but sensitive to the specifications of the signs of the correlations of the random effects.|BAYESIAN DESIGNS FOR HIERARCHICAL LINEAR MODELS|http://www.jstor.org/stable/24310154|24310154|2012-01-01|2012|['eng']|['Mathematics - Mathematical logic']|['Mathematics', 'Science and Mathematics', 'Statistics']
"A regular supply of applicants to Queen's University in Kingston, Ontario is provided by 65 high schools. Each high school can be characterized by a series of grading standards which change from year to year. To aid admissions decisions, it is desirable to forecast the current year's grading standards for all 65 high schools using grading standards estimated from past year's data. We develop and apply a Bayesian break-point time-series model that generates forecasts which involve smoothing across time for each school and smoothing across schools. ""Break point"" refers to a point in time which divides the past into the ""old past"" and the ""recent past"" where the yearly observations in the recent past are exchangeable with the observations in the year to be forecast. We show that this model works fairly well when applied to 11 years of Queen's University data. The model can be applied to other data sets with the parallel time-series structure and short history, and can be extended in several ways to more complicated structures. /// Une bonne partie de la clientèle de l'université Queen's (à Kingston, en Ontario) provient chaque année des mêmes écoles secondaires. Le niveau moyen de préparation des finissants de ces 65 écoles peut être représenté numériquement par un indice dont la valeur change d'année en année. Pour faciliter l'évaluation des dossiers, on désirait prévoir cette valeur pour chacune des écoles à partir des données des années antérieures. A cette fin, nous avons élaboré et testé un modèle bayésien de ces séries chronologiques d'indices. Les données ont été lissées afin de tenir compte des variations dans le temps et entre les écoles. Des points de rupture permettant de discerner entre le passé récent et ancien ont également été incorporés au modèle et les données des années récentes ont été considérées comme échangeables avec les observations prédites. Ce modèle donne des résultats acceptables pour les données accumulées pendant 11 ans par l'université Queen's. Il se prête bien aux situations où l'on a affaire à plusieurs petites séries chronologiques parallèles et il peut être généralisé de diverses manières afin de tenir compte de structures plus compliquées."|Bayesian Break-Point Forecasting in Parallel Time Series, with Application to University Admissions|http://www.jstor.org/stable/3314857|3314857|1987-03-01|1987|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Multivariate hierarchical Bayesian models provide a flexible framework for comprehensive study of biological systems with more than one outcome. Recent methodological developments facilitate modeling of heterogeneous associations between outcomes by specifying a linear mixed model on (co)variances at different levels of the data structure. Motivated by previous evidence for heterogeneous correlations in animal agriculture, we apply the proposed hierarchical Bayesian models to study the nature of the correlations between key performance outcomes in dairy cattle production systems, namely milk yield and reproduction. That is, the association between these outcomes might depend upon various fixed and random effect sources of heterogeneity both at the individual cow (residual) level as well as the herd (cluster) level. We thus propose a sequential modeling approach based on the deviance information criterion to select relevant explanatory variables on both types of associations. Furthermore, we extend the proposed methodology to accommodate right-censored outcomes, as common for dairy reproduction data, and use it to analyze field data from the Michigan dairy industry. The nature of the associations between milk production and reproduction in dairy cattle was inferred to be strongly heterogeneous and driven by multiple farm management practices and herd attributes, as well as by random clustering effects, at both cow and herd levels, thereby suggesting potential between-herd and within-herd intervention strategies to optimize performance of dairy production systems. Supplementary materials are available online.|Inferring Upon Heterogeneous Associations in Dairy Cattle Performance Using a Bivariate Hierarchical Model|http://www.jstor.org/stable/23208850|23208850|2012-04-01|2012|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Agriculture', 'Statistics']
The sampling/importance resampling algorithm is an approximate noniterative sampling method. The algorithm has been used on many occasions to select an approximate random sample of size m from a target distribution from M input random variates. The selection mechanism is an unequal probability sampling with weights being the importance weights. As the weights are random, sampling without replacement is not always possible and some input variates may have more than one copy in the final sample.Duplication of values in the final output is undesirable as it means dependence among the output variates. In this paper a general and simple determination rule for M is proposed. It keeps the duplication problem at a tolerably low level when a tight resampling method is used. We show that (a) M = O(m) if and only if the importance weight is bounded above, (b) if the importance weight has a moment generating function, the suggested M is of order O(mln(m)), and (c) M may need to be as large as O(mc/(c−1)) if the importance weight has finite c-th moment for a c &gt; 1. A procedure is suggested to determine M numerically. The method is tested on the Pareto, Gamma and Beta distributions, and gives satisfactory results.|POOL SIZE SELECTION FOR THE SAMPLING/IMPORTANCE RESAMPLING ALGORITHM|http://www.jstor.org/stable/24307704|24307704|2007-07-01|2007|['eng']|['Mathematics - Applied mathematics']|['Mathematics', 'Science and Mathematics', 'Statistics']
Spline and generalized spline smoothing is shown to be equivalent to Bayesian estimation with a partially improper prior. This result supports the idea that spline smoothing is a natural solution to the regression problem when one is given a set of regression functions but one also wants to hedge against the possibility that the true model is not exactly in the span of the given regression functions. A natural measure of the deviation of the true model from the span of the regression functions comes out of the spline theory in a natural way. An appropriate value of this measure can be estimated from the data and used to constrain the estimated model to have the estimated deviation. Some convergence results and computational tricks are also discussed.|Improper Priors, Spline Smoothing and the Problem of Guarding Against Model Errors in Regression|http://www.jstor.org/stable/2984701|2984701|1978-01-01|1978|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
"In this article, we present new methods to analyze data from an experiment using rodent models to investigate the role of p27, an important cell-cycle mediator, in early colon carcinogenesis. The responses modeled here are essentially functions nested within a two-stage hierarchy. Standard functional data analysis literature focuses on a single stage of hierarchy and conditionally independent functions with near white noise. However, in our experiment, there is substantial biological motivation for the existence of spatial correlation among the functions, which arise from the locations of biological structures called colonic crypts: this possible functional correlation is a phenomenon we term crypt signaling. Thus, as a point of general methodology, we require an analysis that allows for functions to be correlated at the deepest level of the hierarchy. Our approach is fully Bayesian and uses Markov chain Monte Carlo methods for inference and estimation. Analysis of this data set gives new insights into the structure of p27 expression in early colon carcinogenesis and suggests the existence of significant crypt signaling. Our methodology uses regression splines, and because of the hierarchical nature of the data, dimension reduction of the covariance matrix of the spline coefficients is important: we suggest simple methods for overcoming this problem. /// Dans cet article nous présentons de nouvelles méthodes pour analyser les données d'une expérience utilisant des modèles de rongeur pour investiguer le rôle du p27, un important médiateur du cycle cellulaire, dans la carcinogenèse précoce du colon. Les réponses étudiées ici sont essentiellement des fonctions emboîtées dans une hiérarchie à 2 niveaux. La littérature concernant l'analyse standard de données fonctionnelles se concentre sur un seul niveau de hiérarchie, et sur des fonctions conditionnellement indépendantes avec bruit presque blanc. Cependant, dans notre expérience, il y a des arguments biologiques substantiels en faveur de l'existence de corrélations entre les fonctions, qui tirent leur origine de la localisation de structures biologiques appelées ""cryptes de colonisation"": cette possible corrélation fonctionnelle est un phénomène que nous appelons ""signalisation entre cryptes"". Ainsi, en tant qu'aspect de méthodologie générale, nous avons besoin d'une analyse qui permette que les fonctions soient corrélées au niveau le plus profond de la hiérarchie. Notre approche est complètement bayesienne et utilise des méthodes de Monte Carlo par Chaînes de Markov pour l'inférence et l'estimation. L'analyse de ce jeu de données donne un nouvel aperçu de la structure de l'expression du p27 dans la carcinogénèse précoce du colon, et suggère l'existence d'une ""signalisation entre cryptes"" significative. Notre méthodologie utilise des splines de régression, et à cause de la nature hiérarchique des données, la réduction de dimension de la matrice de covariance des coefficients de spline est importante: nous suggérons des méthodes simples pour surmonter ce problème."|Bayesian Hierarchical Spatially Correlated Functional Data Analysis with Application to Colon Carcinogenesis|http://www.jstor.org/stable/25502022|25502022|2008-03-01|2008|['eng']|['Applied sciences - Engineering', 'Biological sciences - Biology']|['Science & Mathematics', 'Statistics']
"This paper deals with the analysis of multivariate survival data from a Bayesian perspective using Markov-chain Monte Carlo methods. The Metropolis along with the Gibbs algorithm is used to calculate some of the marginal posterior distributions. A multivariate survival model is proposed, since survival times within the same group are correlated as a consequence of a frailty random block effect. The conditional proportional-hazards model of Clayton and Cuzick is used with a martingale structured prior process (Arjas and Gasbarra) for the discretized baseline hazard. Besides the calculation of the marginal posterior distributions of the parameters of interest, this paper presents some Bayesian EDA diagnostic techniques to detect model adequacy. The methodology is exemplified with kidney infection data where the times to infections within the same patients are expected to be correlated. /// Cet article a pour objet l'analyse de données de survie multivariées dans un perspective bayesienne utilisant des méthodes de chaîne de Markov Monte Carlo. Metropolis, ainsi que l'algorithme de Gibbs est utilisé pour calculer les distributions marginales a posteriori. Nous proposons un modèle de survie multivariée puisque les temps de survie au sein d'un même ""groupe"" sont en corrélation en conséquence d'un effet de bloc aléatoire de fragillité. Le modèle des hasards conditionnels proportionnels de Clayton et Cuzick est utilisé avec un processus a priori structuré en martingale (Arjas et Gasbarra) pour le hasard de référence rendu discret. A part le calcul des distributions a posteriori marginales des paramètres d'intérêt, cet article présente certaines techniques bayesiennes de diagnostic EDA pour détecter l'insuffisance du modèle. La méthodologie est illustrée par des données sur l'infection du rein où l'on s'attend à ce que les temps avant les infections chez les même patients soient en corrélation."|Bayesian Analysis of Multivariate Survival Data Using Monte Carlo Methods|http://www.jstor.org/stable/3315671|3315671|1998-03-01|1998|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
We consider the on-line Bayesian analysis of data by using a hidden Markov model, where inference is tractable conditional on the history of the state of the hidden component. A new particle filter algorithm is introduced and shown to produce promising results when analysing data of this type. The algorithm is similar to the mixture Kalman filter but uses a different resampling algorithm. We prove that this resampling algorithm is computationally efficient and optimal, among unbiased resampling algorithms, in terms of minimizing a squared error loss function. In a practical example, that of estimating break points from well-log data, our new particle filter outperforms two other particle filters, one of which is the mixture Kalman filter, by between one and two orders of magnitude.|On-Line Inference for Hidden Markov Models via Particle Filters|http://www.jstor.org/stable/3647589|3647589|2003-01-01|2003|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
1. To ensure the successful detection, control and eradication of invasive plant species, we need information that can identify areas prone to invasions and criteria that can point out which particular populations may become foci of further spread. Specifically, our work aimed to develop statistical models that identify hotspots of invasive plant species and evaluate the conditions that give rise to successful populations of invasive species. 2. We combined extensive data sets on invasive species richness and on species per cent ground cover, together with climate, local habitat and land cover data. We then estimated invasive species richness as a function of those environmental variables by developing a spatially explicit generalized linear model within a hierarchical Bayesian framework. In a second analysis, we used an ordinal logistic regression model to quantify invasive species abundance as a function of the same set of predictor variables. 3. Our results show which locations in the studied region, north-eastern USA, are prone to plant species invasions given the combination of climatic and land cover conditions particular to the sites. Predictions were also generated under a range of climate scenarios forecasted for the region, which pointed out at an increase in invasive species incidence under the most moderate forecast. Predicted abundance for some of the most common invasive plant species, Berberis thumbergii, Celastrus orbiculatus, Euonymus alata, Elaeagnus umbellata and Rosa multiflora, allowed us to identify the specific conditions that promote successful population growth of these species, populations that could become foci of further spread. 4. Synthesis and applications. Reliable predictions of plants' invasive potential are crucial for the successful implementation of control and eradication management plans. By following a multivariate approach the parameters estimated in this study can now be used on targeted locations to evaluate the risk of invasions given the local climate and landscape structure; they can also be applied under different climate scenarios and changing landscapes providing an array of possible outcomes. In addition, this modelling approach can be easily used in other regions and for other species.|Identifying Hotspots for Plant Invasions and Forecasting Focal Points of Further Spread|http://www.jstor.org/stable/25623112|25623112|2009-12-01|2009|['eng']|['Biological sciences - Biogeography', 'Biological sciences - Biology', 'Biological sciences - Ecology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
Colorectal cancer (CRC) IS the third leading cause of cancer death in the US, and stage at diagnosis is the primary prognostic factor. To date, the interplay between geographic place and individual characteristics such as cancer stage with CRC survival is unexplored. We used a Bayesian geosurvival statistical model to evaluate whether the spatial patterns of CRC survival at the census tract level varies by stage at diagnosis (in situ/local, regional, distant), controlling for patient characteristics, surveillance test use, and treatment using linked 1991–2005 SEER-Medicare data of patients ≥ 66 years old in two US metropolitan areas. The spatial pattern of survival varied by stage at diagnosis for both cancer sites and registries. Significant spatial effects were identified in all census tracts for colon cancer and the majority of census tracts for rectal cancer. Geographic disparities appeared to be highest for distant-stage rectal cancer. Compared to those with in situ/local stage in the same census tracts, patients with distant-stage cancer were at most 7.73 times and 4.69 times more likely to die of colon and rectal cancer, respectively. Moreover, frailty areas for CRC at in situ/local stage more likely have a higher relative risk at regional stage, but not at distant stage. We identified geographic areas with excessive risk of CRC death and demonstrated that spatial patterns varied by both cancer type and cancer stage. More research is needed to understand the moderating pathways between geographic and individuallevel factors on CRC survival.|The modifying effect of patient location on stage-specific survival following colorectal cancer using geosurvival models|http://www.jstor.org/stable/24717872|24717872|2013-03-01|2013|['eng']|['Health sciences - Medical specialties']|['Medicine & Allied Health', 'Health Sciences']
Colorectal cancer is the second leading cause of cancer related deaths in the United States, with more than 130,000 new cases of colorectal cancer diagnosed each year. Clinical studies have shown that genetic alterations lead to different responses to the same treatment, despite the morphologic similarities of tumors. A molecular test prior to treatment could help in determining an optimal treatment for a patient with regard to both toxicity and efficacy. This article introduces a statistical method appropriate for predicting and comparing multiple endpoints given different treatment options and molecular profiles of an individual. A latent variable-based multivariate regression model with structured variance covariance matrix is considered here. The latent variables account for the correlated nature of multiple endpoints and accommodate the fact that some clinical endpoints are categorical variables and others are censored variables. The mixture normal hierarchical structure admits a natural variable selection rule. Inference was conducted using the posterior distribution sampling Markov chain Monte Carlo method. We analyzed the finite-sample properties of the proposed method using simulation studies. The application to the advanced colorectal cancer study revealed associations between multiple endpoints and particular biomarkers, demonstrating the potential of individualizing treatment based on genetic profiles.|Bayesian Variable Selection with Joint Modeling of Categorical and Survival Outcomes: An Application to Individualizing Chemotherapy Treatment in Advanced Colorectal Cancer|http://www.jstor.org/stable/20640623|20640623|2009-12-01|2009|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
This paper describes a general Bayesian iterative algorithm with entropy prior for image reconstruction. It solves the cases of both pure Poisson data and Poisson data with Gaussian readout noise. The algorithm maintains positivity of the solution; it includes case-specific prior information (default map) and flatfield corrections; it removes background and can be accelerated to be faster than the Richardson-Lucy algorithm. In order to determine the hyperparameter that balances the entropy and likelihood terms in the Bayesian approach, we have used a likelihood cross-validation technique. Cross-validation is more robust than other methods because it is less demanding in terms of the knowledge of exact data characteristics and of the point-spread function. We have used the algorithm to reconstruct successfully images obtained in different space-and ground-based imaging situations. It has been possible to recover most of the original intended capabilities of the Hubble Space Telescope wide field and planetary camera and faint object camera from images obtained in their present state. Semireal simulations for the future wide field planetary camera 2 show that even after the repair of the spherical aberration problem, image reconstruction can play a key role in improving the resolution of the cameras, well beyond the design of the Hubble instruments. We also show that ground-based images can be reconstructed successfully with the algorithm. A technique which consists of dividing the CCD observations into two frames, with one-half the exposure time each, emerges as a recommended procedure for the utilization of the described algorithms. We have compared our technique with two commonly used reconstruction algorithms: the Richardson-Lucy and the Cambridge maximum entropy algorithms.|A General Bayesian Image Reconstruction Algorithm with Entropy Prior. Preliminary Application to HST Data|http://www.jstor.org/stable/40680178|40680178|1993-10-01|1993|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Astronomy']
A major practical impediment when implementing adaptive dose-finding designs is that the toxicity outcome used by the decision rules may not be observed shortly after the initiation of the treatment. To address this issue, we propose the data augmentation continual reassessment method (DA-CRM) for dose finding. By naturally treating the unobserved toxicities as missing data, we show that such missing data are nonignorable in the sense that the missingness depends on the unobserved outcomes. The Bayesian data augmentation approach is used to sample both the missing data and model parameters from their posterior full conditional distributions. We evaluate the performance of the DA-CRM through extensive simulation studies and also compare it with other existing methods. The results show that the proposed design satisfactorily resolves the issues related to late-onset toxicities and possesses desirable operating characteristics: treating patients more safely and also selecting the maximum tolerated dose with a higher probability. The new DA-CRM is illustrated with two phase I cancer clinical trials.|BAYESIAN DATA AUGMENTATION DOSE FINDING WITH CONTINUAL REASSESSMENT METHOD AND DELAYED TOXICITY|http://www.jstor.org/stable/23566457|23566457|2013-12-01|2013|['eng']|['Physical sciences - Astronomy', 'Health sciences - Health and wellness', 'Health sciences - Medical treatment']|['Mathematics', 'Science and Mathematics', 'Statistics']
Climate change will affect the insurance industry. We develop a Bayesian hierarchical statistical approach to explain and predict insurance losses due to weather events at a local geographic scale. The number of weather-related insurance claims is modelled by combining generalized linear models with spatially smoothed variable selection. Using Gibbs sampling and reversible jump Markov chain Monte Carlo methods, this model is fitted on daily weather and insurance data from each of the 319 municipalities which constitute southern and central Norway for the period 1997—2006. Precise out-of-sample predictions validate the model. Our results show interesting regional patterns in the effect of different weather covariates. In addition to being useful for insurance pricing, our model can be used for short-term predictions based on weather forecasts and for long-term predictions based on downscaled climate models.|A Bayesian hierarchical model with spatial variable selection: the effect of weather on insurance claims|http://www.jstor.org/stable/23360978|23360978|2013-01-01|2013|['eng']|['Physical sciences - Astronomy']|['Science and Mathematics', 'Statistics']
Although both clustering and identification of differentially expressed genes are equally essential in most microarray studies, the two tasks are often conducted without regard to each other. This is clearly not the most efficient way of extracting information. The main aim of this article is to develop a coherent statistical method that can simultaneously cluster and detect differentially expressed genes. Through information sharing between the two tasks, the proposed approach gives more sensible clustering among genes and is more sensitive in identifying differentially expressed genes. The improvement, over existing methods is illustrated in both our simulation results and a case study.|A Unified Approach for Simultaneous Gene Clustering and Differential Expression Identification|http://www.jstor.org/stable/4124530|4124530|2006-12-01|2006|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
In this paper we introduce a Bayesian analysis for binary data in the presence of covariates and misclassifications. As a special situation in diagnostic medical testing, we obtain Bayesian inferences for the sensitivity and the specificity in the presence of covariates. We consider a situation where the individuals can be verified or unverified about their real disease status after a test. When part or even all individuals are not verified, usually we have great difficulties to get classical inference results for the parameters of interest. For this situation, the introduction of latent variables gives a good alternative to deal with missing data under the Bayesian approach, specially using Markov chain monte Carlo (MCMC) methods to obtain the posterior summaries of interest. We illustrate the proposed methodology on three real data sets.|Binary data in the presence of covariates and misclassifications: a Bayesian approach|http://www.jstor.org/stable/43601056|43601056|2005-06-01|2005|['eng']|['Applied sciences - Engineering', 'Health sciences - Health and wellness']|['Science & Mathematics', 'Mathematics', 'Statistics']
Growth curve data consist of repeated measurements of a continuous growth process over time in a population of individuals. These data are classically analyzed by nonlinear mixed models. However, the standard growth functions used in this context prescribe monotone increasing growth and can fail to model unexpected changes in growth rates. We propose to model these variations using stochastic differential equations (SDEs) that are deduced from the standard deterministic growth function by adding random variations to the growth dynamics. A Bayesian inference of the parameters of these SDE mixed models is developed. In the case when the SDE has an explicit solution, we describe an easily implemented Gibbs algorithm. When the conditional distribution of the diffusion process has no explicit form, we propose to approximate it using the Euler-Maruyama scheme. Finally, we suggest validating the SDE approach via criteria based on the predictive posterior distribution. We illustrate the efficiency of our method using the Gompertz function to model data on chicken growth, the modeling being improved by the SDE approach.|Bayesian Analysis of Growth Curves Using Mixed Models Defined by Stochastic Differential Equations|http://www.jstor.org/stable/40962444|40962444|2010-09-01|2010|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Despite its shortcomings, cross-level or ecological inference remains a necessary part of some areas of quantitative inference, including in United States voting rights litigation. Ecological inference suffers from a lack of identification that, most agree, is best addressed by incorporating individuallevel data into the model. In this paper we test the limits of such an incorporation by attempting it in the context of drawing inferences about racial voting patterns using a combination of an exit poll and precinct-level ecological data; accurate information about racial voting patterns is needed to assess triggers in voting rights laws that can determine the composition of United States legislative bodies. Specifically, we extend and study a hybrid model that addresses two-way tables of arbitrary dimension. We apply the hybrid model to an exit poll we administered in the City of Boston in 2008. Using the resulting data as well as simulation, we compare the performance of a pure ecological estimator, pure survey estimators using various sampling schemes and our hybrid. We conclude that the hybrid estimator offers substantial benefits by enabling substantive inferences about voting patterns not practicably available without its use.|EXIT POLLING AND RACIAL BLOC VOTING: COMBINING INDIVIDUAL-LEVEL AND R x C ECOLOGICAL DATA|http://www.jstor.org/stable/23362448|23362448|2010-12-01|2010|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
Because of the decreasing cost and high digital resolution, next-generation sequencing (NGS) is expected to replace the traditional hybridization-based microarray technology. For genetics study, the first-step analysis of NGS data is often to identify genomic variants among sequenced samples. Several statistical models and tests have been developed for variant calling in NGS study. The existing approaches, however, are based on either conventional Bayesian or frequentist methods, which are unable to address the multiplicity and testing efficiency issues simultaneously. In this paper, we derive an optimal empirical Bayes testing procedure to detect variants for NGS study. We utilize the empirical Bayes technique to exploit the across-site information among many testing sites in NGS data. We prove that our testing procedure is valid and optimal in the sense of rejecting the maximum number of nonnulls while the Bayesian false discovery rate is controlled at a given nominal level. We show by both simulation studies and real data analysis that our testing efficiency can be greatly enhanced over the existing frequentist approaches that fail to pool and utilize information across the multiple testing sites.|AN EMPIRICAL BAYES TESTING PROCEDURE FOR DETECTING VARIANTS IN ANALYSIS OF NEXT GENERATION SEQUENCING DATA|http://www.jstor.org/stable/23566461|23566461|2013-12-01|2013|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science and Mathematics', 'Statistics']
"We propose new approaches for choosing the shrinkage parameter in ridge regression, a penalized likelihood method for regularizing linear regression coefficients, when the number of observations is small relative to the number of parameters. Existing methods may lead to extreme choices of this parameter, either shrinking the coefficients insufficiently or by too much. Within this ""small-n, large-p"" context, we suggest a correction to the common generalized cross-validation (GCV) method that preserves the asymptotic optimality of the original GCV. We also introduce the notion of a ""hyperpenalty"", which shrinks the shrinkage parameter itself, and make a specific recommendation regarding the choice of hyperpenalty that empirically works well in a broad range of scenarios. A simple algorithm jointly estimates the shrinkage parameter and regression coefficients in the hyperpenalized likelihood. In a comprehensive simulation study of small-sample scenarios and in the analysis of a gene-expression dataset, our proposed approaches offer superior prediction over nine other existing methods."|A SMALL-SAMPLE CHOICE OF THE TUNING PARAMETER IN RIDGE REGRESSION|http://www.jstor.org/stable/24721227|24721227|2015-07-01|2015|['eng']|['Mathematics - Applied mathematics']|['Mathematics', 'Science & Mathematics', 'Statistics']
Gaussian process models have been widely used in spatial statistics but face tremendous computational challenges for very large data sets. The model fitting and spatial prediction of such models typically require ○(n³) operations for a data set of size n. Various approximations of the covariance functions have been introduced to reduce the computational cost. However, most existing approximations cannot simultaneously capture both the large-and the small-scale spatial dependence. A new approximation scheme is developed to provide a high quality approximation to the covariance function at both the large and the small spatial scales. The new approximation is the summation of two parts: a reduced rank covariance and a compactly supported covariance obtained by tapering the covariance of the residual of the reduced rank approximation. Whereas the former part mainly captures the large-scale spatial variation, the latter part captures the small-scale, local variation that is unexplained by the former part. By combining the reduced rank representation and sparse matrix techniques, our approach allows for efficient computation for maximum likelihood estimation, spatial prediction and Bayesian inference. We illustrate the new approach with simulated and real data sets.|A full scale approximation of covariance functions for large spatial data sets|http://www.jstor.org/stable/41430931|41430931|2012-01-01|2012|['eng']|['Mathematics - Applied mathematics']|['Science and Mathematics', 'Statistics']
For multivariate failure time data, we propose a new class of shared gamma frailty models by imposing the Box-Cox transformation on the hazard function, and the product of the baseline hazard and the frailty. This novel class of models allows for a very broad range of shapes and relationships between the hazard and baseline hazard functions. It includes the well-known Cox gamma frailty model and a new additive gamma frailty model as two special cases. Due to the nonnegative hazard constraint, this shared gamma frailty model is computationally challenging in the Bayesian paradigm. The joint priors are constructed through a conditional-marginal specification, in which the conditional distribution is univariate, and it absorbs the nonlinear parameter constraints. The marginal part of the prior specification is free of constraints. The prior distributions allow us to easily compute the full conditionals needed for Gibbs sampling, while incorporating the constraints. This class of shared gamma frailty models is illustrated with a real dataset.|A Class of Bayesian Shared Gamma Frailty Models with Multivariate Failure Time Data|http://www.jstor.org/stable/3695664|3695664|2005-03-01|2005|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
The stylized facts of macroeconomic time series can be presented by fitting structural time series models. Within this framework, we analyse the consequences of the widely used detrending technique popularised by Hodrick and Prescott (1980). It is shown that mechanical detrending based on the Hodrick-Prescott filter can lead investigators to report spurious cyclical behaviour, and this point is illustrated with empirical examples. Structural time-series models also allow investigators to deal explicitly with seasonal and irregular movements that may distort estimated cyclical components. Finally, the structural framework provides a basis for exposing the limitations of ARIMA methodology and models based on a deterministic trend with a single break.|Detrending, Stylized Facts and the Business Cycle|http://www.jstor.org/stable/2284917|2284917|1993-07-01|1993|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Business', 'Economics']
This study provides a Bayesian investigation of rank-ordered multinominal logit models employing conjugate priors for standard multinomial logit models as well as other priors. Also considered is a specification test of the independence of irrelevant alternatives assumption. The proposed techniques are demonstrated in an empirical investigation of Ontario voter preferences before the 1988 Canadian Federal Election.|Rank-Ordered Logit Models: An Empirical Analysis of Ontario Voter Preferences|http://www.jstor.org/stable/2285287|2285287|1994-10-01|1994|['eng']|['Mathematics - Mathematical logic']|['Business & Economics', 'Business', 'Economics']
Bayesian analysis of data from the general linear mixed model is challenging because any nontrivial prior leads to an intractable posterior density. However, if a conditionally conjugate prior density is adopted, then there is a simple Gibbs sampler that can be employed to explore the posterior density. A popular default among the conditionally conjugate priors is an improper prior that takes a product form with a flat prior on the regression parameter, and so-called power priors on each of the variance components. In this paper, a convergence rate analysis of the corresponding Gibbs sampler is undertaken. The main result is a simple, easily-checked sufficient condition for geometric ergodicity of the Gibbs-Markov chain. This result is close to the best possible result in the sense that the sufficient condition is only slightly stronger than what is required to ensure posterior propriety. The theory developed in this paper is extremely important from a practical standpoint because it guarantees the existence of central limit theorems that allow for the computation of valid asymptotic standard errors for the estimates computed using the Gibbs sampler.|CONVERGENCE ANALYSIS OF THE GIBBS SAMPLER FOR BAYESIAN GENERAL LINEAR MIXED MODELS WITH IMPROPER PRIORS|http://www.jstor.org/stable/41806558|41806558|2012-12-01|2012|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
We study the consistency of a Bayesian variable selection procedure for generalized linear models. Specifically, we consider the consistency of a Bayes factor based on g-priors proposed by Sabanés Bové and Held [Bayesian Analysis 6 (2011) 387–410]. The integrals necessary for the computation of this Bayes factor are performed with Laplace approximation and Gaussian quadrature. We show that, under certain regularity conditions, the resulting Bayes factor is consistent. Furthermore, a simulation study confirms our theoretical results. Finally, we illustrate this model selection procedure with an application to a real ecological dataset.|Consistency of hyper-g-prior-based Bayesian variable selection for generalized linear models|http://www.jstor.org/stable/24913888|24913888|2016-11-01|2016|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Mathematics', 'Statistics']
In this paper we describe the use of modern numerical integration methods for making posterior inferences in composed error stochastic frontier models for panel data or individual cross-sections. Two Monte Carlo methods have been used in practical applications. We survey these two methods in some detail and argue that Gibbs sampling methods can greatly reduce the computational difficulties involved in analyzing such models.|Numerical Tools for the Bayesian Analysis of Stochastic Frontier Models|http://www.jstor.org/stable/41770863|41770863|1998-07-01|1998|['eng']|['Mathematics - Mathematical logic']|['Business', 'Business & Economics Collection']
"Wildlife management is increasingly guided by analyses of large and complex datasets. The description of such datasets often requires a large number of parameters, among which certain patterns might be discernible. For example, one may consider a long-term study producing estimates of annual survival rates; of interest is the question whether these rates have declined through time. Several statistical methods exist for examining pattern in collections of parameters. Here, I argue for the superiority of ""random effects models"" in which parameters are regarded as random variables, with distributions governed by ""hyperparameters"" describing the patterns of interest. Unfortunately, implementation of random effects models is sometimes difficult. Ultrastructural models, in which the postulated pattern is built into the parameter structure of the original data analysis, are approximations to random effects models. However, this approximation is not completely satisfactory: failure to account for natural variation among parameters can lead to overstatement of the evidence for pattern among parameters. I describe quasi-likelihood methods that can be used to improve the approximation of random effects models by ultrastructural models."|Modeling Pattern in Collections of Parameters|http://www.jstor.org/stable/3802817|3802817|1999-07-01|1999|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Biological Sciences', 'Zoology']
We develop a method to carry out MAP estimation for a class of Bayesian regression models in which coefficients are assigned with Gaussian-based spike and slab priors. The objective function in the corresponding optimization problem has a Lagrangian form in that regression coefficients are regularized by a mixture of squared l 2 and l 0 norms. A tight approximation to the l 0 norm using majorization—minimization techniques is derived, and a coordinate descent algorithm in conjunction with a soft-thresholding scheme is used in searching for the optimizer of the approximate objective. Simulation studies show that the proposed method can lead to more accurate variable selection than other benchmark methods. Theoretical results show that under regular conditions, sign consistency can be established, even when the Irrepresentable Condition is violated. Results on posterior model consistency and estimation consistency, and an extension to parameter estimation in the generalized linear models are provided.|A MAJORIZATION—MINIMIZATION APPROACH TO VARIABLE SELECTION USING SPIKE AND SLAB PRIORS|http://www.jstor.org/stable/23033614|23033614|2011-06-01|2011|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Finite sample properties of multiple imputation estimators under the linear regression model are studied. The exact bias of the multiple imputation variance estimator is presented. A method of reducing the bias is presented and simulation is used to make comparisons. We also show that the suggested method can be used for a general class of linear estimators.|Finite Sample Properties of Multiple Imputation Estimators|http://www.jstor.org/stable/3448485|3448485|2004-04-01|2004|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
We propose a probability model for random partitions in the presence of covariates. In other words, we develop a model-based clustering algorithm that exploits available covariates. The motivating application is predicting time to progression for patients in a breast cancer trial. We proceed by reporting a weighted average of the responses of clusters of earlier patients. The weights should be determined by the similarity of the new patient's covariate with the covariates of patients in each cluster. We achieve the desired inference by defining a random partition model that includes a regression on covariates. Patients with similar covariates are a priori more likely to be clustered together. Posterior predictive inference in this model formalizes the desired prediction. We build on product partition models (PPM). We define an extension of the PPM to include a regression on covariates by including in the cohesion function a new factor that increases the probability of experimental units with similar covariates to be included in the same cluster. We discuss implementations suitable for any combination of continuous, categorical, count, and ordinal covariates. An implementation of the proposed model as R-package is available for download.|A Product Partition Model With Regression on Covariates|http://www.jstor.org/stable/23113387|23113387|2011-03-01|2011|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Computer Science', 'Statistics']
This package performs specialized tasks related to specification, estimation, prediction and diagnostic checking in the context of a particular class of unobserved-components models. It is menu-driven and easy to use. Although the present (first) release suffers from a number of limitations, omissions and bugs, it is nevertheless a useful tool in the hands of a knowledgeable user.|Structural Time Series Analysis and Modelling Package: A Review|http://www.jstor.org/stable/2096469|2096469|1989-04-01|1989|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical analysis']|['Business & Economics', 'Business', 'Economics']
This article surveys various strategies for modeling ordered categorical (ordinal) response variables when the data have some type of clustering, extending a similar survey for binary data by Pendergast, Gange, Newton, Lindstrom, Palta &amp; Fisher (1996). An important special case is when repeated measurement occurs at various occasions for each subject, such as in longitudinal studies. A much greater variety of models and fitting methods are available than when a similar survey for repeated ordinal response data was prepared a decade ago (Agresti, 1989). The primary emphasis of the review is on two classes of models, marginal models for which effects are averaged over all clusters at particular levels of predictors, and cluster-specific models for which effects apply at the cluster level. We present the two types of models in the ordinal context, review the literature for each, and discuss connections between them. Then, we summarize some alternative modeling approaches and ways of estimating parameters, including a Bayesian approach. We also discuss applications and areas likely to be popular for future research, such as ways of handling missing data and ways of modeling agreement and evaluating the accuracy of diagnostic tests. Finally, we review the current availability of software for using the methods discussed in this article. /// Cet article passe en revue diverses stratégies pour la modélisation de variables de réponse qualitatives et ordinales, quand les données présentent des phénomènes de grappes. Il prolonge une étude similaire réalisée pour les variables dichotomiques par Pendergast et al. (1996). Les mesures répétées à plusieurs reprises pour chaque individu, par exemple dans les études longitudinales, constituent un cas particulier important. Il existe à présent une variété beaucoup plus grande de modèles et de méthodes d'ajustement, que lors de la réalisation d'une étude similaire pour les variables ordinales et répétées il y a une dizaine d'années (Agresti, 1989). Il est mis particulièrement l'accent sur deux types de modèles, les modèles marginaux pour lesquels on prend la moyenne des effets sur toutes les grappes pour des niveaux donnés des variables explicatives, et les modèles par grappes pour lesquels les effets sont pris en compte au niveau de chaque grappe. Nous présentons les deux types de modèles dans le cas de variables ordinales, brossons un panorama de la littérature pour chacun des deux types, et discutons les relations entre eux. Puis nous résumons d'autres approches pour la modélisation et l'estimation de paramètres, notamment une approche bayésienne. Nous discutons aussi des applications et des domaines qui devraient susciter de nouvelles recherches, par exemple les méthodes de traitement des données manquantes ou d'évaluation de l'exactitude de tests. Enfin, nous considérons la disponibilité actuelle de logiciels pour mettre en oeuvre les méthodes discutées dans cet article.|Modeling Clustered Ordered Categorical Data: A Survey|http://www.jstor.org/stable/1403450|1403450|2001-12-01|2001|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
It is a common practice to analyze complex longitudinal data using semiparametric nonlinear mixed-effects (SNLME) models with a normal distribution. Normality assumption of model errors may unrealistically obscure important features of subject variations. To partially explain between- and within-subject variations, covariates are usually introduced in such models, but some covariates may often be measured with substantial errors. Moreover, the responses may be missing and the missingness may be nonignorable. Inferential procedures can be complicated dramatically when data with skewness, missing values, and measurement error are observed. In the literature, there has been considerable interest in accommodating either skewness, incompleteness or covariate measurement error in such models, but there has been relatively little study concerning all three features simultaneously. In this article, our objective is to address the simultaneous impact of skewness, missingness, and covariate measurement error by jointly modeling the response and covariate processes based on a flexible Bayesian SNLME model. The method is illustrated using a real AIDS data set to compare potential models with various scenarios and different distribution specifications.|Bayesian Semiparametric Nonlinear Mixed-Effects Joint Models for Data with Skewness, Missing Responses, and Measurement Errors in Covariates|http://www.jstor.org/stable/23270612|23270612|2012-09-01|2012|['eng']|['Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
Our objective is to build output gap estimates that benefit from information provided by Phillips curve theory and business cycle studies. For this, we develop a Bayesian analysis of the bivariate Phillips curve model proposed by Kuttner for estimating potential output. Given our priors, we obtain samples from parameters and state variables joint posterior distribution following a Gibbs sampling strategy. We sample the state variables given parameters using the Carter-Kohn procedure, and we exploit a likelihood factorization to draw parameters given the state. A Metropolis-Hastings step is used to remove the conditioning on starting values. To accommodate the variance moderation that has been observed on U.S. gross domestic product, Kuttner's model is extended for a change in variance parameters. We apply this methodology to the analysis of the output gap in the United States and in the European Monetary Union. Finally, some important extensions to the original Kuttner model are discussed.|Bayesian Analysis of the Output Gap|http://www.jstor.org/stable/27638958|27638958|2008-01-01|2008|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
Predicting the functional roles of proteins based on various genome-wide data, such as protein-protein association networks, has become a canonical problem in computational biology. Approaching this task as a binary classification problem, we develop a network-based extension of the spatial auto-probit model. In particular, we develop a hierarchical Bayesian probit-based framework for modeling binary network-indexed processes, with a latent multivariate conditional autoregressive Gaussian process. The latter allows for the easy incorporation of protein-protein association network topologies—either binary or weighted—in modeling protein functional similarity. We use this framework to predict protein functions, for functions defined as terms in the Gene Ontology (GO) database, a popular rigorous vocabulary for biological functionality. Furthermore, we show how a natural extension of this framework can be used to model and correct for the high percentage of false negative labels in training data derived from GO, a serious shortcoming endemic to biological databases of this type. Our method performance is evaluated and compared with standard algorithms on weighted yeast protein-protein association networks, extracted from a recently developed integrative database called Search Tool for the Retrieval of INteracting Genes/proteins (STRING). Results show that our basic method is competitive with these other methods, and that the extended method—incorporating the uncertainty in negative labels among the training data—can yield nontrivial improvements in predictive accuracy.|Network-based Auto-probit Modeling for Protein Function Prediction|http://www.jstor.org/stable/41242544|41242544|2011-09-01|2011|['eng']|['Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
Many multivariate data-analysis techniques for an m × n matrix Y are related to the model Y = M + E, where Y is an m × n matrix of full rank and M is an unobserved mean matrix of rank K &lt; (m Λ n). Typically the rank of M is estimated in a heuristic way and then the least-squares estimate of M is obtained via the singular value decomposition of Y, yielding an estimate that can have a very high variance. In this article we suggest a model-based alternative to the preceding approach by providing prior distribution and posterior estimation for the rank of M and the components of its singular value decomposition. In addition to providing more accurate inference, such an approach has the advantage of being extendable to more general data-analysis situations, such as inference in the presence of missing data and estimation in a generalized linear modeling framework.|Model Averaging and Dimension Selection for the Singular Value Decomposition|http://www.jstor.org/stable/27639896|27639896|2007-06-01|2007|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
High-density single-nucleotide polymorphism (SNP) microarrays provide a useful tool for the detection of copy number variants (CNVs). The analysis of such large amounts of data is complicated, especially with regard to determining where copy numbers change and their corresponding values. In this article, we propose a Bayesian multiple change-point model (BMCP) for segmentation and estimation of SNP microarray data. Segmentation concerns separating a chromosome into regions of equal copy number differences between the sample of interest and some reference, and involves the detection of locations of copy number difference changes. Estimation concerns determining true copy number for each segment. Our approach not only gives posterior estimates for the parameters of interest, namely locations for copy number difference changes and true copy number estimates, but also useful confidence measures. In addition, our algorithm can segment multiple samples simultaneously, and infer both common and rare CNVs across individuals. Finally, for studies of CNVs in tumors, we incorporate an adjustment factor for signal attenuation due to tumor heterogeneity or normal contamination that can improve copy number estimates.|Segmentation and Estimation for SNP Microarrays: A Bayesian Multiple Change-Point Approach|http://www.jstor.org/stable/40962438|40962438|2010-09-01|2010|['eng']|['Biological sciences - Biology', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Sequential sampling problems arise in stochastic simulation and many other applications. Sampling is used to infer the unknown performance of several alternatives before one alternative is selected as best. This paper presents new economically motivated fully sequential sampling procedures to solve such problems, called economics of selection procedures. The optimal procedure is derived for comparing a known standard with one alternative whose unknown reward is inferred with sampling. That result motivates heuristics when multiple alternatives have unknown rewards. The resulting procedures are more effective in numerical experiments than any previously proposed procedure of which we are aware and are easily implemented. The key driver of the improvement is the use of dynamic programming to model sequential sampling as an option to learn before selecting an alternative. It accounts for the expected benefit of adaptive stopping policies for sampling, rather than of one-stage policies, as is common in the literature.|Sequential Sampling with Economics of Selection Procedures|http://www.jstor.org/stable/41431670|41431670|2012-03-01|2012|['eng']|['Applied sciences - Engineering']|['Business', 'Business & Economics Collection', 'Management & Organizational Behavior']
We present a hierarchical Bayesian framework for the selection of force fields in molecular dynamics (MD) simulations. The framework associates the variability of the optimal parameters of the MD potentials under different environmental conditions with the corresponding variability in experimental data. The high computational cost associated with the hierarchical Bayesian framework is reduced by orders of magnitude through a parallelized Transitional Markov Chain Monte Carlo method combined with the Laplace Asymptotic Approximation. The suitability of the hierarchical approach is demonstrated by performing MD simulations with prescribed parameters to obtain data for transport coefficients under different conditions, which are then used to infer and evaluate the parameters of the MD model. We demonstrate the selection of MD models based on experimental data and verify that the hierarchical model can accurately quantify the uncertainty across experiments; improve the posterior probability density function estimation of the parameters, thus, improve predictions on future experiments; identify the most plausible force field to describe the underlying structure of a given dataset. The framework and associated software are applicable to a wide range of nanoscale simulations associated with experimental data with a hierarchical structure.|A hierarchical Bayesian framework for force field selection in molecular dynamics simulations|http://www.jstor.org/stable/24758900|24758900|2016-02-13|2016|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Mathematics', 'General Science']
Wood density is thought to be an important indicator of plant life history because it is coupled to many aspects of whole-plant form and function. We used a hierarchical Bayesian approach to explain variation in mortality rates with wood density, drawing on data for 765 500 trees from 1639 species at 10 sites located across the Old and New World tropics. Mortality rates declined with increasing wood density at five of 10 sites. Similar negative trends were detected at four additional sites, while one site showed no relationship. Our model explained 40% of variation in mortality on average. Both wood density and mortality rates show a high degree of phylogenetic conservatism. Grouping species by family across sites in a second analysis, we found considerable variation in the relationship between wood density and mortality, with 10 of 27 families demonstrating a strong negative relationship. Our results highlight the importance of wood density as a functional trait in tropical forests, as it is strongly linked to variation in survival. However, the relationship varied among families, plots, and even census intervals within sites, indicating that the factors responsible for the relationship between wood density and mortality vary spatially, taxonomically and temporally.|The relationship between wood density and mortality in a global tropical forest data set|http://www.jstor.org/stable/40960864|40960864|2010-12-01|2010|['eng']|['Physical sciences - Astronomy', 'Biological sciences - Paleontology']|['Science & Mathematics', 'Botany & Plant Sciences', 'Biological Sciences']
Earthquakes occur because of abrupt slips on faults due to accumulated stress in the Earth's crust. Because most of these faults and their mechanisms are not readily apparent, deterministic earthquake prediction is difficult. For effective prediction, complex conditions and uncertain elements must be considered, which necessitates stochastic prediction. In particular, a large amount of uncertainty lies in identifying whether abnormal phenomena are precursors to large earthquakes, as well as in assigning urgency to the earthquake. Any discovery of potentially useful information for earthquake prediction is incomplete unless quantitative modeling of risk is considered. Therefore, this manuscript describes the prospect of earthquake predictability research to realize practical operational forecasting in the near future.|A Prospect of Earthquake Prediction Research|http://www.jstor.org/stable/43288434|43288434|2013-11-01|2013|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
The class of periodic autoregressive (PAR) models, suitably extended so as to allow for 'periodic integration', has recently found widespread application to economic time series as an alternative to the time-invariant models available in the literature. An elaborate modelling strategy has been proposed, and new tests for periodic integration have been envisaged, whose empirical performance tends to support the notion that the kind of non-stationary stochastic dynamics observed in time series arises as a consequence of periodic integration. This paper aims at challenging this view by means of a Monte Carlo experiment: we generate data according to a trend with a seasonality model such that the trend is a random walk with drift and the seasonal component is generated according to a stochastic trigonometric model. It is found that all the fundamental tools of PAR modelling will tend to provide spurious evidence in favour of a periodic model, and conclude that, as long as macroeconomic time series are concerned, PAR models are an overelaborate way of capturing essential features, such as indeterministic trends and seasonals, that are more parsimoniously accommodated by a time-invariant model.|Spurious periodic autoregressions|http://www.jstor.org/stable/23114950|23114950|1998-01-01|1998|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Business', 'Economics']
Proteins are the workhorses of all living systems, and protein bioinformatics deals with analysis of protein sequences (one dimensional) and structures (three dimensional). The paper reviews statistical advances in three major active areas of protein structural bioinformatics: structure comparison, Ramachandran plots and structure prediction. These topics play a key role in understanding one of the greatest unsolved problems in biology, how proteins fold from one dimension to three dimensions, and have relevance to protein functionality, drug discovery and evolutionary biology. For each area, we give the biological background and review one of the main bioinformatics solutions to a specific problem in that area. We then present statistical tools recently developed to investigate these problems, consisting of Bayesian alignment, directional distributions and hidden Markov models. We illustrate each problem with a new case-study and describe what statistics can offer to these problems. We highlight challenges facing these areas and conclude with an overall discussion.|Statistical approaches to three key challenges in protein structural bioinformatics|http://www.jstor.org/stable/24771817|24771817|2013-05-01|2013|['eng']|['Biological sciences - Biology', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
The analysis of GWAS data has long been restricted to simple models that cannot fully capture the genetic architecture of complex human diseases. As a shift from standard approaches, we propose here a general statistical framework for multi-SNP analysis of GWAS data based on a Bayesian graphical model. Our goal is to develop a general approach applicable to a wide range of genetic association problems, including GWAS and fine-mapping studies, and, more specifically, be able to: (1) Assess the joint effect of multiple SNPs that can be linked or unlinked and interact or not; (2) Explore the multi-SNP model space efficiently using the Mode Oriented Stochastic Search (MOSS) algorithm and determine the best models. We illustrate our new methodology with an application to the CGEM breast cancer GWAS data. Our algorithm selected several SNPs embedded in multi-locus models with high posterior probabilities. Most of the SNPs selected have a biological relevance. Interestingly, several of them have never been detected in standard single-SNP analyses. Finally, our approach has been implemented in the open source R package genMOSS.|A BAYESIAN GRAPHICAL MODEL FOR GENOME-WIDE ASSOCIATION STUDIES (GWAS)|http://www.jstor.org/stable/43957077|43957077|2016-06-01|2016|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
In determining differential expression in cDNA microarray experiments, the expression level of an individual gene is usually assumed to be independent of the expression levels of other genes, but many recent studies have shown that a gene's expression level tends to be similar to that of its neighbors on a chromosome, and differentially expressed (DE) genes are likely to form clusters of similar transcriptional activity along the chromosome. When modeled as a one-dimensional spatial series, the expression level of genes on the same chromosome frequently exhibit significant spatial correlation, reflecting spatial patterns in transcription. By modeling these spatial correlations, we can obtain improved estimates of transcript levels. Here, we demonstrate the existence of spatial correlations in transcriptional activity in the Escherichia coli (E. coli) chromosome across more than 50 experimental conditions. Based on this finding, we propose a hierarchical Bayesian model that borrows information from neighboring genes to improve the estimation of the expression level of a given gene and hence the detection of DE genes. Furthermore, we extend the model to account for the circular structure of E. coli chromosome and the intergenetic distance between gene neighbors. The simulation studies and analysis of real data examples in E. coli and yeast Saccharomyces cerevisiae show that the proposed method outperforms the commonly used significant analysis of microarray (SAM) t-statistic in detecting DE genes.|Improved Detection of Differentially Expressed Genes through Incorporation of Gene Locations|http://www.jstor.org/stable/20640578|20640578|2009-09-01|2009|['eng']|['Applied sciences - Engineering', 'Biological sciences - Biology']|['Science & Mathematics', 'Statistics']
A study conducted by Vriens, Wedel, and Wilms (1996) and published in Journal of Marketing Research found that finite mixture (FM) conjoint models had the best overall performance of nine conjoint segmentation methods in terms of fit, prediction, and parameter recovery. Since that study, hierarchical Bayes (HB) conjoint analysis methods have been proposed to estimate individual-level partworths and have received much attention in the marketing research literature. However, no study has compared the relative effectiveness of FM and HB conjoint analysis models in terms of fit, prediction, and parameter recovery. To conduct such a comparison, the authors employ the simulation methodology proposed by Vriens, Wedel, and Wilms with some modification. The authors estimate traditional individual-level conjoint models as well. The authors show that FM and HB models are equally effective in recovering individual-level parameters and predicting ratings of holdout profiles. Two surprising findings are that (1) HB performs well even when partworths come from a mixture of distributions and (2) FM produces good parameter estimates, even at the individual level. The authors show that both models are quite robust to violations of underlying assumptions and that traditional individual-level models overfit the data.|Hierarchical Bayes versus Finite Mixture Conjoint Analysis Models: A Comparison of Fit, Prediction, and Partworth Recovery|http://www.jstor.org/stable/1558586|1558586|2002-02-01|2002|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Business', 'Business & Economics Collection', 'Marketing & Advertising']
"Subjectivism has become the dominant philosophical foundation for Bayesian inference. Yet in practice, most Bayesian analyses are performed with so-called ""noninformative"" priors, that is, priors constructed by some formal rule. We review the plethora of techniques for constructing such priors and discuss some of the practical and philosophical issues that arise when they are used. We give special emphasis to Jeffreys's rules and discuss the evolution of his viewpoint about the interpretation of priors, away from unique representation of ignorance toward the notion that they should be chosen by convention. We conclude that the problems raised by the research on priors chosen by formal rules are serious and may not be dismissed lightly: When sample sizes are small (relative to the number of parameters being estimated), it is dangerous to put faith in any ""default"" solution; but when asymptotics take over, Jeffreys's rules and their variants remain reasonable choices. We also provide an annotated bibliography."|The Selection of Prior Distributions by Formal Rules|http://www.jstor.org/stable/2291752|2291752|1996-09-01|1996|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
This article proposes a multiresolution genetic algorithm that allows efficient estimation of parameters in large-dimensional models. Such models typically rely on complex numerical methods that require large amounts of computing power for estimating parameters. Unfortunately, the numerical maximization and sampling techniques used to fit such complex models often explore the parameter space slowly resulting in unreliable estimates. Our algorithm improves this exploration by incorporating elements of simulated tempering into a genetic algorithm framework for maximization. Our algorithm can also be adapted to perform Markov chain Monte Carlo sampling from a posterior distribution in a Bayesian setting, which can greatly improve mixing and exploration of the posterior compared to ordinary MCMC methods. The proposed algorithm can be used to estimate parameters in any model where the solution can be solved on different scales, even if the data are not inherently multiscale. We address parallel implementation of the algorithms and demonstrate their use on examples from single photon emission computed tomography and groundwater hydrology.|Multiresolution Genetic Algorithms and Markov Chain Monte Carlo|http://www.jstor.org/stable/27594217|27594217|2006-12-01|2006|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Computer Science', 'Statistics']
The problem of estimating probabilistic deformable template models in the field of computer vision or of probabilistic atlases in the field of computational anatomy has not yet received a coherent statistical formulation and remains a challenge. We provide a careful definition and analysis of a well-defined statistical model based on dense deformable templates for grey level images of deformable objects. We propose a rigorous Bayesian framework for which we prove asymptotic consistency of the maximum a posteriori estimate and which leads to an effective iterative estimation algorithm of the geometric and photometric parameters in the small sample setting. The model is extended to mixtures of finite numbers of such components leading to a fine description of the photometric and geometric variations of an object class. We illustrate some of the ideas with images of handwritten digits and apply the estimated models to classification through maximum likelihood.|Towards a Coherent Statistical Framework for Dense Deformable Template Estimation|http://www.jstor.org/stable/4623251|4623251|2007-01-01|2007|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
We propose a class of kernel stick-breaking processes for uncountable collections of dependent random probability measures. The process is constructed by first introducing an infinite sequence of random locations. Independent random probability measures and beta-distributed random weights are assigned to each location. Predictor-dependent random probability measures are then constructed by mixing over the locations, with stick-breaking probabilities expressed as a kernel multiplied by the beta weights. Some theoretical properties of the process are described, including a covariate-dependent prediction rule. A retrospective Markov chain Monte Carlo algorithm is developed for posterior computation, and the methods are illustrated using a simulated example and an epidemiological application.|Kernel Stick-Breaking Processes|http://www.jstor.org/stable/20441466|20441466|2008-06-01|2008|['eng']|['Applied sciences - Engineering', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Analyzing high-throughput genomic, proteomic, and metabolomic data usually involves estimating high-dimensional location parameters. Thresholding estimators can significantly improve such estimation when many parameters are zero, i.e., parameters are sparse. Several such estimators have been constructed to be adaptive to parameter sparsity. However, they assume that the underlying parameter spaces are symmetric. Since many applications present asymmetry parameter spaces, we introduce a class of generalized thresholding estimators. A construction of these estimators is developed using a Bayes approach, where an important constraint on the hyperparameters is identified. A generalized empirical Bayes implementation is presented for estimating high-dimensional yet sparse normal means. This implementation provides generalized thresholding estimators which are adaptive to both sparsity and asymmetry of high-dimensional parameters.|GENERALIZED THRESHOLDING ESTIMATORS FOR HIGH-DIMENSIONAL LOCATION PARAMETERS|http://www.jstor.org/stable/24309028|24309028|2010-04-01|2010|['eng']|['Physical sciences - Astronomy', 'Mathematics - Mathematical logic']|['Mathematics', 'Science and Mathematics', 'Statistics']
We consider the specification of prior distributions for Bayesian model comparison, focusing on regression-type models. We propose a particular joint specification of the prior distribution across models so that sensitivity of posterior model probabilities to the dispersion of prior distributions for the parameters of individual models (Lindley's paradox) is diminished. We illustrate the behavior of inferential and predictive posterior quantities in linear and log-linear regressions under our proposed prior densities with a series of simulated and real data examples.|Joint Specification of Model Space and Parameter Space Prior Distributions|http://www.jstor.org/stable/41714796|41714796|2012-05-01|2012|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
"In a series of recent articles on nonparametric regression, Donoho and Johnstone developed wavelet-shrinkage methods for recovering unknown piecewise-smooth deterministic signals from noisy data. Wavelet shrinkage based on the Bayesian approach involves specifying a prior distribution on the wavelet coefficients, which is usually assumed to have a distribution with zero mean. There is no a priori reason why all prior means should be 0; indeed, one can imagine certain types of signals in which this is not a good choice of model. In this article, we take an empirical Bayes approach in which we propose an estimator for the prior mean that is ""plugged into"" the Bayesian shrinkage formulas. Another way we are more general than previous work is that we assume that the underlying signal is composed of a piecewise-smooth deterministic part plus a zero-mean stochastic part; that is, the signal may contain a reasonably large number of nonzero wavelet coefficients. Our goal is to predict this signal from noisy data. We also develop a new estimator for the noise variance based on a geostatistical method that considers the behavior of the variogram near the origin. Simulation studies show that our method (DecompShrink) outperforms the well-known VisuShrink and SureShrink methods for recovering a wide variety of signals. Moreover, it is insensitive to the choice of the lowest-scale cut-off parameter, which is typically not the case for other wavelet-shrinkage methods."|Deterministic/Stochastic Wavelet Decomposition for Recovery of Signal from Noisy Data|http://www.jstor.org/stable/1271081|1271081|2000-08-01|2000|['eng']|['Applied sciences - Engineering', 'Information science - Coding theory']|['Science & Mathematics', 'Statistics']
The conventional multivariate analysis of repeated measures is applicable in a wide variety of circumstances, in part, because assumptions regarding the pattern of covariances among the repeated measures are not required. If sample sizes are small, however, then the estimators of the covariance parameters lack precision and, as a result, the power of the multivariate analysis is low. If the covariance matrix associated with estimators of orthogonal contrasts is spherical, then the conventional univariate analysis of repeated measures is applicable and has greater power than the multivariate analysis. If sphericity is not satisfied, an adjusted univariate analysis can be conducted, and this adjusted analysis may still be more powerful than the multivariate analysis. As sample size increases, the power advantage of the adjusted univariate test decreases, and, for moderate sample sizes, the multivariate test can be more powerful. This article proposes a hybrid analysis that takes advantage of the strengths of each of the two procedures. The proposed analysis employs an empirical Bayes estimator of the covariance matrix. Existing software for conventional multivariate analyses can, with minor modifications, be used to perform the proposed analysis. The new analysis behaves like the univariate analysis when samples size is small or sphericity is nearly satisfied. When sample size is large or sphericity is strongly violated, then the proposed analysis behaves like the multivariate analysis. Simulation results suggest that the proposed analysis controls test size adequately and can be more powerful than either of the other two analyses under a wide range of non-null conditions.|Analysis of Repeated Measures under Second-Stage Sphericity: An Empirical Bayes Approach|http://www.jstor.org/stable/1165376|1165376|1997-07-01|1997|['eng']|['Mathematics - Mathematical objects']|['Science & Mathematics', 'Education', 'Statistics', 'Social Sciences']
For postwar U.S. data, this paper uses Bayesian methods to account for the four sources of uncertainty in a random coefficients vector autoregression for inflation, unemployment, and an interest rate. We use the model to assemble evidence about the evolution of measures of the persistence of inflation, prospective long-horizon forecasts (means) of inflation and unemployment, statistics for testing an approximation to the natural-unemployment-rate hypothesis, and a version of the Taylor rule. We relate these measures to stories that interpret the conquest of U.S. inflation under Volcker and Greenspan as reflecting how the monetary policy authority came to learn an approximate version of the natural-unemployment-rate hypothesis. We study Taylor's warning that defects in that approximation may cause the monetary authority to forget the natural-rate hypothesis as the persistence of inflation attenuates.|Evolving Post-World War II U.S. Inflation Dynamics|http://www.jstor.org/stable/3585375|3585375|2001-01-01|2001|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Business', 'Economics']
We study the computational complexity of Markov chain Monte Carlo (MCMC) methods for high-dimensional Bayesian linear regression under sparsity constraints. We first show that a Bayesian approach can achieve variable-selection consistency under relatively mild conditions on the design matrix. We then demonstrate that the statistical criterion of posterior concentration need not imply the computational desideratum of rapid mixing of the MCMC algorithm. By introducing a truncated sparsity prior for variable selection, we provide a set of conditions that guarantee both variable-selection consistency and rapid mixing of a particular Metropolis-Hastings algorithm. The mixing time is linear in the number of covariates up to a logarithmic factor. Our proof controls the spectral gap of the Markov chain by constructing a canonical path ensemble that is inspired by the steps taken by greedy algorithms for variable selection.|ON THE COMPUTATIONAL COMPLEXITY OF HIGH-DIMENSIONAL BAYESIAN VARIABLE SELECTION|http://www.jstor.org/stable/44245760|44245760|2016-12-01|2016|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Summary Trees capable of symbiotic nitrogen (N) fixation (‘N fixers’) are abundant in many tropical forests. In temperate forests, it is well known that N fixers specialize in early-successional niches, but in tropical forests, successional trends of N-fixing species are poorly understood. We used a long-term census study (1997–2013) of regenerating lowland wet tropical forests in Costa Rica to document successional patterns of N fixers vs non-fixers, and used an individual-based model to determine the demographic drivers of these trends. N fixers increased in relative basal area during succession. In the youngest forests, N fixers grew 2.5 times faster, recruited at a similar rate and were 15 times less likely to die as nonfixers. As succession proceeded, the growth and survival disparities decreased, whereas N fixer recruitment decreased relative to non-fixers. According to our individual-based model, high survival was the dominant driver of the increase in basal area of N fixers. Our data suggest that N fixers are successful throughout secondary succession in tropical rainforests of north-east Costa Rica, and that attempts to understand this success should focus on tree survival.|Higher survival drives the success of nitrogen-fixing trees through succession in Costa Rican rainforests|http://www.jstor.org/stable/newphytologist.209.3.965|newphytologist.209.3.965|2016-02-01|2016|['eng']|['Biological sciences - Ecology', 'Biological sciences - Biology']|['Science & Mathematics', 'Botany & Plant Sciences', 'Biological Sciences']
"Theoretical and empirical evidence suggests that body size is a major life-history trait impacting on the structure and functioning of complex food webs. However, long-term analyses of size-dependent interactions within simpler network modules, for instance, competitive guilds, are scant. Here, we model the assembly dynamics of the largest breeding seabird community in the Mediterranean basin during the last 30 years. This unique data set allowed us to test, through a ""natural experiment,"" whether body size drove the assembly and dynamics of an ecological guild growing from very low numbers after habitat protection. Although environmental stochasticity accounted for most of community variability, the population variance explained by interspecific interactions, albeit small, decreased sharply with increasing body size. Since we found a demographic gradient along a body size continuum, in which population density and stability increase with increasing body size, the numerical effects of interspecific interactions were proportionally higher on smaller species than on larger ones. Moreover, we found that the per capita interaction coefficients were larger the higher the size ratio among competing species, but only for the set of interactions in which the species exerting the effect was greater. This provides empirical evidence for long-term asymmetric interspecific competition, which ultimately prompted the local extinction of two small species during the study period. During the assembly process stochastic predation by generalist carnivores further triggered community reorganizations and global decays in population synchrony, which disrupted the pattern of interspecific interactions. These results suggest that the major patterns detected in complex food webs can hold as well for simpler sub-modules of these networks involving non-trophic interactions, and highlight the shifting ecological processes impacting on assembling vs. asymptotic communities."|Size-mediated non-trophic interactions and stochastic predation drive assembly and dynamics in a seabird community|http://www.jstor.org/stable/23034828|23034828|2011-10-01|2011|['eng']|['Biological sciences - Biology', 'Biological sciences - Ecology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
We examine the question of which statistic or statistics should be used in order to recover information important for inference. We take a global geometric viewpoint, developing the local geometry of Amari. By examining the behaviour of simple geometric models, we show how not only the local curvature properties of parametric families but also the global geometric structure can be of crucial importance in finite-sample analysis. The tool we use to explore this global geometry is the Karhunen-Loève decomposition. Using global geometry, we show that the maximum likelihood estimate is the most important one-dimensional summary of information, but that traditional methods of information recovery beyond the maximum likelihood estimate can perform poorly. We also use the global geometry to construct better information summaries to be used with the maximum likelihood estimate.|On the Global Geometry of Parametric Models and Information Recovery|http://www.jstor.org/stable/3318819|3318819|2004-08-01|2004|['eng']|['Mathematics - Mathematical objects']|['Mathematics', 'Science and Mathematics', 'Statistics']
"We consider the problem of estimating the effect of exposure on multiple continuous outcomes, when the outcomes are measured on different scales and are nested within multiple outcome classes, or ""domains."" Our Bayesian model extends the linear mixed models approach to allow the exposure effect to differ across domains and across outcomes within domains. Our model can be parameterized to allow shrinkage of the effects within the different levels of nesting, or to allow fixed domain-specific effects with no shrinkage. Our model also allows covariate effects to differ across outcomes and domains. Our methodology is applied to data on prenatal methylmercury exposure and multiple outcomes in four domains measured at 9 years of age on children enrolled in the Seychelles Child Development Study. We use three different priors and found that our main conclusions were not sensitive to the choice of prior. Simulation studies examine the model performance under alternative scenarios. Our results demonstrate that a sizeable increase in power is possible."|Bayesian Models for Multiple Outcomes Nested in Domains|http://www.jstor.org/stable/20640628|20640628|2009-12-01|2009|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
A simulation-based approach is proposed for approximating a Bayesian analysis. Parameters and data are simulated from a Bayesian model and inference about a parameter is performed by exploring the set of simulated parameter values conditional on a set of values of a simulated statistic. The approach is used to learn about parameters of a streaky model on the basis of a statistic used to measure streakiness. The method is illustrated to detect streakiness in baseball hitting data and basketball shooting data.|Using Model/Data Simulations to Detect Streakiness|http://www.jstor.org/stable/2685528|2685528|2001-02-01|2001|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Demographic analysis of data on births, deaths, and migration and coverage measurement surveys that use capture-recapture methods have both been used to assess U.S. Census counts. These approaches have established that unadjusted Census counts are seriously flawed for groups such as young and middle-aged African-American men. There is considerable interest in methods that combine information from the Census, coverage measurement surveys, and demographic information to improve Census estimates of the population. This article describes a number of models that have been proposed to accomplish this synthesis when the demographic information is in the form of sex ratios stratified by age and race. A key difficulty is that methods for combining information require modeling assumptions that are difficult to assess based on fit to the data. We propose some general principles for aiding the choice among alternative models. We then pick a particular model based on these principles and imbed it within a more comprehensive Bayesian model for counts in poststrata of the population. Our Bayesian approach provides a principled solution to the existence of negative estimated counts in some subpopulations; provides for smoothing of estimates across poststrata, reducing the problem of isolated outlying adjustments; allows a test of whether negative cell counts are due to sampling variability or more egregious problems such as bias in Census or coverage measurement survey counts; and can be easily extended to provide estimates of precision that incorporate uncertainty in the estimates from demographic analysis and other sources. The model is applied to data for African-American age 30-49 from the 1990 Census, and results are compared with those from existing methods.|A Bayesian Approach to Combining Information from a Census, a Coverage Measurement Survey, and Demographic Analysis|http://www.jstor.org/stable/2669372|2669372|2000-06-01|2000|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
We develop a framework for estimating expected returns—a predictive system—that allows predictors to be imperfectly correlated with the conditional expected return. When predictors are imperfect, the estimated expected return depends on past returns in a manner that hinges on the correlation between unexpected returns and innovations in expected returns. We find empirically that prior beliefs about this correlation, which is most likely negative, substantially affect estimates of expected returns as well as various inferences about predictability, including assessments of a predictor's usefulness. Compared to standard predictive regressions, predictive systems deliver different expected returns with higher estimated precision.|Predictive Systems: Living with Imperfect Predictors|http://www.jstor.org/stable/27735145|27735145|2009-08-01|2009|['eng']|['Physical sciences - Astronomy']|['Business & Economics', 'Business', 'Finance']
"Across multiply imputed data sets, variable selection methods such as stepwise regression and other criterion-based strategies that include or exclude particular variables typically result in models with different selected predictors, thus presenting a problem for combining the results from separate complete-data analyses. Here, drawing on a Bayesian framework, we propose two alternative strategies to address the problem of choosing among linear regression models when there are missing covariates. One approach, which we call ""impute, then select"" (ITS) involves initially performing multiple imputation and then applying Bayesian variable selection to the multiply imputed data sets. A second strategy is to conduct Bayesian variable selection and missing data imputation simultaneously within one Gibbs sampling process, which we call ""simultaneously impute and select"" (SIAS). The methods are implemented and evaluated using the Bayesian procedure known as stochastic search variable selection for multivariate normal data sets, but both strategies offer general frameworks within which different Bayesian variable selection algorithms could be used for other types of data sets. A study of mental health services utilization among children in foster care programs is used to illustrate the techniques. Simulation studies show that both ITS and SIAS outperform complete-case analysis with stepwise variable selection and that SIAS slightly outperforms ITS."|Imputation and Variable Selection in Linear Regression Models with Missing Covariates|http://www.jstor.org/stable/3695970|3695970|2005-06-01|2005|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
"The motivation for this work was to investigate the possibility of accurately determining the age of a tern chick using easily obtained body measurements. We describe the construction of a nonlinear multivariate hierarchical model for chick growth and show how it can be estimated using Markov chain Monte Carlo techniques. A simple extension of the analysis allows for estimation of the ages of unknown chicks. Posterior distributions of the unknown ages are derived, so that the accuracy of age determination can be examined. We further extend our model and analysis to include the possibility that chicks fall into distinct groups with different growth characteristics. The technique is illustrated using data on the weight and wing length of black-fronted terns from the Ohau River, New Zealand. It is found that dating to within one day is possible, but only in some areas of the data space. The concept of ""braiding"" of multivariate growth curves is introduced to explain the varying accuracy of age determination."|Dating Chicks: Calibration and Discrimination in a Nonlinear Multivariate Hierarchical Growth Model|http://www.jstor.org/stable/27595565|27595565|2005-09-01|2005|['eng']|['Applied sciences - Engineering', 'Biological sciences - Biology']|['Science & Mathematics', 'Agriculture', 'Statistics']
We consider the problem of estimating the density of a random variable when precise measurements on the variable are not available, but replicated proxies contaminated with measurement error are available for sufficiently many subjects. Under the assumption of additive measurement errors this reduces to a problem of deconvolution of densities. Deconvolution methods often make restrictive and unrealistic assumptions about the density of interest and the distribution of measurement errors, for example, normality and homoscedasticity and thus independence from the variable of interest. This article relaxes these assumptions and introduces novel Bayesian semiparametric methodology based on Dirichlet process mixture models for robust deconvolution of densities in the presence of conditionally heteroscedastic measurement errors. In particular, the models can adapt to asymmetry, heavy tails, and multimodality. In simulation experiments, we show that our methods vastly outperform a recent Bayesian approach based on estimating the densities via mixtures of splines. We apply our methods to data from nutritional epidemiology. Even in the special case when the measurement errors are homoscedastic, our methodology is novel and dominates other methods that have been proposed previously. Additional simulation results, instructions on getting access to the dataset and R programs implementing our methods are included as part of online supplementary materials.|Bayesian Semiparametric Density Deconvolution in the Presence of Conditionally Heteroscedastic Measurement Errors|http://www.jstor.org/stable/43304800|43304800|2014-12-01|2014|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Computer Science', 'Statistics']
We have an unknown function h(x) which we want to estimate within a finite interval. The observed values of h(x) are independent observations of a random variable y whose mean is to be approximated by a polynomial of unknown degree. The problem of estimating h(x) then translates into that of predicting y. We assume that the mean of y is a polynomial of an arbitrarily large degree and derive a prior distribution for its coefficients which expresses the belief that these coefficients will tend to decrease in absolute value as the power of x increases. A prior-posterior analysis for the coefficients is carried out from which we obtain modal estimates of them. We derive the predictive distribution of y for the case when all parameters other than the mean are known. Two examples comparing the performance of this procedure with some of the usual least squares ones are presented.|A Bayesian Approach to Prediction Using Polynomials|http://www.jstor.org/stable/2335698|2335698|1977-08-01|1977|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Bayesian hierarchical models are built to fit multiple health endpoints from a dose-response study of a chemical contaminant, perchlorate. Perchlorate exposure results in iodine uptake inhibition in the thyroid, with health effects manifested by changes in blood hormone concentrations and histopathological effects on the thyroid. We propose empirical models to fit blood hormone concentration and thyroid histopathology data for rats exposed to perchlorate in the 90-day study of Springborn Laboratories Inc. (1998), based upon a mechanistic model derived from the assumed toxicological relationships between dose and the various endpoints. All of the models are fit in a Bayesian framework, and predictions about each endpoint in response to dose are simulated based on the posterior predictive distribution. A hierarchical model tries to exploit possible similarities between different combinations of sex and exposure duration, and it allows us to produce more stable estimates of dose-response curves. We also illustrate how the Bayesian model specification allows us to address additional questions that arise after the analysis.|Bayesian Hierarchical Analysis for Multiple Health Endpoints in a Toxicity Study|http://www.jstor.org/stable/20778470|20778470|2010-09-01|2010|['eng']|['Biological sciences - Biology']|['Science & Mathematics', 'Agriculture', 'Statistics']
We consider the problem of eliciting expert knowledge about the output of a deterministic computer code, where the output is a function of a vector of input variables. A Gaussian process prior is assumed for the unknown function, and expert judgments about the output at various inputs are used to find suitable hyperparameters of the Gaussian process prior distribution. An example is presented involving the movement of radionuclides in the food chain.|Eliciting Gaussian Process Priors for Complex Computer Codes|http://www.jstor.org/stable/3650392|3650392|2002-01-01|2002|['eng']|['Mathematics - Mathematical analysis']|['Science & Mathematics', 'Statistics']
We address the important practical problem of how to select the random effects component in a linear mixed model. A hierarchical Bayesian model is used to identify any random effect with zero variance. The proposed approach reparameterizes the mixed model so that functions of the covariance parameters of the random effects distribution are incorporated as regression coefficients on standard normal latent variables. We allow random effects to effectively drop out of the model by choosing mixture priors with point mass at zero for the random effects variances. Due to the reparameterization, the model enjoys a conditionally linear structure that facilitates the use of normal conjugate priors. We demonstrate that posterior computation can proceed via a simple and efficient Markov chain Monte Carlo algorithm. The methods are illustrated using simulated data and real data from a study relating prenatal exposure to polychlorinated biphenyls and psychomotor development of children.|Random Effects Selection in Linear Mixed Models|http://www.jstor.org/stable/3695314|3695314|2003-12-01|2003|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Bounds for matrix weighted averages of pairs of vectors are presented. The weight matrices are constrained to certain classes suggested by the Bayesian analysis of the linear regression model and the multivariate normal model. The bounds identify the region within which the posterior location vector must lie if the prior comes from a certain class of priors.|Matrix Weighted Averages and Posterior Bounds|http://www.jstor.org/stable/2984831|2984831|1976-01-01|1976|['eng']|['Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
We develop the relational topic model (RTM), a hierarchical model of both network structure and node attributes. We focus on document networks, where the attributes of each document are its words, that is, discrete observations taken from a fixed vocabulary. For each pair of documents, the RTM models their link as a binary random variable that is conditioned on their contents. The model can be used to summarize a network of documents, predict links between them, and predict words within them. We derive efficient inference and estimation algorithms based on variational methods that take advantage of sparsity and scale with the number of links. We evaluate the predictive performance of the RTM for large networks of scientific abstracts, web documents, and geographically tagged news.|HIERARCHICAL RELATIONAL MODELS FOR DOCUMENT NETWORKS|http://www.jstor.org/stable/27801582|27801582|2010-03-01|2010|['eng']|['Applied sciences - Engineering', 'Mathematics - Mathematical logic']|['Mathematics', 'Science & Mathematics', 'Statistics']
Whereas literature in marketing shows that individuals often use noncompensatory decision rules, existing research on dyadic choice is based on compensatory models. In this paper we present a dyadic considerthen-choose model that investigates both compensatory and noncompensatory aspects of the joint decision process. The intersection of individual consideration sets at the dyad level gives rise to dyadic decision processes (DDPs) where dyad members are in concordance or discordance about alternatives to consider. We empirically investigate the implications of different DDPs on outcomes such as decision efficiency and dyadic welfare. The methodological approach merges choice experiments with Bayesian statistical models to uncover nuances of the dyadic choice process. Data were collected using a multiphase nationwide study of 265 husband-andwife dyads. Results across three categories indicate that both concordant and discordant dyads exist. Among concordant dyads, the noncompensatory dyads make quicker decisions that result in higher dyadic welfare. Among discordant dyads, those that restrict their consideration set make quicker decisions that result in higher welfare than those that expand their consideration set. These findings have important implications for buyers looking to maximize dyadic welfare when making joint choices and for sellers making pricing and new product design decisions.|Noncompensatory Dyadic Choices|http://www.jstor.org/stable/41408416|41408416|2011-11-01|2011|['eng']|['Mathematics - Applied mathematics']|['Business', 'Business & Economics Collection', 'Marketing & Advertising']
We propose and fit a Bayesian model to infer palaeoclimate over several thousand years. The data that we use arise as ancient pollen counts taken from sediment cores together with radiocarbon dates which provide (uncertain) ages. When combined with a modern pollen– climate data set, we can calibrate ancient pollen into ancient climate. We use a normal–inverse Gaussian process prior to model the stochastic volatility of palaeoclimate over time, and we present a novel modularized Markov chain Monte Chain algorithm to enable fast computation. We illustrate our approach with a case-study from Sluggan Moss, Northern Ireland, and provide an R package, Bclim, for use at other sites.|Bayesian inference for palaeoclimate with time uncertainty and stochastic volatility|http://www.jstor.org/stable/24771866|24771866|2015-01-01|2015|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
We present some easy-to-construct random probability measures which approximate the Dirichlet process and an extension which we will call the beta two-parameter process. The nature of these constructions makes it simple to implement Markov chain Monte Carlo algorithms for fitting nonparametric hierarchical models and mixtures of non-parametric hierarchical models. For the Dirichlet process, we consider a truncation approximation as well as a weak limit approximation based on a mixture of Dirichlet processes. The same type of truncation approximation can also be applied to the beta two-parameter process. Both methods lead to posteriors which can be fitted using Markov chain Monte Carlo algorithms that take advantage of blocked coordinate updates. These algorithms promote rapid mixing of the Markov chain and can be readily applied to normal mean mixture models and to density estimation problems. We prefer the truncation approximations, since a simple device for monitoring the adequacy of the approximation can be easily computed from the output of the Gibbs sampler. Furthermore, for the Dirichlet process, the truncation approximation offers an exponentially higher degree of accuracy over the weak limit approximation for the same computational effort. We also find that a certain beta two-parameter process may be suitable for finite mixture modelling because the distinct number of sampled values from this process tends to match closely the number of components of the underlying mixture distribution.|Markov Chain Monte Carlo in Approximate Dirichlet and Beta Two-Parameter Process Hierarchical Models|http://www.jstor.org/stable/2673470|2673470|2000-06-01|2000|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
AbstractWe have observed W Hya, one of the closest and best-studied oxygen-rich evolved stars, in the dust sensitive mid-IR spectral domain with the interferometric instrument MIDI. Images could be obtained for the first time with MIDI in 25 wavelengths bins with the image reconstruction software MiRA using only the modulus of the visibilities. This still remains one of the few cases in which images could be successfully recovered due to the difficulties inherent to optical/infrared interferometry concerning the sparseness of the UV-plane and the missing Fourier phase information. Different regularization terms were compared and the influence of the UV-coverage was investigated. The lack of Fourier phase information, however, still limits the interpretation of the images. W Hya appears clearly nonsymmetric and the size is wavelength dependent. The photosphere, molecular layers, and dust formation zone could be resolved with an photospheric Gaussian FWHM diameter of 42 ± 2 mas (corresponding to 4.1 AU) and a dust layer of presumably amorphous aluminum oxide (Al2O3) at around two photospheric radii. The position angle of the major axis of the elongated structure could be determined to be (15 ± 10)° with a less well defined axis ratio between 0.4 and 0.6 showing that the dust forms primarily along a N–S axis. By comparing the elongated structure seen with MIDI with the Herschel/PACS 70 μm image at much larger scales, one can conclude that the asymmetry in the mass-loss most likely originates in the very close vicinity of the star and is thus not due to an interaction with the ambient media.|Spectro-Imaging of the Asymmetric Inner Molecular and Dust Shell Region of the Mira Variable W Hya with MIDI/VLTI|http://www.jstor.org/stable/10.1086/682261|10.1086/682261|2015-08-01|2015|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Astronomy']
As the dimensionality of the alternative hypothesis increases, the power of classical tests tends to diminish quite rapidly. This is especially true for high dimensional data in which there are more parameters than observations. We discuss a score test on a hyperparameter in an empirical Bayesian model as an alternative to classical tests. It gives a general test statistic which can be used to test a point null hypothesis against a high dimensional alternative, even when the number of parameters exceeds the number of samples. This test will be shown to have optimal power on average in a neighbourhood of the null hypothesis, which makes it a proper generalization of the locally most powerful test to multiple dimensions. To illustrate this new locally most powerful test we investigate the case of testing the global null hypothesis in a linear regression model in more detail. The score test is shown to have significantly more power than the F-test whenever under the alternative the large variance principal components of the design matrix explain substantially more of the variance of the outcome than do the small variance principal components. The score test is also useful for detecting sparse alternatives in truly high dimensional data, where its power is comparable with the test based on the maximum absolute t-statistic.|Testing against a High Dimensional Alternative|http://www.jstor.org/stable/3879286|3879286|2006-01-01|2006|['eng']|['Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
We propose extensions of penalized spline generalized additive models for analyzing space-time regression data and study them from a Bayesian perspective. Non-linear effects of continuous covariates and time trends are modelled through Bayesian versions of penalized splines, while correlated spatial effects follow a Markov random field prior. This allows to treat all functions and effects within a unified general framework by assigning appropriate priors with different forms and degrees of smoothness. Inference can be performed either with full (FB) or empirical Bayes (EB) posterior analysis. FB inference using MCMC techniques is a slight extension of previous work. For EB inference, a computationally efficient solution is developed on the basis of a generalized linear mixed model representation. The second approach can be viewed as posterior mode estimation and is closely related to penalized likelihood estimation in a frequentist setting. Variance components, corresponding to inverse smoothing parameters, are then estimated by marginal likelihood. We carefully compare both inferential procedures in simulation studies and illustrate them through data applications. The methodology is available in the open domain statistical package BayesX and as an S-plus/R function.|PENALIZED STRUCTURED ADDITIVE REGRESSION FOR SPACE-TIME DATA: A BAYESIAN PERSPECTIVE|http://www.jstor.org/stable/24307414|24307414|2004-07-01|2004|['eng']|['Mathematics - Applied mathematics']|['Mathematics', 'Science and Mathematics', 'Statistics']
Recent studies suggest that climate warming in interior Alaska may result in major shifts from spruce-dominated forests to broadleaf-dominated forests or even grasslands. To quantify patterns in tree distribution and abundance and to investigate the potential for changes in forest dynamics through time, we initiated a spatially extensive vegetation monitoring program covering 1.28 million ha in Denali National Park and Preserve (DNPP). Using a probabilistic sampling design, we collected field measurements throughout the study area to develop spatially explicit Bayesian hierarchical models of tree occupancy and abundance. These models demonstrated a strong partitioning of the landscape among the six tree species in DNPP, and allowed us to account for and examine residual spatial autocorrelation in our data. Tree distributions were governed by two primary ecological gradients: (1) the gradient from low elevation, poorly drained, permafrost-influenced sites with shallow active layers and low soil pH (dominated by Picea mariana) to deeply thawed and more productive sites at mid-elevation with higher soil pH on mineral substrate (dominated by Picea glauca); and (2) the gradient from older, less recently disturbed sites dominated by conifers to those recently affected by disturbance in the form of fire and flooding with increased occupancy and abundance of broadleaf species. We found that the establishment of broadleaf species was largely dependent on disturbance, and mixed forests and pure stands of broadleaf trees were relatively rare and occurred in localized areas. Contrary to recent work in nearby areas of interior Alaska, our results suggest that P. glauca distribution may actually increase in DNPP under warming conditions rather than decline as previously predicted, as P. glauca expands into areas formerly underlain by permafrost. We found no evidence of a shift to broadleaf forests in DNPP, particularly in the poorly drained basin landscape positions that may be resistant to such changes. Overall, our results indicate that probabilistic sampling conducted at a landscape scale can improve inference relative to the habitat associations driving the distribution and abundance of trees in the boreal forest and the potential effects of climate change on them.|Landscape-scale patterns in tree occupancy and abundance in subarctic Alaska|http://www.jstor.org/stable/23596740|23596740|2013-02-01|2013|['eng']|['Biological sciences - Ecology', 'Biological sciences - Paleontology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
Bayesian analysis using Monte Carlo integration is a powerful method for univariate inference. This approach makes possible multiparameter flexibility within families of univariate distributions. These distributions are defined in this article by increasing spline functions superimposed on probability paper coordinate systems. Smoothing is controlled by the prior distribution. The prior distribution also can express uncertainties about the form of the tails when extrapolation beyond the range of the data is required. The handling of difficult forms of data (e.g., quantal response data) is straightforward. Posterior distributions for functions of the parameters can be easily computed.|Multiparameter Univariate Bayesian Analysis|http://www.jstor.org/stable/2286992|2286992|1979-09-01|1979|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
New techniques for the analysis of stochastic volatility models in which the logarithm of conditional variance follows an autoregressive model are developed. A cyclic Metropolis algorithm is used to construct a Markov-chain simulation tool. Simulations from this Markov chain converge in distribution to draws from the posterior distribution enabling exact finite-sample inference. The exact solution to the filtering/smoothing problem of inferring about the unobserved variance states is a by-product of our Markov-chain method. In addition, multistep-ahead predictive densities can be constructed that reflect both inherent model variability and parameter uncertainty. We illustrate our method by analyzing both daily and weekly data on stock returns and exchange rates. Sampling experiments are conducted to compare the performance of Bayes estimators to method of moments and quasi-maximum likelihood estimators proposed in the literature. In both parameter estimation and filtering, the Bayes estimators outperform these other approaches.|Bayesian Analysis of Stochastic Volatility Models|http://www.jstor.org/stable/1392151|1392151|2002-01-01|2002|['eng']|['Applied sciences - Engineering']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
Data structures in modern applications frequently combine the necessity of flexible regression techniques handling, for example, non-linear and spatial effects with high dimensional covariate vectors. Whereas estimation of the former is typically achieved by supplementing the likelihood with a suitable smoothness penalty, the latter are usually assigned shrinkage penalties that enforce sparse models. We consider a Bayesian unifying perspective, where conditionally Gaussian priors can be assigned to all types of regression effects. Suitable hyperprior assumptions on the variances of the Gaussian distributions then induce the desired smoothness or sparseness properties. As a major advantage, general Markov chain Monte Carlo simulation algorithms can be developed that allow for the joint estimation of smooth and spatial effects and regularized coefficient vectors. Two applications demonstrate the usefulness of the procedure proposed: a geoadditive regression model for data from a rental guide in Munich and an additive probit model for the prediction of consumer credit defaults. In both cases, high dimensional vectors of categorical covariates will be included in the regression models. The predictive ability of the resulting high dimensional structured additive regression models compared with expert models is of particular relevance and will be evaluated on cross-validation test data.|High dimensional structured additive regression models: Bayesian regularization, smoothing and predictive performance|http://www.jstor.org/stable/41057551|41057551|2011-01-01|2011|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
In many biomedical studies, patients may experience the same type of recurrent event repeatedly over time, such as bleeding, multiple infections and disease. In this article, we propose a Bayesian design to a pivotal clinical trial in which lower risk myelodysplastic syndromes (MDS) patients are treated with MDS disease modifying therapies. One of the key study objectives is to demonstrate the investigational product (treatment) effect on reduction of platelet transfusion and bleeding events while receiving MDS therapies. In this context, we propose a new Bayesian approach for the design of superiority clinical trials using recurrent events frailty regression models. Historical recurrent events data from an already completed phase 2 trial are incorporated into the Bayesian design via the partial borrowing power prior of Ibrahim et al. (2012, Biometrics 68, 578–586). An efficient Gibbs sampling algorithm, a predictive data generation algorithm, and a simulation-based algorithm are developed for sampling from the fitting posterior distribution, generating the predictive recurrent events data, and computing various design quantities such as the type I error rate and power, respectively. An extensive simulation study is conducted to compare the proposed method to the existing frequentist methods and to investigate various operating characteristics of the proposed design.|Bayesian Design of Superiority Clinical Trials for Recurrent Events Data with Applications to Bleeding and Transfusion Events in Myelodyplastic Syndrome|http://www.jstor.org/stable/24538384|24538384|2014-12-01|2014|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
We estimate a DSGE (dynamic stochastic general equilibrium) model where rare large shocks can occur, by replacing the commonly used Gaussian assumption with a Student’s t-distribution. Results from the Smets and Wouters (American Economic Review 2007; 97: 586–606) model estimated on the usual set of macroeconomic time series over the 1964–2011 period indicate that (i) the Student’s t specification is strongly favored by the data even when we allow for low-frequency variation in the volatility of the shocks, and (ii)) the estimated degrees of freedom are quite low for several shocks that drive US business cycles, implying an important role for rare large shocks. This result holds even if we exclude the Great Recession period from the sample. We also show that inference about low-frequency changes in volatility—and, in particular, inference about the magnitude of Great Moderation—is different once we allow for fat tails.|RARE SHOCKS, GREAT RECESSIONS|http://www.jstor.org/stable/26609008|26609008|2014-11-01|2014|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Business', 'Economics']
Large-scale natural disturbances, such as hurricanes, have profound effects on populations, either directly by causing mortality, or indirectly by altering ecological conditions or the quantity, quality, and spatial distribution of resources. In the last 20 years, two major disturbances, Hurricane Hugo in 1989 and Hurricane Georges in 1998, struck the Luquillo Mountains of Puerto Rico, providing an unique opportunity to understand the long-term effects of recurrent disturbances on the abundance of species. Nenia tridens is one of the most abundant and pervasive terrestrial gastropods in the Luquillo Mountains. Estimates of yearly abundance of N. tridens from 40 sites on the Luquillo Forest Dynamics Plot from 1991 to 2007 facilitate the development of a spatiotemporal model with intervention effects on the mean abundance over time in response to each hurricane. Intervention effects characteristically decay over time, similar to those in a time series analysis. Model parameters were estimated in a Bayesian framework. Model comparison and diagnostics suggest that our intervention model provides a plausible description of hurricanes effects on the abundances of N. tridens and may be useful for studying long-term spatiotemporal dynamics from the perspective of disturbance and succession.|Intervention Analysis of Hurricane Effects on Snail Abundance in a Tropical Forest Using Long-Term Spatiotemporal Data|http://www.jstor.org/stable/23208312|23208312|2011-03-01|2011|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Agriculture', 'Statistics']
"In Bayesian analysis of multi-way contingency tables, the selection of a prior distribution for either the log-linear parameters or the cell probabilities parameters is a major challenge. In this paper, we define a flexible family of conjugate priors for the wide class of discrete hierarchical log-linear models, which includes the class of graphical models. These priors are defined as the Diaconis—Ylvisaker conjugate priors on the log-linear parameters subject to ""baseline constraints"" under multinomial sampling. We also derive the induced prior on the cell probabilities and show that the induced prior is a generalization of the hyper Dirichlet prior. We show that this prior has several desirable properties and illustrate its usefulness by identifying the most probable decomposable, graphical and hierarchical log-linear models for a six-way contingency table."|A CONJUGATE PRIOR FOR DISCRETE HIERARCHICAL LOG-LINEAR MODELS|http://www.jstor.org/stable/25662199|25662199|2009-12-01|2009|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
We present a novel semiparametric survival model with a log-linear median regression function. As a useful alternative to existing semiparametric models, our large model class has many important practical advantages, including interpretation of the regression parameters via the median and the ability to address heteroscedasticity. We demonstrate that our modeling technique facilitates the ease of prior elicitation and computation for both parametric and semiparametric Bayesian analysis of survival data. We illustrate the advantages of our modeling, as well as model diagnostics, via a reanalysis of a small-cell lung cancer study. Results of our simulation study provide further support for our model in practice.|Semiparametric Bayesian Survival Analysis using Models with Log-Linear Median|http://www.jstor.org/stable/41806032|41806032|2012-12-01|2012|['eng']|['Mathematics - Mathematical analysis', 'Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
Motivated by examples from genetic association studies, this article considers the model selection problem in a general complex linear model system and in a Bayesian framework. We discuss formulating model selection problems and incorporating context-dependent a priori information through different levels of prior specifications. We also derive analytic Bayes factors and their approximations to facilitate model selection and discuss their theoretical and computational properties. We demonstrate our Bayesian approach based on an implemented Markov Chain Monte Carlo (MCMC) algorithm in simulations and a real data application of mapping tissue-specific eQTLs. Our novel results on Bayes factors provide a general framework to perform efficient model comparisons in complex linear model systems.|Bayesian Model Selection in Complex Linear Systems, as Illustrated in Genetic Association Studies|http://www.jstor.org/stable/24537889|24537889|2014-03-01|2014|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Malaria represents one of the major worldwide challenges to public health. A recent breakthrough in the study of the disease follows the annotation of the genome of the malaria parasite Plasmodiumfalciparum and the mosquito vector (an organism that spreads an infectious disease) Anopheles. Of particular interest is the molecular biology underlying the immune response system of Anopheles, which actively fights against Plasmodium infection. This article reports a statistical analysis of gene expression time profiles from mosquitoes that have been infected with a bacterial agent. Specifically, we introduce a Bayesian model-based hierarchical clustering algorithm for curve data to investigate mechanisms of regulation in the genes concerned; that is, we aim to cluster genes having similar expression profiles. Genes displaying similar, interesting profiles can then be highlighted for further investigation by the experimenter. We show how our approach reveals structure within the data not captured by other approaches. One of the most pertinent features of the data is the sample size, which records the expression levels of 2,771 genes at 6 time points. Additionally, the time points are unequally spaced, and there is expected nonstationary behavior in the gene profiles. We demonstrate our approach to be readily implementable under these conditions, and highlight some crucial computational savings that can be made in the context of a fully Bayesian analysis.|A Quantitative Study of Gene Regulation Involved in the Immune Response of Anopheline Mosquitoes: An Application of Bayesian Hierarchical Clustering of Curves|http://www.jstor.org/stable/30047437|30047437|2006-03-01|2006|['eng']|['Biological sciences - Biology', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
"We investigate the utility to computational Bayesian analyses of a particular family of recursive marginal likelihood estimators characterized by the (equivalent) algorithms known as ""biased sampling"" or ""reverse logistic regression"" in the statistics literature and ""the density of states"" in physics. Through a pair of numerical examples (including mixture modeling of the well-known galaxy data set) we highlight the remarkable diversity of sampling schemes amenable to such recursive normalization, as well as the notable efficiency of the resulting pseudo-mixture distributions for gauging prior sensitivity in the Bayesian model selection context. Our key theoretical contributions are to introduce a novel heuristic ("" thermodynamic integration via importance sampling"") for qualifying the role of the bridging sequence in this procedure and to reveal various connections between these recursive estimators and the nested sampling technique."|Recursive Pathways to Marginal Likelihood Estimation with Prior-Sensitivity Analysis|http://www.jstor.org/stable/43288518|43288518|2014-08-01|2014|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
"Link-tracing sampling designs can be used to study human populations that contain ""hidden"" groups who tend to be linked together by a common social trait. These links can be used to increase the sampling intensity of a hidden domain by tracing links from individuals selected in an initial wave of sampling to additional domain members. Chow and Thompson (2003, Survey Methodology 29, 197-205) derived a Bayesian model to estimate the size or proportion of individuals in the hidden population for certain link-tracing designs. We propose an addition to their model that will allow for the modeling of a quantitative response. We assess properties of our model using a constructed population and a real population of at-risk individuals, both of which contain two domains of hidden and nonhidden individuals. Our results show that our model can produce good point and interval estimates of the population mean and domain means when our population assumptions are satisfied."|A Bayesian Model for Estimating Population Means Using a Link-Tracing Sampling Design|http://www.jstor.org/stable/41434049|41434049|2012-03-01|2012|['eng']|['Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
Stochastic processes are natural models for the progression of many individual and team sports. Such models have been applied successfully to select strategies and to predict outcomes in the context of games, tournaments and leagues. This information is useful to participants and gamblers, who often need to make decisions while the sports are in progress. In order to apply these models, much of the published research uses parameters estimated from historical data, thereby ignoring the uncertainty of the parameter values and the most relevant information that arises during competition. In this paper, we investigate candidate stochastic processes for familiar sporting applications that include cricket, football and badminton, reviewing existing models and offering some new suggestions. We then consider how to model parameter uncertainty with prior and posterior distributions, how to update these distributions dynamically during competition and how to use these results to make optimal decisions. Finally, we combine these ideas in a case study aimed at predicting the winners of next year's University Boat Race.|Open: Strategy selection and outcome prediction in sport using dynamic learning for stochastic processes|http://www.jstor.org/stable/43830584|43830584|2015-11-01|2015|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Business & Economics', 'Business']
To address an important risk classification issue that arises in clinical practice, we propose a new mixture model via latent cure rate markers for survival data with a cure fraction. In the proposed model, the latent cure rate markers are modeled via a multinomial logistic regression and patients who share the same cure rate are classified into the same risk group. Compared to available cure rate models, the proposed model fits better to data from a prostate cancer clinical trial. In addition, the proposed model can be used to determine the number of risk groups and to develop a predictive classification algorithm.|A New Latent Cure Rate Marker Model for Survival Data|http://www.jstor.org/stable/30242880|30242880|2009-09-01|2009|['eng']|['Biological sciences - Biology']|['Mathematics', 'Science & Mathematics', 'Statistics']
For mixture models with unknown number of components, Bayesian approaches, as considered by Escobar and West (1995) and Richardson and Green (1997), are reconciled here through a simple Gibbs sampling approach. Specifically, we consider exactly the same direct set up as used by Richardson and Green (1997), but put Dirichlet process prior on the mixture components; the latter has also been used by Escobar and West (1995) albeit in a different set up. The reconciliation we propose here yields a simple Gibbs sampling scheme for learning about all the'unknowns, including the unknown number of components. Thus, we completely avoid complicated reversible jump Markov chain Monte Carlo (RJMCMC) methods, yet tackle variable dimensionality simply and efficiently. Moreover, we demonstrate, using both simulated and real data sets, and pseudo-Bayes factors, that our proposed model outperforms that of Escobar and West (1995), while enjoying, at the same time, computational superiority over the methods proposed by Richardson and Green (1997) and Escobar and West (1995). We also discuss issues related to clustering and argue that in principle, our approach is capable of learning about the number of clusters in the sample as well as in the population, while tttfe approach of Escobar and West (1995) is suitable for learning about the number of clusters in ftie sample only.|Gibbs Sampling Based Bayesian Analysis of Mixtures with Unknown Number of Components|http://www.jstor.org/stable/41234427|41234427|2008-05-01|2008|['eng']|['Applied sciences - Engineering', 'Mathematics - Mathematical logic']|['Science and Mathematics', 'Statistics']
The authors consider the optimal design of sampling schedules for binary sequence data. They propose an approach which allows a variety of goals to be reflected in the utility function by including deterministic sampling cost, a term related to prediction, and if relevant, a term related to learning about a treatment effect. To this end, they use a nonparametric probability model relying on a minimal number of assumptions. They show how their assumption of partial exchangeability for the binary sequence of data allows the sampling distribution to be written as a mixture of homogeneous Markov chains of order k. The implementation follows the approach of Quintana &amp; Müller (2004), which uses a Dirichlet process prior for the mixture. /// Les auteurs s'intéressent à l'optimisation de plans d'échantillonnage pour des mesures binaires répétées. Ils proposent une approche permettant de refléter divers objectifs par l'inclusion dans la fonction d'utilité d'un coût d'échantillonnage déterministe, d'un terme associé à la prévision et, au besoin, d'un terme lié à l'apprentissage concernant l'effet de traitement. Ils utilisent pour ce faire un modèle probabiliste non paramétrique reposant sur un nombre minimal de présupposés. Ils montrent comment leur postulat d'échangeabilité partielle sur la suite binaire des résultats permet d'écrire la loi d'échantillonnage comme un mélange de chaînes de Markov homogènes d'ordre k. Le tout est mis en œuvre selon l'approche de Quintana &amp; Müller (2004), laquelle utilise un processus de Dirichlet comme a priori sur le mélange.|Optimal Sampling for Repeated Binary Measurements|http://www.jstor.org/stable/3316000|3316000|2004-03-01|2004|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Spatio-temporal prediction of levels of an environmental exposure is an important problem in environmental epidemiology. Our work is motivated by multiple studies on the spatio-temporal distribution of mobile source, or traffic related, particles in the greater Boston area. When multiple sources of exposure information are available, a joint model that pools information across sources maximizes data coverage over both space and time, thereby reducing the prediction error. We consider a Bayesian hierarchical framework in which a joint model consists of a set of submodels, one for each data source, and a model for the latent process that serves to relate the submodels to one another. If a submodel depends on the latent process nonlinearly, inference using standard MCMC techniques can be computationally prohibitive. The implications are particularly severe when the data for each submodel are aggregated at different temporal scales. To make such problems tractable, we linearize the nonlinear components with respect to the latent process and induce sparsity in the covariance matrix of the latent process using compactly supported covariance functions. We propose an efficient MCMC scheme that takes advantage of these approximations. We use our model to address a temporal change of support problem whereby interest focuses on pooling daily and multiday black carbon readings in order to maximize the spatial coverage of the study region.|NONLINEAR PREDICTIVE LATENT PROCESS MODELS FOR INTEGRATING SPATIO-TEMPORAL EXPOSURE DATA FROM MULTIPLE SOURCES|http://www.jstor.org/stable/24522274|24522274|2014-09-01|2014|['eng']|['Physical sciences - Astronomy']|['Mathematics', 'Science & Mathematics', 'Statistics']
This article aims to develop a semiparametric latent variable model, in which outcome latent variables are related to explanatory latent variables and covariates through an additive structural equation formulated by a series of unspecified smooth functions. The Bayesian P-splines approach, together with a Markov chain Monte Carlo algorithm, is proposed to estimate smooth functions, unknown parameters, and latent variables in the model. The performance of the developed methodology is demonstrated by a simulation study. An illustrative example in analyzing bone mineral density in older men is provided. An Appendix which includes technical details of the proposed MCMC algorithm and an R code in implementing the algorithm are available as the online supplemental materials.|Semiparametric Latent Variable Models With Bayesian P-Splines|http://www.jstor.org/stable/25765360|25765360|2010-09-01|2010|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Computer Science', 'Statistics']
This study examines the role of elections in determining electricity prices in Quebec. The legislation governing Hydro-Québec is used to develop a model incorporating its stated policy objectives and the partisan interests of the governing party. Bayesian methods are used to incorporate available non-sample information to test the restrictions imposed by the strategic pricing hypothesis. The data provide broad but limited support for the null. Electricity prices appear to be consistent with the behaviour of governments who wish to manipulate electricity prices for partisan gain and who also wish to avoid detection. /// Les prix de l'électricité et les élections au Québec. Dans cette étude, nous examinons le rôle des élections dans la détermination des prix de l'électricité au Québec. Nous utilisons le cadre légal régissant l'établissement des tarifs pour développer des modèles qui incorporent à la fois les critères financiers d'Hydro-Québec et les intérêts partisans du parti politique au pouvoir. Nous faisons appel à l'approche bayesienne pour incorporer l'information disponible a priori et pour tester les restrictions sur les modèles qui découlent de l'hypothèse de comportement stratégique de la part du gouvernement. Les données empiriques tendent à supporter notre hypothèse principale sur l'usage de la détermination des prix de l'électricité de façon stratégique par les gouvernements au Québec. L'évolution des prix de l'électricité indique que les gouvernements les utilisent de façon partisane tout en essayant d'éviter que ce ne soit trop évident.|Electricity Prices and Elections in Quebec|http://www.jstor.org/stable/136231|136231|1997-08-01|1997|['eng']|['Physical sciences - Astronomy']|['Business & Economics', 'Business', 'Political Science', 'Economics', 'Social Sciences']
One of the major objections to the standard multiple-recapture approach to population estimation is the assumption of homogeneity of individual 'capture' probabilities. Modelling individual capture heterogeneity is complicated by the fact that it shows up as a restricted form of interaction among lists in the contingency table cross-classifying list memberships for all individuals. Traditional log-linear modelling approaches to capture-recapture problems are well suited to modelling interactions among lists but ignore the special dependence structure that individual heterogeneity induces. A random-effects approach, based on the Rasch model from educational testing and introduced in this context by Darroch and co-workers and Agresti, provides one way to introduce the dependence resulting from heterogeneity into the log-linear model; however, previous efforts to combine the Rasch-like heterogeneity terms additively with the usual log-linear interaction terms suggest that a more flexible approach is required. In this paper we consider both classical multilevel approaches and fully Bayesian hierarchical approaches to modelling individual heterogeneity and list interactions. Our framework encompasses both the traditional log-linear approach and various elements from the full Rasch model. We compare these approaches on two examples, the first arising from an epidemiological study of a population of diabetics in Italy, and the second a study intended to assess the 'size' of the World Wide Web. We also explore extensions allowing for interactions between the Rasch and log-linear portions of the models in both the classical and the Bayesian contexts.|Classical Multilevel and Bayesian Approaches to Population Size Estimation Using Multiple Lists|http://www.jstor.org/stable/2680485|2680485|1999-01-01|1999|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Previous research indicates mixed conclusions on the potential mispricing of equity index put options. We examine the returns of put writing and other option strategies by comparing historical option returns with returns generated using option pricing models. We find it is generally possible to reject the hypothesis that put returns are consistent with option pricing models. An implication is that the average cost of buying out-of-the-money put options to provide tail-risk protection to a portfolio may include a significant premium. Our results are based on a sample period of 1987–2012 that includes periods of high volatility in equity returns.|Index Option Returns: Still Puzzling|http://www.jstor.org/stable/24465654|24465654|2014-06-01|2014|['eng']|['Physical sciences - Astronomy']|['Business & Economics', 'Business', 'Finance']
The Gaussian process (GP) model provides a powerful methodology for calibrating a computer model in the presence of model uncertainties. However, if the data contain systematic experimental errors, then the GP model can lead to an unnecessarily complex adjustment of the computer model. In this work, we introduce an adjustment procedure that brings the computer model closer to the data by making minimal changes to it. This is achieved by applying a lasso-based variable selection on the systematic experimental error terms while fitting the GP model. Two real examples and simulations are presented to demonstrate the advantages of the proposed approach. This article has supplementary material available online.|Model Calibration Through Minimal Adjustments|http://www.jstor.org/stable/24587034|24587034|2014-11-01|2014|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
This article proposes a new Bayesian approach to prediction on continuous covariates. The Bayesian partition model constructs arbitrarily complex regression and classification surfaces by splitting the covariate space into an unknown number of disjoint regions. Within each region the data are assumed to be exchangeable and come from some simple distribution. Using conjugate priors, the marginal likelihoods of the models can be obtained analytically for any proposed partitioning of the space where the number and location of the regions is assumed unknown a priori. Markov chain Monte Carlo simulation techniques are used to obtain predictive distributions at the design points by averaging across posterior samples of partitions.|Bayesian Prediction via Partitioning|http://www.jstor.org/stable/27594151|27594151|2005-12-01|2005|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Computer Science', 'Statistics']
We consider Bayesian methodology for comparing two or more unlabeled point sets. Application of the technique to a set of steroid molecules illustrates its potential utility involving the comparison of molecules in chemoinformatics and bioinformatics. We initially match a pair of molecules, where one molecule is regarded as random and the other fixed. A type of mixture model is proposed for the point set coordinates, and the parameters of the distribution are a labeling matrix (indicating which pairs of points match) and a concentration parameter. An important property of the likelihood is that it is invariant under rotations and translations of the data. Bayesian inference for the parameters is carried out using Markov chain Monte Carlo simulation, and it is demonstrated that the procedure works well on the steroid data. The posterior distribution is difficult to simulate from, due to multiple local modes, and we also use additional data (partial charges on atoms) to help with this task. An approximation is considered for speeding up the simulation algorithm, and the approximating fast algorithm leads to essentially identical inference to that under the exact method for our data. Extensions to multiple molecule alignment are also introduced, and an algorithm is described which also works well on the steroid data set. After all the steroid molecules have been matched, exploratory data analysis is carried out to examine which molecules are similar. Also, further Bayesian inference for the multiple alignment problem is considered. /// Nous envisageons une méthodologie bayesienne pour comparer deux ou plus de deux ensembles de points non identifiés. L'application de la technique à un ensembles de molecules stéroïdes illustre son utilité potentielle, qui inclut la comparaison de molécules en chimio- informatique et en bioinformatique. Nous commençons par constituer une paire de molécules, l'une étant vue comme aléatoire, et l'autre comme fixe. Nous proposons un type de modèle de mélange pour les coordonnées de l'ensemble de points, et les paramètres de la distribution constituent une matrice identifiée (où sont indiqués les points en correspondance) et un paramètre de concentration. Une propriété importante de la vraisemblance est qu'elle est invariante par translation et rotation des données. Une inférence bayesienne est obtenue pour les paramètres au moyen de simulations de Monte Carlo par chaînes de Markov, et l'on montre que cette procédure marche bien sur les données de stéroïdes. Il est difficile de simuler à partir de la distribution a posteriori, à cause des multiples modes locaux, et nous utilisons aussi des données supplémentaires (des charges partielles sur les atomes) comme aide dans cette opération. Nous envisageons une approximation pour rendre plus rapide l'algorithme de simulation, et l'algorithme rapide approché produit sur nos données une inférence essentiellement identique à celle produite par la méthode exacte. Nous introduisons des extensions à plus de deux molécules, et décrivons un algorithme qui marche également bien sur les données de stéroïdes. Après que toutes les molécules ont été appariées, une analyse exploratoire des données est effectuées pour examiner quelles molécules sont similaires. D'autres inférences bayesiennes sont aussi envisagées pour le probleme à plus de deux molécules.|Statistical Analysis of Unlabeled Point Sets: Comparing Molecules in Chemoinformatics|http://www.jstor.org/stable/4541320|4541320|2007-03-01|2007|['eng']|['Information science - Coding theory', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
"The mixed or heterogeneous multinomial logit (MIXL) model has become popular in a number of fields, especially marketing, health economics, and industrial organization. In most applications of the model, the vector of consumer utility weights on product attributes is assumed to have a multivariate normal (MVN) distribution in the population. Thus, some consumers care more about some attributes than others, and the IIA property of multinomial logit (MNL) is avoided (i.e., segments of consumers will tend to switch among the subset of brands that possess their most valued attributes). The MIXL model is also appealing because it is relatively easy to estimate. Recently, however, some researchers have argued that the MVN is a poor choice for modelling taste heterogeneity. They argue that much of the heterogeneity in attribute weights is accounted for by a pure scale effect (i.e., across consumers, all attribute weights are scaled up or down in tandem). This implies that choice behaviour is simply more random for some consumers than others (i.e., holding attribute coefficients fixed, the scale of their error term is greater). This leads to a ""scale heterogeneity"" MNL model (S-MNL). Here, we develop a generalized multinomial logit model (G-MNL) that nests S-MNL and MIXL. By estimating the S-MNL, MIXL, and G-MNL models on 10 data sets, we provide evidence on their relative performance. We find that models that account for scale heterogeneity (i.e., G-MNL or S-MNL) are preferred to MIXL by the Bayes and consistent Akaike information criteria in all 10 data sets. Accounting for scale heterogeneity enables one to account for ""extreme"" consumers who exhibit nearly lexicographic preferences, as well as consumers who exhibit very ""random"" behaviour (in a sense we formalize below)."|The Generalized Multinomial Logit Model: Accounting for Scale and Coefficient Heterogeneity|http://www.jstor.org/stable/40608156|40608156|2010-05-01|2010|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Marketing & Advertising', 'Business & Economics', 'Business']
The main purpose of this article is to investigate a nonlinear structural equation model with covariates and mixed continuous and ordered categorical outcomes, in the presence of missing observations and missing covariates that are missing with a nonignorable mechanism. The nonignorable missingness mechanism is specified by a logistic regression model. A Bayesian approach is proposed for obtaining the joint Bayesian estimates of structural parameters, latent variables and parameters in the logistic regression model. An algorithm that combines the Gibbs sampler and the Metropolis-Hastings algorithm is developed for sampling observations from the posterior distributions, and for obtaining the Bayesian solution. A procedure for computing the Bayes factor for model comparison is developed via path sampling. Sensitivity analyses of the results with respect to the assumed model for the missingness mechanism, the prior inputs, and the missing covariate distributions are conducted via simulation studies. An example is presented to illustrate the newly developed Bayesian methodologies.|ANALYSIS OF NONLINEAR STRUCTURAL EQUATION MODELS WITH NONIGNORABLE MISSING COVARIATES AND ORDERED CATEGORICAL DATA|http://www.jstor.org/stable/24307780|24307780|2006-10-01|2006|['eng']|['Mathematics - Applied mathematics', 'Physical sciences - Astronomy', 'Mathematics - Mathematical logic']|['Mathematics', 'Science and Mathematics', 'Statistics']
Carroll et al. (2009) summarize the similarities and differences between the NOMINATE and IDEAL methods of fitting spatial voting models to binary roll-call data. As those authors note, for the class of problems with which either NOMINATE and the Bayesian quadratic-normal model can be used, the ideal point estimates almost always coincide, and when they do not, the discrepancy is due to the somewhat arbitrary identification and computational constraints imposed by each method. There are, however, many problems for which the Bayesian quadratic-normal model can be easily generalized, so as to address a broad array of questions and take advantage of additional data. Given the nature and source of the differences between NOMINATE and the Bayesian approach—as well as the fact that both approaches are approximations of the decision-making processes being modeled—we believe that it is preferable to choose the more flexible Bayesian approach.|To Simulate or NOMINATE?|http://www.jstor.org/stable/20680258|20680258|2009-11-01|2009|['eng']|['Political science - Government', 'Information science - Informetrics']|['Political Science', 'Social Sciences']
We investigate the implications of time-varying expected return and volatility on asset allocation in a high dimensional setting. We propose a dynamic factor multivariate stochastic volatility (DFMSV) model that allows the first two moments of returns to vary over time for a large number of assets. We then evaluate the economic significance of the DFMSV model by examining the performance of various dynamic portfolio strategies chosen by mean-variance investors in a universe of 36 stocks. We find that the DFMSV dynamic strategies significantly outperform various bench-mark strategies out of sample. This outperformance is robust to different performance measures, investor's objective functions, time periods, and assets.|Asset Allocation with a High Dimensional Latent Factor Stochastic Volatility Model|http://www.jstor.org/stable/3598036|3598036|2006-04-01|2006|['eng']|['Physical sciences - Astronomy']|['Business & Economics', 'Business', 'Finance']
"Neuroimaging meta-analysis is an important tool for finding consistent effects over studies that each usually have 20 or fewer subjects. Interest in meta-analysis in brain mapping is also driven by a recent focus on so-called ""reverse inference"": where as traditional ""forward inference"" identifies the regions of the brain involved in a task, a reverse inference identifies the cognitive processes that a task engages. Such reverse inferences, however, require a set of meta-analysis, one for each possible cognitive domain. However, existing methods for neuroimaging meta-analysis have significant limitations. Commonly used methods for neuroimaging meta-analysis are not model based, do not provide interpretable parameter estimates, and only produce null hypothesis inferences; further, they are generally designed for a single group of studies and cannot produce reverse inferences. In this work we address these limitations by adopting a nonparametric Bayesian approach for meta-analysis data from multiple classes or types of studies. In particular, foci from each type of study are modeled as a cluster process driven by a random intensity function that is modeled as a kernel convolution of a gamma random field. The type-specific gamma random fields are linked and modeled as a realization of a common gamma random field, shared by all types, that induces correlation between study types and mimics the behavior of a univariate mixed effects model. We illustrate our model on simulation studies and a meta-analysis of five emotions from 219 studies and check model fit by a posterior predictive assessment. In addition, we implement reverse inference by using the model to predict study type from a newly presented study. We evaluate this predictive performance via leave-one-out cross-validation that is efficiently implemented using importance sampling techniques."|A BAYESIAN HIERARCHICAL SPATIAL POINT PROCESS MODEL FOR MULTI-TYPE NEUROIMAGING META-ANALYSIS|http://www.jstor.org/stable/24522285|24522285|2014-09-01|2014|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
Common cluster models for multi-type point processes model the aggregation of points of the same type. In complete contrast, in the study of Anglo-Saxon settlements it is hypothesized that administrative clusters involving complementary names tend to appear. We investigate the evidence for such a hypothesis by developing a Bayesian Random Partition Model based on clusters formed by points of different types (complementary clustering). As a result, we obtain an intractable posterior distribution on the space of matchings contained in a k-partite hypergraph. We apply the Metropolis-Hastings (MH) algorithm to sample from this posterior. We consider the problem of choosing an efficient MH proposal distribution and we obtain consistent mixing improvements compared to the choices found in the literature. Simulated Tempering techniques can be used to overcome multimodality and a multiple proposal scheme is developed to allow for parallel programming. Finally, we discuss results arising from the careful use of convergence diagnostic techniques. This allows us to study a data set including locations and place-names of 1316 Anglo-Saxon settlements dated approximately around 750-850 AD. Without strong prior knowledge, the model allows for explicit estimation of the number of clusters, the average intra-cluster dispersion and the level of interaction among place-names. The results support the hypothesis of organization of settlements into administrative clusters based on complementary names.|RANDOM PARTITION MODELS AND COMPLEMENTARY CLUSTERING OF ANGLO-SAXON PLACE-NAMES|http://www.jstor.org/stable/43826445|43826445|2015-12-01|2015|['eng']|['Mathematics - Applied mathematics']|['Mathematics', 'Science & Mathematics', 'Statistics']
Consider the conditionally independent hierarchical model (CIHM) in which observations yi are independently distributed from f(yi∣θi), the parameters θi are independently distributed from distributions g(θ∣λ), and the hyperparameters λ are distributed according to a distribution h(λ). The posterior distribution of all parameters of the CIHM can be efficiently simulated by Markov chain Monte Carlo (MCMC) algorithms. Although these simulation algorithms have facilitated the application of CIHMs, they generally have not addressed the problem of computing quantities useful in model selection. This article explores how MCMC simulation algorithms and other related computational algorithms can be used to compute Bayes factors that are useful in criticizing a particular CIHM. In the case where the CIHM models a belief that the parameters are exchangeable or lie on a regression surface, the Bayes factor can measure the consistency of the data with the structural prior belief. Bayes factors can also be used to judge the suitability of particular assumptions in CIHMs, including the choice of link function, the nonexistence or existence of outliers, and the prior belief in exchangeability. The methods are illustrated in the situation in which a CIHM is used to model structural prior information about a set of binomial probabilities.|Bayesian Tests and Model Diagnostics in Conditionally Independent Hierarchical Models|http://www.jstor.org/stable/2965555|2965555|1997-09-01|1997|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
The model based on Gaussian process (GP) prior and a kernel covariance function can be used to fit nonlinear data with multidimensional covariates. It has been used as a flexible nonparametric approach for curve fitting, classification, clustering, and other statistical problems, and has been widely applied to deal with complex nonlinear systems in many different areas particularly in machine learning. However, it is a challenging problem when the model is used for the largescale data sets and high-dimensional data, for example, for the meat data discussed in this article that have 100 highly correlated covariates. For such data, it suffers from large variance of parameter estimation and high predictive errors, and numerically, it suffers from unstable computation. In this article, penalized likelihood framework will be applied to the model based on GPs. Different penalties will be investigated, and their ability in application given to suit the characteristics of GP models will be discussed. The asymptotic properties will also be discussed with the relevant proofs. Several applications to real biomechanical and bioinformatics data sets will be reported.|Penalized Gaussian Process Regression and Classification for High-Dimensional Nonlinear Data|http://www.jstor.org/stable/41434433|41434433|2011-12-01|2011|['eng']|['Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
This article focuses on the distribution of price sensitivity across consumers. We employ a random-coefficient logit model in which brand-specific intercepts and price-slope coefficients are allowed to vary across households. The model is estimated with panel data for two product categories. The implications of the estimated model are deduced through an optimal retail pricing analysis that combines the panel data with chain-level cost figures. We test parametric distributional assumptions using semiparametric density estimates based on series expansions.|Modeling the Distribution of Price Sensitivity and Implications for Optimal Retail Pricing|http://www.jstor.org/stable/1392189|1392189|1995-07-01|1995|['eng']|['Applied sciences - Engineering']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
While statisticians are well-accustomed to performing exploratory analysis in the modeling stage of an analysis, the notion of conducting preliminary general-purpose exploratory analysis in the Monte Carlo stage (or more generally, the model-fitting stage) of an analysis is an area that we feel deserves much further attention. Toward this aim, this article proposes a general-purpose algorithm for automatic density exploration. The proposed exploration algorithm combines and expands upon components from various adaptive Markov chain Monte Carlo methods, with the Wang-Landau algorithm at its heart. Additionally, the algorithm is run on interacting parallel chains—a feature that both decreases computational cost as well as stabilizes the algorithm, improving its ability to explore the density. Performance of this new parallel adaptive Wang-Landau algorithm is studied in several applications. Through a Bayesian variable selection example, we demonstrate the convergence gains obtained with interacting chains. The ability of the algorithm's adaptive proposal to induce mode-jumping is illustrated through a Bayesian mixture modeling application. Last, through a two-dimensional Ising model, the authors demonstrate the ability of the algorithm to overcome the high correlations encountered in spatial models. Supplemental materials are available online.|An Adaptive Interacting Wang-Landau Algorithm for Automatic Density Exploration|http://www.jstor.org/stable/43304858|43304858|2013-09-01|2013|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Computer Science', 'Statistics']
We show in this paper how a hierarchical Bayes modelling can lead to better insights in a pharmaceutical experiment assessing the performances of a drug against vertigo, by providing direct access to different extensions of the model. The availability of such extensions relies on the use of Gibbs sampling techniques which allow for simulation from arbitrary distributions. The Bayesian processing of the experiment is noninformative, which allows for an objective perspective, and we consider in detail the extension when the population is heterogeneous at some stage of the experiment, and the corresponding modelling by a normal mixture distribution.|Bayesian Modelling of a Pharmaceutical Experiment with Heterogeneous Responses|http://www.jstor.org/stable/25053027|25053027|1998-04-01|1998|['eng']|['Mathematics - Mathematical logic', 'Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
Bayesian analysis of hierarchically structured data with random intercept and heterogeneous within-group (Level-1) variance is presented. Inferences about all parameters, including the Level-1 variance and intercept for each group, are based on their marginal posterior distributions approximated via the Gibbs sampler. Analysis of artificial data with varying degrees of heterogeneity and varying Level-2 sample sizes illustrates the likely benefits of using a Bayesian approach to model heterogeneity of variance (Bayes/Het). Results are compared to those based on now-standard restricted maximum likelihood with homogeneous Level-1 variance (RML/Hom). Bayes/Het provides sensible interval estimates for Level-1 variances and their heterogeneity, and, relatedly, for each group's intercept. RML/Hom inferences about Level-2 regression coefficients appear surprisingly robust to heterogeneity, and conditions under which such robustness can be expected are discussed. Application is illustrated in a reanalysis of High School and Beyond data. It appears informative and practically feasible to obtain approximate marginal posterior distributions for all Level-1 and Level-2 parameters when analyzing large- or small-scale survey data. A key advantage of the Bayes approach is that inferences about any parameter appropriately reflect uncertainty about all remaining parameters.|Application of Gibbs Sampling to Nested Variance Components Models with Heterogeneous Within-Group Variance|http://www.jstor.org/stable/1165316|1165316|1998-07-01|1998|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Education', 'Statistics', 'Social Sciences']
"Mild sufficient conditions for the ergodicity of the Gibbs sampler are obtained. The geometric ergodicity of the Gibbs sampler is then discussed, where the main tool is the ""drift"" criterion. This criterion requires the construction of a generalized energy function so that the chain is dissipating energy, at a suitable rate, whenever it lies outside the ""center"" of the state-space. For the ""two-variable"" case, a useful recipe is provided for constructing appropriate energy functions. These results are illustrated with four examples."|Asymptotic Behavior of the Gibbs Sampler|http://www.jstor.org/stable/2290727|2290727|1993-03-01|1993|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Several state-of-the-art binary classification techniques are experimentally evaluated in the context of expert automobile insurance claim fraud detection. The predictive power of logistic regression, C4.5 decision tree, k-nearest neighbor, Bayesian learning multilayer perceptron neural network, least-squares support vector machine, naive Bayes, and tree-augmented naive Bayes classification is contrasted. For most of these algorithm types, we report on several operationalizations using alternative hyperparameter or design choices. We compare these in terms of mean percentage correctly classified (PCC) and mean area under the receiver operating characteristic (AUROC) curve using a stratified, blocked, ten-fold cross-validation experiment. We also contrast algorithm type performance visually by means of the convex hull of the receiver operating characteristic (ROC) curves associated with the alternative operationalizations per algorithm type. The study is based on a data set of 1,399 personal injury protection claims from 1993 accidents collected by the Automobile Insurers Bureau of Massachusetts. To stay as close to real-life operating conditions as possible, we consider only predictors that are known relatively early in the life of a claim. Furthermore, based on the qualification of each available claim by both a verbal expert assessment of suspicion of fraud and a ten-point-scale expert suspicion score, we can compare classification for different target/class encoding schemes. Finally, we also investigate the added value of systematically collecting nonflag predictors for suspicion of fraud modeling purposes. From the observed results, we may state that: (1) independent of the target encoding scheme and the algorithm type, the inclusion of nonflag predictors allows us to significantly boost predictive performance; (2) for all the evaluated scenarios, the performance difference in terms of mean PCC and mean AUROC between many algorithm type operationalizations turns out to be rather small; visual comparison of the algorithm type ROC curve convex hulls also shows limited difference in performance over the range of operating conditions; (3) relatively simple and efficient techniques such as linear logistic regression and linear kernel least-squares support vector machine classification show excellent overall predictive capabilities, and (smoothed) naive Bayes also performs well; and (4) the C4.5 decision tree operationalization results are rather disappointing; none of the tree operationalizations are capable of attaining mean AUROC performance in line with the best. Visual inspection of the evaluated scenarios reveals that the C4.5 algorithm type ROC curve convex hull is often dominated in large part by most of the other algorithm type hulls.|A Comparison of State-of-the-Art Classification Techniques for Expert Automobile Insurance Claim Fraud Detection|http://www.jstor.org/stable/1558683|1558683|2002-09-01|2002|['eng']|['Information science - Coding theory', 'Mathematics - Applied mathematics']|['Business & Economics', 'Business', 'Economics']
Tumour multiplicity is a frequently measured phenotype in animal studies of cancer biology. Poisson variation of this measurement represents a biological and statistical reference point that is usually violated, even in highly controlled experiments, owing to sources of variation in the stochastic process of tumour formation. A recent experiment on murine intestinal tumours presented conditions which seem to generate Poisson-distributed tumour counts. If valid, this would support a claim about mechanisms by which the adenomatous polyposis coli gene is inactivated during tumour initiation. In considering hypothesis testing strategies, model choice and Bayesian approaches, we quantify the positive evidence favouring Poisson variation in this experiment. Statistical techniques used include likelihood ratio testing, the Bayes and Akaike information criteria, negative binomial modelling, reversible jump Markov chain Monte Carlo methods and posterior predictive checking. The posterior approximation that is based on the Bayes information criterion is found to be quite accurate in this small n case-study.|Assessing Poisson Variation of Intestinal Tumour Multiplicity in Mice Carrying a Robertsonian Translocation|http://www.jstor.org/stable/3592592|3592592|2006-01-01|2006|['eng']|['Biological sciences - Biology', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Ideal point estimation is an important tool to study legislative and judicial voting behaviours. We propose a hierarchical ideal point estimation framework that directly models complex voting behaviours on the basis of the characteristics of the political actors and the votes that they cast. Through simulations and empirical examples we show that this framework holds good promise for resolving many unsettled issues, such as the multi-dimensional aspects of ideology, and the effects of political parties. As a companion to this paper, we offer an easy-to-use R package that implements the methods discussed.|Understanding complex legislative and judicial behaviour via hierarchical ideal point estimation|http://www.jstor.org/stable/41057553|41057553|2011-01-01|2011|['eng']|['Mathematics - Applied mathematics', 'Information science - Informetrics']|['Science & Mathematics', 'Statistics']
This article considers the problem of identifying the irregular boundary of a magnetic domain in a thin multilayer film, using data in the form of an electron micrograph. Two approaches are illustrated, both of which are based on the concept of two smooth intensity surfaces, one corresponding to the domain and one corresponding to the background, so that the objective is to determine the boundary between the parts of the image where the two surfaces are present. In the first, a hierarchical Bayesian approach, priors are assumed for the set of domain/background states and for the two intensity surfaces, and restoration is carried out using a version of Besag's ICM procedure. It is shown that it is important to initialize the method efficiently. The second approach uses a template-like model for the boundary and also relies on a Bayesian approach. The results from the second approach can be used as an end in themselves or as a way of initializing the more flexible first approach. Several illustrations are provided.|An Image Analysis Problem in Electron Microscopy|http://www.jstor.org/stable/2291713|2291713|1996-09-01|1996|['eng']|['Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
This paper explores the role that the imperfect knowledge of the structure of the economy plays in the uncertainty surrounding the effects of rule-based monetary policy on unemployment dynamics in the euro area and the United States. We employ a Bayesian model averaging procedure on a wide range of models which differ in several dimensions to account for the uncertainty that the policymaker faces when setting the monetary policy and evaluating its effect on real economy. We find evidence of a high degree of dispersion across models in both policy rule parameters and impulse response functions. Moreover, monetary policy shocks have very similar recessionary effects on the two economies with a different role played by the participation rate in the transmission mechanism. Finally, we show that a policymaker who does not take model uncertainty into account and selects die results on the basis of a single model may come to misleading conclusions not only about the transmission mechanism, but also about the differences between the euro area and the United States, which are on average essentially small.|The Effects of Monetary Policy on Unemployment Dynamics under Model Uncertainty: Evidence from the United States and the Euro Area|http://www.jstor.org/stable/40271633|40271633|2009-10-01|2009|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics']|['Business & Economics', 'Business', 'Economics', 'Finance']
This article presents a new prior setting for high-dimensional generalized linear models, which leads to a Bayesian subset regression (BSR) with the maximum a posteriori model approximately equivalent to the minimum extended Bayesian information criterion model. The consistency of the resulting posterior is established under mild conditions. Further, a variable screening procedure is proposed based on the marginal inclusion probability, which shares the same properties of sure screening and consistency with the existing sure independence screening (SIS) and iterative sure independence screening (ISIS) procedures. However, since the proposed procedure makes use of joint information from all predictors, it generally outperforms SIS and ISIS in real applications. This article also makes extensive comparisons of BSR with the popular penalized likelihood methods, including Lasso, elastic net, SIS, and ISIS. The numerical results indicate that BSR can generally outperform the penalized likelihood methods. The models selected by BSR tend to be sparser and, more importantly, of higher prediction ability. In addition, the performance of the penalized likelihood methods tends to deteriorate as the number of predictors increases, while this is not significant for BSR. Supplementary materials for this article are available online.|Bayesian Subset Modeling for High-Dimensional Generalized Linear Models|http://www.jstor.org/stable/24246466|24246466|2013-06-01|2013|['eng']|['Biological sciences - Biology']|['Science & Mathematics', 'Statistics']
This article compares two methodologies for modeling and forecasting statistical time series models of demographic processes: Box-Jenkins ARIMA and structural time series analysis. The Lee-Carter method is used to construct nonlinear demographic models of U.S. mortality rates for the total population, gender, and race and gender combined. Single time varying parameters of k, the index of mortality, are derived from these model and fitted and forecasted using the two methodologies. Forecasts of life expectancy at birth, e0, are generated from these indexes of k. Results show marginal differences in fit and forecasts between the two statistical approaches with a slight advantage to structural models. Stability across models for both methodologies offers support for the robustness of this approach to demographic forecasting.|Forecasting U.S. Mortality: A Comparison of Box-Jenkins ARIMA and Structural Time Series Models|http://www.jstor.org/stable/4121306|4121306|1996-01-01|1996|['eng']|['Physical sciences - Astronomy']|['Sociology', 'Social Sciences']
Tree mortality is an important process determining forest dynamics. However, in species-rich tropical forests it is largely unknown, how species differ in their response of mortality to resource availability and individual condition. We use a hierarchical Bayesian approach to quantify the impact of light availability, tree size and past growth on mortality of 284 woody species in a 50-ha long-term forest census plot in Barro Colorado Island, Panama. Light reaching each individual tree was estimated from yearly vertical censuses of vegetation density. Across the community, 78% of the species showed increasing mortality with increasing light. Considering species individually, just 29 species showed a significant response to light, all with increasing mortality at high light. Past growth had a significant impact on all but three species, with higher growth leading to lower mortality. For the majority of species, mortality decreased sharply with diameter in saplings, then levelled off or increased slightly towards the maximum diameter of the species. Diameter had the biggest impact on mortality, followed by past growth and finally light availability. Our results challenge many previous reports of higher mortality in shade, and we suggest that it is crucial to control for size effects when assessing the impact of environmental conditions on mortality.|Determinants of mortality across a tropical lowland rainforest community|http://www.jstor.org/stable/23015007|23015007|2011-07-01|2011|['eng']|['Physical sciences - Astronomy']|['Biological Sciences', 'Ecology & Evolutionary Biology', 'Science and Mathematics', 'Zoology']
P-splines are an attractive approach for modeling nonlinear smooth effects of covariates within the additive and varying coefficient models framework. In this article, we first develop a Bayesian version for P-splines and generalize in a second step the approach in various ways. First, the assumption of constant smoothing parameters can be replaced by allowing the smoothing parameters to be locally adaptive. This is particularly useful in situations with changing curvature of the underlying smooth function or with highly oscillating functions. In a second extension, one-dimensional P-splines are generalized to two-dimensional surface fitting for modeling interactions between metrical covariates. In a last step, the approach is extended to situations with spatially correlated responses allowing the estimation of geoadditive models. Inference is fully Bayesian and uses recent MCMC techniques for drawing random samples from the posterior. In a couple of simulation studies the performance of Bayesian P-splines is studied and compared to other approaches in the literature. We illustrate the approach by two complex application on rents for flats in Munich and on human brain mapping.|Bayesian P-Splines|http://www.jstor.org/stable/1391151|1391151|2004-03-01|2004|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Computer Science', 'Statistics']
Follow-up medical studies often collect longitudinal data on patients. Multistate transitional models are useful for analysis in such studies where at any point in time, individuals may be said to occupy one of a discrete set of states and interest centers on the transition process between states. For example, states may refer to the number of recurrences of an event, or the stage of a disease. We develop a hierarchical modeling framework for the analysis of such longitudinal data when the processes corresponding to different subjects may be correlated spatially over a region. Continuous-time Markov chains incorporating spatially correlated random effects are introduced. Here, joint modeling of both spatial dependence as well as dependence between different transition rates is required and a multivariate spatial approach is employed. A proportional intensities frailty model is developed where baseline intensity functions are modeled using parametric Weibull forms, piecewise-exponential formulations, and flexible representations based on cubic B-splines. The methodology is developed within the context of a study examining invasive cardiac procedures in Quebec. We consider patients admitted for acute coronary syndrome throughout the 139 local health units of the province and examine readmission and mortality rates over a 4-year period. /// Les études médicales de suivi à long terme collectent souvent des données longitudinales sur des patients. Les modèles de transition multi-états sont utiles dans de telles études, où à n'importe quel point dans le temps, les individus sont dits occupés un état d'un ensemble discret et où l'intérêt porte sur le processus de transition entre états. Par exemple, les états peuvent se référer au nombre de récurrences d'un événement ou au stade de la maladie. Nous développons une modélisation hiérarchique pour l'analyse de telles données longitudinales où les processus correspondants à chaque sujet peuvent être corrélés spatialement sur une région. Des chaînes de Markov à temps continu incorporant des effets aléatoires spatialement corrélés sont introduites. Ici, une modélisation jointe de la dépendance spatiale aussi bien que la dépendance entre les différents taux de transition est requise et une approche multivariée spatiale est employée. Un modèle de fragilité à intensités proportionnelles est développé où les fonctions d'intensité de base sont modélisées soit en utilisant une forme paramétrique de Weibull, soit une formulation exponentielle par morceaux, soit une représentation flexible par des fonctions B-splines cubiques. La méthodologie est développée dans le contexte d'une étude examinant des procédures cardiaques invasives au Québec. Nous considérons des patients admis pour un syndrome coronaire aigu dans 139 unités locales de santé de la province et examinons les taux de réadmission et de mortalité sur une période de quatre ans.|Spatial Multistate Transitional Models for Longitudinal Event Data|http://www.jstor.org/stable/25502045|25502045|2008-03-01|2008|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
We consider the problem of multiple comparisons from a Bayesian viewpoint. The family of Dirichlet process priors is applied in the form of baseline prior/likelihood combinations to obtain posterior probabilities for various hypotheses of equality among population means. The baseline prior/likelihood combinations considered here are beta/binomial and normal/inverted gamma with equal variances on treatment means. The prior probabilities of the hypotheses depend directly on the concentration parameter of the Dirichlet process prior. Finding posterior distributions is analytically intractable; we use Gibbs sampling. The posterior probabilities of hypotheses of interest are easily obtained as a by-product in evaluating the marginal posterior distributions of the parameters. The proposed procedure is compared to Duncan's multiple range test and shown to be more powerful under certain alternative hypotheses.|Bayesian Multiple Comparisons Using Dirichlet Process Priors|http://www.jstor.org/stable/2669856|2669856|1998-09-01|1998|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
We describe and illustrate Bayesian inference in models for density estimation using mixtures of Dirichlet processes. These models provide natural settings for density estimation and are exemplified by special cases where data are modeled as a sample from mixtures of normal distributions. Efficient simulation methods are used to approximate various prior, posterior, and predictive distributions. This allows for direct inference on a variety of practical issues, including problems of local versus global smoothing, uncertainty about density estimates, assessment of modality, and the inference on the numbers of components. Also, convergence results are established for a general class of normal mixture models.|Bayesian Density Estimation and Inference Using Mixtures|http://www.jstor.org/stable/2291069|2291069|1995-06-01|1995|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Discrete-time or grouped duration data, with one or multiple types of terminating events, are often observed in social sciences or economics. In this paper we suggest and discuss dynamic models for flexible Bayesian nonparametric analysis of such data. These models allow simultaneous incorporation and estimation of baseline hazards and time-varying covariate effects, without imposing particular parametric forms. Methods for exploring the possibility of time-varying effects, as for example the impact of nationality or unemployment insurance benefits on the probability of reemployment, have recently gained increasing interest. Our modeling and estimation approach is fully Bayesian and makes use of Markov Chain Monte Carlo (MCMC) simulation techniques. A detailed analysis of unemployment duration data, with full-time job, part-time job and other causes as terminating events, illustrates our methods and shows how they can be used to obtain refined results and interpretations.|Dynamic Discrete-Time Duration Models: Estimation via Markov Chain Monte Carlo|http://www.jstor.org/stable/271112|271112|1997-01-01|1997|['eng']|['Mathematics - Applied mathematics']|['Sociology', 'Social Sciences']
"In multicenter studies, one often needs to make inference about a population survival curve based multiple, possibly heterogeneous survival data from individual centers. We investigate a flexible Bayesian method for estimating a population survival curve based on a semiparametric multiresolution hazard model that can incorporate covariates and account for center heterogeneity. The method yields a smooth estimate of the survival curve for ""Multiple resolutions"" or time scales of interest. The Bayesian model used has the capability to accommodate general forms of censoring and a priori smoothness assumptions. We develop a model checking and diagnostic technique based on the posterior predictive distribution and use it to identify departures from the model assumptions. The hazard estimator is used to analyze data from 110 centers that participated in a multicenter randomized clinical trial to evaluate tamoxifen in the treatment of early stage breast cancer. Of particular interest are the estimates of center heterogeneity in the baseline hazard curves and in the treatment effects, after adjustment for a few key clinical covariates. Our analysis suggests that the treatment effect estimates are rather robust, even for a collection of small trial centers, despite variations in center characteristics."|A Multiresolution Hazard Model for Multicenter Survival Studies: Application to Tamoxifen Treatment in Early Stage Breast Cancer|http://www.jstor.org/stable/27639966|27639966|2007-12-01|2007|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
The first five sections of the paper describe the Bayesian paradigm for statistics and its relationship with other attitudes towards inference. Section 1 outlines Wald's major contributions and explains how they omit the vital consideration of coherence. When this point is included the Bayesian view results, with the main difference that Waldean ideas require the concept of the sample space, whereas the Bayesian approach may dispense with it, using a probability distribution over parameter space instead. Section 2 relates statistical ideas to the problem of inference in science. Scientific inference is essentially the passage from observed, past data to unobserved, future data. The roles of models and theories in doing this are explored. The Bayesian view is that all this should be accomplished entirely within the calculus of probability and Section 3 justifies this choice by various axiom systems. The claim is made that this leads to a quite different paradigm from that of classical statistics and, in particular, problems in the latter paradigm cease to have importance within the other. Point estimation provides an illustration. Some counter-examples to the Bayesian view are discussed. It is important that statistical conclusions should be usable in making decisions. Section 4 explains how the Bayesian view achieves this practicality by introducing utilities and the principle of maximizing expected utility. Practitioners are often unhappy with the ideas of basing inferences on one number, probability, or action on another, an expectation, so these points are considered and the methods justified. Section 5 discusses why the Bayesian viewpoint has not achieved the success that its logic suggests. Points discussed include the relationship between the inferences and the practical situation, for example with multiple comparisons; and the lack of the need to confine attention to normality or the exponential family. Its extensive use by nonstatisticians is documented. The most important objection to the Bayesian view is that which rightly says that probabilities are hard to assess. Consequently Section 6 considers how this might be done and an attempt is made to appreciate how accurate formulae like the extension of the conversation, the product law and Bayes rule are in evaluating probabilities.|The 1988 Wald Memorial Lectures: The Present Position in Bayesian Statistics|http://www.jstor.org/stable/2245880|2245880|1990-02-01|1990|['eng']|['Philosophy - Logic', 'Mathematics - Mathematical logic', 'Mathematics - Applied mathematics']|['Science and Mathematics', 'Statistics']
We consider the empirical Bayes estimation of a distribution using binary data via the Dirichlet process. Let D(α) denote a Dirichlet process with α being a finite measure on [0, 1]. Instead of having direct samples from an unknown random distribution F from D(α), we assume that only indirect binomial data are observable. This paper presents a new interpretation of Lo's formula, and thereby relates the predictive density of the observations based on a Dirichlet process model to likelihoods of much simpler models. As a consequence, the log-likelihood surface, as well as the maximum likelihood estimate of c = α([ 0, 1]), is found when the shape of α is assumed known, together with a formula for the Fisher information evaluated at the estimate. The sequential imputation method of Kong, Liu and Wong is recommended for overcoming computational difficulties commonly encountered in this area. The related approximation formulas are provided. An analysis of the tack data of Beckett and Diaconis, which motivated this study, is supplemented to illustrate our methods.|Nonparametric Hierarchical Bayes Via Sequential Imputations|http://www.jstor.org/stable/2242574|2242574|1996-06-01|1996|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
We investigate empirically the existence of a heterogeneous relationship between foreign direct investment (FDI) and economic growth across developing countries. We argue that, across countries, differences in institutional quality are correlated with heterogeneous absorptive capacities and hence a heterogeneous FDI-growth relationship. Our empirical results show substantial heterogeneity in the FDI-growth relationship. We find that controlling for certain measures of institutional quality reduces the degree of heterogeneity. These findings question the orthodox assumption of a homogeneous return to FDI in the existing empirical literature and highlight the importance of specific aspects of institutional quality in the FDI-growth relationship.|Institutions, foreign direct investment and growth: a hierarchical Bayesian approach|http://www.jstor.org/stable/41409709|41409709|2012-01-01|2012|['eng']|['Biological sciences - Ecology']|['Science and Mathematics', 'Statistics']
This article discusses the problem of obtaining short-run and long-run elasticities of energy demand for each of 49 states in the United States using data for 21 years. Estimation using the time series data by each state gave several wrong signs for the coefficients. Estimation using pooled data was not valid because the hypothesis of homogeneity of the coefficients was rejected. Shrinkage estimators gave more reasonable results. The article presents in a unified framework the classical, empirical Bayes, and Bayes approaches for deriving these estimators.|Estimation of Short-Run and Long-Run Elasticities of Energy Demand from Panel Data Using Shrinkage Estimators|http://www.jstor.org/stable/1392078|1392078|1997-01-01|1997|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
In this paper, we propose a Bayesian estimation and forecasting procedure for noncausal autoregressive (AR) models. Specifically, we derive the joint posterior density of the past and future errors and the parameters, yielding predictive densities as a by-product. We show that the posterior model probabilities provide a convenient model selection criterion in discriminating between alternative causal and noncausal specifications. As an empirical application, we consider US inflation. The posterior probability of noncausality is found to be high—over 98%. Furthermore, the purely noncausal specifications yield more accurate inflation forecasts than alternative causal and noncausal AR models.|BAYESIAN MODEL SELECTION AND FORECASTING IN NONCAUSAL AUTOREGRESSIVE MODELS|http://www.jstor.org/stable/23257586|23257586|2012-08-01|2012|['eng']|['Physical sciences - Astronomy', 'Mathematics - Applied mathematics']|['Business', 'Business & Economics Collection', 'Economics']
This article describes a new approach to Bayesian selection of decomposable models with incomplete data. This approach requires the characterization of new ignorability conditions for the missing-data mechanism and the development of new computational methods. Both issues are considered, and solutions are proposed. Theory and methods are assessed in controlled experiments and in the analysis of one real-life incomplete dataset.|Bayesian Selection of Decomposable Models with Incomplete Data|http://www.jstor.org/stable/3085906|3085906|2001-12-01|2001|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
We consider function estimation in nonparametric regression over Besov spaces and under pointwise lu-risks (1 ≤ u &lt; ∞). First we derive both non-adaptive and adaptive minimax pointwise rates of convergence in the standard nonparametric regression model, complementing recent related results obtained in the Gaussian white noise model. Then we investigate theoretical performance of Bayes factor estimators at a single point in wavelet regression models with independent and identically distributed errors that are not necessarily normally distributed. We compare both non-adaptive and adaptive Bayes factor estimators in terms of their frequentist optimality over Besov spaces and under pointwise lu-risks (1 ≤ u &lt; ∞) for various combinations of error and prior distributions, extending recent non-adaptive results obtained for error and prior models with exponential descents and under pointwise l2-risks. We provide sufficient conditions that determine whether the unknown response function belongs to a Besov space a-priori with probability one, and identify regions wherein the response function enjoys both pointwise optimality, for the proposed non-adaptive and adaptive Bayes factor estimators, and a-priori Besov membership. A simulation study is conducted to illustrate the performance of the proposed adaptive Bayes factor estimation procedure with hyperparameters estimated in a fully Bayesian framework.|MINIMAX RATES OF CONVERGENCE AND OPTIMALITY OF BAYES FACTOR WAVELET REGRESSION ESTIMATORS UNDER POINTWISE RISKS|http://www.jstor.org/stable/24308908|24308908|2009-10-01|2009|['eng']|['Mathematics - Mathematical logic']|['Mathematics', 'Science & Mathematics', 'Statistics']
"This article presents a multivariate spatial prediction methodology in a Bayesian framework. The method is especially suited for use in environmetrics, where vector-valued responses are observed at a small set of ambient monitoring stations ""(gauged sites)"" at successive time points. However, the stations may have varying start-up times so that the data have a ""staircase"" pattern (""monotone"" pattern in the terminology of Rubin and Shaffer). The lowest step corresponds to the newest station in the monitoring network. We base our approach on a hierarchical Bayes prior involving a Gaussian generalized inverted Wishart model. For given hyperparameters, we derive the predictive distribution for currently gauged sites at times before their start-up when no measurements were taken. The resulting predictive distribution is a matric t distribution with appropriate covariance parameters and degrees of freedom. We estimate the hyperparameters using the method of moments (MOM) as an easy-to-implement alternative to the more complex EM algorithm. The MOM in particular gives exact parameter estimates and involves less cumbersome calculations than the EM algorithm. Finally, we obtain the predictive distribution for unmeasured responses at ""ungauged"" sites. The results obtained here allow us to pool the data from different sites that measure different pollutants and also to treat cases where the observed data monitoring stations have a monotonic ""staircase"" structure. We demonstrate the use of this methodology by mapping PM2.5 fields for Philadelphia during the period of May 1992 to September 1993. Large amounts of data missing by design make this application particularly challenging. We give empirical evidence that the method performs well."|Bayesian Spatial Prediction of Random Space-Time Fields with Application to Mapping PM<sub>2.5</sub> Exposure|http://www.jstor.org/stable/3085763|3085763|2002-03-01|2002|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
In many longitudinal studies it is desired to estimate and test the rate over time of a particular recurrent event. Often only the event counts corresponding to the elapsed time intervals between each subject's successive observation times, and baseline covariate data, are available. The intervals may vary substantially in length and number between subjects, so that the corresponding vectors of counts are not directly comparable. A family of Poisson likelihood regression models incorporating a mixed random multiplicative component in the rate function of each subject is proposed for this longitudinal data structure. A related empirical Bayes estimate of random-effect parameters is also described. These methods are illustrated by an analysis of dyspepsia data from the National Cooperative Gallstone Study.|Mixed Poisson Likelihood Regression Models for Longitudinal Interval Count Data|http://www.jstor.org/stable/2531907|2531907|1988-03-01|1988|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Proactive management of web server farms requires accurate prediction of workload. An exemplary measure of workload is the amount of service requests per unit time. As a time series, the workload exhibits not only short-term random fluctuations, but also prominent periodic (daily) patterns that evolve randomly from one period to another. A hierarchical framework with multiple time scales is proposed to model such time series. This framework leads to an adaptive procedure that provides both long-term (in days) and short-term (in minutes) predictions with simultaneous confidence bands that accommodate not only serial correlation, but also heavy tailedness, heteroscedasticity, and nonstationarity of the data.|A Hierarchical Framework for Modeling and Forecasting Web Server Workload|http://www.jstor.org/stable/27590612|27590612|2005-09-01|2005|['eng']|['Applied sciences - Engineering', 'Information science - Data products']|['Science & Mathematics', 'Statistics']
Phycas is open source, freely available Bayesian phylogenetics software written primarily in C++ but with Python interface. Phycas specializes in Bayesian model selection for nucleotide sequence data, particularly the estimation of marginal likelihoods, central to computing Bayes Factors. Marginal likelihoods can be estimated using newer methods (Thermodynamic Integration and Generalized Steppingstone) that are more accurate than the widely used Harmonic Mean estimator. In addition, Phycas supports two posterior predictive approaches to model selection: Gelfand—Ghosh and Conditional Predictive Ordinates. The General Time Reversible family of substitution models, as well as a codon model, are available, and data can be partitioned with all parameters unlinked except tree topology and edge lengths. Phycas provides for analyses in which the prior on tree topologies allows polytomous trees as well as fully resolved trees, and provides for several choices for edge length priors, including a hierarchical model as well as the recently described compound Dirichlet prior, which helps avoid overly informative induced priors on tree length.|Phycas: Software for Bayesian Phylogenetic Analysis|http://www.jstor.org/stable/43700028|43700028|2015-05-01|2015|['eng']|['Applied sciences - Engineering']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
This paper addresses the problem of quantifying expert opinion about a normal linear regression model when there is uncertainty as to which independent variables should be included in the model. Opinion is modeled as a mixture of natural conjugate prior distributions with each distribution in the mixture corresponding to a different subset of the independent variables. It is shown that for certain values of the independent variables, the predictive distribution of the dependent variable simplifies from a mixture of t-distributions to a single t-distribution. Using this result, a method of eliciting the conjugate distributions of the mixture is developed. The method is illustrated in an example.|Elicitation of Prior Distributions for Variable-Selection Problems in Regression|http://www.jstor.org/stable/2242364|2242364|1992-12-01|1992|['eng']|['Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
In clinical trials, a biomarker (S) that is measured after randomization and is strongly associated with the true endpoint (T) can often provide information about Tand hence the effect of a treatment (Z) on T. A useful biomarker can be measured earlier than T and cost less than T. In this article, we consider the use of S as an auxiliary variable and examine the information recovery from using S for estimating the treatment effect on T, when S is completely observed and T is partially observed. In an ideal but often unrealistic setting, when S satisfies Prentice's definition for perfect surrogacy, there is the potential for substantial gain in precision by using data from S to estimate the treatment effect on T. When S is not close to a perfect surrogate, it can provide substantial information only under particular circumstances. We propose to use a targeted shrinkage regression approach that data-adaptively takes advantage of the potential efficiency gain yet avoids the need to make a strong surrogacy assumption. Simulations show that this approach strikes a balance between bias and efficiency gain. Compared with competing methods, it has better mean squared error properties and can achieve substantial efficiency gain, particularly in a common practical setting when S captures much but not all of the treatment effect and the sample size is relatively small. We apply the proposed method to a glaucoma data example.|A Shrinkage Approach for Estimating a Treatment Effect Using Intermediate Biomarker Data in Clinical Trials|http://www.jstor.org/stable/41434449|41434449|2011-12-01|2011|['eng']|['Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
We consider the problem of estimating the number of components d and the unknown mixing distribution in a finite mixture model, in which d is bounded by some fixed finite number N. Our approach relies on the use of a prior over the space of mixing distributions with at most N components. By decomposing the resulting marginal density under this prior, we discover a weighted Bayes factor method for consistently estimating d that can be implemented by an iid generalized weighted Chinese restaurant (GWCR) Monte Carlo algorithm. We also discuss a Gibbs sampling method (the blocked Gibbs sampler) for estimating d and also the mixing distribution. We show that our resulting posterior is consistent and achieves the frequentist optimal Op(n-1/4) rate of estimation. We compare the performance of the new GWCR model selection procedure with that of the Akaike information criterion and the Bayes information criterion implemented through an EM algorithm. Applications of our methods to five real datasets and simulations are considered.|Bayesian Model Selection in Finite Mixtures by Marginal Density Decompositions|http://www.jstor.org/stable/3085902|3085902|2001-12-01|2001|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
In this paper, we propose a novel multiphase support vector regression (mp-SVR) technique to approximate a true relationship for the case where the effect of input on output changes abruptly at some break-points. A new formulation for mp-SVR is presented to allow such structural changes in regression function. And then, we present a new hybrid-encoding scheme in genetic algorithms to select the best combination of the kernel functions and to determine both break-points and hyperparameters of mp-SVR. The proposed method has a major advantage over the conventional ones that different kernel functions can be possibly adapted to different regions of the data domain. Computational results in two examples including a real-life data demonstrate its capability in capturing the local characteristics of the data more effectively. Consequently, the mp-SVR has a high potential value in a wide range of applications for function approximations.|Multiphase support vector regression for function approximation with break-points|http://www.jstor.org/stable/23407020|23407020|2013-05-01|2013|['eng']|['Applied sciences - Engineering']|['Business', 'Business & Economics Collection']
Bayesian versions of the classical one-way random effects model are widely used to analyze data. If the standard diffuse prior is adopted, there is a simple block Gibbs sampler that can be employed to explore the intractable posterior distribution. In this article, theoretical and methodological results are developed that allow one to use this block Gibbs sampler with the same level of confidence that one would have using classical (iid) Monte Carlo. Indeed, a regenerative simulation method is developed that yields simple, asymptotically valid standard errors for the ergodic averages that are used to estimate intractable posterior expectations. These standard errors can be used to choose an appropriate (Markov chain) Monte Carlo sample size. The regenerative method rests on the assumption that the underlying Markov chain converges to its stationary distribution at a geometric rate. Another contribution of this article is a result showing that, unless the dataset is extremely small and unbalanced, the block Gibbs Markov chain is geometrically ergodic. We illustrate the use of the regenerative method with data from a styrene exposure study. R code for the simulation is posted as an online supplement.|Block Gibbs Sampling for Bayesian Random Effects Models With Improper Priors: Convergence and Regeneration|http://www.jstor.org/stable/25651282|25651282|2009-12-01|2009|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Computer Science', 'Statistics']
"The performance of Bayes estimates are studied, under an assumption of conditional exchangeability. More exactly, for each subject in a data set, let ξ be a vector of binary covariates and let η be a binary response variable, with P{η = 1∣ ξ} = f(ξ). Here, f is an unknown function to be estimated from the data; the subjects are independent, and satisfy a natural ""balance"" condition. Define a prior distribution on f as ∑kw kπk/∑kw k, where πk is uniform on the set of f which only depend on the first k covariates and $w_k &gt; 0$ for infinitely many k. Bayes estimates are consistent at all f if wk decreases rapidly as k increase. Otherwise, the estimates are inconsistent at $f \equiv 1/2$."|Nonparametric Binary Regression: A Bayesian Approach|http://www.jstor.org/stable/2242332|2242332|1993-12-01|1993|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
We propose algorithms based on random draws from predictive distributions of unknown quantities (missing values, for instance). This procedure can either be iterative, which is a special variation of the Gibbs sampler, or be sequential, which is a variation of sequential imputation. In the latter case one can update the posterior distribution with new observations easily. The methods proposed have intuitive statistical implications and can be generalized to accommodate other Bayesian-like procedures. We display some applications of the method in connection with the Bayesian bootstrap, classification, hierarchical models and selection of variables. In particular, as an application of the method, we present a unified treatment of switching regression models driven by a general binary process, and we develop a Bayesian testing procedure. Some simulations and a real example are used to illustrate the methods proposed.|Predictive Updating Methods with Application to Bayesian Classification|http://www.jstor.org/stable/2345984|2345984|1996-01-01|1996|['eng']|['Mathematics - Mathematical logic', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
We present a method for estimating event-count models when the data is generated from a persistent time-series process. A Kalman filter is used to estimate a Poisson exponentially weighted moving average (PEWMA) model. The model is compared to extant methods (Poisson regression, negative binomial regression, and ARIMA models). Using Monte Carlo experiments, we demonstrate that the PEWMA provides significant improvements in efficiency. As an example, we present an analysis of Pollins (1996) models of long cycles in international relations.|Dynamic Modeling for Persistent Event-Count Time Series|http://www.jstor.org/stable/2669284|2669284|2000-10-01|2000|['eng']|['Physical sciences - Astronomy']|['Political Science', 'American Studies', 'Social Sciences', 'Area Studies']
Recent guidance from the Food and Drug Administration for the evaluation of new therapies in the treatment of type 2 diabetes (T2DM) calls for a program-wide meta-analysis of cardiovascular (CV) outcomes. In this context, we develop a new Bayesian meta-analysis approach using survival regression models to assess whether the size of a clinical development program is adequate to evaluate a particular safety endpoint. We propose a Bayesian sample size determination methodology for meta-analysis clinical trial design with a focus on controlling the type I error and power. We also propose the partial borrowing power prior to incorporate the historical survival meta data into the statistical design. Various properties of the proposed methodology are examined and an efficient Markov chain Monte Carlo sampling algorithm is developed to sample from the posterior distributions. In addition, we develop a simulation-based algorithm for computing various quantities, such as the power and the type I error in the Bayesian meta-analysis trial design. The proposed methodology is applied to the design of a phase 2/3 development program including a noninferiority clinical trial for CV risk assessment in T2DM studies.|Bayesian Meta-Experimental Design: Evaluating Cardiovascular Risk in New Antidiabetic Therapies to Treat Type 2 Diabetes|http://www.jstor.org/stable/23270460|23270460|2012-06-01|2012|['eng']|['Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
We consider the problem of comparing parametric models using a Bayesian approach. A new method of developing prior distributions for the model parameters is presented, called the expected-posterior prior approach. The idea is to define the priors for all models from a common underlying predictive distribution, in such a way that the resulting priors are amenable to modern Markov chain Monte Carlo computational techniques. The approach has subjective Bayesian and default Bayesian implementations, and overcomes the most significant impediment to Bayesian model selection, that of ensuring that prior distributions for the various models are appropriately compatible.|Expected-Posterior Prior Distributions for Model Selection|http://www.jstor.org/stable/4140597|4140597|2002-09-01|2002|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
This paper uses recent advances in Bayesian estimation methods to exploit fully and efficiently the time-series and cross-sectional empirical restrictions of the Cox, Ingersoll, and Ross model of the term structure. We examine the extent to which the cross-sectional data (five different instruments) provide information about the model. We find that the time-series restrictions of the two-factor model are generally consistent with the data. However, the model's cross-sectional restrictions are not. We show that adding a third factor produces a significant statistical improvement, but causes the average time-series fit to the yields themselves to deteriorate.|Empirical Analysis of the Yield Curve: The Information in the Data Viewed through the Window of Cox, Ingersoll, and Ross|http://www.jstor.org/stable/2697785|2697785|2002-06-01|2002|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Applied mathematics']|['Business & Economics', 'Business', 'Finance']
We propose a simulation-based approach to decision theoretic Bayesian optimal design. The underlying probability model is a population pharmacokinetic model which allows for correlated responses (drug concentrations) and patient-to-patient heterogeneity. We consider the problem of choosing sampling times for the anticancer agent paclitaxel, using criteria related to the total area under the curve, the time above a critical threshold and the sampling cost.|Optimal Sampling Times in Population Pharmacokinetic Studies|http://www.jstor.org/stable/2680875|2680875|2001-01-01|2001|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
The systems that statisticians are asked to assess, such as nuclear weapons, infrastructure networks, supercomputer codes and munitions, have become increasingly complex. It is often costly to conduct full system tests. As such, we present a review of methodology that has been proposed for addressing system reliability with limited full system testing. The first approaches presented in this paper are concerned with the combination of multiple sources of information to assess the reliability of a single component. The second general set of methodology addresses the combination of multiple levels of data to determine system reliability. We then present developments for complex systems beyond traditional series/parallel representations through the use of Bayesian networks and flowgraph models. We also include methodological contributions to resource allocation considerations for system relability assessment. We illustrate each method with applications primarily encountered at Los Alamos National Laboratory.|Advances in Data Combination, Analysis and Collection for System Reliability Assessment|http://www.jstor.org/stable/27645792|27645792|2006-11-01|2006|['eng']|['Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
We construct empirical Bayes intervals for a large number p of means. The existing intervals in the literature assume that variances $\sigma _{i}^{2}$ are either equal or unequal but known. When the variances are unequal and unknown, the suggestion is typically to replace them by unbiased estimators $S_{i}^{2}$ . However, when p is large, there would be advantage in 'borrowing strength' from each other. We derive double-shrinkage intervals for means on the basis of our empirical Bayes estimators that shrink both the means and the variances. Analytical and simulation studies and application to a real data set show that, compared with the t-intervals, our intervals have higher coverage probabilities while yielding shorter lengths on average. The double-shrinkage intervals are on average shorter than the intervals from shrinking the means alone and are always no longer than the intervals from shrinking the variances alone. Also, the intervals are explicitly defined and can be computed immediately.|Empirical Bayes Confidence Intervals Shrinking Both Means and Variances|http://www.jstor.org/stable/20203887|20203887|2009-01-01|2009|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
The combination of graphical models and reference analysis represents a powerful tool for Bayesian inference in highly multivariate settings. It is typically difficult to derive reference priors in complex problems. In this paper we present a suitable mixed parameterisation for a discrete decomposable graphical model and derive the corresponding reference prior.|Reference Priors for Discrete Graphical Models|http://www.jstor.org/stable/20441257|20441257|2006-03-01|2006|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
When experimentation on a real system is expensive, data are often collected by using cheaper, lower fidelity surrogate systems. The paper concerns response surface methods in the context of variable fidelity experimentation. We propose the use of generalized least squares to generate the predictions. We also present perhaps the first optimal designs for variable fidelity experimentation, using an extension of the expected integrated mean-squared error criterion. Numerical tests are used to compare the performance of the method with alternatives and to investigate the robustness to incorporated assumptions. The method is applied to automotive engine valve heat treatment process design in which real world data were mixed with data from two types of computer simulation.|Design and Analysis of Variable Fidelity Experimentation Applied to Engine Valve Heat Treatment Process Design|http://www.jstor.org/stable/3592648|3592648|2005-01-01|2005|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
We propose an adaptive two-stage Bayesian design for finding one or more acceptable dose combinations of two cytotoxic agents used together in a Phase I clinical trial. The method requires that each of the two agents has been studied previously as a single agent, which is almost invariably the case in practice. A parametric model is assumed for the probability of toxicity as a function of the two doses. Informative priors for parameters characterizing the single-agent toxicity probability curves are either elicited from the physician(s) planning the trial or obtained from historical data, and vague priors are assumed for parameters characterizing two-agent interactions. A method for eliciting the single-agent parameter priors is described. The design is applied to a trial of gemcitabine and cyclophosphamide, and a simulation study is presented.|Dose-Finding with Two Agents in Phase I Oncology Trials|http://www.jstor.org/stable/3695424|3695424|2003-09-01|2003|['eng']|['Health sciences - Medical sciences', 'Health sciences - Health and wellness']|['Science & Mathematics', 'Statistics']
We build and estimate a two-sector dynamic stochastic general equilibrium model with two types of inventories: Input inventories facilitate the production of finished goods, output inventories yield utility services. The estimated model replicates the volatility and cyclically of inventory investment and inventory-to-target ratios. Although inventories are an important element of the model's propagation mechanism, shocks to inventory efficiency are not an important source of business cycles. When the model is estimated over two subperiods (pre-and post-1984), changes in the volatility of inventory shocks or in structural parameters associated with inventories play a small role in reducing the volatility of output.|INPUT AND OUTPUT INVENTORIES IN GENERAL EQUILIBRIUM|http://www.jstor.org/stable/41349192|41349192|2011-11-01|2011|['eng']|['Economics - Economic disciplines']|['Business & Economics', 'Business', 'Economics']
"Fusarium Head Blight (FHB), or ""scab,"" is a very destructive disease that affects wheat crops. Recent research has resulted in accurate weather-driven models that estimate the probability of an FHB epidemic based on experiments. However, these predictions ignore two crucial aspects of FHB epidemics: (1) An epidemic is very unlikely to occur unless the plants are flowering, and (2) FHB spreads by its spores, resulting in spatial and temporal dependence in risk. We develop a new approach that combines existing weather-based probabilities with information on flowering dates from survey data, while simultaneously accounting for spatial and temporal dependence. Our model combines two space-time processes, one associated with pure weather-based FHB risks and the other associated with flowering date probabilities. To allow for scalability, we model spatiotemporal dependence via a process convolutions approach. Our sample-based approach produces a realistic assessment of areas that are persistently at high risk (where the probability of an epidemic is elevated for extended time periods), along with associated estimates of uncertainty. We conclude with the application of our approach to a case study from North Dakota."|Estimating the Risk of a Crop Epidemic From Coincident Spatio-temporal Processes|http://www.jstor.org/stable/20778459|20778459|2010-06-01|2010|['eng']|['Applied sciences - Engineering', 'Biological sciences - Biology']|['Science & Mathematics', 'Agriculture', 'Statistics']
In this article, a Bayesian model for age-specific nest survival rates is presented to handle the irregular visit case. Both informative priors and noninformative priors are investigated. The reference prior under this model is derived, and, therefore, the hyperparameter specification problem is solved to some extent. The Bayesian method provides a more accurate estimate of the total survival rate than the standard Mayfield method, if the age-specific hazard rates are not constant. The Bayesian method also lets the biologist look for high- and low-survival rates during the whole nesting period. In practice, it is common for data of several types to be collected in a single study. That is, some nests may be aged, others are not. Some nests are visited regularly; others are visited irregularly. The Bayesian method accommodates any mix of these sampling techniques by assuming that the aging and visiting activities have no effect on the survival rate. The methods are illustrated by an analysis of the Missouri northern bobwhite data set.|Bayesian Modeling of Age-Specific Survival in Bird Nesting Studies under Irregular Visits|http://www.jstor.org/stable/3695336|3695336|2003-12-01|2003|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
This paper empirically investigates whether the theoretical conditions for government expenditure expansions to be effective, hold for the data. We ask whether the necessary conditions for fiscal effectiveness are relevant on average, and in special circumstances that capture features of the recent crisis. Fiscal policy can be an effective countercyclical tool if monetary policy accommodates the fiscal expansion, if expectations about future output growth and inflation are constant, and if structural relationships are invariant to the policy change. Recent expansions are unlikely to produce large output multipliers or have important debt or inflation effects. Credible deficit and debt reduction schemes can produce sizeable output multipliers.|Fiscal policy, pricing frictions and monetary accommodation|http://www.jstor.org/stable/41262005|41262005|2011-10-01|2011|['eng']|['Business - Business operations', 'Economics - Economic disciplines']|['Business', 'Business & Economics Collection', 'Economics']
"We describe studies in molecular profiling and biological pathway analysis that use sparse latent factor and regression models for microarray gene expression data. We discuss breast cancer applications and key aspects of the modeling and computational methodology. Our case studies aim to investigate and characterize heterogeneity of structure related to specific oncogenic pathways, as well as links between aggregate patterns in gene expression profiles and clinical biomarkers. Based on the metaphor of statistically derived ""factors"" as representing biological ""subpathway"" structure, we explore the decomposition of fitted sparse factor models into pathway subcomponents and investigate how these components overlay multiple aspects of known biological activity. Our methodology is based on sparsity modeling of multivariate regression, ANOVA, and latent factor models, as well as a class of models that combines all components. Hierarchical sparsity priors address questions of dimension reduction and multiple comparisons, as well as scalability of the methodology. The models include practically non-Gaussian/nonparametric components for latent structure, underlying often quite complex non-Gaussianity in multivariate expression patterns. Model search and fitting are addressed through stochastic simulation and evolutionary stochastic search methods that are exemplified in the oncogenic pathway studies. Supplementary supporting material provides more details of the applications, as well as examples of the use of freely available software tools for implementing the methodology."|High-Dimensional Sparse Factor Modeling: Applications in Gene Expression Genomics|http://www.jstor.org/stable/27640194|27640194|2008-12-01|2008|['eng']|['Applied sciences - Engineering', 'Biological sciences - Biology']|['Science & Mathematics', 'Statistics']
Grouped survival data with possible interval censoring arise in a variety of settings. This paper presents nonparametric Bayes methods for the analysis of such data. The random cumulative hazard, common to every subject, is assumed to be a realization of a Lévy process. A time-discrete beta process, introduced by Hjort, is considered for modeling the prior process. A sampling-based Monte Carlo algorithm is used to find posterior estimates of several quantities of interest. The methodology presented here is used to check further modeling assumptions. Also, the methodology developed in this paper is illustrated with data for the times to cosmetic deterioration of breast-cancer patients. An extension of the methodology is presented to deal with two interval-censored times in tandem data (as with some AIDS incubation data). /// Des données de survie groupées, avec possiblement censuration d'intervalle, apparaissent dans une variété de situations. Cet article présente des méthodes bayesiennes non-paramétriques pour l'analyse de ce type de données. Le risque cumulatif aléatoire, commun à chacun des sujets, est supposé être la réalisation d'un processus de Lévy. Un processus bêta à temps discret, introduit par Hjort, a été considéré pour modéliser le processus a priori. Un échantillonnage utilisant un algorithme de Monte Carlo permet de trouver des estimations a posteriori de plusieurs quantités d'intérêt. La méthodologie présentée ici est utilisée pour vérifier d'autres hypothèses au sujet dela modélisation. Egalement, la méthodologie développée dans cet article est illustrée à l'aide de données relatives au temps de détérioration cosmétique des patientes atteintes du cancer du sein. Une généralisation de la méthodologie est présentée, afin de traiter deux temps censurés par intervalle dans les données en tandem (telles que retrouvées avec certaines données d'incubation du SIDA).|Time-Discrete Beta-Process Model for Interval-Censored Survival Data|http://www.jstor.org/stable/3315340|3315340|1997-12-01|1997|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
"Networks are a natural and effective tool to study relational data, in which observations are collected on pairs of units. The units are represented by nodes and their relations by edges. In biology, for example, proteins and their interactions may be the nodes and the edges of the network; in social science, the nodes and edges may be people and interpersonal relations. In this article we address the question of clustering vertices in networks as a way to uncover homogeneity patterns in data that enjoy a network representation. We use a mixture model for random graphs and propose a reversible jump Markov chain Monte Carlo algorithm to infer its parameters. Applications of the algorithm to one simulated dataset and three real datasets, which describe friendships among members of a university karate club, social interactions of dolphins, and gap junctions in the ""C. Elegans"", are given."|Vertex Clustering in Random Graphs via Reversible Jump Markov Chain Monte Carlo|http://www.jstor.org/stable/27594313|27594313|2008-06-01|2008|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Science & Mathematics', 'Computer Science', 'Statistics']
The identification of factors that increase the chances of a certain disease is one of the classical and central issues in epidemiology. In this context, a typical measure of the association between a disease and risk factor is the odds ratio. We deal with design problems that arise for Bayesian inference on the odds ratio in the analysis of case-control studies. We consider sample size determination and allocation criteria for both interval estimation and hypothesis testing. These criteria are then employed to determine the sample size and proportions of units to be assigned to cases and controls for planning a study on the association between the incidence of a non-Hodgkin's lymphoma and exposition to pesticides by eliciting prior information from a previous study.|Optimal Predictive Sample Size for Case-Control Studies|http://www.jstor.org/stable/3592563|3592563|2004-01-01|2004|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Monetary policy, the yield curve and the private sector behaviour of the US economy are modelled as a time-varying structural vector autoregression. The monetary policy shocks of the early 1980s explain a large portion of the persistence of inflation and the level of the term structure. Changes in inflation expectations implied by the yield curve account for the persistence of the federal funds rate. Failures of the expectations hypothesis are rare, and coincided with the credibility building of Paul Volcker's Fed tenure at the beginning of the 1980s and the sequence of consecutive policy rate cuts around the time of the early 1990s recession.|Time-Varying Yield Curve Dynamics and Monetary Policy|http://www.jstor.org/stable/25608771|25608771|2009-09-01|2009|['eng']|['Physical sciences - Astronomy']|['Business & Economics', 'Business', 'Economics']
Empirical Bayes approaches have often been applied to the problem of estimating small-area parameters. As a compromise between synthetic and direct survey estimators, an estimator based on an empirical Bayes procedure is not subject to the large bias that is sometimes associated with a synthetic estimator, nor is it as variable as a direct survey estimator. Although the point estimates perform very well, naïve empirical Bayes confidence intervals tend to be too short to attain the desired coverage probability, since they fail to incorporate the uncertainty which results from having to estimate the prior distribution. Several alternative methodologies for interval estimation which correct for the deficiencies associated with the naïve approach have been suggested. Laird and Louis (1987) proposed three types of bootstrap for correcting naïve empirical Bayes confidence intervals. Calling the methodology of Laird and Louis (1987) an unconditional bias-corrected naïve approach, Carlin and Gelfand (1991) suggested a modification to the Type III parametric bootstrap which corrects for bias in the naïve intervals by conditioning on the data. Here we empirically evaluate the Type II and Type III bootstrap proposed by Laird and Louis, as well as the modification suggested by Carlin and Gelfand (1991), with the objective of examining coverage properties of empirical Bayes confidence intervals for small-area proportions. /// On a souvent utilisé des approches empiriques Bayesiennes pour résoudre le problème d'estimer des paramètres d'aire petite. En tant que compromis entre les estimateurs d'enquẽte synthétique et direct, un estimateur fondé sur une procédure empirique Bayesienne n'est ni sujet au biais important souvent associé aux estimateurs synthétiques, ni aussi variable qu'un estimateur d'enquẽte direct. Bien que les estimateurs ponctuels aient une très bonne performance, les intervalles de confiance naïfs empiriques de Bayes tendent à être trop courts pour atteindre la probabilité de couverture désirée, puisqu'ils n'incorporent pas l'incertitude créée par la nécessité d'estimer la distribution a priori. Nous suggérons différentes méthodologies d'estimation d'intervalle corrigeant les déficiences associées à l'approche naïve. Laird et Louis (1987) ont proposé trois types de bootstrap pour corriger les intervalles de confiance naïfs empiriques de Bayes. Ayant dénommé l'approche de Laird et Louis (1987) une approche inconditionnelle corrigeant le biais, Carlin et Gelfand (1991) ont suggéré une modification au bootstrap paramétrique de type III corrigeant le biais dans les intervalles naïfs en conditionnant les données. Ici, nous évaluons empiriquement les bootstrap de type II et III proposés par Laird et Louis (1987), ainsi que la modification suggérée par Carlin et Gelfand (1991), dans le but d'examiner les propriétés de couverture des intervalles de confiance empiriques pour des proportions d'aire petite.|Bootstrap Adjustments for Empirical Bayes Interval Estimates of Small-Area Proportions|http://www.jstor.org/stable/3315358|3315358|1997-03-01|1997|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
The original version of Bayesian reconstruction, which is a method for estimating age-specific fertility, mortality, migration and population counts of the recent past with uncertainty, produced estimates for female-only populations. Here we show how two-sex populations can be similarly reconstructed and probabilistic estimates of various sex ratio quantities obtained. We demonstrate the method by reconstructing the populations of India from 1971 to 2001, Thailand from 1960 to 2000 and Laos from 1985 to 2005. We found evidence that, in India, the sex ratio at birth exceeded its conventional upper limit of 1.06, and, further, increased over the period of study, with posterior probability above 0.9. In addition, almost uniquely, we found evidence that life expectancy at birth, e₀, was lower for females than for males in India (posterior probability for 1971-1976 equal to 0.79), although there was strong evidence for a reversal of the gap through to 2001. In both Thailand and Laos, we found strong evidence for the more usual result that e₀ was greater for females and, in Thailand, that the difference increased over the period of study.|Bayesian reconstruction of two-sex populations by age: estimating sex ratios at birth and sex ratios of mortality|http://www.jstor.org/stable/43965780|43965780|2015-10-01|2015|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
Understanding the relationship between an index of biological community state and habitat is important for policy makers and environmental managers. A common approach to modeling this relationship is to use regression. However, this simple method becomes complicated when the data are clustered and have both within-cluster and between-cluster spatial correlation. This article proposes a Bayesian hierarchical model that incorporates both types of spatial correlation. This model yields both an understanding of the within-cluster relationships as well as an overall relationship between these variables. We apply this method to evaluate the relationship between the index of biotic integrity (a common measure of fish condition) and the qualitative habitat evaluation index (a common measure of habitat quality). This method allows us to show that there is a relationship between the biological community state and habitat and that this relationship varies across river basins, while accounting for the within- and between-spatial correlations.|Evaluating the Relationship between Ecological and Habitat Conditions Using Hierarchical Models|http://www.jstor.org/stable/27595550|27595550|2005-06-01|2005|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Agriculture', 'Statistics']
In this paper, we propose a class of informative prior distributions for Cox's proportional hazards model. A novel construction of the prior is developed based on the notion of the availability of historical data. In many situations, especially in clinical trials, the investigator has historical data from past studies which are similar to the current study. We take a semi-parametric approach in that a non-parametric prior is specified for the hazard rate and a fully parametric prior is specified for the regression coefficients. The prior specifications focus on the observables in that the elicitation is based on a prior prediction $y_{0}$ for the response vector and a quantity $a_{0}$ quantifying the uncertainty in $y_{0}$. Then, $y_{0}$ and $a_{0}$ are used to specify a prior for the regression coefficients in a semi-automatic fashion. One of the main applications of our proposed priors is for model selection. Efficient computational methods are proposed for sampling from the posterior distribution and computing posterior model probabilities. A real data set is used to demonstrate our methodology.|Prior Distributions and Bayesian Computation for Proportional Hazards Models|http://www.jstor.org/stable/25053022|25053022|1998-04-01|1998|['eng']|['Applied sciences - Engineering', 'Mathematics - Mathematical logic']|['Science and Mathematics', 'Statistics']
Ecology and genetics are both of general interest to evolutionary biologists as they can influence the phenotypic and genetic response to selection. The stick insects Timema podura and Timema cristinae exhibit a green/melanistic body color polymorphism that is subject to different ecologically based selective regimes in the two species. Here, we describe aspects of the genetics of this color polymorphism in T. podura, and compare this to previous results in T. cristinae. We first show that similar color phenotypes of the two species cluster in phenotypic space. We then use genome-wide association mapping to show that in both species, color is controlled by few loci, dominance relationships between color alleles are the same, and SNPs associated with color phenotypes colocalize to the same linkage group. Regions within this linkage group that harbor genetic variants associated with color exhibit elevated linkage disequilibrium relative to genome wide expectations, but more strongly so in T. cristinae. We use these results to discuss predictions regarding how the genetics of color could influence levels of phenotypic and genetic variation that segregate within and between populations of T. podura and T. cristinae, drawing parallels with other organisms.|Color phenotypes are under similar genetic control in two distantly related species of Timema stick insect|http://www.jstor.org/stable/24761461|24761461|2016-06-01|2016|['eng']|['Biological sciences - Biology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
We describe a comprehensive modeling approach to combining genomic and clinical data for personalized prediction in disease outcome studies. This integrated clinicogenomic modeling framework is based on statistical classification tree models that evaluate the contributions of multiple forms of data, both clinical and genomic, to define interactions of multiple risk factors that associate with the clinical outcome and derive predictions customized to the individual patient level. Gene expression data from DNA microarrays is represented by multiple, summary measures that we term metagenes; each metagene characterizes the dominant common expression pattern within a cluster of genes. A case study of primary breast cancer recurrence demonstrates that models using multiple metagenes combined with traditional clinical risk factors improve prediction accuracy at the individual patient level, delivering predictions more accurate than those made by using a single genomic predictor or clinical data alone. The analysis also highlights issues of communicating uncertainty in prediction and identifies combinations of clinical and genomic risk factors playing predictive roles. Implicated metagenes identify gene subsets with the potential to aid biological interpretation. This framework will extend to incorporate any form of data, including emerging forms of genomic data, and provides a platform for development of models for personalized prognosis.|Integrated Modeling of Clinical and Gene Expression Information for Personalized Prediction of Disease Outcomes|http://www.jstor.org/stable/3372204|3372204|2004-06-01|2004|['eng']|['Applied sciences - Engineering', 'Biological sciences - Biology']|['Science & Mathematics', 'Biological Sciences', 'General Science']
Background: During the past 2 decades, the observed incidence of in situ and early-stage invasive breast cancer has increased substantially as a result of increased use of mammography. Geographic variability in the increase in breast cancer incidence has been observed among large areas. Examining the variability among small areas in the incidence over time will facilitate appropriate geographic allocation of resources aimed at increasing screening. Methods: We examined county-specific increases in breast cancer incidence over time, specifically the variability and spatial correlation in the increase in breast cancer incidence. The analyses were based on county-level data (1973-1997) from the Iowa Surveillance, Epidemiology, and End Results program. A spatiotemporal hierarchical Bayesian model was used to examine variability in county-specific rates (intercepts, slopes, and spatial correlations) among white women at least 40 years of age. Results: Posterior values indicate there was little variability among counties in the change in breast cancer incidence over time (slope) but substantial variation among intercepts. There was considerable spatial correlation among the county-specific intercepts but a lack of a spatial correlation among the county-specific slopes. There was no correlation between the county-specific intercept and slope. Conclusions: Breast cancer incidence increased over time, but county-specific rates increased independently relative to their neighboring counties or their initial rate.|Small-Area Incidence Trends in Breast Cancer|http://www.jstor.org/stable/20485895|20485895|2004-05-01|2004|['eng']|['Physical sciences - Astronomy']|['Health Sciences', 'Medicine and Allied Health']
We introduce an approach for semiparametric inference in dynamic binary choice models that does not impose distributional assumptions on the state variables unobserved by the econometrician. The proposed framework combines Bayesian inference with partial identification results. The method is applicable to models with finite space of observed states. We demonstrate the method on Rust's model of bus engine replacement. The estimation experiments show that the parametric assumptions about the distribution of the unobserved states can have a considerable effect on the estimates of per-period payoffs. At the same time, the effect of these assumptions on counterfactual conditional choice probabilities can be small for most of the observed states.|Semiparametric Inference in Dynamic Binary Choice Models|http://www.jstor.org/stable/43551625|43551625|2014-07-01|2014|['eng']|['Mathematics - Mathematical logic']|['Business & Economics', 'Business', 'Economics']
ABSTRACT A Monte Carlo framework is adopted for propagating uncertainty in dynamically downscaled seasonal forecasts of area-averaged daily precipitation to associated streamflow response calculations. Daily precipitation is modeled as a mixture of two stochastic processes: a binary occurrence process and a continuous intensity process, both exhibiting serial correlation. The parameters of these processes (e.g., the proportion of wet days and the average wet-day precipitation intensity in a month) are derived from the forecast record. Parameter uncertainty is characterized via an empirical Bayesian model, whereby such parameters are modeled as random with a specific joint probability distribution. The hyperparameters specifying this probability distribution are derived from historical precipitation records at the study basin. Simulated parameter values are then generated using the Bayesian model, leading to alternative synthetic daily precipitation records simulated via the stochastic precipitation model. The set of such synthetic precipitation records is finally input to a physically based deterministic hydrologic model for propagating uncertainty in forecasted precipitation to hydrologic impact assessment studies. The stochastic simulation approach is applied for generating an ensemble (set) of synthetic area-averaged daily precipitation records at the Hopland basin in the northern California Coast Range for the winter months (December through February: DJF) of 1997/98. The parameters of the stochastic precipitation model are derived from a seasonal precipitation forecast based on the Regional Climate System Model (RCSM), available at a 36-km² grid spacing. The large-scale forcing input to RCSM for dynamical downscaling was a seasonal prediction of the University of California, Los Angeles, Atmospheric General Circulation Model. A semidistributed deterministic hydrologic model (“TOPMODEL”) is then used for calculating the streamflow response for each member of the area-averaged precipitation ensemble set. Uncertainty in the parameters of the stochastic precipitation model is finally propagated to associated streamflow response, by considering parameter values derived from historical (DJF 1958–92) area-averaged precipitation records at Hopland.|Uncertainty Propagation of Regional Climate Model Precipitation Forecasts to Hydrologic Impact Assessment|http://www.jstor.org/stable/24909333|24909333|2001-04-01|2001|['eng']|['Physical sciences - Chemistry', 'Biological sciences - Biochemistry']|['Science & Mathematics', 'Environmental Science']
Prior specification is an essential component of parameter estimation and model comparison in Approximate Bayesian computation (ABC). Oaks et al. present a simulation-based power analysis of msBayes and conclude that msBayes has low power to detect genuinely random divergence times across taxa, and suggest the cause is Lindley's paradox. Although the predictions are similar, we show that their findings are more fundamentally explained by insufficient prior sampling that arises with poorly chosen wide priors that critically undersample nonsimultaneous divergence histories of high likelihood. In a reanalysis of their data on Philippine Island vertebrates, we show how this problem can be circumvented by expanding upon a previously developed procedure that accommodates uncertainty in prior selection using Bayesian model averaging. When these procedures are used, msBayes supports recent divergences without support for synchronous divergence in the Oaks et al. data and we further present a simulation analysis that demonstrates that msBayes can have high power to detect asynchronous divergence under narrower priors for divergence time. Our findings highlight the need for exploration of plausible parameter space and prior sampling efficiency for ABC samplers in high dimensions. We discus potential improvements to msBayes and conclude that when used appropriately with model averaging, msBayes remains an effective and powerful tool.|RECOMMENDATIONS FOR USING MSBAYES TO INCORPORATE UNCERTAINTY IN SELECTING AN ABC MODEL PRIOR: A RESPONSE TO OAKS ET AL.|http://www.jstor.org/stable/24032865|24032865|2014-01-01|2014|['eng']|['Applied sciences - Engineering']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
Multivariate receptor modeling is used to estimate profiles and contributions of pollution sources from concentrations of pollutants such as particulate matter in the air. The majority of previous approaches to multivariate receptor modeling assume pollution source profiles are constant through time. In an effort to relax this assumption, this article uses the Dirichlet distribution in a dynamic linear receptor model for pollution source profiles. The receptor model developed herein is evaluated using simulated datasets and then applied to a physical dataset of chemical species concentrations measured at the U.S. Environmental Protection Agency's St. Louis-Midwest supersite. Supplemental materials to this articles are available online.|Incorporating Time-Dependent Source Profiles Using the Dirichlet Distribution in Multivariate Receptor Models|http://www.jstor.org/stable/40586681|40586681|2010-02-01|2010|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
In this paper, we present a rapid Bayesian variable selection technique which can be used when the number of variables is much greater than the number of samples. The method can handle tens of thousands of variables, such as might be measured using biological array technologies. A general formulation is first given, followed by specific details for the class of generalised linear models.|A Bayesian Approach to Variable Selection When the Number of Variables Is Very Large|http://www.jstor.org/stable/4356182|4356182|2003-01-01|2003|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
Automobile insurance premiums in Massachusetts are determined by a Bayesian credibility scheme utilizing the experience of drivers classified as to three driver types and 360 places of residence.|The 1982 Massachusetts Automobile Insurance Classification Scheme|http://www.jstor.org/stable/2987593|2987593|1983-03-01|1983|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
In many practical situations, it is desirable to restrict the flexibility of nonparametric estimation to accommodate a presumed monotonic relationship between a covariate and the response variable. We follow a Bayesian approach using penalized B-splines and incorporate the assumption of monotonicity in a natural way by an appropriate specification of the respective prior distributions. We illustrate the methodology in an empirical application modeling demand for brands of orange juice and show that imposing monotonicity constraints for own-and cross-item price effects improves the predictive validity of the estimated sales response functions considerably.|Monotonic Regression Based on Bayesian P-Splines: An Application to Estimating Price Response Functions From Store-Level Scanner Data|http://www.jstor.org/stable/27638964|27638964|2008-01-01|2008|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
The computation and selection of constrained regressions may be motivated by prior information and, if so, a regression selection strategy reveals the implicit prior. The selection strategies of principal component regression, stepwise regression, and imposing equality constraints are connected with prior densities which are uniform on spheres, hyperbolas, and cones, respectively. Omitting variables in a predetermined order reveals lexicographic priors.|Regression Selection Strategies and Revealed Priors|http://www.jstor.org/stable/2286604|2286604|1978-09-01|1978|['eng']|['Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
The analysis of spatial data by means of Markov random fields usually is based on strict stationarity assumptions. Although these assumptions rarely hold, they are necessary for standard estimation methods to work. The assumptions required for Gaussian spatial data are mean and covariance stationarity. Whereas simple techniques are available to deal with violations of mean stationarity, the same is not true for covariance stationarity. To handle mean non-stationarity as well as covariance non-stationarity, we propose modelling by spatially varying coefficients. This approach not only yields more appropriate models for non-stationary data but also can be used to detect violations of the stationarity assumptions. The method is illustrated by use of the well-known wheat yield data.|Non-Stationary Conditional Models for Spatial Data Based on Varying Coefficients|http://www.jstor.org/stable/2681153|2681153|2001-01-01|2001|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
We consider the properties of the highest posterior probability model in a linear regression setting. Under a spike and slab hierarchy we find that although highest posterior model selection is total risk consistent, it possesses hidden undesirable properties. One such property is a marked underfitting in finite samples, a phenomenon well noted for Bayesian information criterion (BIC) related procedures but not often associated with highest posterior model selection. Another concern is the substantial effect the prior has on model selection. We employ a rescaling of the hierarchy and show that the resulting rescaled spike and slab models mitigate the effects of underfitting because of a perfect cancellation of a BIC-like penalty term. Furthermore, by drawing upon an equivalence between the highest posterior model and the median model, we find that the effect of the prior is less influential on model selection, as long as the underlying true model is sparse. Nonsparse settings are, however, problematic. Using the posterior mean for variable selection instead of posterior inclusion probabilities avoids these issues.|An In-Depth Look at Highest Posterior Model Selection|http://www.jstor.org/stable/20142497|20142497|2008-04-01|2008|['eng']|['Mathematics - Mathematical objects']|['Business', 'Business & Economics Collection', 'Economics']
We describe a novel model and algorithm for simultaneously estimating multiple molecular sequence alignments and the phylogenetic trees that relate the sequences. Unlike current techniques that base phylogeny estimates on a single estimate of the alignment, we take alignment uncertainty into account by considering all possible alignments. Furthermore, because the alignment and phylogeny are constructed simultaneously, a guide tree is not needed. This sidesteps the problem in which alignments created by progressive alignment are biased toward the guide tree used to generate them. Joint estimation also allows us to model rate variation between sites when estimating the alignment and to use the evidence in shared insertion/deletions (indels) to group sister taxa in the phylogeny. Our indel model makes use of affine gap penalties and considers indels of multiple letters. We make the simplifying assumption that the indel process is identical on all branches. As a result, the probability of a gap is independent of branch length. We use a Markov chain Monte Carlo (MCMC) method to sample from the posterior of the joint model, estimating the most probable alignment and tree and their support simultaneously. We describe a new MCMC transition kernel that improves our algorithm's mixing efficiency, allowing the MCMC chains to converge even when started from arbitrary alignments. Our software implementation can estimate alignment uncertainty and we describe a method for summarizing this uncertainty in a single plot.|Joint Bayesian Estimation of Alignment and Phylogeny|http://www.jstor.org/stable/20061243|20061243|2005-06-01|2005|['eng']|['Applied sciences - Engineering', 'Biological sciences - Biology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
This article is concerned with the analysis of correlated count data. A class of models is proposed in which the correlation among the counts is represented by correlated latent effects. Special cases of the model are discussed and a tuned and efficient Markov chain Monte Carlo algorithm is developed to estimate the model under both multivariate normal and multivariate-t assumptions on the latent effects. The methods are illustrated with two real data examples of six and sixteen variate correlated counts.|Markov Chain Monte Carlo Analysis of Correlated Count Data|http://www.jstor.org/stable/1392277|1392277|2001-10-01|2001|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Applied mathematics']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
"We develop scalar-on-image regression models when images are registered multi-dimensional manifolds. We propose a fast and scalable Bayes' inferential procedure to estimate the image coefficient. The central idea is the combination of an Ising prior distribution, which controls a latent binary indicator map, and an intrinsic Gaussian Markov random field, which controls the smoothness of the nonzero coefficients. The model is fit using a single-site Gibbs sampler, which allows fitting within minutes for hundreds of subjects with predictor images containing thousands of locations. The code is simple and is provided in the online Appendix (see the ""Supplementary Materials"" section). We apply this method to a neuroimaging study where cognitive outcomes are regressed on measures of white-matter microstructure at every voxel of the corpus callosum for hundreds of subjects."|Smooth Scalar-on-Image Regression via Spatial Bayesian Variable Selection|http://www.jstor.org/stable/43305714|43305714|2014-03-01|2014|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Computer Science', 'Statistics']
The possibility of improving on the usual multivariate normal confidence was first discussed in Stein (1962). Using the ideas of shrinkage, through Bayesian and empirical Bayesian arguments, domination results, both analytic and numerical, have been obtained. Here we trace some of the developments in confidence set estimation.|Shrinkage Confidence Procedures|http://www.jstor.org/stable/23208823|23208823|2012-02-01|2012|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Objective: To examine the secular effects of opportunistic screening for cervical cancer in a rich, developed community where most other such populations have long adopted organised screening. Design, setting, and participants: The analysis was based on 15 140 cases of invasive cervical cancer from 1972 to 2001. The effects of chronological age, time period, and birth cohort were decomposed using both maximum likelihood and Bayesian methods. Results: The overall age adjusted incidence decreased from 24.9 in 1972-74 to 9.5 per 100,000 in 1999-2001, in a log-linear fashion, yielding an average annual reduction of 4.0% (p&lt; 0.001 ) during the 30 year period. There were two second order and thus identifiable changes: (1) around the mid-1920s cohort curve representing an age-period interaction masquerading as a cohort change that denotes the first availability of Pap testing during the 1960s concentrated among women in their 40s; (2) a hook around the calendar years 1982-83 when cervical cytology became a standard screening test for pregnant women. Conclusions: Hong Kong's cervical cancer rates have declined since Pap tests first became available in the 1960s, most probably because of increasing population coverage over time and in successive generations in a haphazard fashion and punctuated by the systematic introduction of routine cytology as part of antenatal care in the 1980s.|Age-period-cohort analysis of cervical cancer incidence in Hong Kong from 1972 to 2001 using maximum likelihood and Bayesian methods|http://www.jstor.org/stable/40794027|40794027|2006-08-01|2006|['eng']|['Health sciences - Medical specialties']|['Health Sciences', 'Medicine and Allied Health']
"Our article analyzes the determinants of local growth control decisions, which are modeled as the result of a political struggle between different groups of voters and organized lobbies. We show that under specific hypotheses, a higher homeownership rate can induce lower levels of controls. Considering residential choices as endogenous to growth control policies, the local decisions to control growth become strategically interdependent. Assuming imperfect mobility, we show that a spatial econometric specification can be directly derived from our theoretical model. Our empirical analysis concerning the determinants of the ""taxe locale d'équipement"", a French local development tax, is thus naturally based on spatial econometrics. Its results confirm the major predictions of our model."|Modeling local growth control decisions in a multi-city case: Do spatial interactions and lobbying efforts matter?|http://www.jstor.org/stable/23326395|23326395|2013-01-01|2013|['eng']|['Economics - Economic disciplines']|['Business', 'Business & Economics Collection', 'Economics', 'Political Science', 'Social Sciences']
Markov chain Monte Carlo methods for Bayesian computation have until recently been restricted to problems where the joint distribution of all variables has a density with respect to some fixed standard underlying measure. They have therefore not been available for application to Bayesian model determination, where the dimensionality of the parameter vector is typically not fixed. This paper proposes a new framework for the construction of reversible Markov chain samplers that jump between parameter subspaces of differing dimensionality, which is flexible and entirely constructive. It should therefore have wide applicability in model determination problems. The methodology is illustrated with applications to multiple change-point analysis in one and two dimensions, and to a Bayesian comparison of binomial experiments.|Reversible Jump Markov Chain Monte Carlo Computation and Bayesian Model Determination|http://www.jstor.org/stable/2337340|2337340|1995-12-01|1995|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Symmetric binary matrices representing relations are collected in many areas. Our focus is on dynamically evolving binary relational matrices, with interest being on inference on the relationship structure and prediction. We propose a nonparametric Bayesian dynamic model, which reduces dimensionality in characterizing the binary matrix through a lower-dimensional latent space representation, with the latent coordinates evolving in continuous time via Gaussian processes. By using a logistic mapping function from the link probability matrix space to the latent relational space, we obtain a flexible and computationally tractable formulation. Employing Pólya-gamma data augmentation, an efficient Gibbs sampler is developed for posterior computation, with the dimension of the latent space automatically inferred. We provide theoretical results on flexibility of the model, and illustrate its performance via simulation experiments. We also consider an application to co-movements in world financial markets.|Nonparametric Bayes dynamic modelling of relational data|http://www.jstor.org/stable/43304694|43304694|2014-12-01|2014|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Mathematical logic', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
"Preterm birth, defined as delivery before 37 completed weeks' gestation, is a leading cause of infant morbidity and morbidity. Identifying factors related to preterm delivery is an important goal of public health professionals who wish to identify etiologic pathways to target for prevention. Validation studies are often conducted in nutritional epidemiology in order to study measurement error in instruments that are generally less invasive or less expensive than ""gold standard"" instruments. Data from such studies are then used in adjusting estimates based on the full study sample. However, measurement error in nutritional epidemiology has recently been shown to be complicated by correlated error structures in the study-wide and validation instruments. Investigators of a study of preterm birth and dietary intake designed a validation study to assess measurement error in a food frequency questionnaire (FFQ) administered during pregnancy and with the secondary goal of assessing whether a single administration of the FFQ could be used to describe intake over the relatively short pregnancy period, in which energy intake typically increases. Here, we describe a likelihood-based method via Markov chain Monte Carlo to estimate the regression coefficients in a generalized linear model relating preterm birth to covariates, where one of the covariates is measured with error and the multivariate measurement error model has correlated errors among contemporaneous instruments (i.e., FFQs, 24-hour recalls, and biomarkers). Because of constraints on the covariance parameters in our likelihood, identifiability for all the variance and covariance parameters is not guaranteed, and, therefore, we derive the necessary and sufficient conditions to identify the variance and covariance parameters under our measurement error model and assumptions. We investigate the sensitivity of our likelihood-based model to distributional assumptions placed on the true folate intake by employing semiparametric Bayesian methods through the mixture of Dirichlet process priors framework. We exemplify our methods in a recent prospective cohort study of risk factors for preterm birth. We use long-term folate as our error-prone predictor of interest, the FFQ and 24-hour recall as two biased instruments, and the serum folate biomarker as the unbiased instrument. We found that folate intake, as measured by the FFQ, led to a conservative estimate of the estimated odds ratio of preterm birth (.76) when compared to the odds ratio estimate from our likelihood-based approach, which adjusts for the measurement error (.63). We found that our parametric model led to similar conclusions to the semiparametric Bayesian model."|Structured Measurement Error in Nutritional Epidemiology: Applications in the Pregnancy, Infection, and Nutrition (PIN) Study|http://www.jstor.org/stable/27639930|27639930|2007-09-01|2007|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Motivated by the increasing use of and rapid changes in array technologies, we consider the prediction problem of fitting a linear regression relating a continuous outcome Y to a large number of covariates X, for example, measurements from current, state-of-the-art technology. For most of the samples, only the outcome Y and surrogate covariates, W, are available. These surrogates may be data from prior studies using older technologies. Owing to the dimension of the problem and the large fraction of missing information, a critical issue is appropriate shrinkage of model parameters for an optimal bias-variance trade-off. We discuss a variety of fully Bayesian and Empirical Bayes algorithms which account for uncertainty in the missing data and adaptively shrink parameter estimates for superior prediction. These methods are evaluated via a comprehensive simulation study. In addition, we apply our methods to a lung cancer data set, predicting survival time (Y) using qRT-PCR (X) and microarray (W) measurements.|BAYESIAN SHRINKAGE METHODS FOR PARTIALLY OBSERVED DATA WITH MANY PREDICTORS|http://www.jstor.org/stable/23566463|23566463|2013-12-01|2013|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Mathematics', 'Science and Mathematics', 'Statistics']
We consider variable selection in the Cox regression model (Cox, 1975, Biometrika 362, 269-276) with covariates missing at random. We investigate the smoothly clipped absolute deviation penalty and adaptive least absolute shrinkage and selection operator (LASSO) penalty, and propose a unified model selection and estimation procedure. A computationally attractive algorithm is developed, which simultaneously optimizes the penalized likelihood function and penalty parameters. We also optimize a model selection criterion, called the IC Q statistic (Ibrahim, Zhu, and Tang, 2008, Journal of the American Statistical Association 103, 1648-1658), to estimate the penalty parameters and show that it consistently selects all important covariates. Simulations are performed to evaluate the finite sample performance of the penalty estimates. Also, two lung cancer data sets are analyzed to demonstrate the proposed methodology.|<strong>Variable Selection in the Cox Regression Model with Covariates Missing at Random</strong>|http://www.jstor.org/stable/40663156|40663156|2010-03-01|2010|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
This article extends the nonnegative garrote method to a component selection method in a nonparametric additive model in which each univariate function is estimated with P-splines. We also establish the consistency of the procedure. An advantage of P-splines is that the fitted function is represented in a rather small basis of B-splines. A numerical study illustrates the finite-sample performance of the method and includes a comparison with other methods. The nonnegative garrote method with P-splines has the advantage of being computationally fast and performs, with an appropriate parameter selection procedure implemented, overall very well. Real data analysis leads to interesting findings. Supplementary materials for this article (technical proofs, additional numerical results, R code) are available online.|Variable Selection in Additive Models Using P-Splines|http://www.jstor.org/stable/41714926|41714926|2012-11-01|2012|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
This study investigates the empirical evidence on the effects of unanticipated changes in nominal money on real output in 47 countries when viewed through a window (i.e., likelihood function) that assumes the neutrality of anticipated changes. Using a Bayesian predictivist approach, it provides a pedagogical Bayesian analysis of generated regressor models in the face of specification uncertainty involving, among other things, multiple unit roots and trend stationary alternatives.|A Bayesian View of Nominal Money and Real Output through a New Classical Macroeconomic Window|http://www.jstor.org/stable/1391778|1391778|1991-04-01|1991|['eng']|['Physical sciences - Astronomy']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
Whether hierarchical logistic regression can reduce the sample size requirement for estimating optimal cutoff scores in a course placement service where predictive validity is measured by a threshold utility function is explored. Data from courses with varying class size were randomly partitioned into two halves per course. Nonhierarchical and hierarchical analyses were performed on each half. Compared to their nonhierarchical counterparts, hierarchically estimated cutoff scores from different halves were more stable and predicted course outcomes in the other half more accurately. These differences were most pronounced with small samples. Sample size requirements for developing cutoff scores for course placement can be substantially reduced if hierarchical logistic regression is used.|Hierarchical Logistic Regression in Course Placement|http://www.jstor.org/stable/1435317|1435317|2004-10-01|2004|['eng']|['Mathematics - Applied mathematics']|['Education', 'Social Sciences']
The paper introduces general Bayesian inference procedures for the analysis of matched case-control data. The general results hold for a wide class of likelihoods and priors. A real life example is considered to illustrate the methodology. Also, for the logistic regression model, we have shown how Bayesian inference based on the unconditional likelihood can differ substantially from one based on the conditional likelihood.|Bayesian Inference for Matched Case-Control Studies|http://www.jstor.org/stable/25053211|25053211|2002-08-01|2002|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics']|['Science and Mathematics', 'Statistics']
We present a novel nonparametric Bayesian approach based on Levy Adaptive Regression Kernels (LARK) to model spectral data arising from MALDI-TOF (Matrix Assisted Laser Desorption Ionization Time-of-Flight) mass spectrometry. This model-based approach provides identification and quantification of proteins through model parameters that are directly interpretable as the number of proteins, mass and abundance of proteins and peak resolution, while having the ability to adapt to unknown smoothness as in wavelet based methods. Informative prior distributions on resolution are key to distinguishing true peaks from background noise and resolving broad peaks into individual peaks for multiple protein species. Posterior distributions are obtained using a reversible jump Markov chain Monte Carlo algorithm and provide inference about the number of peaks (proteins), their masses and abundance. We show through simulation studies that the procedure has desirable true-positive and false-discovery rates. Finally, we illustrate the method on five example spectra: a blank spectrum, a spectrum with only the matrix of a low-molecular-weight substance used to embed target proteins, a spectrum with known proteins, and a single spectrum and average of ten spectra from an individual lung cancer patient.|BAYESIAN NONPARAMETRIC MODELS FOR PEAK IDENTIFICATION IN MALDI-TOF MASS SPECTROSCOPY1|http://www.jstor.org/stable/23024860|23024860|2011-06-01|2011|['eng']|['Physical sciences - Astronomy']|['Mathematics', 'Science & Mathematics', 'Statistics']
Consider simple right censored survival data with a common unknown hazard rate. The hazard rate is here modelled nonparametrically, as a jump process having a martingale structure with respect to the prior distribution. For an evaluation of posterior probabilities, given the data, sample paths of the hazard rate are generated from the posterior distribution by using a dynamic version of the Gibbs sampler. The algorithm is described in detail. It is also shown how, by slightly modifying the algorithm, the procedure can be altered to correspond to a constrained estimation problem where the hazard rate is known to be increasing (or decreasing). The methods are illustrated by simulation examples.|NONPARAMETRIC BAYESIAN INFERENCE FROM RIGHT CENSORED SURVIVAL DATA, USING THE GIBBS SAMPLER|http://www.jstor.org/stable/24305530|24305530|1994-07-01|1994|['eng']|['Mathematics - Applied mathematics']|['Mathematics', 'Science and Mathematics', 'Statistics']
While studying various features of the posterior distribution of a vector-valued parameter using an MCMC sample, a subsample is often all that is available for analysis. The goal of benchmark estimation is to use the best available information, that is, the full MCMC sample, to improve future estimates made on the basis of the subsample. We discuss a simple approach to do this and provide a theoretical basis for the method. The methodology and benefits of benchmark estimation are illustrated using a well-known example from the literature. We obtain nearly a 90% reduction in MSE with the technique based on a 1-in-10 subsample and show that greater benefits accrue with the thinner subsamples that are often used in practice.|Benchmark Estimation for Markov Chain Monte Carlo Samples|http://www.jstor.org/stable/1391000|1391000|2004-09-01|2004|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Computer Science', 'Statistics']
"In many environmental valuation applications standard sample sizes for choice modelling surveys are impractical to achieve. One can improve data quality using more in-depth surveys administered to fewer respondents. We report on a study using high quality rank-ordered data elicited with the bestworst approach. The resulting ""exploded logit"" choice model, estimated on 64 responses per person, was used to study the willingness to pay for external benefits by visitors for policies which maintain the cultural heritage of alpine grazing commons. We find evidence supporting this approach and reasonable estimates of mean WTP, which appear theoretically valid and policy informative."|Exploring Scale Effects of Best/Worst Rank Ordered Choice Data to Estimate Benefits of Tourism in Alpine Grazing Commons|http://www.jstor.org/stable/41240360|41240360|2011-04-01|2011|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Science & Mathematics', 'Agriculture', 'Business', 'Economics']
This paper presents a new methodological approach for carrying out Bayesian inference about dynamic models for exponential family observations. The approach is simulation-based and involves the use of Markov chain Monte Carlo techniques. A Metropolis-Hastings algorithm is combined with the Gibbs sampler in repeated use of an adjusted version of normal dynamic linear models. Different alternative schemes based on sampling from the system disturbances and state parameters separately and in a block are derived and compared. The approach is fully Bayesian in obtaining posterior samples with state parameters and unknown hyperparameters. Illustrations with real datasets with sparse counts and missing values are presented. Extensions to accommodate more general evolution forms and distributions for observations and disturbances are outlined.|Markov Chain Monte Carlo for Dynamic Generalised Linear Models|http://www.jstor.org/stable/2337321|2337321|1998-03-01|1998|['eng']|['Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
It is common to observe a vector of discrete and/or continuous responses in scientific problems where the objective is to characterize the dependence of each response on explanatory variables and to account for the association between the outcomes. The response vector can comprise repeated observations on one variable, as in longitudinal studies or genetic studies of families, or can include observations for different variables. This paper discusses a class of models for the marginal expectations of each response and for pairwise associations. The marginal models are contrasted with log-linear models. Two generalized estimating equation approaches are compared for parameter estimation. The first focuses on the regression parameters; the second simultaneously estimates the regression and association parameters. The robustness and efficiency of each is discussed. The methods are illustrated with analyses of two data sets from public health research.|Multivariate Regression Analyses for Categorical Data|http://www.jstor.org/stable/2345947|2345947|1992-01-01|1992|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Current phylogenomic data sets highlight the need for species tree methods able to deal with several sources of gene tree/species tree incongruence. At the same time, we need to make most use of all available data. Most species tree methods deal with single processes of phylogenetic discordance, namely, gene duplication and loss, incomplete lineage sorting (ILS) or horizontal gene transfer. In this manuscript, we address the problem of species tree inference from multilocus, genome-wide data sets regardless of the presence of gene duplication and loss and ILS therefore without the need to identify orthologs or to use a single individual per species. We do this by extending the idea of Maximum Likelihood (ML) supertrees to a hierarchical Bayesian model where several sources of gene tree/species tree disagreement can be accounted for in a modular manner. We implemented this model in a computer program called guenomu whose inputs are posterior distributions of unrooted gene tree topologies for multiple gene families, and whose output is the posterior distribution of rooted species tree topologies. We conducted extensive simulations to evaluate the performance of our approach in comparison with other species tree approaches able to deal with more than one leaf from the same species. Our method ranked best under simulated data sets, in spite of ignoring branch lengths, and performed well on empirical data, as well as being fast enough to analyze relatively large data sets. Our Bayesian supertree method was also very successful in obtaining better estimates of gene trees, by reducing the uncertainty in their distributions. In addition, our results show that under complex simulation scenarios, gene tree parsimony is also a competitive approach once we consider its speed, in contrast to more sophisticated models.|A Bayesian Supertree Model for Genome-Wide Species Tree Reconstruction|http://www.jstor.org/stable/44028765|44028765|2016-05-01|2016|['eng']|['Applied sciences - Engineering', 'Biological sciences - Biology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
The paper investigates the link between area-based socio-economic deprivation and the incidence of child pedestrian casualties. The analysis is conducted by using data for small spatial zones within major British cities over the period 2001-2007. Spatial longitudinal generalized linear mixed models, estimated by using frequentist and Bayesian approaches, are used to address issues of confounding, spatial dependence and transmission of deprivation effects across zones (i.e. interference). The results show a consistent strong deprivation effect across model specifications. The incidence of child pedestrian casualties in the most deprived zones is typically greater than 10 times that in the least deprived zones. Modelling interference through a spatially auto-regressive covariate uncovers a substantially larger effect.|Quantifying the effect of area deprivation on child pedestrian casualties by using longitudinal mixed models to adjust for confounding, interference and spatial dependence|http://www.jstor.org/stable/43965365|43965365|2013-10-01|2013|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
Statistical clustering of criminal events can be used by crime analysts to create lists of potential suspects for an unsolved crime, to identify groups of crimes that may have been committed by the same individuals or group of individuals, for offender profiling and for predicting future events. We propose a Bayesian model-based clustering approach for criminal events. Our approach is semisupervised, because the offender is known for a subset of the events, and utilizes spatiotemporal crime locations as well as crime features describing the offender's modus operandi. The hierarchical model naturally handles complex features that are often seen in crime data, including missing data, interval-censored event times and a mix of discrete and continuous variables. In addition, our Bayesian model produces posterior clustering probabilities which allow analysts to act on model output only as warranted. We illustrate the approach by using a large data set of burglaries in 2009-2010 in Baltimore County, Maryland.|Partially supervised spatiotemporal clustering for burglary crime series identification|http://www.jstor.org/stable/43965483|43965483|2015-02-01|2015|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
A two-groups mixed-effects model for the comparison of (normalized) microarray data from two treatment groups is considered. Most competing parametric methods that have appeared in the literature are obtained as special cases or by minor modification of the proposed model. Approximate maximum likelihood fitting is accomplished via a fast and scalable algorithm, which we call LEMMA (Laplace approximated EM Microarray Analysis). The posterior odds of treatment x gene interactions, derived from the model, involve shrinkage estimates of both the interactions and of the gene specific error variances. Genes are classified as being associated with treatment based on the posterior odds and the local false discovery rate (f. d. r.) with a fixed cutoff. Our model-based approach also allows one to declare the non-null status of a gene by controlling the false discovery rate (FDR). It is shown in a detailed simulation study that the approach outperforms wellknown competitors. We also apply the proposed methodology to two previously analyzed microarray examples. Extensions of the proposed method to paired treatments and multiple treatments are also discussed.|Laplace Approximated EM Microarray Analysis: An Empirical Bayes Approach for Comparative Microarray Experiments|http://www.jstor.org/stable/41058954|41058954|2010-08-01|2010|['eng']|['Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
A key question in evaluation of computer models is Does the computer model adequately represent reality? A six-step process for computer model validation is set out in Bayarri et al. [Technometrics 49 (2007) 138-154] (and briefly summarized below), based on comparison of computer model runs with field data of the process being modeled. The methodology is particularly suited to treating the major issues associated with the validation process: quantifying multiple sources of error and uncertainty in computer models; combining multiple sources of information; and being able to adapt to different, but related scenarios. Two complications that frequently arise in practice are the need to deal with highly irregular functional data and the need to acknowledge and incorporate uncertainty in the inputs. We develop methodology to deal with both complications. A key part of the approach utilizes a wavelet representation of the functional data, applies a hierarchical version of the scalar validation methodology to the wavelet coefficients, and transforms back, to ultimately compare computer model output with field output. The generality of the methodology is only limited by the capability of a combination of computational tools and the appropriateness of decompositions of the sort (wavelets) employed here. The methods and analyses we present are illustrated with a test bed dynamic stress analysis for a particular engineering system.|Computer Model Validation with Functional Output|http://www.jstor.org/stable/25464566|25464566|2007-10-01|2007|['eng']|['Applied sciences - Engineering', 'Information science - Coding theory', 'Philosophy - Logic']|['Science & Mathematics', 'Statistics']
A Bayesian semiparametric proportional hazards model is presented to describe the failure behavior of machine tools. The semiparametric setup is introduced using a mixture of Dirichlet processes prior. A Bayesian analysis is performed on real machine tool failure data using the semiparametric setup, and development of optimal replacement strategies are discussed. The results of the semiparametric analysis and the replacement policies are compared with those under a parametric model.|A Bayesian Semiparametric Analysis of the Reliability and Maintenance of Machine Tools|http://www.jstor.org/stable/25046975|25046975|2003-02-01|2003|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
This paper studies the determinants of urban sprawl in France using panel datasets for the four largest metropolitan areas (Paris, Lyon, Marseille, Lille) over the period 1985-98. A measure of urban sprawl is proposed at municipality level. Due to the huge heterogeneity of the panels, it seems difficult to make the fundamental homogeneity assumption underlying pooled models. Thus, random coefficient models under heteroscedasticity of the disturbances are estimated for each metropolitan area using a hierarchical Bayes approach based on the Markov chain Monte Carlo simulation method. It is found that urban sprawl is positively related to the income growth of the tax payers' fiscal households for the period of rapid growth in the late 1980s. At the opposite extreme, the income effects are negative for non-tax payers. During the recession, income effects are significant neither for tax payers' nor for non-tax payers' fiscal households and are significantly positive for tax payers and negative for non-tax payers over the recovery. Finally, on average, the inequality index—difference of average net income between tax payers and exempted fiscal households—has a lower impact on urban sprawl than the income effect.|Determinants of Urban Sprawl in France: An Analysis Using a Hierarchical Bayes Approach on Panel Data|http://www.jstor.org/stable/43081895|43081895|2011-10-01|2011|['eng']|['Mathematics - Applied mathematics']|['Social Sciences', 'Urban Studies']
Selection of significant genes via expression patterns is important in a microarray problem. Owing to small sample size and large number of variables (genes), the selection process can be unstable. This paper considers hierarchical Bayesian gene selection model for survival data. In survival analysis the popular models are usually well suited for data with few covariates and many observations (subjects). In contrast for a typical setting of gene expression data from DNA microarray, we need to consider the case where the number of covariates p exceeds the number of samples n. For a given vector of response values which are times to event (death or censored times) and p gene expressions (covariates), we address the issue of how to reduce the dimension by selecting the significant genes. This approach enables us to estimate the survival curve when n &lt; &lt; p. In our approach, rather than fixing the number of selected genes, we assign a prior distribution to this number. That way it creates additional flexibility by allowing the imposition of constraints, such as bounding the dimension via a prior, which in effect works as a penalty. To implement our methodology, we use a Markov Chain Monte Carlo (MCMC) method. We demonstrate the use of the methodology to diffuse large B-cell lymphoma (DLBCL) complementary DNA (cDNA) data and Breast Carcinomas data.|Bayesian Methods for Variable Selection in Survival Models with Application to DNA Microarray Data|http://www.jstor.org/stable/25053399|25053399|2004-11-01|2004|['eng']|['Applied sciences - Engineering', 'Biological sciences - Biology']|['Science and Mathematics', 'Statistics']
We consider the asymptotic behavior of posterior distributions and Bayes estimators based on observations which are required to be neither independent nor identically distributed. We give general results on the rate of convergence of the posterior measure relative to distances derived from a testing criterion. We then specialize our results to independent, nonidentically distributed observations, Markov processes, stationary Gaussian time series and the white noise model. We apply our general results to several examples of infinite-dimensional statistical models including nonparametric regression with normal errors, binary regression, Poisson regression, an interval censoring model, Whittle estimation of the spectral density of a time series and a nonlinear autoregressive model.|Convergence Rates of Posterior Distributions for Noniid Observations|http://www.jstor.org/stable/25463553|25463553|2007-02-01|2007|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
Adoption of conservation tillage practice in agriculture offers the potential to mitigate greenhouse gas emissions. Studies comparing conservation tillage methods to traditional tillage pair fields under the two management systems and obtain soil core samples from each treatment. Cores are divided into multiple increments, and matching increments from one or more cores are aggregated and analyzed for carbon stock. These data represent not the actual value at a specific depth, but rather the total or average over a depth increment. A semiparametric mixed model is developed for such increment-averaged data. The model uses parametric fixed effects to represent covariate effects, random effects to capture correlation within studies, and an integrated smooth function to describe effects of depth. The depth function is specified as an additive model, estimated with penalized splines using standard mixed model software. Smoothing parameters are automatically selected using restricted maximum likelihood. The methodology is applied to the problem of estimating a change in carbon stock due to a change in tillage practice.|Semiparametric Mixed Models for Increment-Averaged Data with Application to Carbon Sequestration in Agricultural Soils|http://www.jstor.org/stable/27639926|27639926|2007-09-01|2007|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
The authors develop and estimate a model of online buying using clickstream data from a Web site that sells cars. The model predicts online buying by linking the purchase decision to what visitors do and to what they are exposed to while at the site. To overcome the challenges of predicting Internet buying, the authors decompose the purchase process into the completion of sequential nominal user tasks and account for heterogeneity across visitors at the county level. Using a sequence of binary probits, the authors model the visitor's decision of whether to complete each task for the first time, given that the visitor has completed the previous tasks at least once. The results indicate that visitors' browsing experiences and navigational behavior predict task completion for all decision levels. The results also indicate that the number of repeat visits per se is not diagnostic of buying propensity and that a site's offering of sophisticated decision aids does not guarantee increased conversion rates. The authors also compare the predictive performance of the task-completion approach with single-stage benchmark models in a holdout sample. The proposed approach provides superior prediction and better identifies likely buyers, especially early in the task sequence. The authors also discuss implications for Web site managers.|Modeling Purchase Behavior at an E-Commerce Web Site: A Task-Completion Approach|http://www.jstor.org/stable/30162341|30162341|2004-08-01|2004|['eng']|['Information science - Data products', 'Information science - Coding theory']|['Business', 'Business & Economics Collection', 'Marketing & Advertising']
The empirical literature on the transmission of international shocks is based on small-scale VARs. In this paper, we use a large panel of data for 17 industrialized countries to investigate the international transmission mechanism, and revisit the anomalies that arise in the empirical literature. We propose a factor augmented VAR (FAVAR) that extends the model in Bernanke, Boivin, and Eliasz (2005) to the open economy. The main results can be summarized as follows. First, the dynamic effects on the UK economy of an unanticipated fall of short-term interest rates in the rest of the world are: real house price inflation, investment, GDP and consumption growth peak after 1 year, wages peak after 2 years, and CPI and GDP deflator inflation peak during the third year. Second, a positive international supply shock makes the distribution of the components of the UK consumption deflator negatively skewed. Third, in response to a domestic monetary shock, we find little evidence of the exchange rate and liquidity puzzles and little evidence of the forward discount and price anomalies.|The Transmission of International Shocks: A Factor-Augmented VAR Approach|http://www.jstor.org/stable/25483527|25483527|2009-02-01|2009|['eng']|['Mathematics - Mathematical logic']|['Business & Economics', 'Business', 'Economics', 'Finance']
Job displacement entails a substantial wage reduction for most displaced workers. We show that the mean reduction, the tenure profile of reductions, and the experience profile of reductions all vary substantially across industries. We then link this interindustry variation to analogous variation in firm size, unionization, wage levels, and the incidence of employer-provided training. While these industry characteristics explain some of the interindustry variation in mean wage reductions, they do not explain variation in the tenure or experience profiles of wage reductions.|Interindustry Variation in the Costs of Job Displacement|http://www.jstor.org/stable/2535276|2535276|1994-04-01|1994|['eng']|['Information science - Information analysis', 'Information science - Informetrics', 'Mathematics - Applied mathematics', 'Business - Business administration', 'Applied sciences - Research methods']|['Business & Economics', 'Business', 'Economics', 'Labor & Employment Relations']
In this paper we propose a Bayesian methodology for predicting match outcomes. The methodology is illustrated on the 2006 Soccer World Cup. As prior information, we make use of the specialists' opinions and the FIFA ratings. The method is applied to calculate the win, draw and loss probabilities at each match and also to simulate the whole competition in order to estimate classification probabilities in group stage and winning tournament chances for each team. The prediction capability of the proposed methodology is determined by the DeFinetti measure and by the percentage of correct forecasts.|A Bayesian approach for predicting match outcomes: The 2006 (Association) Football World Cup|http://www.jstor.org/stable/40802329|40802329|2010-10-01|2010|['eng']|['Physical sciences - Astronomy']|['Business', 'Business & Economics Collection']
This paper presents a unified treatment of Gaussian process models that extends to data from the exponential dispersion family and to survival data. Our specific interest is in the analysis of data sets with predictors that have an a priori unknown form of possibly nonlinear associations to the response. The modeling approach we describe incorporates Gaussian processes in a generalized linear model framework to obtain a class of nonparametric regression models where the covariance matrix depends on the predictors. We consider, in particular, continuous, categorical and count responses. We also look into models that account for survival outcomes. We explore alternative covariance formulations for the Gaussian process prior and demonstrate the flexibility of the construction. Next, we focus on the important problem of selecting variables from the set of possible predictors and describe a general framework that employs mixture priors. We compare alternative MCMC strategies for posterior inference and achieve a computationally efficient and practical approach. We demonstrate performances on simulated and benchmark data sets.|Variable Selection for Nonparametric Gaussian Process Priors: Models and Computational Strategies|http://www.jstor.org/stable/23059160|23059160|2011-02-01|2011|['eng']|['Mathematics - Mathematical logic']|['Science and Mathematics', 'Statistics']
A method for the Bayesian restoration of noisy binary images portraying an object with constant grey level on a background is presented. The restoration, performed by fitting a polygon with any number of sides to the object's outline, is driven by a new probabilistic model for the generation of polygons in a compact subset of R2, which is used as a prior distribution for the polygon. Some measurability issues raised by the correct specification of the model are addressed. The simulation from the prior and the calculation of the a posteriori mean of grey levels are carried out through reversible jump Markov chain Monte Carlo computation, whose implementation and convergence properties are also discussed. One example of restoration of a synthetic image is presented and compared with existing pixel-based methods.|Boundary Detection Through Dynamic Polygons|http://www.jstor.org/stable/2985934|2985934|1998-01-01|1998|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
We focus on developing nonparametric Bayes methods for collections of dependent random functions, allowing individual curves to vary flexibly while adaptively borrowing information. A prior is proposed, which is expressed as a hierarchical mixture of weighted kernels placed at unknown locations. The induced prior for any individual function is shown to fall within a reproducing kernel Hilbert space. We allow flexible borrowing of information through the use of a hierarchical Dirichlet process prior for the random locations, along with a functional Dirichlet process for the weights. Theoretical properties are considered and an efficient MCMC algorithm is developed, relying on stick-breaking truncations. The methods are illustrated using simulation examples and an application to reproductive hormone data.|NONPARAMETRIC BAYES KERNEL-BASED PRIORS FOR FUNCTIONAL DATA ANALYSIS|http://www.jstor.org/stable/24308846|24308846|2009-04-01|2009|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
By explicativity is meant the extent to which one proposition or event explains why another one should be believed. Detailed mathematical and philosophical arguments are given for accepting a specific formula for explicativity that was previously proposed by the author with much less complete discussion. Some implications of the formula are discussed, and it is applied to several problems of statistical estimation and significance testing with intuitively appealing results. The work is intended to be a contribution to both philosophy and statistics.|Explicativity: A Mathematical Theory of Explanation with Statistical Applications|http://www.jstor.org/stable/79232|79232|1977-05-30|1977|['eng']|['Mathematics - Mathematical logic']|['General Science', 'Mathematics', 'Science and Mathematics']
Constructing maps of dry deposition pollution levels is vital for air quality management, and presents statistical problems typical of many environmental and spatial applications. Ideally, such maps would be based on a dense network of monitoring stations, but this does not exist. Instead, there are two main sources of information for dry deposition levels in the United States: one is pollution measurements at a sparse set of about 50 monitoring stations called CASTNet, and the other is the output of the regional scale air quality models, called Models-3. A related problem is the evaluation of these numerical models for air quality applications, which is crucial for control strategy selection. We develop formal methods for combining sources of information with different spatial resolutions and for the evaluation of numerical models. We specify a simple model for both the Models-3 output and the CASTNet observations in terms of the unobserved ground truth, and we estimate the model in a Bayesian way. This provides improved spatial prediction via the posterior distribution of the ground truth, allows us to validate Models-3 via the posterior predictive distribution of the CASTNet observations, and enables us to remove the bias in the Models-3 output. We apply our methods to data on SO2 concentrations, and we obtain high-resolution SO2 distributions by combining observed data with model output. We also conclude that the numerical models perform worse in areas closer to power plants, where the SO2 values are overestimated by the models.|Model Evaluation and Spatial Interpolation by Bayesian Combination of Observations with Outputs from Numerical Models|http://www.jstor.org/stable/3695645|3695645|2005-03-01|2005|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
The issue of using informative priors for estimation of mixtures at multiple time points is examined. Several different informative priors and an independent prior are compared using samples of actual and simulated aerosol particle size distribution (PSD) data. Measurements of aerosol PSDs refer to the concentration of aerosol particles in terms of their size, which is typically multimodal in nature and collected at frequent time intervals. The use of informative priors is found to better identify component parameters at each time point and more clearly establish patterns in the parameters over time. Some caveats to this finding are discussed.|USING INFORMATIVE PRIORS IN THE ESTIMATION OF MIXTURES OVER TIME WITH APPLICATION TO AEROSOL PARTICLE SIZE DISTRIBUTIONS|http://www.jstor.org/stable/24521732|24521732|2014-03-01|2014|['eng']|['Applied sciences - Engineering', 'Physical sciences - Astronomy']|['Mathematics', 'Science & Mathematics', 'Statistics']
Intervention studies in school systems are sometimes aimed not at changing curriculum or classroom technique, but rather at changing the way that teachers, teaching coaches, and administrators in schools work with one another—in short, changing the professional social networks of educators. Current methods of social network analysis are ill-suited to modeling the multiple partially exchangeable networks that arise in randomized field trials and observational studies in which multiple classrooms, schools, or districts are involved, and to detecting the effect of an intervention on the social network itself To address these needs, we introduce a new modeling framework, the Hierarchical Network Models (HNM) framework. The HNM framework can be used to extend single-network statistical network models to multiple networks, using a hierarchical modeling approach. We show how to generalize the latent space model for a single network to the HNM/multiple-network setting, and illustrate our approach with real and simulated social network data among education professionals.|Hierarchical Network Models for Education Research: Hierarchical Latent Space Models|http://www.jstor.org/stable/41999426|41999426|2013-06-01|2013|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Education', 'Science and Mathematics', 'Social Sciences', 'Statistics']
An approach is proposed to optimal design of experiments for estimating random-effects regression models. The population designs are defined by the number of subjects and the individual designs to be performed. Cost functions associated with individual designs are incorporated. For a given maximal cost, an algorithm is proposed for finding the statistical population design that maximises the determinant of the Fisher information matrix of the population parameters. The Fisher information matrix is formulated for linear models and normal distributions. The approach is applied to the design of an optimal experiment in toxicokinetics using a first-order linearisation of the model. Several cost functions and designs of various orders are studied. An example illustrates the optimal population designs and the increased efficiency of some optimal designs over more standard designs.|Optimal Design in Random-Effects Regression Models|http://www.jstor.org/stable/2337468|2337468|1997-06-01|1997|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
We investigate the transmission dynamics of a certain type of foot-and-mouth disease (FMD) virus under experimental conditions. Previous analyses of experimental data from FMD outbreaks in non-homo-geneously mixing populations of sheep have suggested a decline in viraemic level through serial passage of the virus., but these do not take into account possible variation in the length of the chain of viral transmission for each animal, which is implicit in the non-observed transmission process. We consider a susceptible-exposed-infectious-removed non-Markovian compartmental model for partially observed epidemic processes, and we employ powerful methodology (Markov chain Monte Carlo) for statistical inference, to address epidemiological issues under a Bayesian framework that accounts for all available information and associated uncertainty in a coherent approach. The analysis allows us to investigate the posterior distribution of the hidden transmission history of the epidemic, and thus to determine the effect of the length of the infection chain on the recorded viraemic levels, based on the posterior distribution of a p-value. Parameter estimates of the epidemiological characteristics of the disease are also obtained. The results reveal a possible decline in viraemia in one of the two experimental outbreaks. Our model also suggests that individual infectivity is related to the level of viraemia.|Bayesian Analysis of Experimental Epidemics of Foot-and-Mouth Disease|http://www.jstor.org/stable/4142613|4142613|2004-06-07|2004|['eng']|['Applied sciences - Engineering']|['Biological Sciences', 'General Science', 'Science and Mathematics']
The United Nations regularly publishes projections of the populations of all the world's countries broken down by age and sex. These projections are the de facto standard and are widely used by international organizations, governments and researchers. Like almost all other population projections, they are produced using the standard deterministic cohortcomponent projection method and do not yield statements of uncertainty. We describe a Bayesian method for producing probabilistic population projections for most countries which are projections that the United Nations could use. It has at its core Bayesian hierarchical models for the total fertility rate and life expectancy at birth. We illustrate the method and show how it can be extended to address concerns about the UN's current assumptions about the long-term distribution of fertility. The method is implemented in the R packages bayesTFR, bayesLife, bayesPop and bayesDem.|Bayesian Population Projections for the United Nations|http://www.jstor.org/stable/43288451|43288451|2014-02-01|2014|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
In fitting a generalized linear model, many authors have noticed that data sets can show greater residual variability than predicted under the exponential family. Two main approaches have been used to model this overdispersion. The first approach uses a sampling density which is a conjugate mixture of exponential family distributions. The second uses a quasilikelihood which adds a new scale parameter to the exponential likelihood. The approaches are compared by means of a Bayesian analysis using noninformative priors. In examples, it is indicated that the posterior analysis can be significantly different using the two approaches. /// Plusieurs auteurs ont remarqué que lors de l'ajustement d'un modèle linéaire généralisé, les données peuvent présenter plus de variabilité résiduelle que celle prédite sous un modèle exponentiel. Deux approches sont utilisées pour représenter cette dispersion excédentaire. La première utilise une densité échantillonnale qui est un mélange conjugé de densités de la famille exponentielle. La deuxième approche utilise une fonction de quasi-vraisemblance qui ajoute un nouveau paramètre de dispersion à la fonction de vraisemblance exponentielle. Les deux approches sont comparées en faisant une analyse bayesienne avec des lois a priori non informatives. Des exemples illustrent que les deux approches peuvent mener à des analyses a posteriori différentes.|A Bayesian Approach to Some Overdispersion Models|http://www.jstor.org/stable/3315529|3315529|1989-09-01|1989|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Objectives. We compared national and state-based estimates for the prevalence of mammography screening from the National Health Interview Survey (NHIS), the Behavioral Risk Factor Surveillance System (BRFSS), and a modelbased approach that combines information from the two surveys. Methods. At the state and national levels, we compared the three estimates of prevalence for two time periods (1997-1999 and 2000-2003) and the estimated difference between the periods. We included state-level covariates in the model-based approach through principal components. Results. The national mammography screening prevalence estimate based on the BRFSS was substantially larger than the NHIS estimate for both time periods. This difference may have been due to nonresponse and noncoverage biases, response mode (telephone vs. in-person) differences, or other factors. However, the estimated change between the two periods was similar for the two surveys. Consistent with the model assumptions, the model-based estimates were more similar to the NHIS estimates than to the BRFSS prevalence estimates. The state-level covariates (through the principal components) were shown to be related to the mammography prevalence with the expected positive relationship for socioeconomic status and urbanicity. In addition, several principal components were significantly related to the difference between NHIS and BRFSS telephone prevalence estimates. Conclusions. Model-based estimates, based on information from the two surveys, are useful tools in representing combined information about mammography prevalence estimates from the two surveys. The model-based approach adjusts for the possible nonresponse and noncoverage biases of the telephone survey while using the large BRFSS state sample size to increase precision.|State-Based Estimates of Mammography Screening Rates Based on Information from Two Health Surveys|http://www.jstor.org/stable/41435282|41435282|2010-07-01|2010|['eng']|['Physical sciences - Astronomy']|['Health Policy', 'Medicine and Allied Health']
Doan, Litterman, and Sims (DLS) have suggested using conditional forecasts to do policy analysis with Bayesian vector autoregression (BVAR) models. Their method seems to violate the Lucas critique, which implies that coefficients of a BVAR model will change when there is a change in policy rules. In this article, we attempt to determine whether the Lucas critique is important quantitatively in a BVAR macro model that we construct. We find evidence following two candidate policy rule changes of significant coefficient instability and of a deterioration in the performance of the DLS method.|The Quantitative Significance of the Lucas Critique|http://www.jstor.org/stable/1391237|1391237|1991-10-01|1991|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
The evaluation of the performance of a continuous diagnostic measure is a commonly encountered task in medical research. We develop Bayesian non-parametric models that use Dirichlet process mixtures and mixtures of Polya trees for the analysis of continuous serologic data. The modelling approach differs from traditional approaches to the analysis of receiver operating characteristic curve data in that it incorporates a stochastic ordering constraint for the distributions of serologic values for the infected and non-infected populations. Biologically such a constraint is virtually always feasible because serologic values from infected individuals tend to be higher than those for non-infected individuals. The models proposed provide data-driven inferences for the infected and non-infected population distributions, and for the receiver operating characteristic curve and corresponding area under the curve. We illustrate and compare the predictive performance of the Dirichlet process mixture and mixture of Polya trees approaches by using serologic data for Johne's disease in dairy cattle.|Modelling Stochastic Order in the Analysis of Receiver Operating Characteristic Data: Bayesian Non-Parametric Approaches|http://www.jstor.org/stable/20492597|20492597|2008-01-01|2008|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Numerous studies have linked ambient air pollution and adverse health outcomes. Many studies of this nature relate outdoor pollution levels measured at a femonitoring stations with health outcomes. Recently, computational methods have been developed to model the distribution of personal exposures, rather than ambient concentration, and then relate the exposure distribution to the health outcome. Although these methods show great promise, they are limited by the computational demands of the exposure model. We propose a method to alleviate these computational burdens with the eventual goal of implementing a national study of the health effects of air pollution exposure. Our approach is to develop a statistical emulator for the exposure model, i.e. we use Bayesian density estimation to predict the conditional exposure distribution as a function of several variables, such as temperature, human activity and physical characteristics of the pollutant. This poses a challenging statistical problem because there are many predictors of the exposure distribution and density estimation is notoriously difficult in high dimensions. To overcome this challenge, we use stochastic search variable selection to identify a subset of the variables that have more than just additive effects on the mean of the exposure distribution. We apply our method to emulate an ozone exposure model in Philadelphia.|Variable selection for high dimensional Bayesian density estimation: application to human exposure simulation|http://www.jstor.org/stable/41430948|41430948|2012-01-01|2012|['eng']|['Mathematics - Applied mathematics']|['Science and Mathematics', 'Statistics']
This paper consists of three separate parts. The first one deals with the problem of calculating «correct» formulae for the variances and covariances of multistep predictions in a VAR context with deterministic components. The second part proposes a way of eliminating one of the principal drawbacks connected with the use of VAR models (i.e. the lack of parsimony) by imposing particular (automatically accepted) restrictions. Refering to this model we will call it «restricted VAR». In this part we will also use the formulae found for the variances and covariances of unrestricted forecasts adapting them for the restricted VAR case. In the third part we compare unrestricted and restricted VAR's performance both on a theoretical and on an empirical ground.|PREDICTIONS FROM UNRESTRICTED AND RESTRICTED VAR MODELS|http://www.jstor.org/stable/23247093|23247093|1987-05-01|1987|['eng']|['Mathematics - Mathematical logic']|['Business & Economics', 'Business', 'Economics']
Using a unique conjoint data set drawn from 281 college students in Hong Kong, I estimate a random-coefficient discrete choice demand system for Microsoft Office from legal and various illegal sources. Counterfactual results show two things. First, most students would switch to Internet piracy even if the government eradicated street piracy. This explains why software piracy in Hong Kong remains well above 40% despite the government's successful measures to bring down street piracy. Second, the true gain from shutting off all sources of piracy is HK$48.6 (US$6) per person, only 15% of the Business Software Alliance's estimated cost of piracy.|WHAT IS THE TRUE LOSS DUE TO PIRACY? EVIDENCE FROM MICROSOFT OFFICE IN HONG KONG|http://www.jstor.org/stable/43554809|43554809|2013-07-01|2013|['eng']|['Business - Business operations', 'Information science - Coding theory']|['Business & Economics', 'Science & Mathematics', 'Business', 'Statistics', 'Economics']
We propose a method for the computational inference of directed acyclic graphical structures given data from experimental interventions. Order-space Markov chain Monte Carlo, equi-energy sampling, importance weighting, and stream-based computation are combined to create a fast algorithm for learning causal Bayesian network structures.|Learning Causal Bayesian Network Structures from Experimental Data|http://www.jstor.org/stable/27640100|27640100|2008-06-01|2008|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Consider the problem of finding the dose that is as high as possible subject to having a controlled rate of toxicity. The problem is commonplace in oncology Phase I clinical trials. Such a dose is often called the maximum tolerated dose (MTD) since it represents a necessary trade-off between efficacy and toxicity. The continual reassessment method (CRM) is an improvement over traditional up-and-down schemes for estimating the MTD. It is based on a Bayesian approach and on the assumption that the dose-toxicity relationship follows a specific response curve, e.g., the logistic or power curve. The purpose of this paper is to illustrate how the assumption of a specific curve used in the CRM is not necessary and can actually hinder the efficient use of prior inputs. An alternative curve-free method in which the probabilities of toxicity are modeled directly as an unknown multidimensional parameter is presented. To that purpose, a product-of-beta prior (PBP) is introduced and shown to bring about logical improvements. Practical improvements are illustrated by simulation results.|A Curve-Free Method for Phase I Clinical Trials|http://www.jstor.org/stable/2677008|2677008|2000-06-01|2000|['eng']|['Biological sciences - Biology', 'Applied sciences - Engineering', 'Health sciences - Health and wellness', 'Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
A new approach for data-based stochastic parametrization of unresolved scales and processes in numerical weather and climate prediction models is introduced. The subgridscale model is conditional on the state of the resolved scales, consisting of a collection of local models. A clustering algorithm in the space of the resolved variables is combined with statistical modelling of the impact of the unresolved variables. The clusters and the parameters of the associated subgrid models are estimated simultaneously from data. The method is implemented and explored in the framework of the Lorenz '96 model using discrete Markov processes as local statistical models. Performance of the cluster-weighted Markov chain scheme is investigated for long-term simulations as well as ensemble prediction. It clearly outperforms simple parametrization schemes and compares favourably with another recently proposed subgrid modelling scheme also based on conditional Markov chains.|Data-based stochastic subgrid-scale parametrization: an approach using cluster-weighted modelling|http://www.jstor.org/stable/41348432|41348432|2012-03-13|2012|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Mathematics', 'General Science']
The analysis of stochastic models is often greatly complicated if there are censored observations of the random variables. This paper characterizes families of distributions which help keep tractable the analysis of such models. Our primary motivation is to provide guidance to practitioners in the selection of distributions: If a modeler feels that no member of the families we characterize is a reasonable approximation, then he will almost surely encounter serious analytic and computational problems if his data include censored observations. We characterize a family of distributions for which there exist fixed-dimensional sufficient statistics of purely censored observations. We also characterize an important subset of this family, appropriate for situations where data include both censored and exact observations. We derive the corresponding predictive distributions using arbitrary priors and present some general results relating stochastic dominance among predictive distributions to the parameters of the prior. We also analyze the cases of discrete and mixed random variables.|Informational Dynamics of Censored Observations|http://www.jstor.org/stable/2632442|2632442|1991-11-01|1991|['eng']|['Mathematics - Mathematical logic']|['Business', 'Business & Economics Collection', 'Management & Organizational Behavior']
Recent work suggests VAR models of output, inflation, and interest rates may be prone to instabilities. In the face of such instabilities, a variety of estimation or forecasting methods might be used to improve the accuracy of forecasts from a VAR. The uncertainty inherent in any single representation of instability could mean that combining forecasts from a range of approaches will improve forecast accuracy. Focusing on models of US output, prices, and interest rates, this paper examines the effectiveness of combining various models of instability in improving VAR forecasts made with real-time data.|Averaging Forecasts from VARs with Uncertain Instabilities|http://www.jstor.org/stable/25608794|25608794|2010-01-01|2010|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Business', 'Economics']
A model for analyzing rank data obtained from multiple evaluators, possibly using different ranking criteria, is proposed. The model is specified hierarchically within the Bayesian paradigm and includes parameters that represent the probabilities that two items are assigned equal rankings. Also included are parameters that account for the relative precision of rankings obtained from distinct evaluation schemes. The model is illustrated through a meta-analysis of rank data collected to compare the cognitive abilities of various primate genera.|Bayesian Analysis of Rank Data with Application to Primate Intelligence Experiments|http://www.jstor.org/stable/3085754|3085754|2002-03-01|2002|['eng']|['Biological sciences - Biology', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
In the last two decades many random graph models have been proposed to extract knowledge from networks. Most of them look for communities or, more generally, clusters of vertices with homogeneous connection profiles. While the first models focused on networks with binary edges only, extensions now allow to deal with valued networks. Recently, new models were also introduced in order to characterize connection patterns in networks through mixed memberships. This work was motivated by the need of analyzing a historical network where a partition of the vertices is given and where edges are typed. A known partition is seen as a decomposition of a network into subgraphs that we propose to model using a stochastic model with unknown latent clusters. Each subgraph has its own mixing vector and sees its vertices associated to the clusters. The vertices then connect with a probability depending on the subgraphs only, while the types of edges are assumed to be sampled from the latent clusters. A variational Bayes expectation-maximization algorithm is proposed for inference as well as a model selection criterion for the estimation of the cluster number. Experiments are carried out on simulated data to assess the approach. The proposed methodology is then applied to an ecclesiastical network in Merovingian Gaul. An R code, called Rambo, implementing the inference algorithm is available from the authors upon request.|THE RANDOM SUBGRAPH MODEL FOR THE ANALYSIS OF AN ECCLESIASTICAL NETWORK IN MEROVINGIAN GAUL|http://www.jstor.org/stable/24521738|24521738|2014-03-01|2014|['eng']|['Mathematics - Mathematical logic']|['Mathematics', 'Science & Mathematics', 'Statistics']
We present a state-of-the-art application of smoothing for dependent bivariate binomial spatial to Loa loa prevalence mapping in West Africa. This application starts with the nonspatial calibration of survey instruments, continues with the spatial model building and assessment, and ends with robust, tested software intended for use by field workers for online prevalence map updating. From a statistical perspective, we address several important methodological issues: building spatial models that are sufficiently complex to capture the structure of the data but remain computationally usable, reducing the computational burden in the handling of very large covariate data sets, and devising methods for comparing spatial prediction methods for a given exceedance policy threshold.|Bivariate Binomial Spatial Modeling of Loa loa Prevalence in Tropical Africa [with Comments, Rejoinder]|http://www.jstor.org/stable/27640018|27640018|2008-03-01|2008|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Time-frequency analysis has become a fundamental component of many scientific inquiries. Due to improvements in technology, the amount of high-frequency signals that are collected for ecological and other scientific processes is increasing at a dramatic rate. In order to facilitate the use of these data in ecological prediction, we introduce a class of nonlinear multivariate time-frequency functional models that can identify important features of each signal as well as the interaction of signals corresponding to the response variable of interest. Our methodology is of independent interest and utilizes stochastic search variable selection to improve model selection and performs model averaging to enhance prediction. We illustrate the effectiveness of our approach through simulation and by application to predicting spawning success of shovelnose sturgeon in the Lower Missouri River.|Ecological Prediction With Nonlinear Multivariate Time-Frequency Functional Data Models|http://www.jstor.org/stable/26452950|26452950|2013-09-01|2013|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Agriculture', 'Statistics']
This article proposes a Bayesian method for estimating a heteroscedastic regression model with Gaussian errors, where the mean and the log variance are modeled as linear combinations of explanatory variables. We use Bayesian variable selection priors and model averaging to make the estimation more efficient. The model is made semiparametric by allowing explanatory variables to enter the mean and log variance flexibly by representing a covariate effect as a linear combination of basis functions. Our methodology for estimating flexible effects is locally adaptive in the sense that it works well when the flexible effects vary rapidly in some parts of the predictor space but only slowly in other parts. Our article develops an efficient Markov chain Monte Carlo simulation method to sample from the posterior distribution and applies the methodology to a number of simulated and real examples.|Locally Adaptive Semiparametric Estimation of the Mean and Variance Functions in Regression Models|http://www.jstor.org/stable/27594220|27594220|2006-12-01|2006|['eng']|['Applied sciences - Engineering', 'Mathematics - Mathematical objects', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Computer Science', 'Statistics']
A study into the geographical variability of timing of initial child breastfeeding after birth was carried out with the data set from the 1999 Nigeria Demographic and Health Survey. The effect of the metrical covariate of the mother's age at birth was assumed to be nonlinear and estimated nonparametrically. Other categorical covariates are estimated in the usual parametric form. Within a Bayesian context, appropriate priors are assigned for the geographical location, vector of the unknown (nonlinear) smooth functions and a further vector of the fixed effect parameters. For instance, a Markov random field prior is assumed on the spatial effects. Inferences are based on Markov chain Monte Carlo techniques while Bayesian model diagnostics are based on the deviance information criteria.|Bayesian Geoadditive Modelling of Breastfeeding Initiation in Nigeria|http://www.jstor.org/stable/25146280|25146280|2004-03-01|2004|['eng']|['Applied sciences - Engineering', 'Health sciences - Medical specialties']|['Business & Economics', 'Business', 'Economics']
Our paper proposes adaptive Monte Carlo sampling schemes for Bayesian variable selection in linear regression that improve on standard Markov chain methods. We do so by considering Metropolis-Hastings proposals that make use of accumulated information about the posterior distribution obtained during sampling. Adaptation needs to be done carefully to ensure that sampling is from the correct ergodic distribution. We give conditions for the validity of an adaptive sampling scheme in this problem, and for simulating from a distribution on a finite state space in general, and suggest a class of adaptive proposal densities which uses best linear prediction to approximate the Gibbs sampler. Our sampling scheme is computationally much faster per iteration than the Gibbs sampler, and when this is taken into account the efficiency gains when using our sampling scheme compared to alternative approaches are substantial in terms of precision of estimation of posterior quantities of interest for a given amount of computation time. We compare our method with other sampling schemes for examples involving both real and simulated data. The methodology developed in the paper can be extended to variable selection in more general problems.|Adaptive Sampling for Bayesian Variable Selection|http://www.jstor.org/stable/20441233|20441233|2005-12-01|2005|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
ABSTRACT Reliability-based design optimization (RBDO) has been widely used to obtain a reliable design via an existing CAE model considering the variations of input variables. However, most RBDO approaches do not consider the CAE model bias and uncertainty, which may largely affect the reliability assessment of the final design and result in risky design decisions. In this paper, the Gaussian Process Modeling (GPM) approach is applied to statistically correct the model discrepancy which is represented as a bias function, and to quantify model uncertainty based on collected data from either real tests or high-fidelity CAE simulations. After the corrected model is validated by extra sets of test data, it is integrated into the RBDO formulation to obtain a reliable solution that meets the overall reliability targets while considering both model and parameter uncertainties. The proposed technique is demonstrated through a vehicle design problem aiming at minimizing the vehicle weight through gauge optimization while satisfying reliability constraints. The RBDO result considering model uncertainty is compared with the one from conventional RBDO to demonstrate the benefits of the proposed method.|Reliability-Based Design Optimization with Model Bias and Data Uncertainty|http://www.jstor.org/stable/26268547|26268547|2013-06-01|2013|['eng']|['Applied sciences - Engineering']|
"We consider two-stage models of the kind used in parametric empirical Bayes (PEB) methodology, calling them conditionally independent hierarchical models. We suppose that there are k ""units,"" which may be experimental subjects, cities, study centers, etcetera. At the first stage, the observation vectors Yi for units i = 1,...,k are independently distributed with densities p(yi∣θi), or more generally, p(yi∣θi, λ). At the second stage, the unit-specific parameter vectors θi are iid with densities p(θi∣λ). The PEB approach proceeds by regarding the second-stage distribution as a prior and noting that, if λ were known, inference about θ could be based on its posterior. Since λ is not known, the simplest PEB methods estimate the parameter λ by maximum likelihood or some variant, and then treat λ as if it were known to be equal to this estimate. Although this procedure is sometimes satisfactory, a well-known defect is that it neglects the uncertainty due to the estimation of λ. In this article we suggest that approximate Bayesian inference can provide simple and manageable solutions to this problem. In Bayesian inferences, a prior density π(·) on λ is introduced, the posterior p(λ∣y) is calculated, and the posterior density of θi is then equal to the expectation, with respect to p(λ∣y), of the conditional posterior p(θi∣ yi, λ). From the Bayesian point of view, the PEB estimate is of interest because it is a first-order approximation to the posterior mean [having an error of order O(k-1)]. Letting Eλ and Vλ denote the expectation and variance with respect to p(λ∣y), we may write the posterior variance of θi as V(θi∣y) = Eλ{V(θi∣ yi, λ)} + Vλ{E(θi∣ yi, λ)}. The conditional posterior variance $V(\theta_i\mid y_i, \hat\lambda)$, where $\hat\lambda$ is the maximum likelihood estimator, approximates only the first term. When we include an approximation to the second term we obtain a first-order approximation to the posterior variance itself. In many examples, this elementary method, incorporating approximations to both terms, will substantially account for the estimation of λ. We briefly consider second-order approximations, noting that the work of Deely and Lindley (1981) may be extended using expansions derived by Lindley (1980), Mosteller and Wallace (1964), Tierney and Kadane (1986), and Tierney, Kass, and Kadane (1989). We suggest that second-order approximations provide rough and, often, easily computed assessments of accuracy of first-order approximations. Although we confine our data-analytical examples to simple models, we believe the methods will be useful in general settings. An important area of application is longitudinal data analysis."|Approximate Bayesian Inference in Conditionally Independent Hierarchical Models (Parametric Empirical Bayes Models)|http://www.jstor.org/stable/2289653|2289653|1989-09-01|1989|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Mathematical analysis']|['Science & Mathematics', 'Statistics']
We propose a new semiparametric Bayesian model for causal inference in which assignment to treatment depends on potential outcomes. The model uses the probit stick-breaking process mixture proposed by Chung and Dunson (2009), a variant of the Dirichlet process mixture modeling. In contrast to previous Bayesian models, the proposed model directly estimates the parameters of the marginal parametric model of potential outcomes, while it relaxes the strong ignorability assumption, and requires no parametric model assumption for the assignment model and conditional distribution of the covariate vector. The proposed estimation method is more robust than maximum likelihood estimation, in that it does not require knowledge of the full joint distribution of potential outcomes, covariates, and assignments. In addition, the method is more efficient than fully nonparametric Bayes methods. We apply this model to infer the differential effects of cognitive and noncognitive skills on the wages of production and nonproduction workers using panel data from the National Longitudinal Survey of Youth in 1979. The study also presents the causal effect of online word-of-mouth on Web site browsing behavior. Supplementary materials for this article are available online.|Semiparametric Bayesian Estimation for Marginal Parametric Potential Outcome Modeling: Application to Causal Inference|http://www.jstor.org/stable/24247050|24247050|2013-12-01|2013|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
An evolutionary history of a set of organisms is a family tree, or topology, with branches of various lengths between vertices that describe how closely the organisms are related to each other. We consider the K evolutionary histories of K genes from a set of N organisms. Evolutionary similarity (ES) occurs when the branching patterns and relative branch lengths in the K evolutionary histories of the genes are the same or nearly the same across the set of organisms. Evolutionary similarity indicates similarity of evolutionary pressures acting on these genes. Current likelihood approaches identify ES conditional on a given topology. For a variety of reasons, different genes may support different topologies when fit independently. We use Bayesian models and reversible-jump Markov chain Monte Carlo to jointly infer topology and branch lengths for multiple genes simultaneously. We test for ES using Bayes factors, conditionally on a consistent topology over the multiple genes, where the topology is either known or unknown. We relax the single topology assumption by employing a dissimilarity measure between evolutionary histories and testing for ES using both prior and posterior predictive p values. We apply our methodology to three genes (DAX1, SOX9, and SRY) believed to be involved in sex determination in primates. We find support in the data for ES between DAX1 and SRY, but not SOX9. These results are consistent with the hypothesized biological roles of these genes.|Evolutionary Similarity among Genes|http://www.jstor.org/stable/30045292|30045292|2003-09-01|2003|['eng']|['Biological sciences - Biology']|['Science & Mathematics', 'Statistics']
We apply Bayesian image analysis techniques to a problem in a newly developed scanned probe technology that uses commercial magnetoresistive (MR) record and playback heads as probes to sense magnetic fields. This technology can be used for magnetic imaging and for evaluating playback and record processes in magnetic recording. In MR microscopy, an MR head is raster scanned while in physical contact with a magnetic sample (e.g., hard disk media, tape, or fine magnetic particles). By plotting the MR resistance as a function of position, a very high resolution (on the order of .1 × 1.0 μ m) magnetic image of the sample is constructed. This case study focuses on characterizing the head sensitivity function (HSF), which depends on the physical dimensions and the magnetic properties of the MR head. These sensitivity functions are of great practical interest because they ultimately relate to the head's performance in a high-density data storage environment. Estimating the HSF requires a deconvolution that has features that prevent the problem from being straightforward: both the HSF and the source being scanned are unknown, and there is a substantial amount of correlated noise in the scanned image. We take a Bayesian approach to model and estimate the HSF, while accounting for noise and other nuisance effects such as thermal drift. Besides yielding a point estimate, which is a fairly difficult task here, this approach also quantifies uncertainty so we can assess whether certain features of the estimated head sensitivity function appear to be genuine.|Estimation of the Head Sensitivity Function in Scanning Magnetoresistance Microscopy|http://www.jstor.org/stable/2670219|2670219|2001-09-01|2001|['eng']|['Mathematics - Applied mathematics', 'Information science - Coding theory']|['Science & Mathematics', 'Statistics']
The two-phase sampling design is a cost-efficient way of collecting expensive covariate information on a judiciously selected subsample. It is natural to apply such a strategy for collecting genetic data in a subsample enriched for exposure to environmental factors for gene-environment interaction (G × E) analysis. In this paper, we consider two-phase studies of G × E interaction where phase I data are available on exposure, covariates and disease status. Stratified sampling is done to prioritize individuals for genotyping at phase II conditional on disease and exposure. We consider a Bayesian analysis based on the joint retrospective likelihood of phases I and II data. We address several important statistical issues: (i) we consider a model with multiple genes, environmental factors and their pairwise interactions. We employ a Bayesian variable selection algorithm to reduce the dimensionality of this potentially high-dimensional model; (ii) we use the assumption of gene— gene and gene-environment independence to trade off between bias and efficiency for estimating the interaction parameters through use of hierarchical priors reflecting this assumption; (iii) we posit a flexible model for the joint distribution of the phase I categorical variables using the nonparametric Bayes construction of Dunson and Xing [J. Amer. Statist. Assoc. 104 (2009) 1042—1051]. We carry out a small-scale simulation study to compare the proposed Bayesian method with weighted likelihood and pseudo-likelihood methods that are standard choices for analyzing two-phase data. The motivating example originates from an ongoing case-control study of colorectal cancer, where the goal is to explore the interaction between the use of statins (a drug used for lowering lipid levels) and 294 genetic markers in the lipid metabolism/cholesterol synthesis pathway. The subsample of cases and controls on which these genetic markers were measured is enriched in terms of statin users. The example and simulation results illustrate that the proposed Bayesian approach has a number of advantages for characterizing joint effects of genotype and exposure over existing alternatives and makes efficient use of all available data in both phases.|BAYESIAN SEMIPARAMETRIC ANALYSIS FOR TWO-PHASE STUDIES OF GENE-ENVIRONMENT INTERACTION|http://www.jstor.org/stable/23566522|23566522|2013-03-01|2013|['eng']|['Applied sciences - Engineering', 'Biological sciences - Biology']|['Mathematics', 'Science & Mathematics', 'Statistics']
Recursive binary partitioning is a popular tool for regression analysis. Two fundamental problems of exhaustive search procedures usually applied to fit such models have been known for a long time: overfitting and a selection bias towards covariates with many possible splits or missing values. While pruning procedures are able to solve the overfitting problem, the variable selection bias still seriously affects the interpretability of tree-structured regression models. For some special cases unbiased procedures have been suggested, however lacking a common theoretical foundation. We propose a unified framework for recursive partitioning which embeds tree-structured regression models into a well defined theory of conditional inference procedures. Stopping criteria based on multiple test procedures are implemented and it is shown that the predictive performance of the resulting trees is as good as the performance of established exhaustive search procedures. It turns out that the partitions and therefore the models induced by both approaches are structurally different, confirming the need for an unbiased variable selection. Moreover, it is shown that the prediction accuracy of trees with early stopping is equivalent to the prediction accuracy of pruned trees with unbiased variable selection. The methodology presented here is applicable to all kinds of regression problems, including nominal, ordinal, numeric, censored as well as multivariate response variables and arbitrary measurement scales of the covariates. Data from studies on glaucoma classification, node positive breast cancer survival and mammography experience are re-analyzed.|Unbiased Recursive Partitioning: A Conditional Inference Framework|http://www.jstor.org/stable/27594202|27594202|2006-09-01|2006|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Computer Science', 'Statistics']
For rare diseases the observed disease count may exhibit extra Poisson variability, particularly in areas with low or sparse populations. Hence the variance of the estimates of disease risk, the standardized mortality ratios, may be highly unstable. This overdispersion must be taken into account otherwise subsequent maps based on standardized mortality ratios will be misleading and, rather than displaying the true spatial pattern of disease risk, the most extreme values will be highlighted. Neighbouring areas tend to exhibit spatial correlation as they may share more similarities than non-neighbouring areas. The need to address overdispersion and spatial correlation has led to the proposal of Bayesian approaches for smoothing estimates of disease risk. We propose a new model for investigating the spatial variation of disease risks in conjunction with an alternative specification for estimates of disease risk in geographical areas-the multivariate Poisson-gamma model. The main advantages of this new model lie in its simplicity and ability to account naturally for overdispersion and spatial auto-correlation. Exact expressions for important quantities such as expectations, variances and covariances can be easily derived.|A New Approach to Investigating Spatial Variations of Disease|http://www.jstor.org/stable/30130763|30130763|2008-01-01|2008|['eng']|['Applied sciences - Engineering', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Despite multiple calls for the integration of time into behavioral intent measurement, surprisingly little academic research has examined timed intent measures directly. In two empirical studies, the authors estimate individual-level cumulative adoption likelihood curves—curves calibrated on self-reported adoption likelihoods for cumulative time intervals across a fixed horizon—of 478 managerial decision makers, self-predicting whether and when they will adopt a relevant technology. A hierarchical Bayes formulation allows for a heterogeneous account of the individual-level adoption likelihood curves as a function of time and common antecedents of technology adoption. A third study generalizes these results among 354 consumer decision makers and, using behavioral data collected during a two-year longitudinal study involving a subsample of 143 consumer decision makers, provides empirical evidence for the accuracy of cumulative adoption likelihood curves for predicting whether and when a technology is adopted. Cumulative adoption likelihood curves outperform two single-intent measures as well as two widely validated intent models in predicting individual-level adoption for a fixed period of two years. The results hold great promise for further research on using and optimizing cumulative timed intent measures across a variety of application domains.|Cumulative Timed Intent: A New Predictive Tool for Technology Adoption|http://www.jstor.org/stable/20751545|20751545|2010-10-01|2010|['eng']|['Physical sciences - Astronomy', 'Information science - Informetrics']|['Business & Economics', 'Marketing & Advertising', 'Business']
Gaussian time-series models are often specified through their spectral density. Such models present several computational challenges, in particular because of the nonsparse nature of the covariance matrix. We derive a fast approximation of the likelihood for such models. We propose to sample from the approximate posterior (i.e., the prior times the approximate likelihood), and then to recover the exact posterior through importance sampling. We show that the variance of the importance sampling weights vanishes as the sample size goes to infinity. We explain why the approximate posterior may typically be multimodal, and we derive a Sequential Monte Carlo sampler based on an annealing sequence to sample from that target distribution. Performance of the overall approach is evaluated on simulated and real dataseis. In addition, for one real-world dataset, we provide some numerical evidence that a Bayesian approach to semiparametric estimation of spectral density may provide more reasonable results than its frequentist counterparts. The article comes with supplementary materials, available online, that contain an Appendix with a proof of our main Theorem, a Python package that implements the proposed procedure, and the Ethernet dataset.|Computational Aspects of Bayesian Spectral Density Estimation|http://www.jstor.org/stable/43304848|43304848|2013-09-01|2013|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Computer Science', 'Statistics']
"Landscape classification of the well-known biodiversity hotspot, Western Ghats (mountains), on the west coast of India, is an important part of a world-wide program of monitoring biodiversity. To this end, a massive vegetation data set, consisting of 51,834 4-variate observations has been clustered into different landscapes by Nagendra and Gadgil [Current Sci. 75 (1998) 264-271]. But a study of such importance may be affected by nonuniqueness of cluster analysis and the lack of methods for quantifying uncertainty of the clusterings obtained. Motivated by this applied problem of much scientific importance, we propose a new methodology for obtaining the global, as well as the local modes of the posterior distribution of clustering, along with the desired credible and ""highest posterior density"" regions in a nonparametric Bayesian framework. To meet the need of an appropriate metric for computing the distance between any two clusterings, we adopt and provide a much simpler, but accurate modification of the metric proposed in [In Felicitation Volume in Honour of Prof. B. K. Kale (2009) MacMillan]. A very fast and efficient Bayesian methodology, based on [Sankhyā Ser. B 70 (2008) 133-155], has been utilized to solve the computational problems associated with the massive data and to obtain samples from the posterior distribution of clustering on which our proposed methods of summarization are illustrated. Clustering of the Western Ghats data using our methods yielded landscape types different from those obtained previously, and provided interesting insights concerning the differences between the results obtained by Nagendra and Gadgil [Current Sci. 75 (1998) 264-271] and us. Statistical implications of the differences are also discussed in detail, providing interesting insights into methodological concerns of the traditional clustering methods."|"ON BAYESIAN ""CENTRAL CLUSTERING"": APPLICATION TO LANDSCAPE CLASSIFICATION OF WESTERN GHATS"|http://www.jstor.org/stable/23069360|23069360|2011-09-01|2011|['eng']|['Physical sciences - Astronomy']|['Mathematics', 'Science & Mathematics', 'Statistics']
Selection models are appropriate when the probability that a potential datum enters the sample is a nondecreasing function of the numeric value of the datum. It is rarely justifiable to model this function, called the weight function, with a specific parametric form, but appealing to model it with a nonparametric prior centered around a parametric form. The Bayesian analysis with a Dirichlet process prior for the weight function is considered and it is proved that the posterior is consistent under the weak topology.|ON POSTERIOR CONSISTENCY IN SELECTION MODELS|http://www.jstor.org/stable/24306849|24306849|2001-07-01|2001|['eng']|['Mathematics - Mathematical logic']|['Mathematics', 'Science and Mathematics', 'Statistics']
Most subjective probability aggregation procedures use a single probability judgment from each expert, even though it is common for experts studying real problems to update their probability estimates over time. This paper advances into unexplored areas of probability aggregation by considering a dynamic context in which experts can update their beliefs at random intervals. The updates occur very infrequently, resulting in a sparse data set that cannot be modeled by standard time-series procedures. In response to the lack of appropriate methodology, this paper presents a hierarchical model that takes into account the expert's level of self-reported expertise and produces aggregate probabilities that are sharp and well calibrated both in- and out-of-sample. The model is demonstrated on a real-world data set that includes over 2300 experts making multiple probability forecasts over two years on different subsets of 166 international political events.|PROBABILITY AGGREGATION IN TIME-SERIES: DYNAMIC HIERARCHICAL MODELING OF SPARSE EXPERT BELIEFS|http://www.jstor.org/stable/24522095|24522095|2014-06-01|2014|['eng']|['Physical sciences - Astronomy']|['Mathematics', 'Science & Mathematics', 'Statistics']
The random effects model fit to repeated measures data is an extremely common model and data structure in current biostatistical practice. Modern data analysis often involves the selection of models within broad classes of prespecified models, but for models beyond the generalized linear model, few model-selection tools have been actively studied. In a Bayesian analysis, Bayes factors are the natural tool to use to explore these classes of models. In this paper, we develop a predictive approach for specifying the priors of a repeated measures random effects model with emphasis on selecting the fixed effects. The advantage of the predictive approach is that a single predictive specification is used to specify priors for all models considered. The methodology is applied to a pediatric pain data analysis.|Predictive Model Selection for Repeated Measures Random Effects Models Using Bayes Factors|http://www.jstor.org/stable/2533960|2533960|1997-06-01|1997|['eng']|['Applied sciences - Engineering', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
There has been increasing interest in applying Bayesian nonparametric methods in large samples and high dimensions. As Markov chain Monte Carlo (MCMC) algorithms are often infeasible, there is a pressing need for much faster algorithms. This article proposes a fast approach for inference in Dirichlet process mixture (DPM) models. Viewing the partitioning of subjects into clusters as a model selection problem, we propose a sequential greedy search algorithm for selecting the partition. Then, when conjugate priors are chosen, the resulting posterior conditionally on the selected partition is available in closed form. This approach allows testing of parametric models versus nonparametric alternatives based on Bayes factors. We evaluate the approach using simulation studies and compare it with four other fast nonparametric methods in the literature. We apply the proposed approach to three datasets including one from a large epidemiologic study. Matlab codes for the simulation and data analyses using the proposed approach are available online in the supplemental materials.|Fast Bayesian Inference in Dirichlet Process Mixture Models|http://www.jstor.org/stable/23113384|23113384|2011-03-01|2011|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Computer Science', 'Statistics']
Unobserved heterogeneity in random utility choice models can be dealt with by specifying either a multinomial or a normal distribution of the coefficients, leading to finite mixture logit and mixed logit models. Focusing on the former, we show that individual-level estimates and predictions of finite mixtures estimated by maximizing the likelihood function can be improved through integration over the estimation error of the hyperparameters, using an empirical Bayes approach. We investigate the conjecture that this approach is more robust against departures of the underlying assumptions of the finite mixture model in two Monte Carlo studies. We show that our approach improves the performance of the finite mixture model in representing individual-level parameters and producing hold-out forecasts. We illustrate with two examples that our approach may offer advantages in empirical applications involving the analysis of heterogeneous choice data.|An Empirical Bayes Procedure for Improving Individual-Level Estimates and Predictions from Finite Mixtures of Multinomial Logit Models|http://www.jstor.org/stable/27638786|27638786|2004-01-01|2004|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
In the study of immune responses to infectious pathogens, the minimum protective antibody concentration (MPAC) is a quantity of great interest. We use case-control data to estimate the posterior distribution of the conditional risk of disease given a lower bound on antibody concentration in an at-risk subject. The concentration bound beyond which there is high credibility that infection risk is zero or nearly so is a candidate for the MPAC. A very simple Gibbs sampling procedure that permits inference on the risk of disease given antibody level is presented. In problems involving small numbers of patients, the procedure is shown to have favorable accuracy and robustness to choice/misspecification of priors. Frequentist evaluation indicates good coverage probabilities of credibility intervals for antibody-dependent risk, and rules for estimation of the MPAC are illustrated with epidemiological data.|Bayesian Inference on Protective Antibody Levels Using Case-Control Data|http://www.jstor.org/stable/2676851|2676851|2001-03-01|2001|['eng']|['Biological sciences - Biology']|['Science & Mathematics', 'Statistics']
We present an extension of population-based Markov chain Monte Carlo to the transdimensional case. A major challenge is that of simulating from high- and transdimensional target measures. In such cases, Markov chain Monte Carlo methods may not adequately traverse the support of the target; the simulation results will be unreliable. We develop population methods to deal with such problems, and give a result proving the uniform ergodicity of these population algorithms, under mild assumptions. This result is used to demonstrate the superiority, in terms of convergence rate, of a population transition kernel over a reversible jump sampler for a Bayesian variable selection problem. We also give an example of a population algorithm for a Bayesian multivariate mixture model with an unknown number of components. This is applied to gene expression data of 1000 data points in six dimensions and it is demonstrated that our algorithm outperforms some competing Markov chain samplers. In this example, we show how to combine the methods of parallel chains (Geyer, 1991), tempering (Geyer &amp; Thompson, 1995), snooker algorithms (Gilks et al., 1994), constrained sampling and delayed rejection (Green &amp; Mira, 2001).|Population-Based Reversible Jump Markov Chain Monte Carlo|http://www.jstor.org/stable/20441418|20441418|2007-12-01|2007|['eng']|['Mathematics - Mathematical logic', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
We consider the Bayesian analysis of a few complex, highdimensional models and show that intuitive priors, which are not tailored to the fine details of the model and the estimated parameters, produce estimators which perform poorly in situations in which good, simple frequentisi estimators exist. The models we consider are: stratified sampling, the partial linear model, linear and quadratic functionals of white noise and estimation with stopping times. We present a strong version of Doob's consistency theorem which demonstrates that the existence of a uniformly $\sqrt n $-consistent estimator ensures that the Bayes posterior is $\sqrt n $-oensistent for values of the parameter in subsets of prior probability 1. We also demonstrate that it is, at least, in principle, possible to construct Bayes priors giving both global and local minimax rates, using a suitable combination of loss functions. We argue that there is no contradiction in these apparently conflicting findings.|The Bayesian Analysis of Complex, High-Dimensional Models: Can It Be CODA?|http://www.jstor.org/stable/43288502|43288502|2014-11-01|2014|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Bayesian statistics involve substantial changes in the methods and philosophy of science. Before adopting Bayesian approaches, ecologists should consider carefully whether or not scientific understanding will be enhanced. Frequentist statistical methods, while imperfect, have made an unquestioned contribution to scientific progress and are a workhorse of day-to-day research. Bayesian statistics, by contrast, have a largely untested track record. The papers in this special section on Bayesian statistics exemplify the difficulties inherent in making convincing scientific arguments with Bayesian reasoning.|Discussion: Should Ecologists Become Bayesians?|http://www.jstor.org/stable/2269594|2269594|1996-11-01|1996|['eng']|['Mathematics - Applied mathematics', 'Philosophy - Logic']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
Multilocus genealogical approaches are still uncommon in phylogeography and historical demography, fields which have been dominated by microsatellite markers and mitochondrial DNA, particularly for vertebrates. Using 30 newly developed anonymous nuclear loci, we estimated population divergence times and ancestral population sizes of three closely related species of Australian grass finches (Poephila) distributed across two barriers in northern Australia. We verified that substitution rates were generally constant both among lineages and among loci, and that intralocus recombination was uncommon in our dataset, thereby satisfying two assumptions of our multilocus analysis. The reconstructed gene trees exhibited all three possible tree topologies and displayed considerable variation in coalescent times, yet this information provided the raw data for maximum likelihood and Bayesian estimation of population divergence times and ancestral population sizes. Estimates of these parameters were in close agreement with each other regardless of statistical approach and our Bayesian estimates were robust to prior assumptions. Our results suggest that black-throated finches (Poephila cincta) diverged from long-tailed finches (P. acuticauda and P. hecki) across the Carpentarian Barrier in northeastern Australia around 0.6 million years ago (mya), and that P. acuticauda diverged from P. hecki across the Kimberley Plateau-Arnhem Land Barrier in northwestern Australia approximately 0.3 mya. Bayesian 95% credibility intervals around these estimates strongly support Pleistocene timing for both speciation events, despite the fact that many gene divergences across the Carpentarian region clearly predated the Pleistocene. Estimates of ancestral effective population sizes for the basal ancestor and long-tailed finch ancestor were large (about 521,000 and about 384,000, respectively). Although the errors around the population size parameter estimates are considerable, they are the first for birds taking into account multiple sources of variance.|Speciational History of Australian Grass Finches (Poephila) Inferred from Thirty Gene Trees|http://www.jstor.org/stable/3449131|3449131|2005-09-01|2005|['eng']|['Biological sciences - Biology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
A model is introduced here for multivariate failure time data arising from heterogenous populations. In particular, we consider a situation in which the failure times of individual subjects are often temporally clustered, so that many failures occur during a relatively short age interval. The clustering is modelled by assuming that the subjects can be divided into 'internally homogenous' latent classes, each such class being then described by a time-dependent frailty profile function. As an example, we reanalysed the dental caries data presented earlier in Härkänen et al. [Scand. J. Statist. 27 (2000) 577], as it turned out that our earlier model could not adequately describe the observed clustering.|A Non-Parametric Frailty Model for Temporally Clustered Multivariate Failure Times|http://www.jstor.org/stable/4616781|4616781|2003-09-01|2003|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Abstract: We present a Bayesian hierarchical model for the joint spatial dynamics of a host‐parasite system. The model was fitted to long‐term data on regional plague dynamics and metapopulation dynamics of the black‐tailed prairie dog, a declining keystone species of North American prairies. The rate of plague transmission between colonies increases with increasing precipitation, while the rate of infection from unknown sources decreases in response to hot weather. The mean annual dispersal distance of plague is about 10 km, and topographic relief reduces the transmission rate. Larger colonies are more likely to become infected, but colony area does not affect the infectiousness of colonies. The results suggest that prairie dog movements do not drive the spread of plague through the landscape. Instead, prairie dogs are useful sentinels of plague epizootics. Simulations suggest that this model can be used for predicting long‐term colony and plague dynamics as well as for identifying which colonies are most likely to become infected in a specific year.|Climate‐Driven Spatial Dynamics of Plague among Prairie Dog Colonies|http://www.jstor.org/stable/10.1086/525051|10.1086/525051|2008-02-01|2008|['eng']|['Biological sciences - Ecology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
Bayesian small-sample estimation of a multinomial distribution based on its apparent distribution and a matrix of misclassification probabilities is considered.|Bayesian Small-Sample Estimation of Misclassified Multinomial Data|http://www.jstor.org/stable/2533215|2533215|1994-03-01|1994|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
cDNA microarrays permit us to study the expression of thousands of genes simultaneously. They are now used in many different contexts to compare mRNA levels between two or more samples of cells. Microarray experiments typically give us expression measurements on a large number of genes, say 10,000-20,000, but with few, if any, replicates for each gene. Traditional methods using means and standard deviations to detect differential expression are not completely satisfactory in this context, and so a different approach seems desirable. In this paper we present an empirical Bayes method for analysing replicated microarray data. Data from all the genes in a replicate set of experiments are combined into estimates of parameters of a prior distribution. These parameter estimates are then combined at the gene level with means and standard deviations to form a statistic B which can be used to decide whether differential expression has occurred. The statistic B avoids the problems of using averages or t-statistics. The method is illustrated using data from an experiment comparing the expression of genes in the livers of SR-BI transgenic mice with that of the corresponding wild-type mice. In addition we present the results of a simulation study estimating the ROC curve of B and three other statistics for determining differential expression: the average and two simple modifications of the usual t-statistic. B was found to be the most powerful of the four, though the margin was not great. The data were simulated to resemble the SR-BI data.|REPLICATED MICROARRAY DATA|http://www.jstor.org/stable/24307034|24307034|2002-01-01|2002|['eng']|['Applied sciences - Engineering', 'Biological sciences - Biology']|['Mathematics', 'Science and Mathematics', 'Statistics']
We consider a comparative trial of a new treatment against a standard, where the data are Poisson. The novelty is that the likelihood for the data is placed in the scientific context and the model incorporates both data uncertainty and initial uncertainty about the parameters. Both estimation and hypothesis-testing are discussed. The analysis is applied to a clinical trial of the effect of selenium on cancer deaths. Several assumptions have been made to simplify the analysis; in a final section it is shown how these might be relaxed.|A Holistic Analysis of Poisson Data with Application to a Trial of Selenium and Cancer Deaths|http://www.jstor.org/stable/25051424|25051424|2002-10-01|2002|['eng']|['Mathematics - Applied mathematics']|['Science and Mathematics', 'Statistics']
In case-control studies of gene-environment association with disease, when genetic and environmental exposures can be assumed to be independent in the underlying population, one may exploit the independence in order to derive more efficient estimation techniques than the traditional logistic regression analysis (Chatterjee and Carroll, 2005, Biometrika 92, 399-418). However, covariates that stratify the population, such as age, ethnicity and alike, could potentially lead to nonindependence. In this article, we provide a novel semiparametric Bayesian approach to model stratification effects under the assumption of gene-environment independence in the control population. We illustrate the methods by applying them to data from a population-based case-control study on ovarian cancer conducted in Israel. A simulation study is conducted to compare our method with other popular choices. The results reflect that the semiparametric Bayesian model allows incorporation of key scientific evidence in the form of a prior and offers a flexible, robust alternative when standard parametric model assumptions do not hold.|Semiparametric Bayesian Analysis of Case-Control Data under Conditional Gene-Environment Independence|http://www.jstor.org/stable/4541416|4541416|2007-09-01|2007|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
In longitudinal clinical trials, one analysis of interest is an intention-to-treat analysis, which groups subjects according to the randomized treatment regardless of whether they stayed on that treatment or not. When in addition to going off the randomized treatment subjects may also drop out of the study and be lost to follow-up, it is unclear what an intention-to-treat analysis should be. If measurements are made after treatment drop-out on a random sample of subjects who drop the treatment, then Hogan and Laird (1996, Biometrics 52, 1002-1017) present a random effects model, well suited to this type of analysis, which fits a two-piece linear spline to the data with the knot at the time the assigned treatment is dropped. This article presents a Bayesian approach to fitting a similar two-piece linear spline model and shows how the model can be applied to data that have no off-treatment observations.|A Bayesian Framework for Intent-to-Treat Analysis with Missing Data|http://www.jstor.org/stable/2534013|2534013|1998-03-01|1998|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Health status is a complex outcome, often characterized by multiple measures. When assessing changes in health status over time, multiple measures are typically collected longitudinally. Analytic challenges posed by these multivariate longitudinal data are further complicated when the outcomes are combinations of continuous, categorical, and count data. To address these challenges, we propose a fully Bayesian latent transition regression approach for jointly analyzing a mixture of longitudinal outcomes from any distribution. Health status is assumed to be a categorical latent variable, and the multiple outcomes are treated as surrogate measures of the latent health state, observed with error. Using this approach, both baseline latent health state prevalences and the probabilities of transitioning between the health states over time are modeled as functions of covariates. The observed outcomes are related to the latent health states through regression models that include subject-specific effects to account for residual correlation among repeated measures over time, and covariate effects to account for differential measurement of the latent health states. We illustrate our approach with data from a longitudinal study of back pain.|Latent Transition Regression for Mixed Outcomes|http://www.jstor.org/stable/3695448|3695448|2003-09-01|2003|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
We introduce a class of Cox cluster processes called generalised shot noise Cox processes (GSNCPs), which extends the definition of shot noise Cox processes (SNCPs) in two directions: the point process that drives the shot noise is not necessarily Poisson, and the kernel of the shot noise can be random. Thereby, a very large class of models for aggregated or clustered point patterns is obtained. Due to the structure of GSNCPs, a number of useful results can be established. We focus first on deriving summary statistics for GSNCPs and, second, on how to simulate such processes. In particular, results on first- and second-order moment measures, reduced Palm distributions, the J -function, simulation with or without edge effects, and conditional simulation of the intensity function driving a GSNCP are given. Our results are exemplified in important special cases of GSNCPs, and we discuss their relation to the corresponding results for SNCPs.|Generalised Shot Noise Cox Processes|http://www.jstor.org/stable/30037315|30037315|2005-03-01|2005|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
We present a framework that enables computer model evaluation oriented toward answering the question: Does the computer model adequately represent reality? The proposed validation framework is a six-step procedure based on Bayesian and likelihood methodology. The Bayesian methodology is particularly well suited to treating the major issues associated with the validation process: quantifying multiple sources of error and uncertainty in computer models, combining multiple sources of information, and updating validation assessments as new information is acquired. Moreover, it allows inferential statements to be made about predictive error associated with model predictions in untested situations. The framework is implemented in a test bed example of resistance spot welding, to provide context for each of the six steps in the proposed validation process.|A Framework for Validation of Computer Models|http://www.jstor.org/stable/25471307|25471307|2007-05-01|2007|['eng']|['Mathematics - Applied mathematics', 'Information science - Data products']|['Science & Mathematics', 'Statistics']
We discuss several tests to check for the presence of selectivity bias in estimators based on panel data. One approach to test for selectivity bias is to specify the selection mechanism explicitly and estimate it jointly with the model of interest. Alternatively, one can derive the asymptotically efficient LM test. Both approaches are computationally demanding. In this paper, we propose the use of simple variable addition and (quasi-) Hausman tests for selectivity bias that do not require any knowledge of the response process. We compare the power of these tests with the asymptotically efficient test using Monte Carlo methods.|Testing for Selectivity Bias in Panel Data Models|http://www.jstor.org/stable/2527133|2527133|1992-08-01|1992|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering', 'Information science - Coding theory', 'Biological sciences - Biology']|['Business & Economics', 'Business', 'Economics']
"Richardson and Green present a method of performing a Bayesian analysis of data from a finite mixture distribution with an unknown number of components. Their method is a Markov Chain Monte Carlo (MCMC) approach, which makes use of the ""reversible jump"" methodology described by Green. We describe an alternative MCMC method which views the parameters of the model as a (marked) point process, extending methods suggested by Ripley to create a Markov birth-death process with an appropriate stationary distribution. Our method is easy to implement, even in the case of data in more than one dimension, and we illustrate it on both univariate and bivariate data. There appears to be considerable potential for applying these ideas to other contexts, as an alternative to more general reversible jump methods, and we conclude with a brief discussion of how this might be achieved."|Bayesian Analysis of Mixture Models with an Unknown Number of Components- An Alternative to Reversible Jump Methods|http://www.jstor.org/stable/2673981|2673981|2000-02-01|2000|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
The velocity of climate change is defined as an instantaneous rate of change needed to maintain a constant climate. It is developed as the ratio of the temporal gradient of climate change over the spatial gradient of climate change. Ecologically, understanding these rates of climate change is critical since the range limits of plants and animals are changing in response to climate change. Additionally, species respond differently to changes in climate due to varying tolerances and adaptability. A fully stochastic hierarchical model is proposed that incorporates the inherent relationship between climate, time, and space. Space-time processes are employed to capture the spatial correlation in both the climate variable and the rate of change in climate over time. Directional derivative processes yield spatial and temporal gradients and, thus, the resulting velocities for a climate variable. The gradients and velocities can be obtained at any location in any direction and any time. In fact, maximum gradients and their directions can be obtained, hence minimum velocities. Explicit parametric forms for the directional derivative processes provide full inference on the gradients and velocities including estimates of uncertainty. The model is applied to annual average temperature across the eastern United States for the years 1963–2012. Maps of the spatial and temporal gradients are produced as well as velocities of temperature change. Supplementary materials accompanying this paper appear on-line.|Stochastic Modeling for Velocity of Climate Change|http://www.jstor.org/stable/26451836|26451836|2015-09-01|2015|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Agriculture', 'Statistics']
Advances in understanding the biological underpinnings of many cancers have led increasingly to the use of molecularly targeted anticancer therapies. Because the plateletderived growth factor receptor (PDGFR) has been implicated in the progression of prostate cancer bone métastases, it is of great interest to examine possible relationships between PDGFR inhibition and therapeutic outcomes. We analyse the association between change in activated PDGFR (phosphorylated PDGFR) and progression-free survival time based on large within-patient samples of cell-specific phosphorylated PDGFR values taken before and after treatment from each of 88 prostate cancer patients. To utilize these paired samples as covariate data in a regression model for progression-free survival time, and because the phosphorylated PDGFR distributions are bimodal, we first employ a Bayesian hierarchical mixture model to obtain a deconvolution of the pretreatment and post-treatment within-patient phosphorylated PDGFR distributions. We evaluate fits of the mixture model and a non-mixture model that ignores the bimodality by using a supnorm metric to compare the empirical distribution of each phosphorylated PDGFR data set with the corresponding fitted distribution under each model. Our results show that first using the mixture model to account for the bimodality of the within-patient phosphorylated PDGFR distributions, and then using the posterior within-patient component mean changes in phosphorylated PDGFR so obtained as covariates in the regression model for progression-free survival time, provides an improved estimation.|A Bayesian Hierarchical Mixture Model for Platelet-Derived Growth Factor Receptor Phosphorylation to Improve Estimation of Progression-Free Survival in Prostate Cancer|http://www.jstor.org/stable/40541641|40541641|2010-01-01|2010|['eng']|['Biological sciences - Biochemistry']|['Science & Mathematics', 'Statistics']
We analyze the effects of redistricting as revealed in the votes received by the Democratic and Republican candidates for state legislature. We develop measures of partisan bias and the responsiveness of the composition of the legislature to changes in statewide votes. Our statistical model incorporates a mixed hierarchical Bayesian and non-Bayesian estimation, requiring simulation along the lines of Tanner and Wong (1987). This model provides reliable estimates of partisan bias and responsiveness along with measures of their variabilities from only a single year of electoral data. This allows one to distinguish systematic changes in the underlying electoral system from typical election-to-election variability.|Estimating the Electoral Consequences of Legislative Redistricting|http://www.jstor.org/stable/2289762|2289762|1990-06-01|1990|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Large outbreaks of Q fever in The Netherlands have provided a unique opportunity for studying longitudinal serum antibody responses in patients. Results are presented of a cohort of 344 patients with acute symptoms of Q fever with three or more serum samples per patient. In all these serum samples IgM and IgG against phase 1 and 2 Coxiella burnetii were measured by an immunofluorescence assay. A mathematical model of the dynamic interaction of serum antibodies and pathogens was used in a mixed model framework to quantitatively analyse responses to C. burnetii infection. Responses show strong heterogeneity, with individual serum antibody responses widely different in magnitude and shape. Features of the response, peak titre and decay rate, are used to characterize the diversity of the observed responses. Binary mixture analysis of IgG peak levels (phases 1 and 2) reveals a class of patients with high IgG peak titres that decay slowly and may represent potential chronic cases. When combining the results of mixture analysis into an odds score, it is concluded that not only high IgG phase 1 may be predictive for chronic Q fever, but also that high IgG phase 2 may aid in detecting such putative chronic cases.|Time-course of antibody responses against Coxiella burnetii following acute Q fever|http://www.jstor.org/stable/23360553|23360553|2013-01-01|2013|['eng']|['Health sciences - Medical sciences', 'Biological sciences - Biology']|['Health Sciences', 'Medicine and Allied Health']
Underlying dynamic event processes unfolding in continuous time give rise to spatiotemporal patterns that are sometimes observable at only a few discrete times. Such event processes may be modulated simultaneously over several spatial (e.g., latitude and longitude) and temporal (e.g., age, calendar time, and cohort) dimensions. The ecological challenge is to understand the dynamic latent processes that were integrated over several dimensions (space and time) to produce the observed pattern: a so-called inverse problem. An example of such a problem is characterizing epidemiological rate processes from spatially referenced age-specific prevalence data for a wildlife disease such as chronic wasting disease (CWD). With age-specific prevalence data, the exact infection times are not observed, which complicates the direct estimation of rates. However, the relationship between the observed data and the unobserved rate variables can be described with likelihood equations. Typically, for problems with multiple timescales, the likelihoods are integral equations without closed forms. The complexity of the likelihoods often makes traditional maximum-likelihood approaches untenable. Here, using seven years of hunter-harvest prevalence data from the CWD epidemic in white-tailed deer (Odocoileus virginianus) in Wisconsin, USA, we develop and explore a Bayesian approach that allows for a detailed examination of factors modulating the infection rates over space, age, and time, and their interactions. Our approach relies on the Bayesian ability to borrow strength from neighbors in both space and time. Synthesizing a number of areas of event time analysis (current-status data, age/period/cohort models, Bayesian spatial shared frailty models), our general framework has very broad ecological applicability beyond disease prevalence data to a number of important ecological event time analyses, including general survival studies with multiple time dimensions for which existing methodology is limited. We observed strong associations of infection rates with age, gender, and location. The infection rate appears to be increasing with time. We could not detect growth hotspots, or location by time interactions, which suggests that spatial variation in infection rates is determined primarily by when the disease arrives locally, rather than how fast it grows. We emphasize assumptions and the potential consequences of their violations.|Linking process to pattern: estimating spatiotemporal dynamics of a wildlife epidemic from cross-sectional data|http://www.jstor.org/stable/27806884|27806884|2010-05-01|2010|['eng']|['Mathematics - Applied mathematics', 'Biological sciences - Biology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
Benjamini and Yekutieli suggested that it is important to account for multiplicity correction for confidence intervals when only some of the selected intervals are reported. They introduced the concept of the false coverage rate (FCR) for confidence intervals which is parallel to the concept of the false discovery rate in the multiple-hypothesis testing problem and they developed confidence intervals for selected parameters which control the FCR. Their approach requires the FCR to be controlled in the frequentist's sense, i. e. controlled for all the possible unknown parameters. In modern applications, the number of parameters could be large, as large as tens of thousands or even more, as in microarray experiments. We propose a less conservative criterion, the Bayes FCR, and study confidence intervals controlling it for a class of distributions. The Bayes FCR refers to the average FCR with respect to a distribution of parameters. Under such a criterion, we propose some confidence intervals, which, by some analytic and numerical calculations, are demonstrated to have the Bayes FCR controlled at level q for a class of prior distributions, including mixtures of normal distributions and zero, where the mixing probability is unknown. The confidence intervals are shrinkage-type procedures which are more efficient for the θ i , s that have a sparsity structure, which is a common feature of microarray data. More importantly, the centre of the proposed shrinkage intervals reduces much of the bias due to selection. Consequently, the proposed empirical Bayes intervals are always shorter in average length than the intervals of Benjamini and Yekutieli and can be only 50% or 60% as long in some cases. We apply these procedures to the data of Choe and colleagues and obtain similar results.|Empirical Bayes false coverage rate controlling confidence intervals|http://www.jstor.org/stable/41674635|41674635|2012-11-01|2012|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science and Mathematics', 'Statistics']
The financial viability of Social Security, the single largest U.S. government program, depends on accurate forecasts of the solvency of its intergenerational trust fund. We begin by detailing information necessary for replicating the Social Security Administration's (SSA's) forecasting procedures, which until now has been unavailable in the public domain. We then offer a way to improve the quality of these procedures via age- and sex-specific mortality forecasts. The most recent SSA mortality forecasts were based on the best available technology at the time, which was a combination of linear extrapolation and qualitative judgments. Unfortunately, linear extrapolation excludes known risk factors and is inconsistent with longstanding demographic patterns, such as the smoothness of age profiles. Modern statistical methods typically outperform even the best qualitative judgments in these contexts. We show how to use such methods, enabling researchers to forecast using far more information, such as the known risk factors of smoking and obesity and known demographic patterns. Including this extra information makes a substantial difference. For example, by improving only mortality forecasting methods, we predict three fewer years of net surplus, $730 billion less in Social Security Trust Funds, and program costs that are 0.66% greater for projected taxable payroll by 2031 compared with SSA projections. More important than specific numerical estimates are the advantages of transparency, replicability, reduction of uncertainty, and what may be the resulting lower vulnerability to the politicization of program forecasts. In addition, by offering with this article software and detailed replication information, we hope to marshal the efforts of the research community to include ever more informative inputs and to continue to reduce uncertainties in Social Security forecasts.|Statistical Security for Social Security|http://www.jstor.org/stable/23252682|23252682|2012-08-01|2012|['eng']|['Economics - Economic disciplines']|['Population Studies', 'Social Sciences']
1. With biodiversity facing threats, there is a need to improve reserve selection procedures. However, detailed information about different biodiversity measures (e.g. species richness) at potential sites is often lacking, and selecting areas that protect most biodiversity is difficult. To simplify matters, biodiversity surrogate species, that is, species associated with higher biodiversity than average, have been used for area selection. However, consensus about the performance of the surrogate species concept is lacking, and there are few studies investigating potential differences in the effectiveness of multiple predators as surrogates for biodiversity over large spatial scales. 2. We evaluated two avian predators, the northern goshawk Accipiter gentilis and the Ural owl Strix uralensis, as surrogates for biodiversity in the boreal forest biome in Western Finland. We used a study design including nest sites and two reference sites for each nest, the diversity of birds and wood-decaying fungi (polypores). We assessed simultaneously whether surrogacy persisted at the landscape level while moving from one vegetation zone to another. 3. We generally found more birds and polypores around the nest sites for both goshawks and Ural owls than at their respective reference sites. However, the goshawk outperformed the Ural owl. 4. Additionally, although biodiversity was found to decrease at the landscape scale as a result of a decrease in vegetation complexity with increasing longitude, the surrogacy efficiency of the raptors remained unchanged. 5. Synthesis and applications. These findings suggest that the surrogate species concept applied to raptors may be an efficient addition to methods for identifying areas of conservation priority, even across vegetation zones. We conclude that protecting areas around raptor nests is a method to consider in order to halt forest biodiversity loss. Finally, sampling biodiversity along diversity and landscape gradients can improve the necessary assessment of surrogate species.|Raptors as surrogates of biodiversity along a landscape gradient|http://www.jstor.org/stable/24032541|24032541|2014-06-01|2014|['eng']|['Biological sciences - Biogeography', 'Biological sciences - Biology', 'Biological sciences - Ecology']|['Biological Sciences', 'Ecology & Evolutionary Biology', 'Science and Mathematics']
Spatiotemporal disease mapping models have been used extensively to describe the pattern of surveillance data. They are usually formulated in a hierarchical Bayesian framework and posterior marginals are not available in closed form. Hence, the standard method for parameter estimation is Markov chain Monte Carlo algorithms. A new method for approximate Bayesian inference in latent Gaussian models using integrated nested Laplace approximations has recently been proposed as an alternative. This approach promises very precise results in short computational time. The aim of the paper is to show how integrated nested Laplace approximations can be used as an inferential tool for a variety of spatiotemporal models for the analysis of reported cases of bovine viral diarrhoea in cattle from Switzerland. Conclusions concerning the problem of under-reporting in the data are drawn via a multilevel modelling strategy. Furthermore, a comparison with Markov chain Monte Carlo methods with regard to the accuracy of the parameter estimates and the usability of both approaches in practice is conducted. Approaches to model choice using integrated nested Laplace approximations are also presented.|Using integrated nested Laplace approximations for the evaluation of veterinary surveillance data from Switzerland: a case-study|http://www.jstor.org/stable/41057574|41057574|2011-03-01|2011|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
AbstractDespite the existence of many approaches to reference-condition modeling, Bayesian statistical methods have not been used. We assessed whether a hybrid approach that combined features of existing reference-condition approaches with Bayesian model fitting and assessment of test sites could provide superior results to existing established methods. We used 4 Bayesian models of increasing complexity to develop and test reference-condition models for 5 biotic endpoints across 3 data sets. Our best models were comparable or superior to standard approaches (Benthic Assessment of Sediment, Australian River Assessment System) using the same data. Those of our models with the simplest endpoint (species richness) performed best. On average, those models with the simplest model structures also performed best, but differences in performance among models of different complexity were small. All models performed poorly at detecting the lower levels of simulated impact in the test data. However, these impacts were small relative to the variation among validation sites and consequent predictive uncertainty of the models. The Bayesian approach to reference-condition modeling shows promise as an alternative to existing methods. It also has advantages in terms of the ease of interpretation of model outputs. However, for the approach to be relevant, further development work should be driven by a perceived need to revise standard methods used by management agencies.|Bayesian reference condition models achieve comparable or superior performance to existing standard techniques|http://www.jstor.org/stable/10.1086/678949|10.1086/678949|2014-12-01|2014|['eng']|['Biological sciences - Biology', 'Applied sciences - Engineering']|['Ecology & Evolutionary Biology', 'Aquatic Sciences', 'Science & Mathematics', 'Biological Sciences']
"A major goal of evolutionary biology is to understand the dynamics of natural selection within populations. The strength and direction of selection can be described by regressing relative fitness measurements on organismal traits of ecological significance. However, many important evolutionary characteristics of organisms are complex, and have correspondingly complex relationships to fitness. Secondary sexual characteristics such as mating displays are prime examples of complex traits with important consequences for reproductive success. Typically, researchers atomize sexual traits such as mating signals into a set of measurements including pitch and duration, in order to include them in a statistical analysis. However, these researcher-defined measurements are unlikely to capture all of the relevant phenotypic variation, especially when the sources of selection are incompletely known. In order to accommodate this complexity we propose a Bayesian dimension-reduced spectrogram generalized linear model that directly incorporates representations of the entire phenotype (one-dimensional acoustic signal) into the model as a predictor while accounting for multiple sources of uncertainty. The first stage of dimension reduction is achieved by treating the spectrogram as an ""image"" and finding its corresponding empirical orthogonal functions. Subsequently, further dimension reduction is accomplished through model selection using stochastic search variable selection. Thus, the model we develop characterizes key aspects of the acoustic signal that influence sexual selection while alleviating the need to extract higher-level signal traits a priori. This facet of our approach is fundamental and has the potential to provide additional biological insight, as is illustrated in our analysis."|Modeling Complex Phenotypes: Generalized Linear Models Using Spectrogram Predictors of Animal Communication Signals|http://www.jstor.org/stable/40962463|40962463|2010-09-01|2010|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
When costs of producing male versus female offspring differ, parents may vary allocation of resources between sons and daughters. We tested leading sex-allocation theories using an information-theoretic approach and Bayesian hierarchical models to analyse litter sex ratios (proportion males) at weaning for 1,049 litters over 24 years from a population of Richardson's ground squirrels (Urocitellus richardsonii), a polygynandrous, annually reproducing mammal in which litter size averages from six to seven offspring and sons are significantly heavier than daughters at birth and weaning. The model representing random Mendelian sex-chromosome assortment fit the data best; a homeostatic model received similar support but other models performed poorly. Embryo resorption was rare, and 5 years of litter data in a second population revealed no differences in litter size or litter sex ratio between birth and weaning, suggesting that litter size and sex ratio are determined in early pregnancy. Sex ratio did not vary with litter size at weaning in any of 29 years, and the observed distribution of sex ratios did not differ significantly from the binomial distribution for any litter size. For 1,580 weaned litters in the two populations, average sex ratio deviated from parity in only 3 of 29 years. Heavier females made a greater reproductive investment than lighter females, weaning larger and heavier litters composed of smaller sons and daughters, but litter sex ratio was positively related to maternal mass in only 2 of 29 years. Such occasional significant patterns emphasize the importance of multi-season studies in distinguishing infrequent events from normal patterns.|Litter sex ratios in Richardson's ground squirrels: long-term data support random sex allocation and homeostasis|http://www.jstor.org/stable/24035331|24035331|2014-04-01|2014|['eng']|['Physical sciences - Astronomy']|['Biological Sciences', 'Ecology & Evolutionary Biology', 'Science and Mathematics']
Bayesian methods for the Jelinski and Moranda and the Littlewood and Verrall models in software reliability are studied. A Gibbs sampling approach is employed to compute the Bayes estimates. In addition, prediction of future failure times and future reliabilities is examined. Model selection based on the mean squared prediction error and the prequential likelihood of the conditional predictive ordinates is developed.|Bayesian Computation of Software Reliability|http://www.jstor.org/stable/1390628|1390628|1995-03-01|1995|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Science & Mathematics', 'Computer Science', 'Statistics']
In some types of cancer chemoprevention experiments and short-term carcinogenicity bioassays, the data consist of the number of observed tumors per animal and the times at which these tumors were first detected. In such studies, there is interest in distinguishing between treatment effects on the number of tumors induced by a known carcinogen and treatment effects on the tumor growth rate. Since animals may die before all induced tumors reach a detectable size, separation of these effects can be difficult. This paper describes a flexible parametric model for data of this type. Under our model, the tumor detection times are realizations of a delayed Poisson process that is characterized by the age-specific tumor induction rate and a random latency interval between tumor induction and detection. The model accommodates distinct treatment and animal-specific effects on the number of induced tumors (multiplicity) and the time to tumor detection (growth rate). A Gibbs sampler is developed for estimation of the posterior distributions of the parameters. The methods are illustrated through application to data from a breast cancer chemoprevention experiment.|Distinguishing Effects on Tumor Multiplicity and Growth Rate in Chemoprevention Experiments|http://www.jstor.org/stable/2677038|2677038|2000-12-01|2000|['eng']|['Biological sciences - Biology', 'Health sciences - Health and wellness']|['Science & Mathematics', 'Statistics']
The Markov chain simulation method has been successfully used in many problems, including some that arise in Bayesian statistics. We give a self-contained proof of the convergence of this method in general state spaces under conditions that are easy to verify.|On the Convergence of the Markov Chain Simulation Method|http://www.jstor.org/stable/2242609|2242609|1996-02-01|1996|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Electoral systems translate citizens' votes into seats in the legislature, and are thus critical components of democracies. But electoral systems can be unfair, insulating incumbents from adverse electoral trends, or biasing the mapping of votes to seats in favour of one party. I assess methods for measuring bias and responsiveness in electoral systems, highlighting the limitations of the popular 'multi-year' and 'uniform swing' methods. I advocate an approach that incorporates constituency-level and jurisdiction-wide variation in party's vote shares. I show how this method can be used to elaborate both the extent and consequences of malapportionment. I then present election-by-election estimates of partisan bias and responsiveness for ninety-three state and federal elections in Australia since 1949. The empirical results reported show that the coalition parties have generally 'out-biased' the Australian Labor party, despite some notable pro-ALP biases. The overall extent of partisan bias in Australian electoral systems, however, has generally diminished in magnitude over time.|Measuring Electoral Bias: Australia, 1949-93|http://www.jstor.org/stable/194252|194252|1994-07-01|1994|['eng']|['Political science - Government']|['Political Science', 'Social Sciences']
A novel method is proposed to compute the Bayes estimate for a logistic Gaussian process prior for density estimation. The method gains speed by drawing samples from the posterior of a finite-dimensional surrogate prior, which is obtained by imputation of the underlying Gaussian process. We establish that imputation results in quite accurate computation. Simulation studies show that accuracy and high speed can be combined. This fact, along with known flexibility of the logistic Gaussian priors for modeling smoothness and recent results on their large support, makes these priors and the resulting density estimate very attractive.|Towards a Faster Implementation of Density Estimation with Logistic Gaussian Process Priors|http://www.jstor.org/stable/27594263|27594263|2007-09-01|2007|['eng']|['Mathematics - Mathematical objects']|['Science & Mathematics', 'Computer Science', 'Statistics']
Assessing natural selection on a phenotypic trait in wild populations is of primary importance for evolutionary ecologists. To cope with the imperfect detection of individuals inherent to monitoring in the wild, we develop a nonparametric method for evaluating the form of natural selection on a quantitative trait using mark-recapture data. Our approach uses penalized splines to achieve flexibility in exploring the form of natural selection by avoiding the need to specify an a priori parametric function. If needed, it can help in suggesting a new parametric model. We employ Markov chain Monte Carlo sampling in a Bayesian framework to estimate model parameters. We illustrate our approach using data for a wild population of sociable weavers (Philetairus socius) to investigate survival in relation to body mass. In agreement with previous parametric analyses, we found that lighter individuals showed a reduction in survival. However, the survival function was not symmetric, indicating that body mass might not be under stabilizing selection as suggested previously.|Nonparametric Estimation of Natural Selection on a Quantitative Trait Using Mark-Recapture Data|http://www.jstor.org/stable/4095308|4095308|2006-03-01|2006|['eng']|['Biological sciences - Biology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
Cohen's kappa coefficient is a widely popular measure for chance-corrected nominal scale agreement between two raters. This article describes Bayesian analysis for kappa that can be routinely implemented using Markov chain Monte Carlo (MCMC) methodology. We consider the case of m ≥ 2 independent samples of measured agreement, where in each sample a given subject is rated by two rating protocols on a binary scale. A major focus here is on testing the homogeneity of the kappa coefficient across the different samples. The existing frequentist tests for this case assume exchangeability of rating protocols, whereas our proposed Bayesian test does not make any such assumption. Extensive simulation is carried out to compare the performances of the Bayesian and the frequentist tests. The developed methodology is illustrated using data from a clinical trial in ophthalmology.|Bayesian Inference for Kappa from Single and Multiple Studies|http://www.jstor.org/stable/2677003|2677003|2000-06-01|2000|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
Many studies have failed to detect costs of defense and some have even found a positive correlation between growth and the concentrations of chemical defenses. These studies contradict the theoretical assumption that anti-herbivore defenses are costly—produced at the expense of growth and/or reproduction. Costs, however, may be transient and therefore difficult to detect. Here we tested the hypothesis that costs of defense would be pronounced early in development when root growth is prioritized (high percent root allocation), but not later in development. To test this hypothesis, we grew F 2 hybrid willow seedlings from five different families, and harvested cohorts of even-aged seedlings after 6, 7, 8 and 9 weeks of growth. Seedlings were divided into root and shoot tissue and shoots were analyzed for phenolics (condensed tannins and phenolic glycosides). We found evidence for transient costs of defense. The concentrations of phenolics were negatively correlated with total biomass, shoot biomass, and the proportion of biomass allocated to roots in week 6. After week 6, however, the concentrations of phenolics were positively correlated with shoot biomass and total biomass, while phenolics were uncorrelated with the proportion of biomass allocated to roots. These results, the first ever, to our knowledge, with woody plants, suggest that costs of defense were transient; specifically, costs were found in early development, when root establishment was a priority. Our findings suggest that studies should focus more on trade-offs early in plant development.|Growth and chemical defense in willow seedlings: trade-offs are transient|http://www.jstor.org/stable/40606586|40606586|2010-06-01|2010|['eng']|['Biological sciences - Biology']|['Biological Sciences', 'Ecology & Evolutionary Biology', 'Science and Mathematics']
The prediction of sea wave parameters is important for the planning, design, and operation of coastal and ocean structures. Many empirical methods, numerical models, and soft computing techniques for wave parameter forecasting have been investigated, but such forecasting is still a complex ocean engineering problem. In this study, the support vector machine (SVM) approach, using various kernel functions, is presented for wave parameters prediction in an attempt to suggest a new model with superior explanatory power and stability. A genetic algorithm is used to determine the free parameters of the SVM. The SVM results are compared with the field data and with backpropagation and cascade-correlation neural network models. Among the models, the SVM with a radial basis function kernel provides the best generalization capability and the lowest prediction error and can therefore be most successfully used for the prediction of wave parameters.|Sea Wave Parameters Prediction by Support Vector Machine Using a Genetic Algorithm|http://www.jstor.org/stable/43432902|43432902|2015-07-01|2015|['eng']|['Applied sciences - Engineering']|['Ecology & Evolutionary Biology', 'Aquatic Sciences', 'Science & Mathematics', 'Biological Sciences']
Empirical Bayes research has expanded significantly since the ground-breaking paper (1956) of Herbert Robbins, and its province currently incorporates a range of methods in statistics. For example, Stein's famous estimator (James and Stein, 1961) is now best understood from the parametric empirical Bayes viewpoint. Appropriate generalizations and applications of Stein's rule in other settings (Efron and Morris, 1973, 1975; Morris, 1983b) are facilitated dramatically by the empirical Bayes viewpoint, relative to the frequentist perspective -- this will be indicated below.|Empirical Bayes: A Frequency-Bayes Compromise|http://www.jstor.org/stable/4355531|4355531|1986-01-01|1986|['eng']|['Mathematics - Mathematical logic']|['Mathematics', 'Science & Mathematics', 'Statistics']
In geostatistics, it is common to model spatially distributed phenomena through an underlying stationary and isotropic spatial process. However, these assumptions are often untenable in practice because of the influence of local effects in the correlation structure. Therefore, it has been of prolonged interest in the literature to provide flexible and effective ways to model non-stationarity in the spatial effects. Arguably, due to the local nature of the problem, we might envision that the correlation structure would be highly dependent on local characteristics of the domain of study, namely, the latitude, longitude and altitude of the observation sites, as well as other locally defined covariate information. In this work, we provide a flexible and computationally feasible way for allowing the correlation structure of the underlying processes to depend on local covariate information. We discuss the properties of the induced covariance functions and methods to assess its dependence on local covariate information. The proposed method is used to analyze daily ozone in the southeast United States.|A CLASS OF COVARIATE-DEPENDENT SPATIOTEMPORAL COVARIANCE FUNCTIONS FOR THE ANALYSIS OF DAILY OZONE CONCENTRATION|http://www.jstor.org/stable/23069336|23069336|2011-12-01|2011|['eng']|['Physical sciences - Astronomy']|['Mathematics', 'Science & Mathematics', 'Statistics']
BACKGROUND Local migration in developing-world settings, particularly among rural populations, is an important yet understudied demographic process. Research on migration in such populations can help us test and inform anthropological and demographic theory. Furthermore, it can lead to a better understanding of modern population distributions and epidemiologic landscapes. OBJECTIVE The purpose of this study was to investigate the potential relationships between household- and individual-level factors on out-migration among Karen villagers along the Thai-Myanmar border. METHODS We used a random effects hazard model to investigate the relationship of household consumer-producer (C/P) ratios, the number of household members, and an individual’s sex on the odds of outmigration. We then used simulations in order to test the sensitivity of our model to our C/P ratio weighting scheme. RESULTS We found that the number of household members is predictive of increased out-migration. Household C/P ratios were positively associated with out-migration in children but negatively associated with out-migration in working age adults. Finally, adult males were much more likely to move out of the household than were adult females. CONCLUSIONS While household-level factors are important with regard to out-migration, the relationships between such household-level factors and out-migration are complex and vary by the individual’s age and sex. Our study offers two novel concepts to household demography and migration studies. First, this study offers a new approach to evaluating weighting schemes for C/P ratios. Second, we show that household level factors are important at units of time (two-week intervals) that are not normally studied by demographers.|Household ecology and out-migration among ethnic Karen along the Thai-Myanmar border|http://www.jstor.org/stable/26348230|26348230|2014-01-01|2014|['eng']|['Economics - Economic disciplines']|['Population Studies', 'Social Sciences']
We present a Bayesian hierarchical modeling approach to paleoclimate reconstruction using borehole temperature profiles. The approach relies on modeling heat conduction in solids via the heat equation with step function, surface boundary conditions. Our analysis includes model error and assumes that the boundary conditions are random processes. The formulation also enables separation of measurement error and model error. We apply the analysis to data from nine borehole temperature records from the San Rafael region in Utah. We produce ground surface temperature histories with uncertainty estimates for the past 400 years. We pay special attention to use of prior parameter models that illustrate borrowing strength in a combined analysis for all nine boreholes. In addition, we review selected sensitivity analyses.|BAYESIAN HIERARCHICAL MODELING FOR TEMPERATURE RECONSTRUCTION FROM GEOTHERMAL DATA|http://www.jstor.org/stable/23024853|23024853|2011-06-01|2011|['eng']|['Mathematics - Applied mathematics']|['Mathematics', 'Science & Mathematics', 'Statistics']
Survival data often contain small-area geographical or spatial information, such as the residence of individuals. In many cases, the impact of such spatial effects on hazard rates is of considerable substantive interest. Therefore, extensions of known survival or hazard rate models to spatial models have been suggested. Mostly, a spatial component is added to the usual linear predictor of the Cox model. In this article flexible continuous-time geoadditive models are proposed, extending the Cox model with respect to several aspects often needed in applications. The common linear predictor is generalized to an additive predictor, including nonparametric components for the log-baseline hazard, time-varying effects, and possibly nonlinear effects of continuous covariates or further time scales, and a spatial component for geographical effects. In addition, uncorrelated frailty effects or nonlinear two-way interactions can be incorporated. Inference is developed within a unified fully Bayesian framework. Penalized regression splines and Markov random fields are suggested as basic building blocks, and geostatistical (kriging) models are also considered. Posterior analysis uses computationally efficient Markov chain Monte Carlo sampling schemes. Smoothing parameters are an integral part of the model and are estimated automatically. Propriety of posteriors is shown under fairly general conditions, and practical performance is investigated through simulation studies. Our approach is applied to data from a case study in London and Essex that aims to estimate the effect of area of residence and further covariates on waiting times to coronary artery bypass grafting. Results provide clear evidence of nonlinear time-varying effects, and considerable spatial variability of waiting times to bypass grafting.|Geoadditive Survival Models|http://www.jstor.org/stable/27590784|27590784|2006-09-01|2006|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
We propose a unified framework for the analysis of chromatin (Ch) immunoprecipitation (IP) microarray (ChIP-chip) data for detecting transcription factor binding sites (TFBSs) or motifs. ChIP-chip assays are used to focus the genomewide search for TFBSs by isolating a sample of DNA fragments with TFBSs and applying this sample to a microarray with probes corresponding to tiled segments across the genome. Present analytical methods use a two-step approach: (i) analyze array data to estimate IP-enrichment peaks then (ii) analyze the corresponding sequences independently of intensity information. The proposed model integrates peak finding and motif discovery through a unified Bayesian hidden Markov model (HMM) framework that accommodates the inherent uncertainty in both measurements. A Markov chain Monte Carlo algorithm is formulated for parameter estimation, adapting recursive techniques used for HMMs. In simulations and applications to a yeast RAP1 dataset, the proposed method has favorable TFBS discovery performance compared to currently available two-stage procedures in terms of both sensitivity and specificity.|A Bayesian Hidden Markov Model for Motif Discovery through Joint Modeling of Genomic Sequence and ChIP-Chip Data|http://www.jstor.org/stable/20640629|20640629|2009-12-01|2009|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Classical gradient analysis continues to be used to explore and test theories and models in community ecology. Yet the foundations of classical gradient analysis were developed at a time when computational power was limited, relative to current computational power. I argue that this history has left a lasting legacy on the field. Consequently, many gradient analyses do not to take advantage of current computer technology. Here I show how to use computationally intensive Markov-chain Monte Carlo methods to improve gradient analyses of presence–absence community data. The methods that I use were developed by quantitative social scientists in the early 1990s, and therefore tested and efficient software already exists for practical data analysis. As an example, I analyze the classic dune meadow vegetation data. A main advantage of the Bayesian approach to indirect gradient analysis is that, unlike essentially all classical indirect methods, it is able to make empirically testable probabilistic predictions of observed species occurrence patterns. The Bayesian approach also poses challenges for statistical ecology. In particular, the development of Markov-chain Monte Carlo methods for a wider class of Bayesian indirect gradient analysis models would permit more flexible approaches to generating probabilistic predictions.|Indirect gradient analysis by Markov-chain Monte Carlo|http://www.jstor.org/stable/24557662|24557662|2015-05-01|2015|['eng']|['Applied sciences - Engineering']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Botany & Plant Sciences', 'Biological Sciences']
1. Overabundant wildlife can cause economic and ecological damage. Therefore population control typically seeks to maintain species' abundance within desired control limits. Efficient control requires targets, methods for estimating population size before and after control, and reliable means of forecasting population size. Demographic stochasticity, environmental variability and model uncertainty complicate these tasks. Monitoring provides critical feedback in the control process, yet examples of integrated monitoring and management are scarce. 2. We developed an integrated Bayesian population modelling and monitoring algorithm to assist with dynamic cull control of an overabundant population. We describe components of the control algorithm and their combination to produce a structured, sequential prescription for implementing control of a kangaroo population. We demonstrate its application within a single management year and evaluate its performance over a multi-year horizon under a range of scenarios reflecting uncertainties about population dynamics. 3. Simulation testing of the algorithm demonstrates that it provides a coherent, flexible, efficient and robust basis for managing population control. It is coherent in that connections between management objectives, models and operating rules are explicit and logically integrated. It is flexible in that the management objectives can be freely varied. It is both cost and operationally efficient because: (i) it avoids the need for an expensive, dedicated sampling process to estimate population size prior to culling; (ii) a relatively small number of culls produces reasonable population size estimates and (iii) the estimation by removal process enables direct assessment of whether control has been achieved. Lastly, it is robust because even when there is substantial uncertainty about system state and dynamics, the algorithm performs well at keeping the population under control over the duration of the management horizon. 4. Synthesis and applications. We provide a general and flexible framework for integrated monitoring and culling when the objective is to keep a species' abundance within control limits. Our framework explicitly deals with uncertainty arising from demographic stochasticity, ecological complexity and lack of knowledge, and provides the foundation for maximizing efficiency and costeffectiveness of control operations. Our approach could be applied in any instances where control is effected via culling.|Linking modelling, monitoring and management: an integrated approach to controlling overabundant wildlife|http://www.jstor.org/stable/40958946|40958946|2010-12-01|2010|['eng']|['Applied sciences - Engineering', 'Biological sciences - Biology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
"This article introduces a Bayesian methodology for the prediction for computer experiments having quantitative and qualitative inputs. The proposed model is a hierarchical Bayesian model with conditional Gaussian stochastic process components. For each of the qualitative inputs, our model assumes that the outputs corresponding to different levels of the qualitative input have ""similar"" functional behavior in the quantitative inputs. The predictive accuracy of this method is compared with the predictive accuracies of alternative proposals in examples. The method is illustrated in a biomechanical engineering application."|Prediction for Computer Experiments Having Quantitative and Qualitative Input Variables|http://www.jstor.org/stable/40586622|40586622|2009-08-01|2009|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Multi-subject functional magnetic resonance imaging (fMRI) data has been increasingly used to study the population-wide relationship between human brain activity and individual biological or behavioral traits. A common method is to regress the scalar individual response on imaging predictors, known as a scalar-on-image (SI) regression. Analysis and computation of such massive and noisy data with complex spatio-temporal correlation structure is challenging. In this article, motivated by a psychological study on human affective feelings using fMRI, we propose a joint Ising and Dirichlet Process (Ising-DP) prior within the framework of Bayesian stochastic search variable selection for selecting brain voxels in high-dimensional SI regressions. The Ising component of the prior makes use of the spatial information between voxels, and the DP component groups the coefficients of the large number of voxels to a small set of values and thus greatly reduces the posterior computational burden. To address the phase transition phenomenon of the Ising prior, we propose a new analytic approach to derive bounds for the hyperparameters, illustrated on 2- and 3-dimensional lattices. The proposed method is compared with several alternative methods via simulations, and is applied to the fMRI data collected from the KLIFF hand-holding experiment.|SPATIAL BAYESIAN VARIABLE SELECTION AND GROUPING FOR HIGH-DIMENSIONAL SCALAR-ON-IMAGE REGRESSION|http://www.jstor.org/stable/24522598|24522598|2015-06-01|2015|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Mathematics', 'Science & Mathematics', 'Statistics']
This article describes a new class of prior distributions for nonparametric function estimation. The unknown function is modeled as a limit of weighted sums of kernels or generator functions indexed by continuous parameters that control local and global features such as their translation, dilation, modulation and shape. Lévy random fields and their stochastic integrals are employed to induce prior distributions for the unknown functions or, equivalently, for the number of kernels and for the parameters governing their features. Scaling, shape, and other features of the generating functions are location-specific to allow quite different function properties in different parts of the space, as with wavelet bases and other methods employing overcomplete dictionaries. We provide conditions under which the stochastic expansions converge in specified Besov or Sobolev norms. Under a Gaussian error model, this may be viewed as a sparse regression problem, with regularization induced via the Lévy random field prior distribution. Posterior inference for the unknown functions is based on a reversible jump Markov chain Monte Carlo algorithm. We compare the Lévy Adaptive Regression Kernel (LARK) method to wavelet-based methods using some of the standard test functions, and illustrate its flexibility and adaptability in nonstationary applications.|STOCHASTIC EXPANSIONS USING CONTINUOUS DICTIONARIES: LÉVY ADAPTIVE REGRESSION KERNELS|http://www.jstor.org/stable/23033588|23033588|2011-08-01|2011|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
The problem of combining information related to I binomial experiments, each having a distinct probability of success θi, is considered. Instead of using a standard exchangeable prior for θ = (θ1, ..., θI), we propose a more flexible distribution that takes into account various degrees of similarity among the θi's. Using ideas developed by Malec and Sedransk, we consider a partition g of the experiments and take the θi's belonging to the same partition subset to be exchangeable and the θi's belonging to distinct subsets to be independent. Next we perform Bayesian inference on θ conditional on g. Of course, one is typically uncertain about which partition to use, and so a prior distribution is assigned on a set of plausible partitions g. The final inference on θ is obtained by combining the conditional inferences according to the posterior distribution of g. The methodology adopted in this article offers a wide flexibility in structuring the dependence among the θi's. This allows the estimate of θi to borrow strength from all other experiments according to an adaptive process governed by the data themselves. The method may be usefully applied to the analysis of binary response variables in the presence of categorical covariates. The latter are used to identify a collection of suitable partitions g, representing factor main effects and interactions, whose relevance will be summarized in the posterior distribution of g. Besides providing novel interpretations on the role played by the various factors, the procedure will also produce parameter estimates that may differ, sometimes in an appreciable manner, from those obtained using more traditional techniques. Finally, three real data sets are used to illustrate the methodology and compare it with other approaches, such as empirical Bayes (both parametric and nonparametric), logistic regression, and multiple shrinkage estimators.|A Bayesian Method for Combining Results from Several Binomial Experiments|http://www.jstor.org/stable/2291328|2291328|1995-09-01|1995|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
The accuracy of U.S. Social Security Administration (SSA) demographic and financial forecasts is crucial for the solvency of its Trust Funds, other government programs, industry decision-making, and the evidence base of many scholarly articles. Because SSA makes public insufficient replication information and uses antiquated statistical forecasting methods, no external group has ever been able to produce fully independent forecasts or evaluations of policy proposals to change the system. Yet, no systematic evaluation of SSA forecasts has ever been published by SSA or anyone else–until a companion paper to this one. We show that SSA's forecasting errors were approximately unbiased until about 2000, but then began to grow quickly, with increasingly overconfident uncertainty intervals. Moreover, the errors are largely in the same direction, making the Trust Funds look healthier than they are. We extend and then explain these findings with evidence from a large number of interviews with participants at every level of the forecasting and policy processes. We show that SSA's forecasting procedures meet all the conditions the modern social-psychology and statistical literatures demonstrate make bias likely. When those conditions mixed with potent new political forces trying to change Social Security, SSA's actuaries hunkered down, trying hard to insulate their forecasts from strong political pressures. Unfortunately, this led the actuaries into not incorporating the fact that retirees began living longer lives and drawing benefits longer than predicted. We show that fewer than 10% of their scorings of major policy proposals were statistically different from random noise as estimated from their policy forecasting error. We also show that the solution to this problem involves SSA or Congress implementing in government two of the central projects of political science over the last quarter century: (1) transparency in data and methods and (2) replacing with formal statistical models large numbers of ad hoc qualitative decisions too complex for unaided humans to make optimally.|Explaining Systematic Bias and Nontransparency in U.S. Social Security Administration Forecasts|http://www.jstor.org/stable/24573165|24573165|2015-07-01|2015|['eng']|['Information science - Information analysis', 'Business - Business administration']|['Political Science', 'Social Sciences']
New media outlets have been deemed a vital instrument for protesters and opposition groups to coordinate activities in the recent civilian uprisings in the Middle East and North Africa. But what happens when regimes respond by shutting down the internet? I argue that governments have a strategic incentive to implement internet blackouts in conjunction with larger repressive operations against violent opposition forces. Short-term intermissions in communication channels are expected to decrease opposition groups' capabilities to successfully coordinate and implement attacks against the state, allowing regime forces to strengthen their position. Network blackouts should consequently be accompanied by significant increases in military activity. Analyzing daily documented killings by the government in the Syrian civil war, I find that blackouts occur in conjunction with significantly higher levels of state repression, most notably in areas where government forces are actively fighting violent opposition groups. In addition, I estimate the number of undocumented conflict fatalities prior to and during network blackouts to test whether they are implemented to hide atrocities from outside observers, and find no support for this hypothesis. The results indicate that such network blackouts constitute a part of the military's strategy to target and weaken opposition groups, where the underreporting of violence is not systematically linked to outages.|Pulling the plug: Network disruptions and violence in civil conflict|http://www.jstor.org/stable/24557405|24557405|2015-05-01|2015|['eng']|['Law - Computer law']|['Peace & Conflict Studies', 'Political Science', 'Social Sciences']
With many predictors, choosing an appropriate subset of the covariates is a crucial—and difficult—step in nonparametric regression. We propose a Bayesian nonparametric regression model for curve fitting and variable selection. We use the smoothing splines ANOVA framework to decompose the regression function into interpretable main effect and interaction functions, and use stochastic search variable selection through Markov chain Monte Carlo sampling to search for models that fit the data well. We also show that variable selection is highly sensitive to hyperparameter choice, and develop a technique for selecting hyperparameters that control the long-run false-positive rate. We use our method to build an emulator for a complex computer model for two-phase fluid flow.|Variable Selection in Bayesian Smoothing Spline ANOVA Models: Application to Deterministic Computer Codes|http://www.jstor.org/stable/40586589|40586589|2009-05-01|2009|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Bayesian analysis of time-to-event data, usually called survival analysis, has received increasing attention in the last years. In Cox-type models it allows to use information from the full likelihood instead of from a partial likelihood, so that the baseline hazard function and the model parameters can be jointly estimated. In general, Bayesian methods permit a full and exact posterior inference for any parameter or predictive quantity of interest. On the other side, Bayesian inference often relies on Markov chain Monte Carlo (MCMC) techniques which, from the user point of view, may appear slow at delivering answers. In this article, we show how a new inferential tool named integrated nested Laplace approximations can be adapted and applied to many survival models making Bayesian analysis both fast and accurate without having to rely on MCMC-based inference.|Approximate Bayesian Inference for Survival Models|http://www.jstor.org/stable/23015578|23015578|2011-09-01|2011|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Motivated by calibration problems in near-infrared (NIR) spectroscopy, we consider the linear regression setting in which the many predictor variables arise from sampling an essentially continuous curve at equally spaced points and there may be multiple predictands. We tackle this regression problem by calculating the wavelet transforms of the discretized curves, then applying a Bayesian variable selection method using mixture priors to the multivariate regression of predictands on wavelet coefficients. For prediction purposes, we average over a set of likely models. Applied to a particular problem in NIR spectroscopy, this approach was able to find subsets of the wavelet coefficients with overall better predictive performance than the more usual approaches. In the application, the available predictors are measurements of the NIR reflectance spectrum of biscuit dough pieces at 256 equally spaced wavelengths. The aim is to predict the composition (i.e., the fat, flour, sugar, and water content) of the dough pieces using the spectral variables. Thus we have a multivariate regression of four predictands on 256 predictors with quite high intercorrelation among the predictors. A training set of 39 samples is available to fit this regression. Applying a wavelet transform replaces the 256 measurements on each spectrum with 256 wavelet coefficients that carry the same information. The variable selection method could use subsets of these coefficients that gave good predictions for all four compositional variables on a separate test set of samples. Selecting in the wavelet domain rather than from the original spectral variables is appealing in this application, because a single wavelet coefficient can carry information from a band of wavelengths in the original spectrum. This band can be narrow or wide, depending on the scale of the wavelet selected.|Bayesian Wavelet Regression on Curves with Application to a Spectroscopic Calibration Problem|http://www.jstor.org/stable/2670278|2670278|2001-06-01|2001|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
There are several challenges to testing the effectiveness of group-therapy-based interventions in alcohol and other drug use treatment settings. Enrolment into alcohol and other drug use therapy groups typically occurs on an open (rolling) basis. Changes in therapy group membership induce a complex correlation structure between client outcomes, with relatively small numbers of clients attending each therapy group session. Primary outcomes are measured post treatment, so each datum reflects the effect of all sessions attended by a client. The number of post-treatment outcomes assessments is typically very limited. The first feature of our modelling approach relaxes the assumption of independent random effects in the standard multiple-membership model by employing conditional auto-regression to model correlation in random-therapy-group session effects associated with clients' attendance of common group therapy sessions. A second feature specifies a longitudinal growth model under which the posterior distribution of client-specific random effects, or growth parameters, is modelled nonparametrically. The Dirichlet process prior helps to overcome limitations of standard parametric growth models given limited numbers of longitudinal assessments. We motivate and illustrate our approach with a data set from a study of group cognitive behavioural therapy to reduce depressive symptoms among residential alcohol and other drug use treatment clients.|Bayesian hierarchical semiparametric modelling of longitudinal post-treatment outcomes from open enrolment therapy groups|http://www.jstor.org/stable/43965662|43965662|2013-06-01|2013|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Agent-based models have been used to mimic natural processes in a variety of fields, from biology to social science. By specifying mechanistic models that describe how small-scale processes function and then scaling them up, agent-based approaches can result in very complicated large-scale behavior while often relying on only a small set of initial conditions and intuitive rules. Although many agent-based models are used strictly in a simulation context, statistical implementations are less common. To characterize complex dynamic processes, such as the spread of epidemics, we present a hierarchical Bayesian framework for formal statistical agent-based modeling using spatiotemporal binary data. Our approach is based on an intuitive parameterization of the system dynamics and can explicitly accommodate directionally varying dispersal, long distance dispersal, and spatial heterogeneity.|Statistical Agent-Based Models for Discrete Spatio-Temporal Systems|http://www.jstor.org/stable/29747023|29747023|2010-03-01|2010|['eng']|['Applied sciences - Engineering', 'Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
We develop a mixed model approach of quantitative trait locus (QTL) mapping for a hybrid population derived from the crosses of two or more distinguished outbred populations. Under the mixed model, we treat the mean allelic value of each source population as the fixed effect and the allelic deviations from the mean as random effects so that we can partition the total genetic variance into between- and within-population variances. Statistical inference of the QTL parameters is obtained by using the Bayesian method implemented by Markov chain Monte Carlo (MCMC). This unified QTL mapping algorithm treats the fixed and random model approaches as special cases of the general mixed model methodology. Utility and flexibility of the method are demonstrated by using a set of simulated data.|Mixed Model Analysis of Quantitative Trait Loci|http://www.jstor.org/stable/2666332|2666332|2000-12-19|2000|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Biological Sciences', 'General Science']
"New empirical models of consumer demand that incorporate social effects seek to measure the causal effect of past adopter's behavior—the ""installed-base""—on current adoption behavior. Identifying such causal effects is challenging due to several alternative confounds that generate correlation in agents' actions. In the absence of experimental variation, a preferred solution has been to control for these spurious correlations using a rich specification of fixed effects. The authors show that fixedeffects estimators of this sort are inconsistent in the presence of installed-base effects; in simulations, random-effects specifications perform even worse. The analysis reveals the tension the applied empiricist faces in this area: a rich control for unobservables increases the credibility of the reported causal effects, but the incorporation of these controls introduces biases of a new kind in this class of models. The authors present two solutions: a modified version of an instrumental variable approach and a new bias-correction approach, both of which deliver consistent estimates of causal installed-base effects. The empirical application to the adoption of the Toyota Prius Hybrid in California shows evidence for social influence in diffusion and reveals that implementing the bias correction reverses the sign of the measured installed-base effect. The authors also discuss implications of the results for identification of models in marketing involving state dependence in demand, and incorporating discrete games of strategic interaction."|Estimating Causal Installed-Base Effects: A Bias-Correction Approach|http://www.jstor.org/stable/42003066|42003066|2013-02-01|2013|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Business', 'Business & Economics Collection', 'Marketing & Advertising']
Assessing potential association between exposures to complex mixtures and health outcomes may be complicated by a lack of knowledge of causal components of the mixture, highly correlated mixture components, potential synergistic effects of mixture components, and difficulties in measurement. We extend recently proposed nonparametric Bayes shrinkage priors for model selection to investigations of complex mixtures by developing a formal hierarchical modeling framework to allow different degrees of shrinkage for main effects and interactions and to handle truncation of exposures at a limit of detection. The methods are used to shed light on data from a study of endometriosis and exposure to environmental polychlorinated biphenyl congeners.|Nonparametric Bayes Shrinkage for Assessing Exposures to Mixtures Subject to Limits of Detection|http://www.jstor.org/stable/25680610|25680610|2010-07-01|2010|['eng']|['Applied sciences - Engineering', 'Biological sciences - Biology']|['Medicine & Allied Health', 'Health Sciences']
This paper reviews various approaches to the modelling of microstructure evolution in hot deformation, for the purpose of predicting the flow stress during deformation or for predicting the subsequent annealing behaviour. Two contrasting approaches are discussed, and illustrated for the example of hot plane-strain compression testing of Al-Mg alloy. These approaches are (i) physically based state variable models, in which the microstructure and property evolution is modelled explicitly; and (ii) advanced statistical methods, for linking processing conditions empirically to properties, or to annealing rate and final microstructure. The state variable models illustrate some general features of microstructure modelling and the level of experimental work that goes with it. Of particular importance are the accuracy of the data used to calibrate or validate a model, the implications that this makes on the volume of data needed, and the viable level of detail in the model that can realistically be verified. Various sensitivity analyses will be used to illustrate the need for a balanced view of model and experiment if a credible predictive capability is to emerge. The statistical methods provide no physical insight, but, nonetheless, warrant further consideration for hot-deformation problems. They potentially provide a means to optimize time-consuming experimental work, and may provide useful predictive capabilities for industry rather sooner than can be expected from complex physically based modelling.|Modelling of Microstructure Evolution in Hot Deformation [and Discussion]|http://www.jstor.org/stable/55204|55204|1999-06-15|1999|['eng']|['Physical sciences - Materials science']|['General Science', 'Mathematics', 'Science and Mathematics']
"Modern Bayesian statistical methods, such as Gibbs and Metropolis—Hastings sampling, were developed to liberate statisticians from the necessity of making large-sample assumptions and to facilitate the numerical approximation of problems that had previously been analytically intractable. Counter to this trend, we develop a method for constructing asymptotic joint posterior approximations based on models with k blocks of parameters and where the corresponding properly normalized full conditionals are themselves asymptotically normal. We illustrate these techniques by applying them to particular linear and generalized linear mixed models (GLMMs). We also consider the relevance of different parameterizations with regard to our asymptotics. Recent work has indicated that Gibbs samplers based on so-called ""centering parameterizations"" result in better convergence properties for the resulting Markov chains. Our results for the one-way random-effects model shed some light on this issue. For this example, we also consider the distinction between letting the within-group sample size, n, tend to infinity versus letting the number of groups K (as defined by the random-effects part of the model) tend to infinity. Letting n grow results in a proper limiting normal distribution only when the weight on the prior for the variance component grows at a rate comparable to n. With large K, on the other hand, proper limits are obtained without this assumption, and thus it is seen that the information in the data will ultimately swamp standard prior information. We compare results based on simulated data when n and K are large. A dataset involving the effect of smoking on hormone function is analyzed using our asymptotics and compared with results based on Gibbs sampling."|Large-Sample Joint Posterior Approximations When Full Conditionals Are Approximately Normal: Application to Generalized Linear Mixed Models|http://www.jstor.org/stable/27590736|27590736|2006-06-01|2006|['eng']|['Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
Serial analysis of gene expression (SAGE) is a technology for quantifying gene expression in biological tissue that yields count data that can be modeled by a multinomial distribution with two characteristics: skewness in the relative frequencies and small sample size relative to the dimension. As a result of these characteristics, a given SAGE sample may fail to capture a large number of expressed mRNA species present in the tissue. Empirical estimators of mRNA species' relative abundance effectively ignore these missing species, and as a result tend to overestimate the abundance of the scarce observed species comprising a vast majority of the total. We have developed a new Bayesian estimation procedure that quantifies our prior information about these characteristics, yielding a nonlinear shrinkage estimator with efficiency advantages over the MLE. Our prior is mixture of Dirichlets, whereby species are stochastically partitioned into abundant and scarce classes, each with its own multivariate prior. Simulation studies reveal our estimator has lower integrated mean squared error (IMSE) than the MLE for the SAGE scenarios simulated, and yields relative abundance profiles closer in Euclidean distance to the truth for all samples simulated. We apply our method to a SAGE library of normal colon tissue, and discuss its implications for assessing differential expression.|Bayesian Shrinkage Estimation of the Relative Abundance of mRNA Transcripts Using SAGE|http://www.jstor.org/stable/3695423|3695423|2003-09-01|2003|['eng']|['Biological sciences - Biology']|['Science & Mathematics', 'Statistics']
Sample size determination (SSD) is a crucial aspect of experimental design. Two SSD problems are considered here. The first concerns how to select a sample size to achieve specified performance with regard to one or more features of a model. Adopting a Bayesian perspective, we move the Bayesian SSD problem from the rather elementary models addressed in the literature to date in the direction of the wide range of hierarchical models which dominate the current Bayesian landscape. Our approach is generic and thus, in principle, broadly applicable. However, it requires full model specification and computationally intensive simulation, perhaps limiting it practically to simple instances of such models. Still, insight from such cases is of useful design value. In addition, we present some theoretical tools for studying performance as a function of sample size, with a variety of illustrative results. Such results provide guidance with regard to what is achievable. We also offer two examples, a survival model with censoring and a logistic regression model. The second problem concerns how to select a sample size to achieve specified separation of two models. We approach this problem by adopting a screening criterion which in turn forms a model choice criterion. This criterion is set up to choose model 1 when the value is large, model 2 when the value is small. The SSD problem then requires choosing n1 to make the probability of selecting model 1 when model 1 is true sufficiently large and choosing n2 to make the probability of selecting model 2 when model 2 is true sufficiently large. The required n is max (n1, n2). Here, we again provide two illustrations. One considers separating normal errors from t errors, the other separating a common growth curve model from a model with individual growth curves.|A Simulation-Based Approach to Bayesian Sample Size Determination for Performance under a Given Model and for Separating Models|http://www.jstor.org/stable/3182824|3182824|2002-05-01|2002|['eng']|['Mathematics - Mathematical logic']|['Science and Mathematics', 'Statistics']
By extending Schwarz's (1978) basic idea we derive a Bayesian information criterion which enables us to evaluate models estimated by the maximum penalised likelihood method or the method of regularisation. The proposed criterion is applied to the choice of smoothing parameters and the number of basis functions in radial basis function network models. Monte Carlo experiments were conducted to examine the performance of the nonlinear modelling strategy of estimating the weight parameters by regularisation and then determining the adjusted parameters by the Bayesian information criterion. The simulation results show that our modelling procedure performs well in various situations.|Bayesian Information Criteria and Smoothing Parameter Selection in Radial Basis Function Networks|http://www.jstor.org/stable/20441077|20441077|2004-03-01|2004|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
We provide a new characterization of the Dirichlet distribution. Let θij, 1 ≤ i ≤ k, 1 ≤ j ≤ n, be positive random variables that sum to unity. Define θi· = ∑n j=1 θij, θI· = {θi·}k-1 i=1, θj∣ i = θij/∑j θij and θJ∣ i = {θj∣ i}n-1 j=1. We prove that if {θI·, θJ∣ 1, ..., θJ∣ k} are mutually independent and {θ· J, θI∣ 1, ..., θI∣ n} are mutually independent (where θ· J and θI∣ j are defined analogously), and each parameter set has a strictly positive pdf, then the pdf of θij is Dirichlet. This characterization implies that under assumptions made by several previous authors for selecting a Bayesian network structure out of a set of candidate structures, a Dirichlet prior on the parameters is inevitable.|A Characterization of the Dirichlet Distribution through Global and Local Parameter Independence|http://www.jstor.org/stable/2242526|2242526|1997-06-01|1997|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Networks—sets of objects connected by relationships—are important in a number of fields. The study of networks has long been central to sociology, where researchers have attempted to understand the causes and consequences of the structure of relationships in large groups of people. Using insight from previous network research, Killworth et al. and McCarty et al. have developed and evaluated a method for estimating the sizes of hard-to-count populations using network data collected from a simple random sample of Americans. In this article we show how, using a multilevel overdispersed Poisson regression model, these data also can be used to estimate aspects of social structure in the population. Our work goes beyond most previous research on networks by using variation, as well as average responses, as a source of information. We apply our method to the data of McCarty et al. and find that Americans vary greatly in their number of acquaintances. Further, Americans show great variation in propensity to form ties to people in some groups (e.g., males in prison, the homeless, and American Indians), but little variation for other groups (e.g., twins, people named Michael or Nicole). We also explore other features of these data and consider ways in which survey data can be used to estimate network structure.|How Many People Do You Know in Prison?: Using Overdispersion in Count Data to Estimate Social Structure in Networks|http://www.jstor.org/stable/27590705|27590705|2006-06-01|2006|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering', 'Biological sciences - Biology']|['Science & Mathematics', 'Statistics']
In the construction of a leading indicator model of economic activity, economists must select among a pool of variables which lead output growth. Usually the pool of variables is large and a selection of a subset must be carried out. This paper proposes an automatic leading indicator model which, rather than preselection, uses a dynamic factor model to summarize the information content of a pool of variables. Results using quarterly data for France, Germany, Italy and the United Kingdom show that the overall forecasting performance of the automatic leading indicator model appears better than that of more traditional VAR and BVAR models.|An automatic leading indicator of economic activity: forecasting GDP growth for European countries|http://www.jstor.org/stable/23114939|23114939|2001-01-01|2001|['eng']|['Mathematics - Applied mathematics', 'Information science - Informetrics']|['Business & Economics', 'Business', 'Economics']
This article reinvestigates the performance of risk‐based multifactor models. We generalize the Bayesian methodology of Shanken and Kandel, McCulloch, and Stambaugh from mean‐variance to multifactor efficiency. Using informative priors, our flexible framework handles severe small‐sample problems. We introduce a new inefficiency metric that measures the maximum correlation between the market portfolio and any multifactor‐efficient portfolio. Finally, we present new empirical evidence that neither the two additional Fama‐French factors nor the momentum factor move the market portfolio robustly closer to being multifactor efficient or robustly decrease pricing errors relative to the Capital Asset Pricing Model.|Multifactor Efficiency and Bayesian Inference|http://www.jstor.org/stable/10.1086/508005|10.1086/508005|2006-11-01|2006|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical objects']|['Business & Economics', 'Business', 'Finance']
We develop a dependent Dirichlet process (DDP) model for repeated measures multiple membership (MM) data. This data structure arises in studies under which an intervention is delivered to each client through a sequence of elements which overlap with those of other clients on different occasions. Our interest concentrates on study designs for which the overlaps of sequences occur for clients who receive an intervention in a shared or grouped fashion whose memberships may change over multiple treatment events. Our motivating application focuses on evaluation of the effectiveness of a group therapy intervention with treatment delivered through a sequence of cognitive behavioral therapy session blocks, called modules. An open-enrollment protocol permits entry of clients at the beginning of any new module in a manner that may produce unique MM sequences across clients. We begin with a model that composes an addition of client and multiple membership module random effect terms, which are assumed independent. Our MM DDP model relaxes the assumption of conditionally independent client and module random effects by specifying a collection of random distributions for the client effect parameters that are indexed by the unique set of module attendances. We demonstrate how this construction facilitates examining heterogeneity in the relative effectiveness of group therapy modules over repeated measurement occasions.|BAYESIAN NONPARAMETRIC HIERARCHICAL MODELING FOR MULTIPLE MEMBERSHIP DATA IN GROUPED ATTENDANCE INTERVENTIONS|http://www.jstor.org/stable/23566424|23566424|2013-06-01|2013|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science and Mathematics', 'Statistics']
A Bayesian hierarchical mixed model is developed for multiple comparisons under a simple order restriction. The model facilitates inferences on the successive differences of the population means, for which we choose independent prior distributions that are mixtures of an exponential distribution and a discrete distribution with its entire mass at zero. We employ Markov Chain Monte Carlo (MCMC) techniques to obtain parameter estimates and estimates of the posterior probabilities that any two of the means are equal. The latter estimates allow one both to determine if any two means are significantly different and to test the homogeneity of all of the means. We investigate the performance of the model-based inferences with simulated data sets, focusing on parameter estimation and successive-mean comparisons using posterior probabilities. We then illustrate the utility of the model in an application based on data from a study designed to reduce lead blood concentrations in children with elevated levels. Our results show that the proposed hierarchical model can effectively unify parameter estimation, tests of hypotheses and multiple comparisons in one setting. Un modèle Bayésien hiérarchique mixte est développé pour des comparaisons multiples avec une simple restriction d'ordre. Le modèle facilite les inférences sur les différences successives des moyennes de population, pour lesquelles nous choisissons des distributions indépendantes préalables qui sont des mélanges d'une distribution exponentielle et d'une distribution discrète avec sa masse entière à zéro. Nous employons les techniques de la chaîne de Markov Monte Carlo pour obtenir des estimations des paramètres et des estimations des probabilités postérieures que deux quelconques des moyennes sont égales. Les seconds estimateurs permettent à chacun de déterminer si deux quelconques des moyennes sont significativement différentes et de tester l'homogénéité de toutes les moyennes. Nous étudions la performance des inférences basées sur des modèles avec des jeux de données simulées, en se concentrant sur l'estimation du paramètre et des comparaisons successives moyennes utilisant des probabilités postérieures. Nous illustrons ensuite l'utilité du modèle dans une application basée sur les données d'une étude destinée à réduire les concentrations de plomb sanguin chez les enfants avec des niveaux élevés. Nos résultats montrent que le modélé hiérarchique proposé peut efficacement unifier l'estimation des paramètres, les hypothèses de tests et les comparaisons multiples dans un seul cadre.|A Bayesian Multiple Comparison Procedure for Order-Restricted Mixed Models|http://www.jstor.org/stable/27919616|27919616|2008-08-01|2008|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
This paper considers simultaneous estimation of means from several strata. A model-based approach is taken, where the covariates in the superpopulation model are subject to measurement errors. Empirical Bayes (EB) and Hierarchical Bayes estimators of the strata means are developed and asymptotic optimality of EB estimators is proved. Their performances are examined and compared with that of the sample mean in a simulation study as well as in data analysis.|Empirical and Hierarchical Bayesian Estimation in Finite Population Sampling under Structural Measurement Error Models|http://www.jstor.org/stable/4616944|4616944|2006-09-01|2006|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
We review the current state of nonparametric Bayesian inference. The discussion follows a list of important statistical inference problems, including density estimation, regression, survival analysis, hierarchical models and model validation. For each inference problem we review relevant nonparametric Bayesian models and approaches including Dirichlet process (DP) models and variations, Pólya trees, wavelet based models, neural network models, spline regression, CART, dependent DP models and model validation with DP and Pólya tree extensions of parametric models.|Nonparametric Bayesian Data Analysis|http://www.jstor.org/stable/4144375|4144375|2004-02-01|2004|['eng']|['Mathematics - Mathematical logic']|['Science and Mathematics', 'Statistics']
Winter road maintenance is one of the main tasks for the Washington State Department of Transportation. Anti-icing, that is, the preemptive application of chemicals, is often used to keep the roadways free of ice. Given the preventive nature of anti-icing, accurate predictions of road ice are needed. Currently, anti-icing decisions are usually based on deterministic weather forecasts. However, the costs of the two kinds of errors are highly asymmetric because the cost of a road closure due to ice is much greater than that of taking anti-icing measures. As a result, probabilistic forecasts are needed to optimize decision making. We propose two methods for forecasting the probability of ice formation. Starting with deterministic numerical weather predictions, we model temperature and precipitation using distributions centered around the bias-corrected forecasts. This produces a joint predictive probability distribution of temperature and precipitation, which then yields the probability of ice formation, defined here as the occurrence of precipitation when the temperature is below freezing. The first method assumes that temperatures, as well as precipitation, at different spatial locations are conditionally independent given the numerical weather predictions. The second method models the spatial dependence between forecast errors at different locations. The model parameters are estimated using a Bayesian approach via Markov chain Monte Carlo. We evaluate both methods by comparing their probabilistic forecasts with observations of ice formation for Interstate Highway 90 in Washington State for the 2003—2004 and 2004—2005 winter seasons. The use of the probabilistic forecasts reduces costs by about 50% when compared to deterministic forecasts. The spatial method improves the reliability of the forecasts, but does not result in further cost reduction when compared to the first method.|Probabilistic Weather Forecasting for Winter Road Maintenance|http://www.jstor.org/stable/29747061|29747061|2010-06-01|2010|['eng']|['Physical sciences - Astronomy', 'Environmental studies - Atmospheric sciences', 'Biological sciences - Biogeography']|['Science & Mathematics', 'Statistics']
In longitudinal studies of biological markers, different individuals may have different underlying patterns of response. In some applications, a subset of individuals experiences latent events, causing an instantaneous change in the level or slope of the marker trajectory. The paper presents a general mixture of hierarchical longitudinal models for serial biomarkers. Interest centres both on the time of the event and on levels of the biomarker before and after the event. In observational studies where marker series are incomplete, the latent event can be modelled by a survival distribution. Risk factors for the occurrence of the event can be investigated by including covariates in the survival distribution. A combination of Gibbs, Metropolis-Hastings and reversible jump Markov chain Monte Carlo sampling is used to fit the models to serial measurements of forced expiratory volume from lung transplant recipients.|Models for Longitudinal Data with Censored Changepoints|http://www.jstor.org/stable/3592693|3592693|2004-01-01|2004|['eng']|['Applied sciences - Engineering', 'Health sciences - Medical specialties']|['Science & Mathematics', 'Statistics']
The paper develops a method for producing current quarter forecasts of gross domestic product growth with a (possibly large) range of available within-the-quarter monthly observations of economic indicators, such as employment and industrial production, and financial indicators, such as stock prices and interest rates. In light of existing evidence of time variation in the variances of shocks to gross domestic product, we consider versions of the model with both constant variances and stochastic volatility. We use Bayesian methods to estimate the model, to facilitate providing shrinkage on the (possibly large) set of model parameters and conveniently generate predictive densities. We provide results on the accuracy of nowcasts of realtime gross domestic product growth in the USA from 1985 through 2011. In terms of point forecasts, our proposal improves significantly on auto-regressive models and performs comparably with survey forecasts. In addition, it provides reliable density forecasts, for which the stochastic volatility specification is quite useful.|Realtime nowcasting with a Bayesian mixed frequency model with stochastic volatility|http://www.jstor.org/stable/43965773|43965773|2015-10-01|2015|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
Despite recent improvements, New Zealand still has one of the highest per-capita incidence rates of campylobacteriosis in the world. To reduce the incidence, a thorough understanding of the epidemiology of infection is needed. This retrospective analysis of 36 000 notified human cases during a high-risk period between 2001 and 2007 explored the spatial and temporal determinants of Campylobacter notifications at a fine spatial scale in order to improve understanding of the complex epidemiology. Social deprivation was associated with a decreased risk of notification, whereas urban residence was associated with an increased risk. However, for young children rural residence was a risk factor. High dairy cattle density was associated with an increased risk of notification in two of the three regions investigated. Campylobacter notification patterns exhibit large temporal variations; however, few factors were associated with periods of increased risk, in particular temperature did not appear to drive the seasonality in campylobacteriosis.|The spatial and temporal determinants of campylobacteriosis notifications in New Zealand, 2001—2007|http://www.jstor.org/stable/23254492|23254492|2012-09-01|2012|['eng']|['Health sciences - Medical conditions']|['Health Sciences', 'Medicine and Allied Health']
"We develop a novel approach for testing treatment effects in high-throughput data. Most previous works on this topic focused on testing for differences between the means, but recently it has been recognized that testing for differential variation is probably as important. We take it a step further, and introduce a bivariate model modeling strategy which accounts for both differential expression and differential variation. Our model-based approach, in which the differential mean and variance are considered random effects, results in shrinkage estimation and powerful tests as it borrows strength across levels. We show in simulations that the method yields a substantial gain in the power to detect differential means when differential variation is present. Our case studies show that the model is realistic in a wide range of applications. Furthermore, a hierarchical estimation approach implemented using the EM algorithm results in a computationally efficient method which is particularly well-suited for ""multiple testing"" situations. Finally, we develop a power and sample size calculation tool that mirrors the estimation and inference method described in this article, and can be used to design experiments involving thousands of simultaneous tests."|A Bivariate Model for Simultaneous Testing in Bioinformatics Data|http://www.jstor.org/stable/24247184|24247184|2014-06-01|2014|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
In a typical case—control study, exposure information is collected at a single time point for the cases and controls. However, case—control studies are often embedded in existing cohort studies containing a wealth of longitudinal exposure history about the participants. Recent medical studies have indicated that incorporating past exposure history, or a constructed summary measure of cumulative exposure derived from the past exposure history, when available, may lead to more precise and clinically meaningful estimates of the disease risk. In this article, we propose a flexible Bayesian semiparametric approach to model the longitudinal exposure profiles of the cases and controls and then use measures of cumulative exposure based on a weighted integral of this trajectory in the final disease risk model. The estimation is done via a joint likelihood. In the construction of the cumulative exposure summary, we introduce an influence function, a smooth function of time to characterize the association pattern of the exposure profile on the disease status with different time windows potentially having differential influence/weights. This enables us to analyze how the present disease status of a subject is influenced by his/her past exposure history conditional on the current ones. The joint likelihood formulation allows us to properly account for uncertainties associated with both stages of the estimation process in an integrated manner. Analysis is carried out in a hierarchical Bayesian framework using reversible jump Markov chain Monte Carlo algorithms. The proposed methodology is motivated by, and applied to a case—control study of prostate cancer where longitudinal biomarker information is available for the cases and controls.|A Bayesian Semiparametric Approach for Incorporating Longitudinal Information on Exposure History for Inference in Case—Control Studies|http://www.jstor.org/stable/23270437|23270437|2012-06-01|2012|['eng']|['Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
A distributed lag model (DLagM) is a regression model that includes lagged exposure variables as covariates; its corresponding distributed lag (DL) function describes the relationship between the lag and the coefficient of the lagged exposure variable. DLagMs have recently been used in environmental epidemiology for quantifying the cumulative effects of weather and air pollution on mortality and morbidity. Standard methods for formulating DLagMs include unconstrained, polynomial, and penalized spline DLagMs. These methods may fail to take full advantage of prior information about the shape of the DL function for environmental exposures, or for any other exposure with effects that are believed to smoothly approach zero as lag increases, and are therefore at risk of producing suboptimal estimates. In this article, we propose a Bayesian DLagM (BDLagM) that incorporates prior knowledge about the shape of the DL function and also allows the degree of smoothness of the DL function to be estimated from the data. We apply our BDLagM to its motivating data from the National Morbidity, Mortality, and Air Pollution Study to estimate the short-term health effects of particulate matter air pollution on mortality from 1987 to 2000 for Chicago, Illinois. In a simulation study, we compare our Bayesian approach with alternative methods that use unconstrained, polynomial, and penalized spline DLagMs. We also illustrate the connection between BDLagMs and penalized spline DLagMs. Software for fitting BDLagM models and the data used in this article are available online.|Bayesian Distributed Lag Models: Estimating Effects of Particulate Matter Air Pollution on Daily Mortality|http://www.jstor.org/stable/25502268|25502268|2009-03-01|2009|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
"Like other partition-based models, Polya trees suffer the problem of partition dependence. We develop Randomized Polya Trees to address this limitation. This new framework inherits the structure of Polya trees but ""jitters"" partition points and as a result smooths discontinuities in predictive distributions. Some of the theoretical aspects of the new framework are developed, followed by discussion of methodological and computational issues arising in implementation. Examples of data analyses and prediction problems are provided to highlight issues of Bayesian inference in this context."|RANDOMIZED POLYA TREE MODELS FOR NONPARAMETRIC BAYESIAN INFERENCE|http://www.jstor.org/stable/24307143|24307143|2003-04-01|2003|['eng']|['Applied sciences - Engineering', 'Mathematics - Mathematical objects']|['Mathematics', 'Science and Mathematics', 'Statistics']
Ratings of teachers' instructional practices using standardized classroom observation instruments are increasingly being used for both research and teacher accountability. There are multiple instruments in use, each attempting to evaluate many dimensions of teaching and classroom activities, and little is known about what underlying teaching quality attributes are being measured. We use data from multiple instruments collected from 458 middle school mathematics and English language arts teachers to inform research and practice on teacher performance measurement by modeling latent constructs of high-quality teaching. We make inferences about these constructs using a novel approach to Bayesian exploratory factor analysis (EFA) that, unlike commonly used approaches for identifying factor loadings in Bayesian EFA, is invariant to how the data dimensions are ordered. Applying this approach to ratings of lessons reveals two distinct teaching constructs in both mathematics and English language arts: (1) quality of instructional practices; and (2) quality of teacher management of classrooms. We demonstrate the relationships of these constructs to other indicators of teaching quality, including teacher content knowledge and student performance on standardized tests.|INFERRING CONSTRUCTS OF EFFECTIVE TEACHING FROM CLASSROOM OBSERVATIONS: AN APPLICATION OF BAYESIAN EXPLORATORY FACTOR ANALYSIS WITHOUT RESTRICTIONS|http://www.jstor.org/stable/43826430|43826430|2015-09-01|2015|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
Expert panels are playing an increasingly important role in U.S. health policy decision making. A fundamental issue in these applications is how to synthesize the judgments of individual experts into a group judgment. In this paper we propose an approach to synthesis based on Bayesian hierarchical models, and apply it to the problem of determining physician staffing at medical centers operated by the U.S. Department of Veteran Affairs (VA). Our starting point is the supra-Bayesian approach to synthesis, whose principal motivation in the present context is to generate an estimate of the uncertainty associated with a panel's evaluation of the number of physicians required under specified conditions. Hierarchical models are particularly natural in this context since variability in the experts' judgments results in part from heterogeneity in their baseline experiences at different VA medical centers. We derive alternative hierarchical Bayes synthesis distributions for the number of physicians required to handle the (service-mix specific) daily workload in internal medicine at a given VA medical center (VAMC). The analysis appears to be the first to provide a statistical treatment of expert judgment processes for evaluating the appropriate use of resources in health care. Also, while hierarchical models are well established, their application to the synthesis of expert judgment is novel.|Combining Expert Judgment by Hierarchical Modeling: An Application to Physician Staffing|http://www.jstor.org/stable/2634492|2634492|1998-02-01|1998|['eng']|['Applied sciences - Engineering']|['Business', 'Business & Economics Collection', 'Management & Organizational Behavior']
After continued treatment with an insecticide, within the population of the susceptible insects, resistant strains will occur. It is important to know whether there are any resistant strains, what the proportions are, and what the median lethal doses are for the insecticide. Lwin and Martin (1989, Biometrics 45, 721-732) propose a probit mixture model and use the EM algorithm to obtain the maximum likelihood estimates for the parameters. This approach has difficulties in estimating the confidence intervals and in testing the number of components. We propose a Bayesian approach to obtaining the credible intervals for the location and scale of the tolerances in each component and for the mixture proportions by using data augmentation and Gibbs sampler. We use Bayes factor for model selection and determining the number of components. We illustrate the method with data published in Lwin and Martin (1989).|A Bayesian Approach to Finite Mixture Models in Bioassay via Data Augmentation and Gibbs Sampling and Its Application to Insecticide Resistance|http://www.jstor.org/stable/2677065|2677065|2000-12-01|2000|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
For a sample of 2361 patients admitted with suspected acute myocardial infarction to a set of 37 hospitals, recorded patient response variables include eligibility for treatment with aspirin, eligibility for treatment with thrombolytics, treatment with aspirin received, treatment with thrombolytics received and short-term patient survival. Each of these five variables has two levels resulting in a 25 contingency table. The covariate information includes age, sex, race and comorbidity status. Because the responses arrive in sequence, we model these data in three stages: eligibility, then treatment received given eligibility and finally short-term survival given eligibility and treatment received, all given the covariates. Issues of interest include the extent to which the treatment received matches eligibility, whether the probability of survival is affected by treatment status and how the chance of mortality is affected by whether or not the treatment received matches eligibility. The influence of covariate information on these quantities is examined. These quantities are studied at the hospital level adjusted for case mix and also in aggregate, marginalizing over hospitals.|Conditional Categorical Response Models with Application to Treatment of Acute Myocardial Infarction|http://www.jstor.org/stable/2680848|2680848|2000-01-01|2000|['eng']|['Health sciences - Medical treatment', 'Health sciences - Health and wellness']|['Science & Mathematics', 'Statistics']
The decline in the level and persistence of inflation over the 1980s is a common feature of the most industrialized economies in the world. The rise in inflation volatility of the late 1970s and the subsequent fall of the 1980s is country specific for the UK, Canada, and, to a lesser extent, the United States, Italy, and Japan. Since the late 1980s, inflation predictability has declined significantly across the industrialized world. We link the empirical results to recent theories of international inflation.|EVOLVING INTERNATIONAL INFLATION DYNAMICS: WORLD AND COUNTRY-SPECIFIC FACTORS|http://www.jstor.org/stable/23251097|23251097|2012-08-01|2012|['eng']|['Economics - Economic policy', 'Economics - Economic disciplines', 'Applied sciences - Research methods']|['Business & Economics', 'Business', 'European Studies', 'Economics', 'Area Studies']
This article applies Bayesian nonparametric techniques of analysis to the mixed linear model. The distribution of the random effects is specified as a nonparametric prior. A Dirichlet process prior is specified on the space of prior distributions. A modified Dirichlet process is described and applied using a Gibbs sampler. The approach is demonstrated in an investigation of the changes over time of packed cell volume in two breeds of cattle.|An Application of the Mixed Linear Model and the Dirichlet Process Prior in Veterinary Medicine Research|http://www.jstor.org/stable/1400551|1400551|2003-09-01|2003|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Agriculture', 'Statistics']
High-dimensional and highly correlated data leading to non-or weakly identified effects are commonplace. Maximum likelihood will typically fail in such situations and a variety of shrinkage methods have been proposed. Standard techniques, such as ridge regression or the lasso, shrink estimates toward zero, with some approaches allowing coefficients to be selected out of the model by achieving a value of zero. When substantive information is available, estimates can be shrunk to nonnull values; however, such information may not be available. We propose a Bayesian semiparametric approach that allows shrinkage to multiple locations. Coefficients are given a mixture of heavy-tailed double exponential priors, with location and scale parameters assigned Dirichlet process hyperpriors to allow groups of coefficients to be shrunk toward the same, possibly nonzero, mean. Our approach favors sparse, but flexible, structure by shrinking toward a small number of random locations. The methods are illustrated using a study of genetic polymorphisms and Parkinson's disease.|<strong>Bayesian Semiparametric Multiple Shrinkage</strong>|http://www.jstor.org/stable/40663238|40663238|2010-06-01|2010|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
A common theme in marketing literature is the acquisition and retention of customers as they trade up from inexpensive introductory offerings to those of higher quality. We develop a nonhomothetic choice model to accommodate effects of advertising, professional recommendation, and other factors that facilitate the description and management of trade-up. Our model allows advertising to affect the relative superiority or inferiority of products. This allows for a wide variety of trade-up patterns beyond those obtained from a standard random utility formulation of the logit model. Our nonhomothetic model allows for advertising to affect more than just brand intercepts (perceived quality), but also the rate at which consumers are willing to trade up to higher-quality brands. Advertising effects are measured using a randomized treatment and evaluated by considering their direct implications for firm pricing and profits.|A Model for Trade-Up and Change in Considered Brands|http://www.jstor.org/stable/40608088|40608088|2010-01-01|2010|['eng']|['Mathematics - Mathematical objects']|['Marketing & Advertising', 'Business & Economics', 'Business']
The paper proposes a stochastic frontier model with random coefficients to separate technical inefficiency from technological differences across firms, and free the frontier model from the restrictive assumption that all firms must share exactly the same technological possibilities. Inference procedures for the new model are developed based on Bayesian techniques, and computations are performed using Gibbs sampling with data augmentation to allow finite-sample inference for underlying parameters and latent efficiencies. An empirical example illustrates the procedure.|Stochastic Frontier Models with Random Coefficients|http://www.jstor.org/stable/4129247|4129247|2002-03-01|2002|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics', 'Mathematics - Mathematical analysis']|['Business & Economics', 'Business', 'Economics']
We consider a structural component model based on a random walk that incorporates a drift from an unknown point in time, τ, with the objective of providing an on-line estimate of this changepoint. The application to detecting bacteriological growth in routine monitoring of feedstuff motivates the analysis, and the ability of this model to be tuned in different ways for different specific applications is the reason for its choice. The changepoint τ is regarded as a parameter and the posterior distribution (or likelihood function) of τ is computed at each time point by running a triangular multiprocess Kalman filter. The values of other parameters in the structural component model are tuned from previous data. The location and width of an 80% posterior interval give both an estimate of the changepoint and the magnitude of the evidence for a change. A more formal decision rule for on-line and post-sampling detection is derived by application of Bayesian decision analysis.|A Dynamic Changepoint Model for Detecting the Onset of Growth in Bacteriological Infections|http://www.jstor.org/stable/2986261|2986261|1994-01-01|1994|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Making quantified statements about the uncertainty associated with the lifelength of an item is one of the most fundamental tasks of reliability assessment. Most practitioners routinely do this using one of the several available statistical techniques. The purpose of this paper is two-fold. The first is to give the user an overview of the key tenets of two of the most commonly used parametric approaches. The second is to point out that these commonly used approaches involve strategies that are either ad hoc, or are in violation of some of the underlying tenets. A method that is devoid of logical flaws can be proposed, but this method is difficult to implement. The user must therefore resign to using that technique against which the fewest objections can be hurled. /// Une des activités les plus fondamentales du contrôle de la fiabilité d'un objet consiste à faire connaître de façon quantitative l'incertitude associée à sa durée de vie. La plupart des praticiens utilisent couramment pour ce faire une des techniques statistiques disponibles. L'objectif de cet article est double. Il vise d'abord à donner à l'utilisateur une vue d'ensemble des principes clés de deux des approches paramétriques les plus souvent utilisées. Il met ensuite l'accent sur le fait que ces approches habituelles impliquent des stratégies qui sont soit ad hoc, soit en violation de certains des principes sous-jacents. On peut proposer une méthode exempte de problèmes logiques, mais elle est difficile à mettre en oeuvre. L'utilisateur doit donc se résigner à recourir à la technique sur laquelle on peut émettre le moins d'objections.|Some Cracks in the Empire of Chance (Flaws in the Foundations of Reliability)|http://www.jstor.org/stable/1403722|1403722|2002-04-01|2002|['eng']|['Mathematics - Applied mathematics', 'Philosophy - Logic', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Non-invasive marks, including pigmentation patterns, acquired scars, and genetic markers, are often used to identify individuals in mark-recapture experiments. If animals in a population can be identified from multiple, non-invasive marks then some individuals may be counted twice in the observed data. Analyzing the observed histories without accounting for these errors will provide incorrect inference about the population dynamics. Previous approaches to this problem include modeling data from only one mark and combining estimators obtained from each mark separately assuming that they are independent. Motivated by the analysis of data from the ECOCEAN online whale shark (Rhincodon typus) catalog, we describe a Bayesian method to analyze data from multiple, non-invasive marks that is based on the latent-multinomial model of Link et al. (2010, Biometrics 66, 178–185). Further to this, we describe a simplification of the Markov chain Monte Carlo algorithm of Link et al. (2010, Biometrics 66, 178–185) that leads to more efficient computation. We present results from the analysis of the ECOCEAN whale shark data and from simulation studies comparing our method with the previous approaches.|Mark-Recapture with Multiple, Non-Invasive Marks|http://www.jstor.org/stable/24538143|24538143|2013-09-01|2013|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Under the multispecies coalescent model of molecular evolution, gene trees have independent evolutionary histories within a shared species tree. In comparison, supermatrix concatenation methods assume that gene trees share a single common genealogical history, thereby equating gene coalescence with species divergence. The multispecies coalescent is supported by previous studies which found that its predicted distributions fit empirical data, and that concatenation is not a consistent estimator of the species tree. *BEAST, a fully Bayesian implementation of the multispecies coalescent, is popular but computationally intensive, so the increasing size of phylogenetic data sets is both a computational challenge and an opportunity for better systematics. Using simulation studies, we characterize the scaling behavior of *BEAST, and enable quantitative prediction of the impact increasing the number of loci has on both computational performance and statistical accuracy. Follow-up simulations over a wide range of parameters show that the statistical performance of *BEAST relative to concatenation improves both as branch length is reduced and as the number of loci is increased. Finally, using simulations based on estimated parameters from two phylogenomic data sets, we compare the performance of a range of species tree and concatenation methods to show that using *BEAST with tens of loci can be preferable to using concatenation with thousands of loci. Our results provide insight into the practicalities of Bayesian species tree estimation, the number of loci required to obtain a given level of accuracy and the situations in which supermatrix or summary methods will be outperformed by the fully Bayesian multispecies coalescent.|Computational Performance and Statistical Accuracy of *BEAST and Comparisons with Other Methods|http://www.jstor.org/stable/44028764|44028764|2016-05-01|2016|['eng']|['Biological sciences - Biology', 'Applied sciences - Engineering']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
Time varying parameter (TVP) models have enjoyed an increasing popularity in empirical macroeconomics. However, TVP models are parameter-rich and risk over-fitting unless the dimension of the model is small. Motivated by this worry, this article proposes several Time Varying Dimension (TVD) models where the dimension of the model can change over time, allowing for the model to automatically choose a more parsimonious TVP representation, or to switch between different parsimonious representations. Our TVD models all fall in the category of dynamic mixture models. We discuss the properties of these models and present methods for Bayesian inference. An application involving U.S. inflation forecasting illustrates and compares the different TVD models. We find our TVD approaches exhibit better forecasting performance than many standard benchmarks and shrink toward parsimonious specifications. This article has online supplementary materials.|Time Varying Dimension Models|http://www.jstor.org/stable/23243734|23243734|2012-07-01|2012|['eng']|['Mathematics - Applied mathematics']|['Business', 'Business & Economics Collection', 'Economics', 'Science and Mathematics', 'Statistics']
Optimal control theory can be combined with the probability structure of a vector autoregression to investigate the tradeoffs available to policy-makers. Such an approach obtains results based on a minimal set of assumptions about the economy and the structure of policy actions. This paper takes this approach to analyze the potential effectiveness of countercyclical monetary policy. /// La théorie du contrôle optimal peut être combinée avec la structure probabiliste d'un modèle autorégressif vectoriel pour examiner les choix offerts aux responsables de politique économique. Une telle approche obtient des résultats fondés sur des hypothèses minimales portant sur l'économie et la structure des politiques. Cet article utilise cette approche pour analyser l'efficacité potentielle d'une politique monétaire contracyclique.|The Limits of Counter-Cyclical Monetary Policy: An Analysis Based on Optimal Control Theory and Vector Autoregressions|http://www.jstor.org/stable/20075651|20075651|1987-04-01|1987|['eng']|['Philosophy - Logic']|['Business & Economics', 'Mathematics', 'Science & Mathematics', 'Economics', 'Statistics']
We propose a series of Bayesian nonparametric statistical models for community detection in graphs. We model the probability of the presence or absence of edges within the graph. Using these models, we naturally incorporate uncertainty and variability and take advantage of nonparametric techniques, such as the Chinese restaurant process and the Dirichlet process. Some of the contributions include: (a) the community structure is directly modeled without specifying the number of communities a priori; (b) the probabilities of edges within or between communities may be modeled as varying by community or pairs of communities; (c) some nodes can be classified as not belonging to any community; and (d) Bayesian model diagnostics are used to compare models and help with appropriate model selection. We start by fitting an initial model to a well-known network dataset, and we develop a series of increasingly complex models. We propose Markov chain Monte Carlo algorithms to carry out the estimation as well as an approach for community detection using the posterior distributions under a decision theoretical framework. Bayesian nonparametric techniques allow us to estimate the number and structure of communities from the data. To evaluate the proposed models for the example dataset, we discuss model comparison using the deviance information criterion and model checking using posterior predictive distributions. Supplementary materials are available online.|Bayesian Nonparametric Models for Community Detection|http://www.jstor.org/stable/24586994|24586994|2013-11-01|2013|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
In this paper, we establish both naive and formal Bayesian justifications of Cox's (1975) partial likelihood and its various modifications. We extend the original work of Kalbfieisch (1978), who showed that the partial likelihood is a limiting marginal posterior under noninformative priors for baseline hazards. We extend the result to scenarios with time-dependent covariates and time-varying regression parameters. We establish results for continuous time as well as grouped survival data. In addition, we present a Bayesian justification of a modified partial likelihood for handling ties. We also present tools for simplification of the Gibbs sampling algorithm for implementing partial likelihood based Bayesian inference in various practical applications.|A Bayesian Justification of Cox's Partial Likelihood|http://www.jstor.org/stable/30042071|30042071|2003-09-01|2003|['eng']|['Mathematics - Mathematical analysis', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
We describe a general approach using Bayesian analysis for the estimation of parameters in physiological pharmacokinetic models. The chief statistical difficulty in estimation with these models is that any physiological model that is even approximately realistic will have a large number of parameters, often comparable to the number of observations in a typical pharmacokinetic experiment (e.g., 28 measurements and 15 parameters for each subject). In addition, the parameters are generally poorly identified, akin to the well-known ill-conditioned problem of estimating a mixture of declining exponentials. Our modeling includes (a) hierarchical population modeling, which allows partial pooling of information among different experimental subjects; (b) a pharmacokinetic model including compartments for well-perfused tissues, poorly perfused tissues, fat, and the liver; and (c) informative prior distributions for population parameters, which is possible because the parameters represent real physiological variables. We discuss how to estimate the models using Bayesian posterior simulation, a method that automatically includes the uncertainty inherent in estimating such a large number of parameters. We also discuss how to check model fit and sensitivity to the prior distribution using posterior predictive simulation. We illustrate the application to the toxicokinetics of tetrachloroethylene (perchloroethylene [PERC]), the problem that motivated this work.|Physiological Pharmacokinetic Analysis Using Population Modeling and Informative Prior Distributions|http://www.jstor.org/stable/2291566|2291566|1996-12-01|1996|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
In epidemiology, it often is of interest to assess how individuals with different trajectories over time in an environmental exposure or biomarker differ with respect to a continuous response. For ease in interpretation and presentation of results, epidemiologists typically categorize predictors before analysis. To extend this approach to time-varying predictors, individuals can be clustered by their predictor trajectory, with the cluster index included as a predictor in a regression model for the response. This article develops a semiparametric Bayes approach that avoids assuming a prespecified number of clusters and allows the response to vary nonparametrically over predictor clusters. This methodology is motivated by interest in relating trajectories in weight gain during pregnancy to the distribution of birth weight adjusted for gestational age at delivery. In this setting, the proposed approach allows the tails of the birth weight density to vary flexibly over weight gain clusters.|Bayesian Inference on Changes in Response Densities over Predictor Clusters|http://www.jstor.org/stable/27640199|27640199|2008-12-01|2008|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
AbstractMultilevel hierarchical regression was used to examine regional patterns in the responses of benthic macroinvertebrates and algae to urbanization across 9 metropolitan areas of the conterminous USA. Linear regressions established that responses (intercepts and slopes) to urbanization of invertebrates and algae varied among metropolitan areas. Multilevel hierarchical regression models were able to explain these differences on the basis of region-scale predictors. Regional differences in the type of land cover (agriculture or forest) being converted to urban and climatic factors (precipitation and air temperature) accounted for the differences in the response of macroinvertebrates to urbanization based on ordination scores, total richness, Ephemeroptera, Plecoptera, Trichoptera richness, and average tolerance. Regional differences in climate and antecedent agriculture also accounted for differences in the responses of salt-tolerant diatoms, but differences in the responses of other diatom metrics (% eutraphenic, % sensitive, and % silt tolerant) were best explained by regional differences in soils (mean % clay soils). The effects of urbanization were most readily detected in regions where forest lands were being converted to urban land because agricultural development significantly degraded assemblages before urbanization and made detection of urban effects difficult. The effects of climatic factors (temperature, precipitation) on background conditions (biogeographic differences) and rates of response to urbanization were most apparent after accounting for the effects of agricultural development. The effects of climate and land cover on responses to urbanization provide strong evidence that monitoring, mitigation, and restoration efforts must be tailored for specific regions and that attainment goals (background conditions) may not be possible in regions with high levels of prior disturbance (e.g., agricultural development).|Multilevel regression models describing regional patterns of invertebrate and algal responses to urbanization across the USA|http://www.jstor.org/stable/10.1899/10-140.1|10.1899/10-140.1|2011-06-28|2011|['eng']|['Biological sciences - Ecology']|['Ecology & Evolutionary Biology', 'Aquatic Sciences', 'Science & Mathematics', 'Biological Sciences']
This paper illustrates the advantages of a multilevel/hierarchical approach for predictive modeling, including flexibility of model formulation, explicitly accounting for hierarchical structure in the data, and the ability to predict the outcome of new cases. As a generalization of the classical approach, the multilevel modeling approach explicitly models the hierarchical structure in the data by considering both the within- and between-group variances leading to a partial pooling of data across all levels in the hierarchy. The modeling framework provides means for incorporating variables at different spatiotemporal scales. The examples used in this paper illustrate the iterative process of model fitting and evaluation, a process that can lead to improved understanding of the system being studied.|On the application of multilevel modeling in environmental and ecological studies|http://www.jstor.org/stable/25661061|25661061|2010-02-01|2010|['eng']|['Biological sciences - Ecology', 'Biological sciences - Biogeography']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
We discuss the problem of screening a general population for characteristics such as HIV or drug use. Our main approach is Bayesian, which allows for the incorporation of prior information about parameters. In the particular problem we consider, there is currently no information in the data for estimating the sensitivity of the screening test, and consequently, the prevalence of the characteristic among screened negatives cannot be estimated from the collected data alone. Our inferences are straightforward to obtain using Gibbs sampling techniques, and they are valid for large or small samples and for arbitrary prevalence or accuracy of screening tests. We also develop the maximum-likelihood approach using the EM algorithm.|Dual Screening|http://www.jstor.org/stable/2533616|2533616|1999-09-01|1999|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
A variety of flexible approaches have been proposed for functional data analysis, allowing both the mean curve and the distribution about the mean to be unknown. Such methods are most useful when there is limited prior information. Motivated by applications to modeling of temperature curves in the menstrual cycle, this article proposes a flexible approach for incorporating prior information in semiparametric Bayesian analyses of hierarchical functional data. The proposed approach is based on specifying the distribution of functions as a mixture of a parametric hierarchical model and a nonparametric contamination. The parametric component is chosen based on prior knowledge, while the contamination is characterized as a functional Dirichlet process. In the motivating application, the contamination component allows unanticipated curve shapes in unhealthy menstrual cycles. Methods are developed for posterior computation, and the approach is applied to data from a European fecundability study.|Bayesian Hierarchical Functional Data Analysis via Contaminated Informative Priors|http://www.jstor.org/stable/20640575|20640575|2009-09-01|2009|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
"This report is about the analysis of stochastic processes of the form R = S + N, where S is a ""smooth"" functional and N is noise. The proposed methods derive from the assumption that the observed R-values and unobserved values of R, the assumed inferential objectives of the analysis, are linearly related through Taylor series expansions of observed about unobserved values. The expansion errors and all other priori unspecified quantities have a joint multivariate normal distribution which expresses the prior uncertainty about their values. The results include interpolators, predictors, and derivative estimates, with credibility-interval estimates automatically generated in each case. An analysis of an acid-rain wet-deposition time series is included to indicate the efficacy of the proposed method. It was this problem which led to the methodological developments reported in this paper. /// Dans cet article, on s'intéresse à l'analyse des processus stochastiques de la forme R = S + N, où S est une fonctionnelle ""régulière"" et N est un bruit. Les méthodes proposées ici découlent du présupposé que les valeurs observées de R et ses valeurs non observées, qui sont en fait les objectifs de l'inférence dans cette analyse, sont en relation linéaire par l'existence de développements en séries de Taylor des valeurs observées autour des valeurs inobservées. Les termes d'erreur dans ces développements et toute autre quantité non spécifié à priori possèdent une distribution multinormale qui reflète l'incertitude qu'on peut avoir à priori en ce qui a trait à leurs valeurs. Les résultats obtenus comprennent des estimateurs en interpolation, des estimateurs prévisionnels et des estimateurs en de paramètres ayant la forme de dérivées, produisant automatiquement dans chaque cas des estimateurs par intervalle de crédibilité. L'analyse d'une série chronologique se rapportant aux dépôts liquides de pluies acides est incluse afin d'illustrer l'efficacité de la méthode proposé. C'est l'étude de ce problème particulier qui a donné lieu aux développements méthodologiques décrits dans le présent article."|Bayesian Nonparametric Smoothers for Regular Processes|http://www.jstor.org/stable/3315064|3315064|1988-03-01|1988|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
This article develops a set of tools for smoothing and prediction with dependent point event patterns. The methodology is motivated by the problem of tracking weekly maps of violent crime events, but is designed to be straightforward to adapt to a wide variety of alternative settings. In particular, a Bayesian semiparametric framework is introduced for modeling correlated time series of marked spatial Poisson processes. The likelihood is factored into two independent components: the set of total integrated intensities and a series of process densities. For the former it is assumed that Poisson intensities are realizations from a dynamic linear model. In the latter case, a novel class of dependent stick-breaking mixture models are proposed to allow nonparametric density estimates to evolve in discrete time. This, a simple and flexible new model for dependent random distributions, is based on autoregressive time series of marginally beta random variables applied as correlated stick-breaking proportions. The approach allows for marginal Dirichlet process priors at each time and adds only a single new correlation term to the static model specification. Sequential Monte Carlo algorithms are described for online inference with each model component, and marginal likelihood calculations form the basis for inference about parameters governing temporal dynamics. Simulated examples are provided to illustrate the methodology, and we close with results for the motivating application of tracking violent crime in Cincinnati.|Autoregressive Mixture Models for Dynamic Spatial Poisson Processes: Application to Tracking Intensity of Violent Crime|http://www.jstor.org/stable/27920174|27920174|2010-12-01|2010|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
"We investigate statistical properties for a broad class of modern kernel-based regression (KBR) methods. These kernel methods were developed during the last decade and are inspired by convex risk minimization in infinite-dimensional Hilbert spaces. One leading example is support vector regression. We first describe the relationship between the loss function L of the KBR method and the tail of the response variable. We then establish the L-risk consistency for KBR which gives the mathematical justification for the statement that these methods are able to ""learn"". Then we consider robustness properties of such kernel methods. In particular, our results allow us to choose the loss function and the kernel to obtain computationally tractable and consistent KBR methods that have bounded influence functions. Furthermore, bounds for the bias and for the sensitivity curve, which is a finite sample version of the influence function, are developed, and the relationship between KBR and classical M estimators is discussed."|Consistency and Robustness of Kernel-Based Regression in Convex Risk Minimization|http://www.jstor.org/stable/25464905|25464905|2007-08-01|2007|['eng']|['Mathematics - Mathematical logic']|['Mathematics', 'Science and Mathematics', 'Statistics']
Bayesian model selection poses two main challenges: the specification of parameter priors for all models, and the computation of the resulting Bayes factors between models. There is now a large literature on automatic and objective parameter priors in the linear model. One important class are g-priors, which were recently extended from linear to generalized linear models (GLMs). We show that the resulting Bayes factors can be approximated by test-based Bayes factors (Johnson [Scand. J. Stat. 35 (2008) 354–368]) using the deviance statistics of the models. To estimate the hyperparameter g, we propose empirical and fully Bayes approaches and link the former to minimum Bayes factors and shrinkage estimates from the literature. Furthermore, we describe how to approximate the corresponding posterior distribution of the regression coefficients based on the standard GLM output. We illustrate the approach with the development of a clinical prediction model for 30-day survival in the GUSTO-I trial using logistic regression.|Approximate Bayesian Model Selection with the Deviance Statistic|http://www.jstor.org/stable/24780644|24780644|2015-05-01|2015|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Computing integrated likelihoods (ILs) to perform Bayesian model selection is a challenging task, particularly when the models considered involve a large number of parameters. In this article, we propose the use of an adaptive quadrature algorithm to automate the selection of the grid in path sampling (PS), an integration technique recognized as one of the most powerful Monte Carlo integration statistical methods for IL estimation. We begin by examining the impact of two tuning parameters of PS, the choice of the auxiliary density and the specification of the grid, both of which are shown to be potentially very influential. We then present the Grid Selection by Adaptive Quadrature (GSAQ) approach and provide a probabilistic bound for the bias of the PS grid estimator in this context. We perform a comparison between the GSAQ and standard grid implementations of PS using two well-studied datasets; GSAQ is found to yield superior results. We then examine other alternatives for PS implementation. In particular, a comparison of the fixed-GSAQ and random approaches to PS is presented. Finally, GSAQ is successfully applied to a longitudinal hierarchical regression model selection problem in multiple sclerosis research. Supplemental materials for this article are available online (see section Supplemental Materials).|Path Sampling to Compute Integrated Likelihoods: An Adaptive Approach|http://www.jstor.org/stable/25651253|25651253|2009-06-01|2009|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Science & Mathematics', 'Computer Science', 'Statistics']
In the past 10 years a statistical technique, approximate Bayesian computation (ABC), has been developed that can be used to infer parameters and choose between models in the complicated scenarios that are often considered in the environmental sciences. For example, based on gene sequence and microsatellite data, the method has been used to choose between competing models of human demographic history as well as to infer growth rates, times of divergence, and other parameters. The method fits naturally in the Bayesian inferential framework, and a brief overview is given of the key concepts. Three main approaches to ABC have been developed, and these are described and compared. Although the method arose in population genetics, ABC is increasingly used in other fields, including epidemiology, systems biology, ecology, and agent-based modeling, and many of these applications are briefly described.|Approximate Bayesian Computation in Evolution and Ecology|http://www.jstor.org/stable/27896228|27896228|2010-01-01|2010|['eng']|['Applied sciences - Engineering', 'Biological sciences - Biology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
We propose a semiparametric Bayesian method for handling measurement error in nutritional epidemiological data. Our goal is to estimate nonparametrically the form of association between a disease and exposure variable while the true values of the exposure are never observed. Motivated by nutritional epidemiological data, we consider the setting where a surrogate covariate is recorded in the primary data, and a calibration data set contains information on the surrogate variable and repeated measurements of an unbiased instrumental variable of the true exposure. We develop a flexible Bayesian method where not only is the relationship between the disease and exposure variable treated semiparametrically, but also the relationship between the surrogate and the true exposure is modeled semiparametrically. The two nonparametric functions are modeled simultaneously via B-splines. In addition, we model the distribution of the exposure variable as a Dirichlet process mixture of normal distributions, thus making its modeling essentially nonparametric and placing this work into the context of functional measurement error modeling. We apply our method to the NIH-AARP Diet and Health Study and examine its performance in a simulation study.|<strong>Semiparametric Bayesian Analysis of Nutritional Epidemiology Data in the Presence of Measurement Error</strong>|http://www.jstor.org/stable/40663237|40663237|2010-06-01|2010|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
We discuss functional clustering procedures for nested designs, where multiple curves are collected for each subject in the study. We start by considering the application of standard functional clustering tools to this problem, which leads to groupings based on the average profile for each subject. After discussing some of the shortcomings of this approach, we present a mixture model based on a generalization of the nested Dirichlet process that clusters subjects based on the distribution of their curves. By using mixtures of generalized Dirichlet processes, the model induces a much more flexible prior on the partition structure than other popular model-based clustering methods, allowing for different rates of introduction of new clusters as the number of observations increases. The methods are illustrated using hormone profiles from multiple menstrual cycles collected for women in the Early Pregnancy Study.|FUNCTIONAL CLUSTERING IN NESTED DESIGNS: MODELING VARIABILITY IN REPRODUCTIVE EPIDEMIOLOGY STUDIES|http://www.jstor.org/stable/24522269|24522269|2014-09-01|2014|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
A fully Bayesian analysis of directed graphs, with particular emphasis on applications in social networks, is explored. The model is capable of incorporating the effects of covariates, within and between block ties and multiple responses. Inference is straightforward by using software that is based on Markov chain Monte Carlo methods. Examples are provided which highlight the variety of data sets that can be entertained and the ease with which they can be analysed.|Bayesian Analysis of Directed Graphs Data with Applications to Social Networks|http://www.jstor.org/stable/3592539|3592539|2004-01-01|2004|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Heterosis, also known as the hybrid vigor, occurs when the mean phenotype of hybrid offspring is superior to that of its two inbred parents. The heterosis phenomenon is extensively utilized in agriculture though the molecular basis is still unknown. In an effort to understand phenotypic heterosis at the molecular level, researchers have begun to compare expression levels of thousands of genes between parental inbred lines and their hybrid offspring to search for evidence of gene expression heterosis. Standard statistical approaches for separately analyzing expression data for each gene can produce biased and highly variable estimates and unreliable tests of heterosis. To address these shortcomings, we develop a hierarchical model to borrow information across genes. Using our modeling framework, we derive empirical Bayes estimators and an inference strategy to identify gene expression heterosis. Simulation results show that our proposed method outperforms the more traditional strategy used to detect gene expression heterosis. This article has supplementary material online.|Estimation and Testing of Gene Expression Heterosis|http://www.jstor.org/stable/26452911|26452911|2014-09-01|2014|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Agriculture', 'Statistics']
The present work focuses on extensions of the posterior predictive p-value (ppp-value) for models with hierarchical structure, designed for testing assumptions made on underlying processes. The ppp-values are popular as tools for model criticism, yet their lack of a common interpretation limit their practical use. We discuss different extensions of ppp-values to hierarchical models, allowing for discrepancy measures that can be used for checking properties of the model at all stages. Through analytical derivations and simulation studies on simple models, we show that similar to the standard ppp-values, these extensions are typically far from uniformly distributed under the model assumptions and can give poor power in a hypothesis testing framework. We propose a calibration of the p-values, making the resulting calibrated p-values uniformly distributed under the model conditions. Illustrations are made through a real example of multinomial regression to age distributions of fish.|Posterior Predictive p-values in Bayesian Hierarchical Models|http://www.jstor.org/stable/41000323|41000323|2009-06-01|2009|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Populations where items are error free are found in many areas of applied statistics, such as auditing and actuarial science. Decisions are then made by inferring the total error in a population. This parameter is usually modelled by two parametric structures under the assumption of prior independence. This paper explores the usefulness of robust Bayesian techniques in the setting of an applied problem. The results reveal a dramatic lack of robustness with regard to the independence hypothesis.|Analysing the Independence Hypothesis in Models for Rare Errors: An Application to Auditing|http://www.jstor.org/stable/3592718|3592718|2005-01-01|2005|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
For the problem of dual system estimation, we propose a Bayesian treed capture-recapture model to account for heterogeneity of capture probabilities where individual auxiliary information is available. The model uses a binary tree to partition the covariate space into 'homogeneous' regions, within each of which the capture response can be described adequately by a simple model that assumes equal catchability. The attractive features of the proposed model include reduction of correlation bias, robustness and practical flexibility as well as simplicity and interpretability. In addition, it provides a systematic and effective way of forming post-strata for the Sekar-Deming estimator of population size. We compare the performance of estimators based on this model to those of alternative estimators in three scenarios.|Forming Post-Strata via Bayesian Treed Capture-Recapture Models|http://www.jstor.org/stable/20441332|20441332|2006-12-01|2006|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
This article describes advances in statistical computation for large-scale data analysis in structured Bayesian mixture models via graphics processing unit (GPU) programming. The developments are partly motivated by computational challenges arising in fitting models of increasing heterogeneity to increasingly large datasets. An example context concerns common biological studies using high-throughput technologies generating many, very large datasets and requiring increasingly high-dimensional mixture models with large numbers of mixture components. We outline important strategies and processes for GPU computation in Bayesian simulation and optimization approaches, give examples of the benefits of GPU implementations in terms of processing speed and scale-up in ability to analyze large datasets, and provide a detailed, tutorial-style exposition that will benefit readers interested in developing GPU-based approaches in other statistical models. Novel, GPU-oriented approaches to modifying existing algorithms software design can lead to vast speed-up and, critically, enable statistical analyses that presently will not be performed due to compute time limitations in traditional computational environments. Supplemental materials are provided with all source code, example data, and details that will enable readers to implement and explore the GPU approach in this mixture modeling context.|Understanding GPU Programming for Statistical Computation: Studies in Massively Parallel Massive Mixtures|http://www.jstor.org/stable/25703576|25703576|2010-06-01|2010|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Computer Science', 'Statistics']
Process capability analysis is designed to estimate the proportion of parts that do not meet engineering requirements in a stable production process. In this paper, we review and criticize the capability indices that are typically used in industry for this purpose, and we propose a general multivariate Bayesian capability index which contains the conventional index as a limiting case. We further derive its analytical expression under standard assumptions, discuss numerical approximations and illustrate the theory with some examples.|A General Multivariate Bayesian Process Capability Index|http://www.jstor.org/stable/2988547|2988547|1996-01-01|1996|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
A class of confidence sets with constant coverage probability for the mean of a $p$-variate normal distribution is proposed through a pseudo-empirical-Bayes construction. When the dimension is greater than 2, by combining analytical results with some exact numerical calculations the proposed sets are proved to have a uniformly smaller volume than the usual confidence region. Sufficient conditions for the connectedness of the proposed confidence sets are also derived. In addition, our confidence sets could be used to construct tests for point null hypotheses. The resultant tests have convex acceptance regions and hence are admissible by Birnbaum. Tabular results of the comparison between the proposed region and other confidence sets are also given.|Good Exact Confidence Sets for a Multivariate Normal Mean|http://www.jstor.org/stable/2959027|2959027|1997-10-01|1997|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
One important implementation of Bayesian forecasting is the Multi-State Kalman Filter (MSKF) method. It is particularly suited for short and irregular time series data. In certain applications, time series data are available on numerous parallel observational units which, while not having cause-and-effect relationships between them, are subject to the same external forces (e.g., business cycles). Treating them separately may lose useful information for forecasting. For such situations, involving seemingly unrelated time series, this article develops a Bayesian forecasting method called C-MSKF that combines the MSKF method with the Conditionally Independent Hierarchical method. A case study on forecasting income tax revenue for each of forty school districts in Allegheny County, Pennsylvania, based on fifteen years of data, is used to illustrate the application of C-MSKF in comparison with univariate MSKF. Results show that C-MSKF is more accurate than MSKF. The relative accuracy of C-MSKF increases with decreasing length of historical time series data, increasing forecasting horizon, and sensitivity of school districts to the economic cycle.|Bayesian Forecasting for Seemingly Unrelated Time Series: Application to Local Government Revenue Forecasting|http://www.jstor.org/stable/2632644|2632644|1993-03-01|1993|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Business', 'Business & Economics Collection', 'Management & Organizational Behavior']
To assess the protective effects of a time-varying covariate, we develop a stochastic model based on tumor biology. The model assumes that individuals have a Poisson-distributed pool of initiated clones, which progress through predetectable, detectable mortal and detectable immortal stages. Time-independent covariates are incorporated through a log-linear model for the expected number of clones, resulting in a proportional hazards model for disease onset. By allowing time-dependent covariates to induce clone death, with rate dependent on a clone's state, the model is flexible enough to accommodate delayed disease onset and remission or cure of preexisting disease. Inference uses Bayesian methods via Markov chain Monte Carlo. Theoretical properties are derived, and the approach is illustrated through analysis of the effects of childbirth on uterine leiomyoma (fibroids).|A Proportional Hazards Model for Incidence and Induced Remission of Disease|http://www.jstor.org/stable/3068292|3068292|2002-03-01|2002|['eng']|['Health sciences - Medical sciences']|['Science & Mathematics', 'Statistics']
"The specification of a forecasting model is considered in the context of linear multiple regression. Several potential predictor variables are available, but some of them convey little information about the dependent variable which is to be predicted. A technique for selecting the ""best"" set of predictors which takes into account the inherent uncertainty in prediction is detailed. In addition to current data, there is often substantial expert opinion available which is relevant to the forecasting problem. The approach taken here utilizes both data and expert judgment by incorporating them into a Bayesian predictive distribution. Precise forecasting models are constructed by selecting the set of predictors which minimizes a measure of variability in prediction. An empirical demonstration of the technique is provided."|A Bayesian Technique for Selecting a Linear Forecasting Model|http://www.jstor.org/stable/2631362|2631362|1983-05-01|1983|['eng']|['Applied sciences - Engineering']|['Business', 'Business & Economics Collection', 'Management & Organizational Behavior']
We consider the problem of estimating the sum of squared means when the data (x₁ ,. . ., xn) are independent values with ×i ~ N(θi, 1) and θ₁, θ₂ · · · are a priori i.i.d. N(0, σ₂) with σ₂ known. This example has posed difficulties for many approaches to inference. We examine the consistency properties of several estimators derived from Bayesian considerations. We prove that a particular Bayesian estimate (LRSE) is consistent in a wider set of circumstances than other Bayesian estimates like the posterior mean and mode. We show that the LRSE is either equal to the positive part of the UMVUE or differs from it with a relative error no greater than 2/n. We also prove a consistency result for interval estimation and discuss checking for prior-data conflict. While it can be argued that the choice of the N(0, σ₂) prior is inappropriate when σ₂ is chosen large to reflect noninformativity, this argument is not applicable when σ₂ is chosen to reflect knowledge about the unknowns. As such it is important to show that there are consistent Bayesian estimation procedures using this prior.|Consistency of Bayesian Estimates for the Sum of Squared Normal Means with a Normal Prior|http://www.jstor.org/stable/44114219|44114219|2014-02-01|2014|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Mathematical models, usually implemented in computer programs known as simulators, are widely used in all areas of science and technology to represent complex real-world phenomena. Simulators are often so complex that they take appreciable amounts of computer time or other resources to run. In this context, a methodology has been developed based on building a statistical representation of the simulator, known as an emulator. The principal approach to building emulators uses Gaussian processes. This work presents some diagnostics to validate and assess the adequacy of a Gaussian process emulator as surrogate for the simulator. These diagnostics are based on comparisons between simulator outputs and Gaussian process emulator outputs for some test data, known as validation data, defined by a sample of simulator runs not used to build the emulator. Our diagnostics take care to account for correlation between the validation data. To illustrate a validation procedure, we apply these diagnostics to two different data sets.|Diagnostics for Gaussian Process Emulators|http://www.jstor.org/stable/40586652|40586652|2009-11-01|2009|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
"In this paper we propose a unified, probabilistically coherent framework for the analysis of task-related brain activity in multi-subject fMRI experiments. This is distinct from two-stage ""group analysis"" approaches traditionally considered in the fMRI literature, which separate the inference on the individual fMRI time courses from the inference at the population level. In our modeling approach we consider a spatiotemporal linear regression model and specifically account for the between-subjects heterogeneity in neuronal activity via a spatially informed multi-subject nonparametric variable selection prior. For posterior inference, in addition to Markov chain Monte Carlo sampling algorithms, we develop suitable variational Bayes algorithms. We show on simulated data that variational Bayes inference achieves satisfactory results at more reduced computational costs than using MCMC, allowing scalability of our methods. In an application to data collected to assess brain responses to emotional stimuli our method correctly detects activation in visual areas when visual stimuli are presented."|A SPATIOTEMPORAL NONPARAMETRIC BAYESIAN MODEL OF MULTI-SUBJECT FMRI DATA|http://www.jstor.org/stable/43957072|43957072|2016-06-01|2016|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
The aim of this paper is to deal with the empirical aspects of the 'new' monetary policy framework, known as Inflation Targeting. Applying Intervention Analysis to multivariate Structural Time Series models, which avoids certain biases encountered in the use of conventional regression estimators, new empirical evidence is produced in the case of a number of OECD countries. These results demonstrate that although Inflation Targeting has gone hand-in-hand with low inflation, the strategy was introduced well after inflation had begun its downward trend. But, then, Inflation Targeting 'locks in' low inflation rates. The evidence produced in this paper suggests that non-Inflation Targeting central banks have also been successful on this score.|Assessing Inflation Targeting through Intervention Analysis|http://www.jstor.org/stable/25167690|25167690|2008-04-01|2008|['eng']|['Economics - Economic policy', 'Economics - Economic disciplines']|['Business & Economics', 'Business', 'Economics']
There is significant interest in being able to predict where crimes will happen, for example to aid in the efficient tasking of police and other protective measures. We aim to model both the temporal and spatial dependencies often exhibited by violent crimes in order to make such predictions. The temporal variation of crimes typically follows patterns familiar in time series analysis, but the spatial patterns are irregular and do not vary smoothly across the area. Instead we find that spatially disjoint regions exhibit correlated crime patterns. It is this indeterminate inter-region correlation structure along with the discrete nature of small counts of serious crimes that motivates our proposed forecasting tool. In particular, we propose to model the crime counts in each region using an integer-valued first order autoregressive process. We take a Bayesian nonparametric approach to flexibly discover clusters of region-specific time series. We then describe how to account for covariates within this framework. Both approaches adjust for seasonality. We demonstrate our approach through an analysis of weekly reported violent crimes in Washington, D.C. between 2001-2008. Our forecasts outperform those of standard methods while additionally providing information from the posterior distribution of forecasts, such as prediction intervals.|SPATIO-TEMPORAL LOW COUNT PROCESSES WITH APPLICATION TO VIOLENT CRIME EVENTS|http://www.jstor.org/stable/44114349|44114349|2016-10-01|2016|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
Has there been a structural change in the way U.S. presidents use force abroad since the nineteenth century? In this article, I investigate historical changes in the use of force by U.S. presidents using Bayesian changepoint analysis. In doing so, I present an integrated Bayesian approach for analyzing changepoint problems in a Poisson regression model. To find the nature of the breaks, I estimate parameters of the Poisson regression changepoint model using Chib's (1998) hidden Markov model algorithm and Frühwirth-Schnatter and Wagner's (2006) data augmentation method. Then, I utilize transdimensional Markov chain Monte Carlo methods to detect the number of breaks. Analyzing yearly use of force data from 1890 to 1995, I find that, controlling for the effects of the Great Depression and the two world wars, the relationship between domestic conditions and the frequency of the use of force abroad fundamentally shifted in the 1940s.|Structural Change in U.S. Presidents' Use of Force|http://www.jstor.org/stable/27821951|27821951|2010-07-01|2010|['eng']|['Philosophy - Applied philosophy']|['Political Science', 'American Studies', 'Social Sciences', 'Area Studies']
In the absence of relevant prior experience, popular Bayesian estimation techniques usually begin with some form of 'uninformative' prior distribution intended to have minimal inferential influence. The Bayes rule will still produce nice looking estimates and credible intervals, but these lack the logical force that is attached to experience-based priors and require further justification. The paper concerns the frequentist assessment of Bayes estimates. A simple formula is shown to give the frequentist standard deviation of a Bayesian point estimate. The same simulations as required for the point estimate also produce the standard deviation. Exponential family models make the calculations particularly simple and bring in a connection to the parametric bootstrap.|Frequentist accuracy of Bayesian estimates|http://www.jstor.org/stable/24774821|24774821|2015-06-01|2015|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
It is of interest in many applications to study trends over time in relationships among categorical variables, such as age group, ethnicity, religious affiliation, political party, and preference for particular policies. At each time point, a sample of individuals provides responses to a set of questions, with different individuals sampled at each time. In such settings, there tend to be an abundance of missing data and the variables being measured may change over time. At each time point, we obtained a large sparse contingency table, with the number of cells often much larger than the number of individuals being surveyed. To borrow information across time in modeling large sparse contingency tables, we propose a Bayesian autoregressive tensor factorization approach. The proposed model relies on a probabilistic Parafac factorization of the joint pmf characterizing the categorical data distribution at each time point, with autocorrelation included across times. We develop efficient computational methods that rely on Markov chain Monte Carlo. The methods are evaluated through simulation examples and applied to social survey data. Supplementary materials for this article are available online.|Bayesian Modeling of Temporal Dependence in Large Sparse Contingency Tables|http://www.jstor.org/stable/24247064|24247064|2013-12-01|2013|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
We consider a Bayesian model selection strategy based on predicting the signs of the coefficients in a regression model, i.e. we consider identification of coefficients in a full or encompassing model for which we can confidently predict whether they are positive or negative. This is useful when our main purpose in doing model selection is interpretation, since the sign of a coefficient is often of primary importance for this task. In the case of a linear model with standard non-informative prior, we connect our sign coefficient prediction approach to the classical Zheng-Loh procedure for model selection. One advantage of our approach is that only specification of a prior on the full model is required, unlike standard Bayesian variable selection approaches which require specification of prior distributions on parameters in all submodels, and specification of a prior on the model itself. We consider applying our method with proper hierarchical shrinkage priors, which makes the procedure more useful in'large p, small n' regression problems with more predictors than observations and in problems involving multicollinearity. In these problems we may wish to do prediction by using shrinkage methods in the full model, but interpreting which variables are important is also of interest. We compare selection by using our coefficient sign prediction approach with the recently proposed elastic net procedure of Zou and Hastie and observe that our method shares some of the features of the elastic net such as a group selection property. The method can be extended to more complex model selection problems such as selection on variance components in random-effects models. For selection on variance components where the parameter of interest is non-negative and hence prediction of the sign of the parameter not the appropriate way to proceed, we consider instead prediction of the sign of the score component for the parameter at zero, obtaining a method that is related to classical score tests on variance components.|Coefficient Sign Prediction Methods for Model Selection|http://www.jstor.org/stable/4623278|4623278|2007-01-01|2007|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
New car purchases are among the largest and most expensive purchases consumers ever make. While functional and economic concerns are important, the authors examine whether visual influence also plays a role. Using a hierarchical Bayesian probability model and data on 1.6 million new cars sold over nine years, they examine how visual influence affects purchase volume, focusing on three questions: Are people more likely to buy a new car if others around them have recently done so? Are these effects moderated by visibility, the ease of seeing others' behavior? Do they vary according to the identity (e.g., gender) of prior purchasers and the identity relevance of vehicle type? The authors perform an extensive set of tests to rule out alternatives to visual influence and find that visual effects are (1) present (one additional purchase for approximately every seven prior purchases), (2) larger in areas where others' behavior should be more visible (i.e., more people commute in car-visible ways), (3) stronger for prior purchases by men than by women in male-oriented vehicle types, (4) extant only for cars of similar price tiers, and (5) subject to saturation effects.|Visual Influence and Social Groups|http://www.jstor.org/stable/41714473|41714473|2012-12-01|2012|['eng']|['Information science - Coding theory']|['Business', 'Business & Economics Collection', 'Marketing & Advertising']
Nonlinear panel data models arise naturally in economic applications, yet their analysis is challenging. Here we provide a progress report on some recent advances in the area. We start by reviewing the properties of random-effects likelihood approaches. We emphasize a link with Bayesian computation and Markov chain Monte Carlo, which provides a convenient approach to estimation and inference. The relaxation of parametric assumptions on the distribution of individual effects raises serious identification problems. In discrete choice models, common parameters and average marginal effects are generally set identified. The availability of continuous outcomes, however, provides opportunities for point identification. We end by reviewing recent progress on non-fixed-T approaches. In panel applications in which the time dimension is not negligible relative to the size of the cross section, it makes sense to view the estimation problem as a time-series finite-sample bias. Several perspectives to bias reduction are now available. We review their properties, with a special emphasis on randomeffects methods.|Nonlinear Panel Data Analysis|http://www.jstor.org/stable/42940193|42940193|2011-01-01|2011|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Business & Economics Collection', 'Economics']
In earlier articles, we developed an automated methodology for using cubic splines with tail linear constraints to model the logarithm of a univariate density function. This methodology was subsequently modified so that the knots were determined by stepwise addition-deletion and the remaining coefficients were determined by maximum likelihood estimation. An alternative approach, referred to as the free knot spline procedure, is to use the maximum likelihood method to estimate the knot locations as well as the remaining coefficients. This article compares various approaches to constructing confidence intervals for logspline density estimates, for both the stepwise procedure and the free knot procedure. It is concluded that a variation of the bootstrap, in which only a limited number of bootstrap simulations are used to estimate standard errors that are combined with standard normal quantiles, seems to perform the best, especially when coverages and computing time are both taken into account.|Comparison of Parametric and Bootstrap Approaches to Obtaining Confidence Intervals for Logspline Density Estimation|http://www.jstor.org/stable/1391147|1391147|2004-03-01|2004|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Computer Science', 'Statistics']
Although the lymphatic system is clearly linked to the metastasis of most human carcinomas, the mechanisms by which lymphangiogenesis occurs in response to the presence of carcinoma remain unclear. Hierarchical models are presented to investigate the properties of lymphatic vessel production in 2997 fields taken from 20 individuals with invasive carcinoma, 21 individuals with cervical intraepithelial neoplasia and 21 controls. Such data demonstrate a high degree of correlation within tumour samples from the same individual. Joint hierarchical models utilising shared random effects are discussed and fitted in a Bayesian framework to allow for the correlation between two key outcome measures: a random cluster size (the number of lymphatic vessels in a tissue sample) and a continuous outcome (vessel size). Results show that invasive carcinoma samples are associated with increased production of smaller and more irregularly-shaped lymphatic vessels and suggest a mechanistic link between carcinoma of the cervix and lymphangiogenesis.|LYMPHANGIOGENESIS AND CARCINOMA IN THE UTERINE CERVIX: JOINT AND HIERARCHICAL MODELS FOR RANDOM CLUSTER SIZES AND CONTINUOUS OUTCOMES|http://www.jstor.org/stable/43826450|43826450|2015-12-01|2015|['eng']|['Biological sciences - Biology', 'Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
We study Ornstein-Uhlenbeck stochastic processes driven by Lévy processes, and extend them to more general non-Ornstein-Uhlenbeck models. In particular, we investigate the means of making the correlation structure in the volatility process more flexible. For one model, we implement a method for introducing quasi long-memory into the volatility model. We demonstrate that the models can be fitted to real share price returns data.|Simulation and Inference for Stochastic Volatility Models Driven by Lévy Processes|http://www.jstor.org/stable/20441401|20441401|2007-08-01|2007|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical analysis', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
In this article, we describe a Bayesian approach to the calibration of a stochastic computer model of chemical kinetics. As with many applications in the biological sciences, the data available to calibrate the model come from different sources. Furthermore, these data appear to provide somewhat conflicting information about the model parameters. We describe a modeling framework that allows us to synthesize this conflicting information and arrive at a consensus inference. In particular, we show how random effects can be incorporated into the model to account for between-individual heterogeneity that may be the source of the apparent conflict.|<strong>Bayesian Calibration of a Stochastic Kinetic Computer Model Using Multiple Data Sources</strong>|http://www.jstor.org/stable/40663173|40663173|2010-03-01|2010|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
The present paper introduces and illustrates methodological developments intended for so-called optimal fingerprinting methods, which are of frequent use in detection and attribution studies. These methods used to involve three independent steps: preliminary reduction of the dimension of the data, estimation of the covariance associated to internal climate variability, and, finally, linear regression inference with associated uncertainty assessment. It is argued that such a compartmentalized treatment presents several issues; an integrated method is thus introduced to address them. The suggested approach is based on a single-piece statistical model that represents both linear regression and control runs. The unknown covariance is treated as a nuisance parameter that is eliminated by integration. This allows for the introduction of regularization assumptions. Point estimates and confidence intervals follow from the integrated likelihood. Further, it is shown that preliminary dimension reduction is not required for implementability and that computational issues associated to using the raw, high-dimensional, spatiotemporal data can be resolved quite easily. Results on simulated data show improved performance compared to existing methods w.r.t. both estimation error and accuracy of confidence intervals and also highlight the need for further improvements regarding the latter. The method is illustrated on twentieth-century precipitation and surface temperature, suggesting a potentially high informational benefit of using the raw, nondimension-reduced data in detection and attribution (D&amp;A), provided model error is appropriately built into the inference.|Integrated Optimal Fingerprinting|http://www.jstor.org/stable/26385375|26385375|2016-03-15|2016|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics', 'Mathematics - Mathematical analysis', 'Mathematics - Mathematical logic']|
We investigate whether return volatility is predictable by macroeconomic and financial variables to shed light on the economic drivers of financial volatility. Our approach is distinct owing to its comprehensiveness: First, we employ a data-rich forecast methodology to handle a large set of potential predictors in a Bayesian model-averaging approach and, second, we take a look at multiple asset classes (equities, foreign exchange, bonds and commodities) over long time spans. We find that proxies for credit risk and funding liquidity consistently show up as common predictors of volatility across asset classes. Variables capturing time-varying risk premia also perform well as predictors of volatility. While forecasts by macro-finance augmented models also achieve forecasting gains out-of-sample relative to autoregressive benchmarks, the performance varies across asset classes and over time.|A COMPREHENSIVE LOOK AT FINANCIAL VOLATILITY PREDICTION BY ECONOMIC VARIABLES|http://www.jstor.org/stable/23355909|23355909|2012-09-01|2012|['eng']|['Physical sciences - Astronomy', 'Mathematics - Applied mathematics']|['Business', 'Business & Economics Collection', 'Economics']
An important task of the U. S. Nuclear Regulatory Commission is to examine annual operating data from the nation's population of nuclear power plants for trends over time. We are interested here in trends in the scram rate at 66 commercial nuclear power plants based on annual observed scram data from 1984-1993. For an assumed Poisson distribution on the number of unplanned scrams, a gamma prior, and an appropriate hyperprior, a parametric empirical Bayes (PEB) approximation to a full hierarchical Bayes formulation is used to estimate the scram rate for each plant for each year. The PEB-estimated prior and posterior distributions are then subsequently smoothed over time using an exponentially weighted moving average. The results indicate that such bidirectional shrinkage is quite useful for identifying reliability trends over time.|Estimation of Trends in the Scram Rate at Nuclear Power Plants|http://www.jstor.org/stable/1271351|1271351|1999-11-01|1999|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
We estimate a model of voting in Congress that allows for dispersed information about the quality of proposals in an equilibrium context. In equilibrium, the Senate only approves House bills that receive the support of a supermajority of members of the lower chamber. We estimate this endogenous supermajority rule to be about four-fifths on average across policy areas. Our results indicate that the value of information dispersed among legislators is significant, and that in equilibrium a large fraction of House members' (40-50%) votes following their private information. Finally, we show that the probability of a type I error in Congress (not passing a good bill) is on average about twice as high as the probability of a type II error (passing a low-quality bill).|Voting in the Bicameral Congress: Large Majorities as a Signal of Quality|http://www.jstor.org/stable/43774380|43774380|2013-10-01|2013|['eng']|['Information science - Coding theory', 'Applied sciences - Engineering']|['Business & Economics', 'Law', 'Business', 'Economics', 'Law']
This paper uses a unique data set to determine which dynamic model of tacit collusion best describes behaviour in a particular industry. The area investigated is a region of the Vancouver, British Columbia retail-gasoline market. Players are service-station managers who compete daily. Firms choose price in each period using strategies that depend on prices chosen in the previous period. Periodically, unanticipated demand shocks precipitate price wars. When shocks occur, the firms in the market must determine the new demand conditions and adjust their strategies. From an econometric point of view, slopes of intertemporal reaction functions are latent variables. The resulting system of equations with time-varying parameters is estimated via the Kalman filter. Different repeated-game oligopoly models correspond to different transition matrices for the latent variables. The models can thus be assessed in terms of their power to explain firm behaviour in this market.|Vancouver's Gasoline-Price Wars: An Empirical Exercise in Uncovering Supergame Strategies|http://www.jstor.org/stable/2297954|2297954|1992-04-01|1992|['eng']|['Mathematics - Mathematical logic']|['Business & Economics', 'Business', 'Economics']
Nonparametric regression techniques are often sensitive to the presence of correlation in the errors. The practical consequences of this sensitivity are explained, including the breakdown of several popular data-driven smoothing parameter selection methods. We review the existing literature in kernel regression, smoothing splines and wavelet regression under correlation, both for short-range and long-range dependence. Extensions to random design, higher dimensional models and adaptive estimation are discussed.|Nonparametric Regression with Correlated Errors|http://www.jstor.org/stable/2676791|2676791|2001-05-01|2001|['eng']|['Mathematics - Applied mathematics']|['Science and Mathematics', 'Statistics']
We present an approach for estimating physical parameters in nonlinear models that relies on an approximation to the mechanistic model itself for computational efficiency. The proposed methodology is validated and applied in two different modeling scenarios: (a) Simulation and (b) lower trophic level ocean ecosystem model. The approach we develop relies on the ability to predict right singular vectors (resulting from a decomposition of computer model experimental output) based on the computer model input and an experimental set of parameters. Critically, we model the right singular vectors in terms of the model parameters via a nonlinear statistical model. Specifically, we focus our attention on first-order models of these right singular vectors rather than the second-order (covariance) structure.|Assessing First-Order Emulator Inference for Physical Parameters in Nonlinear Mechanistic Models|http://www.jstor.org/stable/23238828|23238828|2011-12-01|2011|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Agriculture', 'Statistics']
Identifying genes differentially expressed between a treatment and a control experimental condition is a common task for gene expression data analysts. Standard existing methods are the two-sample t-test, the regularized t-test (Cyber-T) and the Bayesian t-test. In this paper, we propose a Bayesian approach to identify genes differentially expressed based on the posterior probability of the difference calculated via the Bayes factor. In order to calculate the Bayes factor, we use the predictive density that is constructed by using the previously observed gene expression levels. We perform a simulation study with small sample sizes, which is usual in gene expression data analysis, to verify the performance of the proposed method and compare it with the standard ones. The results revel a better performance of the proposed methodology in identification of difference of means and/or variance. The methodology is also illustrated on the Escherichia coli bacterium dataset.|A predictive Bayes factor approach to identify genes differentially expressed: An application to Escherichia coli bacterium data|http://www.jstor.org/stable/43601349|43601349|2014-05-01|2014|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Mathematics', 'Statistics']
We propose a general Bayesian criterion for model assessment. The criterion is constructed from the posterior predictive distribution of the data, and can be written as a sum of two components, one involving the means of the posterior predictive distribution and the other involving the variances. It can be viewed as a Bayesian goodness-of-fit statistic which measures the performance of a model by a combination of how close its predictions are to the observed data and the variability of the predictions. We call this proposed predictive criterion the L measure, it is motivated by earlier work of Ibrahim and Laud (1994) and related to a criterion of Gelfand and Ghosh (1998). We examine the L measure in detail for the class of generalized linear models and survival models with right censored or interval censored data. We also propose a calibration of the L measure, defined as the prior predictive distribution of the difference between the L measures of the candidate model and the criterion minimizing model, and call it the calibration distribution. The calibration distribution will allow us to formally compare two competing models based on their L measure values. We discuss theoretical properties of the calibration distribution in detail, and provide Monte Carlo methods for computing it. For the linear model, we derive an analytic closed form expression for the L measure and the calibration distribution, and also derive a closed form expression for the mean of the calibration distribution. These novel developments will enable us to fully characterize the properties of the L measure for each model under consideration and will facilitate a direct formal comparison between several models, including non-nested models. Informative priors based on historical data and computational techniques are discussed. Several simulated and real datasets are used to demonstrate the proposed methodology.|CRITERION-BASED METHODS FOR BAYESIAN MODEL ASSESSMENT|http://www.jstor.org/stable/24306870|24306870|2001-04-01|2001|['eng']|['Mathematics - Applied mathematics']|['Mathematics', 'Science and Mathematics', 'Statistics']
We propose a new framework for design under uncertainty based on stochastic computer simulations and multi-level recursive co-kriging. The proposed methodology simultaneously takes into account multi-fidelity in models, such as direct numerical simulations versus empirical formulae, as well as multi-fidelity in the probability space (e.g. sparse grids versus tensor product multi-element probabilistic collocation). We are able to construct response surfaces of complex dynamical systems by blending multiple information sources via auto-regressive stochastic modelling. A computationally efficient machine learning framework is developed based on multi-level recursive co-kriging with sparse precision matrices of Gaussian–Markov random fields. The effectiveness of the new algorithms is demonstrated in numerical examples involving a prototype problem in risk-averse design, regression of random functions, as well as uncertainty quantification in fluid mechanics involving the evolution of a Burgers equation from a random initial state, and random laminar wakes behind circular cylinders.|Multi-fidelity modelling via recursive co-kriging and Gaussian–Markov random fields|http://www.jstor.org/stable/24509980|24509980|2015-07-08|2015|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'General Science']
We propose several Bayesian models for modelling time-to-event data. We consider a piecewise exponential model, a fully parametric cure rate model and a semiparametric cure rate model. For each model, we derive the likelihood function and examine some of its properties for carrying out Bayesian inference with non-informative priors. We also examine model identifiability issues and give conditions which guarantee identifiability. Also, for each model, we construct a class of informative prior distributions based on historical data, i.e. data from similar previous studies. These priors, called power priors, prove to be quite useful in this context. We examine the properties of the power priors for Bayesian inference and, in particular, we study their effect on the current analysis. Tools for model comparison and model assessment are also proposed. A detailed case-study of a recently completed melanoma clinical trial conducted by the Eastern Cooperative Oncology Group is presented and the methodology proposed is demonstrated in detail.|Bayesian Cure Rate Models for Malignant Melanoma: A Case-Study of Eastern Cooperative Oncology Group Trial E1690|http://www.jstor.org/stable/3592743|3592743|2002-01-01|2002|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Spatial statistical analysis of massive amounts of spatial data can be challenging because computation of optimal procedures can break down. The Spatial Random Effects (SRE) model uses a fixed number of known but not necessarily orthogonal (multiresolutional) spatial basis functions, which gives a flexible family of nonstationary covariance functions, results in dimension reduction, and yields optimal spatial predictors whose computations are scalable. By modeling spatial data in a hierarchical manner with a process model that includes the SRE model, the choice is whether to estimate the SRE model's parameters or to take a Bayesian approach and put a prior distribution on them. In this article, we develop Bayesian inference for the SRE model when the spatial basis functions are multiresolutional. Then the covariance matrix of the random effects decomposes naturally in terms of Givens angles and eigenvalues, for which a new class of prior distributions is developed. This approach to prior specification of a spatial covariance matrix offers remarkable improvement over other types of priors used in the random-effects literature (e.g., Wishart priors), as demonstrated in a simulation experiment. Further, a large remote-sensing dataset of aerosol optical depth (AOD), from the Multi-angle Imaging SpectroRadiometer (MISR) instrument on the Terra satellite, is analyzed in a fully Bayesian framework, using the new prior, and compared to an empirical-Bayesian analysis.|Bayesian Inference for the Spatial Random Effects Model|http://www.jstor.org/stable/23427567|23427567|2011-09-01|2011|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
The Mk model was developed for estimating phylogenetic trees from discrete morphological data, whether for living or fossil taxa. Like any model, the Mk model makes a number of assumptions. One assumption is that transitions between character states are symmetric (i.e., the probability of changing from 0 to 1 is the same as 1 to 0). However, some characters in a data matrix may not satisfy this assumption. Here, we test methods for relaxing this assumption in a Bayesian context. Using empirical data sets, we perform model fitting to illustrate cases in which modeling asymmetric transition rates among characters is preferable to the standard Mk model. We use simulated data sets to demonstrate that choosing the best-fit model of transition-state symmetry can improve model fit and phylogenetic estimation.|Modeling Character Change Heterogeneity in Phylogenetic Analyses of Morphology through the Use of Priors|http://www.jstor.org/stable/44028780|44028780|2016-07-01|2016|['eng']|['Biological sciences - Biology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
This paper develops Bayesian methodology for estimating long-term trends in the daily maxima of tropospheric ozone. The methods are then applied to study long-term trends in ozone at six monitoring sites in the state of Texas. The methodology controls for the effects of meteorological variables because it is known that variables such as temperature, wind speed and humidity substantially affect the formation of tropospheric ozone. A semiparametric regression model is estimated in which a nonparametric trivariate surface is used to model the relationship between ozone and these meteorological variables because, while it is known that the relationship is a complex nonlinear one, its functional form is unknown. The model also allows for the effects of wind direction and seasonality. The errors are modeled as an autoregression, which is methodologically challenging because the observations are unequally spaced over time. Each function in the model is represented as a linear combination of basis functions located at all of the design points. We also estimate an appropriate data transformation simultaneously with the functions. The functions are estimated nonparametrically by a Bayesian hierarchical model that uses indicator variables to allow a non-zero probability that the coefficient of each basis term is zero. The entire model, including the nonparametric surfaces, data transformation and autoregression for the unequally spaced errors, is estimated using a Markov chain Monte Carlo sampling scheme with a computationally efficient transition kernel for generating the indicator variables. The empirical results indicate that key meteorological variables explain most of the variation in daily ozone maxima through a nonlinear interaction and that their effects are consistent across the six sites. However, the estimated trends vary considerably from site to site, even within the same city. /// Dans ce papier, une méthode d'estimation pour la prediction de la tendance au long term des maxima journaliers du niveau d'ozone troposphérique est développée. Cette méthode est ensuite appliquée à l'étude de ces maxima pour six sites de l'état du Texas. Des facteurs météorologiques sont utilisés comme variables explicatives car il est connu que la température, le vent et l'humidité sont des facteurs de première importance affectant la formation d'ozone troposphérique. Etant donne que ces facteurs sont lies non linéairement au niveau d'ozone et que leur forme est inconnué, des surfaces non parametriques trivariées sont utilisées pour modeliser ces relations. Chaque fonction consiste en une combinaison linéaire de bases. Les effets de la direction du vent ainsi que des saisons sont aussi démontres. Les erreurs suivent un model autoregressif ce qui est particulièrement intéressant car les observations sont irregulierement espacées dans le temps. Une transformation des observations est aussi estimée simultanement avec les fonctions. Les fonctions sont modellisées en utilisant un model de Bayes hierachique avec variables indicatrices pour chacune des bases permêttant d'avoir une probabilité non nulle que leur coefficient soit zero. Le model contenant les surfaces non parametriques, la transformation des observations et le model autoregressif sur les erreurs est estimé avec Markov chain Monte Carlo en utilisant une méthode numériquement efficiente pour generer les variables indicatrices. Les resultats empiriques montrent que quelques facteurs météorologiques importants expliquent la plupart de la variabilité dans les changements des maxima d'ozone troposphérique journaliers à travers une forme non linéaire. Ces effets sont similaires pour les six sités consideres, contrairement à la tendance qui varié enormement d'un site a un autre, même a l'intérieur de la même ville.|Estimating Long-Term Trends in Tropospheric Ozone Levels|http://www.jstor.org/stable/1403728|1403728|2002-04-01|2002|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
When analyzing microarray data, hierarchical models are often used to share information across genes when estimating means and variances or identifying differential expression. Many methods utilize some form of the two-level hierarchical model structure suggested by Kendziorski et al. [Stat. Med. (2003) 22 3899–3914] in which the first level describes the distribution of latent mean expression levels among genes and among differentially expressed treatments within a gene. The second level describes the conditional distribution, given a latent mean, of repeated observations for a single gene and treatment. Many of these models, including those used in Kendziorski's et al. [Stat. Med. (2003) 22 3899–3914] EBarrays package, assume that expression level changes due to treatment effects have the same distribution as expression level changes from gene to gene. We present empirical evidence that this assumption is often inadequate and propose three-level hierarchical models as extensions to the two-level log-normal based EBarrays models to address this inadequacy. We demonstrate that use of our three-level models dramatically changes analysis results for a variety of microarray data sets and verify the validity and improved performance of our suggested method in a series of simulation studies. We also illustrate the importance of accounting for the uncertainty of gene-specific error variance estimates when using hierarchical models to identify differentially expressed genes.|THE IMPORTANCE OF DISTINCT MODELING STRATEGIES FOR GENE AND GENE-SPECIFIC TREATMENT EFFECTS IN HIERARCHICAL MODELS FOR MICROARRAY DATA|http://www.jstor.org/stable/41713517|41713517|2012-09-01|2012|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
This paper considers the analysis of round robin interaction data whereby individuals from a group of subjects interact with one another, producing a pair of outcomes, one for each individual. The authors provide an overview of the various analyses applied to these types of data and extend the work in several directions. In particular, they provide a fully Bayesian analysis for such data and use a real data example for illustration purposes. /// Cet article concerne l'analyse de données issues de tournois à la ronde dans lesquels l'interaction des membres d'un groupe deux à la fois donne lieu à des paires de résultats, un pour chaque individu. Les auteurs passent en revue les différents types d'analyses existantes pour ce type de données et en proposent diverses généralisations. Ils montrent notamment comment analyser de telles données par une approche bayésienne. Leur propos est illustré par un exemple concret.|Statistical Analyses for Round Robin Interaction Data|http://www.jstor.org/stable/3316080|3316080|2001-06-01|2001|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
This paper first tests the restrictions implied by Hall's [1978] version of the permanent income hypothesis (PIH) obtained from a bivariate system of labor income and savings, using quarterly data over the period of 1947:01-2008:03 for the US economy, and then uses the model to forecast changes in labor income over the period of 1991:01-2008:03, using 1947:01-1990:04 as the in-sample. First, our results indicate the overwhelming rejection of the restrictions on the data implied by the PIH. Second, we found that, when compared to univariate and bivariate versions of classical and Bayesian Vector Autoregressive (BVAR) models, the PIH model, in general, is outperformed by all other models in terms of the average Root Mean Squared Errors for one-to eightquarters-ahead forecasts for the changes in labor income. Finally, as far as forecasting is concerned, we found the most tight Gibbs-sampled univarite BVAR to perform the best. In sum, we do not find evidence for the US data to be consistent with the PIH, neither does the PIH model perform better relative to alternative atheoretical models in forecasting changes in labor income over an out-of-sample horizon that was characterized by high degree of volatility for the variable of interest.|Is the Permanent Income Hypothesis Really Well-Suited for Forecasting?|http://www.jstor.org/stable/41239560|41239560|2011-04-01|2011|['eng']|['Mathematics - Applied mathematics']|['Business & Economics Collection', 'Economics']
One of the contexts where small area estimation techniques have proved their potential is the analysis of data collected in national labour force surveys to obtain estimates for small geographical domains. Applications of small area estimation methods to data from labour force surveys have recently been considered in Italy. This paper gives a review of specific problems, data and opportunities for the application of small area estimation models for producing reliable information at provincial and sub-provincial level in Italy on labour force aggregates. Some new developments stimulated by the application of small area estimation models to the analysis of labour force survey data are also discussed.|LABOUR FORCE ESTIMATES FOR SMALL GEOGRAPHICAL DOMAINS IN ITALY: PROBLEMS, DATA AND MODELS|http://www.jstor.org/stable/41625219|41625219|2008-10-01|2008|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics']|['Business & Economics', 'Sociology', 'Economics', 'Social Sciences']
Categorical data analysis is typically performed by fitting models to the observed counts in a contingency table using maximum likelihood. An inherent problem with maximum likelihood fits is their sensitivity to outlier cells, ones whose counts are not consistent with the presupposed model. Robust alternatives to maximum likelihood estimation, including least median of chi-squared residuals, least median of weighted squared residuals, and analogous methods using least trimmed functions, are proposed in this article. Equivariance and breakdown properties are discussed. Monte Carlo simulation results and three real examples are used to illustrate the properties of the estimators in practice. In particular, whereas the maximum likelihood estimates break down in the presence of outlying cells, the robust estimators do not as long as the contamination does not exceed the breakdown point. The proposed estimators perform similarly in the simulations; they are competitive with median polish when fitting independence, and generalize easily to other, more complex, models.|A Robust Approach to Categorical Data Analysis|http://www.jstor.org/stable/1391031|1391031|2001-03-01|2001|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Computer Science', 'Statistics']
We consider the problem of combining inference in related nonparametric Bayes models. Analogous to parametric hierarchical models, the hierarchical extension formalizes borrowing strength across the related submodels. In the nonparametric context, modelling is complicated by the fact that the random quantities over which we define the hierarchy are infinite dimensional. We discuss a formal definition of such a hierarchical model. The approach includes a regression at the level of the nonparametric model. For the special case of Dirichlet process mixtures, we develop a Markov chain Monte Carlo scheme to allow efficient implementation of full posterior inference in the given model.|A Method for Combining Inference across Related Nonparametric Bayesian Models|http://www.jstor.org/stable/3647503|3647503|2004-01-01|2004|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
"Causal inference with observational data frequently relies on the notion of the propensity score (PS) to adjust treatment comparisons for observed confounding factors. As decisions in the era of ""big data"" are increasingly reliant on large and complex collections of digital data, researchers are frequently confronted with decisions regarding which of a high-dimensional covariate set to include in the PS model to satisfy the assumptions necessary for estimating average causal effects. Typically, simple or ad hoc methods are employed to arrive at a single PS model, without acknowledging the uncertainty associated with the model selection. We propose three Bayesian methods for PS variable selection and model averaging that (a) select relevant variables from a set of candidate variables to include in the PS model and (b) estimate causal treatment effects as weighted averages of estimates under different PS models. The associated weight for each PS model reflects the data-driven support for that model's ability to adjust for the necessary variables. We illustrate features of our proposed approaches with a simulation study, and ultimately use our methods to compare the effectiveness of surgical versus nonsurgical treatment for brain tumors among 2606 Medicare beneficiaries. Supplementary materials for this article are available online."|Uncertainty in Propensity Score Estimation: Bayesian Methods for Variable Selection and Model-Averaged Causal Effects|http://www.jstor.org/stable/24247140|24247140|2014-03-01|2014|['eng']|['Biological sciences - Biochemistry', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
A method for obtaining early forecasts for the sales of new durable products based on Hierarchical Bayes procedures is presented. The Bass model is implemented within this framework by using a nonlinear regression approach. The linear regression model has been shown to have numerous shortcomings. Two stages of prior distributions use sales data from a variety of dissimilar new products. The first prior distribution describes the variation among the parameters of the products, and the second prior distribution expresses the uncertainty about the hyperparameters of the first prior. Before observing sales data for a new product launch, the forecasts are the expectation of the first stage prior distribution. As sales data become available, the forecasts adapt to the unique features of the product. Early forecasting and the adaptive capability are the two major payoffs from using Hierarchical Bayes procedures. This contrasts with other estimation approaches which either use a linear model or provide reasonable forecasts only after the inflection point of the time series of the sales. The paper also indicates how the Hierarchical Bayes procedure can be extended to include exogenous variables.|New Models from Old: Forecasting Product Adoption by Hierarchical Bayes Procedures|http://www.jstor.org/stable/183952|183952|1990-01-01|1990|['eng']|['Mathematics - Applied mathematics', 'Information science - Information analysis']|['Marketing & Advertising', 'Business & Economics', 'Business']
An outcome-adaptive Bayesian design is proposed for choosing the optimal dose pair of a chemotherapeutic agent and a biological agent used in combination in a phase I/II clinical trial. Patient outcome is characterized as a vector of two ordinal variables accounting for toxicity and treatment efficacy. A generalization of the Aranda-Ordaz model (1981, Biometrika 68, 357-363) is used for the marginal outcome probabilities as functions of a dose pair, and a Gaussian copula is assumed to obtain joint distributions. Numerical utilities of all elementary patient outcomes, allowing the possibility that efficacy is inevaluable due to severe toxicity, are obtained using an elicitation method aimed to establish consensus among the physicians planning the trial. For each successive patient cohort, a dose pair is chosen to maximize the posterior mean utility. The method is illustrated by a trial in bladder cancer, including simulation studies of the method's sensitivity to prior parameters, the numerical utilities, correlation between the outcomes, sample size, cohort size, and starting dose pair.|<strong>Utility-Based Optimization of Combination Therapy Using Ordinal Toxicity and Efficacy in Phase I/II Trials</strong>|http://www.jstor.org/stable/40663246|40663246|2010-06-01|2010|['eng']|['Applied sciences - Engineering', 'Health sciences - Health and wellness']|['Science & Mathematics', 'Statistics']
A common problem in Bayesian object recognition using marked point process models is to produce a point estimate of the true underlying object configuration: the number of objects and the size, location and shape of each object. We use decision theory and the concept of loss functions to design a more reasonable estimator for this purpose, rather than using the common zero-one loss corresponding to the maximum a posteriori estimator. We propose to use the squared Δ -metric of Baddeley (1992) as our loss function and demonstrate that the corresponding optimal Bayesian estimator can be well approximated by combining Markov chain Monte Carlo methods with simulated annealing into a two-step algorithm. The proposed loss function is tested using a marked point process model developed for locating cells in confocal microscopy images.|Bayesian Object Recognition with Baddeley's Delta Loss|http://www.jstor.org/stable/1427876|1427876|1998-03-01|1998|['eng']|['Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
In this article, we propose a Bayesian approach to phase I/II dose-finding oncology trials by jointly modeling a binary toxicity outcome and a continuous biomarker expression outcome. We apply our method to a clinical trial of a new gene therapy for bladder cancer patients. In this trial, the biomarker expression indicates biological activity of the new therapy. For ethical reasons, the trial is conducted sequentially, with the dose for each successive patient chosen using both toxicity and activity data from patients previously treated in the trial. The modeling framework that we use naturally incorporates correlation between the binary toxicity and continuous activity outcome via a latent Gaussian variable. The dose-escalation/de-escalation decision rules are based on the posterior distributions of both toxicity and activity. A flexible state-space model is used to relate the activity outcome and dose. Extensive simulation studies show that the design reliably chooses the preferred dose using both toxicity and expression outcomes under various clinical scenarios.|A Bayesian Approach to Jointly Modeling Toxicity and Biomarker Expression in a Phase I/II Dose-Finding Trial|http://www.jstor.org/stable/3695953|3695953|2005-06-01|2005|['eng']|['Health sciences - Health and wellness']|['Science & Mathematics', 'Statistics']
Stan is a free and open-source C++ program that performs Bayesian inference or optimization for arbitrary user-specified models and can be called from the command line, R, Python, Matlab, or Julia and has great promise for fitting large and complex statistical models in many areas of application. We discuss Stan from users 'and developers' perspectives and illustrate with a simple but nontrivial nonlinear regression example.|Stan: A Probabilistic Programming Language for Bayesian Inference and Optimization|http://www.jstor.org/stable/43966398|43966398|2015-10-01|2015|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Education', 'Statistics', 'Social Sciences']
Strategic abstentions—in which legislators abstain from votes for ideological reasons—are a poorly understood feature of legislative voting records. The paper discusses a spatial model for legislators' revealed preferences that accounts for abstentions when missing values are non-ignorable and allows us to measure the pervasiveness of strategic abstention by identifying legislators who consistently engage in strategic abstentions, as well as bills for which the ideology of legislators is a key driver of abstentions. We illustrate the performance of our model through the analysis of the 101st–112th US Senates.|Measuring and accounting for strategic abstentions in the US Senate, 1989–2012|http://www.jstor.org/stable/24773008|24773008|2015-11-01|2015|['eng']|['Philosophy - Applied philosophy', 'Information science - Informetrics', 'Political science - Politics', 'Applied sciences - Research methods']|['Science & Mathematics', 'Statistics']
Recent studies argue that cultural and political-economic shifts have led to a sea change in penal regimes among modern Western societies, resulting in more punitive social policies in general and a trend toward higher incarceration rates in particular. This is a special case of a wider argument that globalization has led to a decline in state autonomy and convergence on a market-based model of economic and social policy. This thesis has been challenged by the empirical literature on welfare states, which finds persistent cross-national diversity in institutional structures, policies, and patterns of inequality. Focusing on incarceration rates as the outcome of interest, this study evaluates these arguments by applying a Bayesian change-point model to four decades of data from 15 countries. Results show that a regime shift did occur but that incarceration rates increased mainly among countries with unregulated labor markets, decentralized polities, or weak labor unions. Profound institutional differences persist and are fateful for incarceration trajectories.|The Transformation of Prison Regimes in Late Capitalist Societies|http://www.jstor.org/stable/10.1086/675300|10.1086/675300|2013-11-01|2013|['eng']|['Philosophy - Applied philosophy']|['Sociology', 'Social Sciences']
Objectives: We modelled exposure to traffic particles using a latent variable approach and investigated whether long-term exposure to traffic particles is associated with an increase in the occurrence of acute myocardial infarction (AMI) using data from a population-based coronary disease registry. Methods: Cases of individually validated AMI were identified between 1995 and 2003 as part of the Worcester Heart Attack Study. Population controls were selected from Massachusetts, USA, resident lists. NO 2 and PM 2.5 filter absorbance were measured at 36 locations throughout the study area. The air pollution data were used to estimate exposure to traffic particles using a semiparametric latent variable regression model. Conditional logistic models were used to estimate the association between exposure to traffic particles and occurrence of AMI. Results: Modelled exposure to traffic particles was highest near the city of Worcester. Cases of AMI were more exposed to traffic and traffic particles compared to controls. An interquartile range increase in modelled traffic particles was associated with a 10% (95% CI 4% to 16%) increase in the odds of AMI. Accounting for spatial dependence at the census tract, but not block group, scale substantially attenuated this association. Conclusions: These results provide some support for an association between long-term exposure to traffic particles and risk of AMI. The results were sensitive to the scale selected for the analysis of spatial dependence, an issue that requires further investigation. The latent variable model captured variation in exposure, although on a relatively large spatial scale.|Traffic particles and occurrence of acute myocardial infarction: a case–control analysis|http://www.jstor.org/stable/27797676|27797676|2009-12-01|2009|['eng']|['Physical sciences - Astronomy']|['Medicine & Allied Health', 'Health Sciences']
The λ-continuum of inductive methods was derived from an assumption, called λ-condition, which says that the probability of finding an individual having property $x_{j}$ depends only on the number of observed individuals having property $x_{j}$ and on the total number of observed individuals. So, according to that assumption, all individuals with properties which are different from $x_{j}$ have equal weight with respect to that probability and, in particular, it does not matter whether any individual was observed having some property similar to $x_{j}$ (the most complete proof of this result is presented in Carnap, 1980). The problem thus remained open to find some general condition, weaker than the λ-condition, which would allow for the derivation of probability functions which might be sensitive to similarity. Carnap himself suggested a weakening of the λ-condition which might allow for similarity sensitive probability functions (Carnap, 1980, p. 45) but he did not find the family of probability functions derivable from that principle. The aim of this paper is to present the family of probability functions derivable from Carnap's suggestion and to show how it is derived. In Section 1 the general problem of analogy by similarity in inductive logic is presented, Section 2 outlines the notation and the conceptual background involved in the proof, Section 3 gives the proof, Section 4 discusses Carnap's principle and the result, Section 5 is a brief review of the solutions which have previously been proposed.|Predictive Probability and Analogy by Similarity in Inductive Logic|http://www.jstor.org/stable/20012663|20012663|1995-11-01|1995|['eng']|['Mathematics - Mathematical logic']|['Humanities', 'Philosophy']
Online chatter, or user-generated content, constitutes an excellent emerging source for marketers to mine meaning at a high temporal frequency. This article posits that this meaning consists of extracting the key latent dimensions of consumer satisfaction with quality and ascertaining the valence, labels, validity, importance, dynamics, and heterogeneity of those dimensions. The authors propose a unified framework for this purpose using unsupervised latent Dirichlet allocation. The sample of user-generated content consists of rich data on product reviews across 15 firms in five markets over four years. The results suggest that a few dimensions with good face validity and external validity are enough to capture quality. Dynamic analysis enables marketers to track dimensions' importance over time and allows for dynamic mapping of competitive brand positions on those dimensions over time. For vertically differentiated markets (e.g., mobile phones, computers), objective dimensions dominate and are similar across markets, heterogeneity is low across dimensions, and stability is high over time. For horizontally differentiated markets (e.g., shoes, toys), subjective dimensions dominate but vary across markets, heterogeneity is high across dimensions, and stability is low over time.|Mining Marketing Meaning from Online Chatter|http://www.jstor.org/stable/26661847|26661847|2014-08-01|2014|['eng']|['Applied sciences - Engineering', 'Information science - Coding theory']|['Business & Economics', 'Marketing & Advertising', 'Business']
Markov chain Monte Carlo (MCMC) methods for Bayesian computation are mostly used when the dominating measure is the Lebesgue measure, the counting measure, or a product of these. Many Bayesian problems give rise to distributions that are not dominated by the Lebesgue measure or the counting measure alone. In this article we introduce a simple framework for using MCMC algorithms in Bayesian computation with mixtures of mutually singular distributions. The idea is to find a common dominating measure that allows the use of traditional Metropolis—Hastings algorithms. In particular, using our formulation, the Gibbs sampler can be used whenever the full conditionals are available. We compare our formulation with the reversible jump approach and show that the two are closely related. We give results for three examples, involving testing a normal mean, variable selection in regression, and hypothesis testing for differential gene expression under multiple conditions. This allows us to compare the three methods considered: Metropolis—Hastings with mutually singular distributions, Gibbs sampler with mutually singular distributions, and reversible jump. In our examples, we found the Gibbs sampler to be more precise and to need considerably less computer time than the other methods. In addition, the full conditionals used in the Gibbs sampler can be used to further improve the estimates of the model posterior probabilities via Rao—Blackwellization, at no extra cost.|Markov Chain Monte Carlo With Mixtures of Mutually Singular Distributions|http://www.jstor.org/stable/25651237|25651237|2008-12-01|2008|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Computer Science', 'Statistics']
Understanding which phenotypic traits are consistently correlated throughout evolution is a highly pertinent problem in modern evolutionary biology. Here, we propose a multivariate phylogenetic latent liability model for assessing the correlation between multiple types of data, while simultaneously controlling for their unknown shared evolutionary history informed through molecular sequences. The latent formulation enables us to consider in a single model combinations of continuous traits, discrete binary traits and discrete traits with multiple ordered and unordered states. Previous approaches have entertained a single data type generally along a fixed history, precluding estimation of correlation between traits and ignoring uncertainty in the history. We implement our model in a Bayesian phylogenetic framework, and discuss inference techniques for hypothesis testing. Finally, we showcase the method through applications to columbine flower morphology, antibiotic resistance in Salmonella and epitope evolution in influenza.|ASSESSING PHENOTYPIC CORRELATION THROUGH THE MULTIVARIATE PHYLOGENETIC LATENT LIABILITY MODEL|http://www.jstor.org/stable/24522611|24522611|2015-06-01|2015|['eng']|['Biological sciences - Biology']|['Mathematics', 'Science & Mathematics', 'Statistics']
In cluster randomized trials, patients seen by the same physician are randomized to the same treatment arm as a group. Besides the natural clustering of patients due to cluster/group randomization, interactions between an individual patient and the attending physician within the group could just as well influence patient care outcomes. Despite the intuitive relevance of these interactions to treatment assessment, few studies have thus far examined their influences. Whether and to what extent these interactions affect assessment of the treatment effect remains unexplored. In fact, few statistical models provide ready accommodation for such interactions. In this research, we propose a general modeling framework based on the nested Dirichlet process (nDP) for assessing treatment effect in cluster randomized trials. The proposed methodology explicitly accounts for physician—patient interactions by assuming that the interactions follow unspecified group-specific distributions from an nDP. In addition to accounting for physician—patient interactions, the model has greatly enhanced the flexibility of traditional mixed effect models by allowing for nonnormally distributed random effects, thus, alleviating concerns about mixed effect misspecification and sidestepping verification of distributional assumptions on random effects. At the same time, the model retains the mixed models' ability to make inferences on fixed effects. The proposed method is easily extendable to more complicated hierarchical clustering structures. We introduce the method in the context of a real cluster randomized trial. A comprehensive simulation study was conducted to assess the operating characteristics of the proposed nDP model.|A Nested Dirichlet Process Analysis of Cluster Randomized Trial Data With Application in Geriatric Care Assessment|http://www.jstor.org/stable/23427511|23427511|2013-03-01|2013|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Determining the magnitude and location of neural sources within the brain that are responsible for generating magnetoencephalography (MEG) signals measured on the surface of the head is a challenging problem in functional neuroimaging. The number of potential sources within the brain exceeds by an order of magnitude the number of recording sites. As a consequence, the estimates for the magnitude and location of the neural sources will be ill-conditioned because of the underdetermined nature of the problem. One well-known technique designed to address this imbalance is the minimum norm estimator (MNE). This approach imposes an L² regularization constraint that serves to stabilize and condition the source parameter estimates. However, these classes of regularizer are static in time and do not consider the temporal constraints inherent to the biophysics of the MEG experiment. In this paper we propose a dynamic state-space model that accounts for both spatial and temporal correlations within and across candidate intracortical sources. In our model, the observation model is derived from the steady-state solution to Maxwell's equations while the latent model representing neural dynamics is given by a random walk process. We show that the Kalman filter (KF) and the Kalman smoother [also known as the fixedinterval smoother (FIS)] may be used to solve the ensuing high-dimensional state-estimation problem. Using a well-known relationship between Bayesian estimation and Kalman filtering, we show that the MNE estimates carry a significant zero bias. Calculating these high-dimensional state estimates is a computationally challenging task that requires High Performance Computing (HPC) resources. To this end, we employ the NSF Teragrid Supercomputing Network to compute the source estimates. We demonstrate improvement in performance of the state-space algorithm relative to MNE in analyses of simulated and actual somatosensory MEG experiments. Our findings establish the benefits of high-dimensional state-space modeling as an effective means to solve the MEG source localization problem.|STATE-SPACE SOLUTIONS TO THE DYNAMIC MAGNETOENCEPHALOGRAPHY INVERSE PROBLEM USING HIGH PERFORMANCE COMPUTING1|http://www.jstor.org/stable/23024849|23024849|2011-06-01|2011|['eng']|['Applied sciences - Engineering', 'Biological sciences - Biology']|['Mathematics', 'Science & Mathematics', 'Statistics']
With rapid improvements in medical treatment and health care, many datasets dealing with time to relapse or death now reveal a substantial portion of patients who are cured (i.e., who never experience the event). Extended survival models called cure rate models account for the probability of a subject being cured and can be broadly classified into the classical mixture models of Berkson and Gage (BG type) or the stochastic tumor models pioneered by Yakovlev and extended to a hierarchical framework by Chen, Ibrahim, and Sinha (YCIS type). Recent developments in Bayesian hierarchical cure models have evoked significant interest regarding relationships and preferences between these two classes of models. Our present work proposes a unifying class of cure rate models that facilitates flexible hierarchical model-building while including both existing cure model classes as special cases. This unifying class enables robust modeling by accounting for uncertainty in underlying mechanisms leading to cure. Issues such as regressing on the cure fraction and propriety of the associated posterior distributions under different modeling assumptions are also discussed. Finally, we offer a simulation study and also illustrate with two datasets (on melanoma and breast cancer) that reveal our framework's ability to distinguish among underlying mechanisms that lead to relapse and cure.|Flexible Cure Rate Modeling under Latent Activation Schemes|http://www.jstor.org/stable/27639886|27639886|2007-06-01|2007|['eng']|['Applied sciences - Engineering', 'Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
Bayesian statistics can be hard to teach at an elementary level due to the difficulty in deriving the posterior distribution for interesting nonconjugate problems. One attractive method of summarizing the posterior distribution is to directly simulate from the probability distribution of interest and then explore the simulated sample. We illustrate the use of Rubin's Sampling-Importance-Resampling (SIR) algorithm to simulate posterior distributions for three inference problems. In each example, we focus on the construction of the prior distribution and then use exploratory data analysis techniques to describe the posterior samples and make inferences. The use of MINITAB macros is presented to illustrate the ease of performing this simulation on standard statistical computer programs.|Teaching Bayesian Statistics Using Sampling Methods and MINITAB|http://www.jstor.org/stable/2684973|2684973|1993-08-01|1993|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
This paper discusses the modelling of rainfall-flow (rainfall-run-off) and flow-routeing processes in river systems within the context of real-time flood forecasting. It is argued that deterministic, reductionist (or 'bottom-up') models are inappropriate for real-time forecasting because of the inherent uncertainty that characterizes river-catchment dynamics and the problems of model over-parametrization. The advantages of alternative, efficiently parametrized data-based mechanistic models, identified and estimated using statistical methods, are discussed. It is shown that such models are in an ideal form for incorporation in a real-time, adaptive forecasting system based on recursive state-space estimation (an adaptive version of the stochastic Kalman filter algorithm). An illustrative example, based on the analysis of a limited set of hourly rainfall-flow data from the River Hodder in northwest England, demonstrates the utility of this methodology in difficult circumstances and illustrates the advantages of incorporating real-time state and parameter adaption.|Advances in Real-Time Flood Forecasting|http://www.jstor.org/stable/3066450|3066450|2002-07-15|2002|['eng']|['Applied sciences - Engineering']|['General Science', 'Mathematics', 'Science and Mathematics']
In this paper, we propose a hierarchical Bayesian framework with a prior Dirichlet process for gene-by-gene multiple comparison analysis. The comparison among experimental conditions are made using the posterior probability for hypothesis of equality or inequality. To calculate the posterior probabilities, we use the Polya urn scheme through latent variables and the Bayes factor. The performance of the proposed method, as well as a comparison with usual Tukey-test, are evaluated on artificial data and on a shotgun proteomics data set. The results reveal a better performance of the proposed methodology in identification of difference of means and/or variance.|A gene-by-gene multiple comparison analysis: A predictive Bayesian approach|http://www.jstor.org/stable/43601324|43601324|2015-02-01|2015|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Mathematics', 'Statistics']
In several areas of application ranging from brain imaging to astrophysics and geostatistics, an important statistical problem is to find regions where the process studied exceeds a certain level. Estimating such regions so that the probability for exceeding the level in the entire set is equal to some predefined value is a difficult problem connected to the problem of multiple significance testing. In this work, a method for solving this problem, as well as the related problem of finding credible regions for contour curves, for latent Gaussian models is proposed. The method is based on using a parametric family for the excursion sets in combination with a sequential importance sampling method for estimating joint probabilities. The accuracy of the method is investigated by using simulated data and an environmental application is presented.|Excursion and contour uncertainty regions for latent Gaussian models|http://www.jstor.org/stable/24774726|24774726|2015-01-01|2015|['eng']|['Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
We discuss two methods of making nonparametric Bayesian inference on probability measures subject to a partial stochastic ordering. The first method involves a nonparametric prior for a measure on partially ordered latent observations, and the second involves rejection sampling. Computational approaches are discussed for each method, and interpretations of prior and posterior information are discussed. An application is presented in which inference is made on the number of independently segregating quantitative trait loci present in an animal population.|Bayesian Methods for Partial Stochastic Orderings|http://www.jstor.org/stable/30042041|30042041|2003-06-01|2003|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
A physiologically based pharmacokinetic model for trichloroethylene (TCE) in rodents and humans was calibrated with published toxicokinetic data sets. A Bayesian statistical framework was used to combine previous information about the model parameters with the data likelihood, to yield posterior parameter distributions. The use of the hierarchical statistical model yielded estimates of both variability between experimental groups and uncertainty in TCE toxicokinetics. After adjustment of the model by Markov chain Monte Carlo sampling, estimates of variability for the animal or human metabolic parameters ranged from a factor of 1.5-2 (geometric standard deviation [GSD]). Uncertainty was of the same order as variability for animals and higher than variability for humans. The model was used to make posterior predictions for several measures of cancer risk. These predictions were affected by both uncertainties and variability and exhibited GSDs ranging from 2 to 6 in mice and rats and from 2 to 10 for humans.|Statistical Analysis of Clewell et al. PBPK Model of Trichloroethylene Kinetics|http://www.jstor.org/stable/4619458|4619458|2000-05-01|2000|['eng']|['Health sciences - Health and wellness', 'Biological sciences - Biochemistry', 'Physical sciences - Chemistry']|['Medicine & Allied Health', 'Health Sciences']
Public transportation systems are an essential component of major cities. The widespread use of smart cards for automated fare collection in these systems offers a unique opportunity to understand passenger behavior at a massive scale. In this study, we use network-wide data obtained from smart cards in the London transport system to predict future traffic volumes, and to estimate the effects of disruptions due to unplanned closures of stations or lines. Disruptions, or shocks, force passengers to make different decisions concerning which stations to enter or exit. We describe how these changes in passenger behavior lead to possible overcrowding and model how stations will be affected by given disruptions. This information can then be used to mitigate the effects of these shocks because transport authorities may prepare in advance alternative solutions such as additional buses near the most affected stations. We describe statistical methods that leverage the large amount of smart-card data collected under the natural state of the system, where no shocks take place, as variables that are indicative of behavior under disruptions. We find that features extracted from the natural regime data can be successfully exploited to describe different disruption regimes, and that our framework can be used as a general tool for any similar complex transportation system.|Predicting traffic volumes and estimating the effects of shocks in massive transportation systems|http://www.jstor.org/stable/26462639|26462639|2015-05-05|2015|['eng']|['Physical sciences - Astronomy', 'Health sciences - Health and wellness']|['Science & Mathematics', 'Biological Sciences', 'General Science']
The general aim of a contingent valuation survey is to elicit the willingness to pay (WTP) of respondents for some (public) commodity without a clear market price. This could be a program to protect some environmental resource or, as in our application, the access to a recreational area of particular interest. In this context, we want to accommodate the possibility of zero WTP and we need to deal with the fact that observations arise as intervals for WTP, rather than point observations. We propose a flexible Bayesian statistical analysis of WTP as a function of characteristics of the respondents that formally incorporates this structure through a mixture model. We consider model uncertainty and pay particular attention to the predictive distribution of revenue if a certain entry price were asked. The latter is an important tool for deriving pricing policies.|Bayesian Analysis of Interval Data Contingent Valuation Models and Pricing Policies|http://www.jstor.org/stable/1392049|1392049|2004-10-01|2004|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Applied mathematics']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
Many popular Bayesian nonparametric priors can be characterized in terms of exchangeable species sampling sequences. However, in some applications, exchangeability may not be appropriate. We introduce a novel and probabilistically coherent family of nonexchangeable species sampling sequences characterized by a tractable predictive probability function with weights driven by a sequence of independent Beta random variables. We compare their theoretical clustering properties with those of the Dirichlet process and the two parameters Poisson–Dirichlet process. The proposed construction provides a complete characterization of the joint process, differently from existing work. We then propose the use of such process as prior distribution in a hierarchical Bayes' modeling framework, and we describe a Markov chain Monte Carlo sampler for posterior inference. We evaluate the performance of the prior and the robustness of the resulting inference in a simulation study, providing a comparison with popular Dirichlet process mixtures and hidden Markov models. Finally, we develop an application to the detection of chromosomal aberrations in breast cancer by leveraging array comparative genomic hybridization (CGH) data. Supplementary materials for this article are available online.|Generalized Species Sampling Priors With Latent Beta Reinforcements|http://www.jstor.org/stable/24247385|24247385|2014-12-01|2014|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Functionally similar species often co-occur within an ecosystem, and they can compete for or facilitate each other's access to resources. The coupled dynamics of such species play an important role in shaping biodiversity and an ecosystem's resilience to perturbations. Here we study two congeneric North American ducks: Redhead Aythya americana and Canvasback A. vaselineria. Both are largely sympatric during the breeding season, and in addition to competition, facultative parasitic egg-laying can lead to interspecific density dependence. Using multi-population integrated models, we combined capturerecovery data, population surveys, and age ratio data in order to simultaneously estimate the mechanistic drivers of fecundity, survival, and population dynamics for both species. Canvasback numbers positively affected Redhead fecundity, whereas Redhead numbers negatively affected Canvasback fecundity, as expected due to parasitism. This interaction was modulated by wetland habitat availability in a way that matched the observation that Redhead hens parasitize Canvasback nests under all conditions but exhibit typical nesting behavior more frequently during years with numerous ponds. Once these effects of density and habitat were statistically controlled for, we found high levels of interspecific synchrony in both fecundity and survival (respectively, 75% and 49% of remaining variation). Thus, both neutral and non-neutral mechanisms affected the dynamics of these functionally similar species. In this and other systems, our method can be used to test hypotheses about species coexistence and to gain insights into the demographic drivers of community dynamics.|Integrated modeling of communities: parasitism, competition, and demographic synchrony in sympatric ducks|http://www.jstor.org/stable/41739316|41739316|2012-11-01|2012|['eng']|['Biological sciences - Ecology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
In this paper we analyze the spatial patterns of the risk of unprotected sexual intercourse for Italian women during their initial experience with sexual intercourse. We rely on geo-referenced survey data from the Italian Fertility and Family Survey, and we use a Bayesian approach relying on weakly informative prior distributions. Our analyses are based on a logistic regression model with a multilevel structure. The spatial pattern uses an intrinsic Gaussian conditional autoregressive (CAR) error component. The complexity of such a model is best handled within a Bayesian framework, and statistical inference is carried out using Markov Chain Monte Carlo simulation. In contrast with previous analyses based on multilevel model, our approach avoids the restrictive assumption of independence between area effects. This model allows us to borrow strength from neighbors in order to obtain estimates for areas that may, on their own, have inadequate sample sizes. We show that substantial geographical variation exists within Italy (Southern Italy has higher risks of unprotected first-time sexual intercourse). The findings are robust with respect to the specification of the prior distribution. We argue that spatial analysis can give useful insights on unmet reproductive health needs.|Bayesian spatial analysis of demographic survey data|http://www.jstor.org/stable/26348078|26348078|2003-01-01|2003|['eng']|['Mathematics - Applied mathematics']|['Population Studies', 'Social Sciences']
A joint dynamic model for the interdependence between infection, immunity and risk of disease is presented. Recurrent latent infections are modelled as realizations from a renewal process and antibody dynamics as a diffusion with a decreasing drift modified by the stimulating effect of the random infections. The augmented submodels are estimated simultaneously in one large Markov chain Monte Carlo algorithm. As an example, we consider the risk of recurrent ear infections when having only partially observed information on bacterial carriage and antibody concentrations.|Joint Modelling of Recurrent Infections and Antibody Response by Bayesian Data Augmentation|http://www.jstor.org/stable/4616796|4616796|2003-12-01|2003|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
In this paper we describe methods for predicting distributions of outcome gains in the framework of a latent variable selection model. We describe such procedures for Student-t selection models and a finite mixture of Gaussian selection models. Importantly, our algorithms for fitting these models are simple to implement in practice, and also permit learning to take place about the non-identified cross-regime correlation parameter. Using data from High School and Beyond, we apply our methods to determine the impact of dropping out of high school on a math test score taken at the senior year of high school. Our results show that selection bias is an important feature of this data, that our beliefs about this non-identified correlation are updated from the data, and that generalized models of selectivity offer an improvement over the 'textbook' Gaussian model. Further, our results indicate that on average dropping out of high school has a large negative impact on senior-year test scores. However, for those individuals who actually drop out of high school, the act of dropping out of high school does not have a significantly negative impact on test scores. This suggests that policies aimed at keeping students in school may not be as beneficial as first thought, since those individuals who must be induced to stay in school are not the ones who benefit significantly (in terms of test scores) from staying in school.|Do Dropouts Suffer from Dropping out? Estimation and Prediction of Outcome Gains in Generalized Selection Models|http://www.jstor.org/stable/25146277|25146277|2004-03-01|2004|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Business', 'Economics']
This paper compares two methods of elicitation for the hyperparameters of the conjugate beta distribution for a binomial sampling model. The PM (posterior mode) method of Chaloner and Duncan appears to be more sensitive to elicitation errors than does a new method proposed here. This new method uses the device of imaginary results, due to I. J. Good (1967). An explicit model for elicitation errors is used in a simulation.|A Comparison of Two Elicitation Methods for a Prior Distribution for a Binomial Parameter|http://www.jstor.org/stable/2632131|2632131|1988-06-01|1988|['eng']|['Mathematics - Mathematical logic']|['Business', 'Business & Economics Collection', 'Management & Organizational Behavior']
We consider the problem of determining an optimal experimental design for estimation of parameters of a class of complex curves characterizing nanowire growth that is partially exponential and partially linear. Locally D-optimal designs for some of the models belonging to this class are obtained by using a geometric approach. Further, a Bayesian sequential algorithm is proposed for obtaining D-optimal designs for models with a closed-form solution, and for obtaining efficient designs in situations where theoretical results cannot be obtained. The advantages of the proposed algorithm over traditional approaches adopted in recently reported nanoexperiments are demonstrated using Monte Carlo simulations. The computer code implementing the sequential algorithm is available as supplementary materials.|A D-Optimal Design for Estimation of Parameters of an Exponential-Linear Growth Curve of Nanostructures|http://www.jstor.org/stable/24587030|24587030|2014-11-01|2014|['eng']|['Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
The prognosis for patients with high grade gliomas is poor, with a median survival of 1 year. Treatment efficacy assessment is typically unavailable until 5-6 months post diagnosis. Investigators hypothesize that quantitative magnetic resonance imaging can assess treatment efficacy 3 weeks after therapy starts, thereby allowing salvage treatments to begin earlier. The purpose of this work is to build a predictive model of treatment efficacy by using quantitative magnetic resonance imaging data and to assess its performance. The outcome is 1 -year survival status. We propose a joint, two-stage Bayesian model. In stage I, we smooth the image data with a multivariate spatiotemporal pairwise difference prior. We propose four summary statistics that are functionals of posterior parameters from the first-stage model. In stage II, these statistics enter a generalized non-linear model as predictors of survival status. We use the probit link and a multivariate adaptive regression spline basis. The hybrid Metropolis-within-Gibbs algorithm and reversible jump Markov chain Monte Carlo methods are applied iteratively between the two stages to estimate the posterior distribution. Through both simulation studies and model performance comparisons we find that we can achieve higher overall correct classification rates by accounting for the spatiotemporal correlation in the images and by allowing for a more complex and flexible decision boundary provided by the generalized non-linear model.|Predicting treatment efficacy via quantitative magnetic resonance imaging: a Bayesian joint model|http://www.jstor.org/stable/41430950|41430950|2012-01-01|2012|['eng']|['Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
We consider the optimal estimation, prediction and control etc. of time series. The essence of a path integral characterization is considered to be that optimal estimates and decisions etc. are obtained by the free extremization of some form. In Sections 2-5 it is shown for some stock situations that this property is ensured by the direct maximization of likelihood (rather than by appeal to the formally equivalent but operationally contrasting least square principle). In Sections 6-9 a risk-sensitive criterion is taken, in that the criterion is an exponential function of cost rather than cost itself. It is shown that this class of problems admits a natural path integral treatment, in which extremization of stress replaces separate extremization of likelihood and cost. A stochastic maximum principle is derived. For the linear-quadratic-Gaussian (LQG) case, results are exact and depend on proof of a certainty equivalence principle. In the non-LQG case they are valid as large deviation approximations, depending on a formalism going back to Bartlett. Finally, in Sections 10 and 11 we note the efficacy (if not indeed the necessity) of direct likelihood arguments in two other time series areas: recursive estimation and structural models.|Likelihood and Cost as Path Integrals|http://www.jstor.org/stable/2345585|2345585|1991-01-01|1991|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
"Shifting from traditional hazard-based food safety management toward risk-based management requires statistical methods for evaluating intermediate targets in food production, such as microbiological criteria (MC), in terms of their effects on human risk of illness. A fully risk-based evaluation of MC involves several uncertainties that are related to both the underlying Quantitative Microbiological Risk Assessment (QMRA) model and the production-specific sample data on the prevalence and concentrations of microbes in production batches. We used Bayesian modeling for statistical inference and evidence synthesis of two sample data sets. Thus, parameter uncertainty was represented by a joint posterior distribution, which we then used to predict the risk and to evaluate the criteria for acceptance of production batches. We also applied the Bayesian model to compare alternative criteria, accounting for the statistical uncertainty of parameters, conditional on the data sets. Comparison of the posterior mean relative risk, E(RR|data) = E (P (illness| criterion is met)/P (illness|data), and relative posterior risk, RPR = P (illness|data, criterion is met)/P (illness|data), showed very similar results, but computing is more efficient for RPR. Based on the sample data, together with the QMRA model, one could achieve a relative risk of 0.4 by insisting that the default criterion be fulfilled for acceptance of each batch."|"A BAYESIAN APPROACH TO THE EVALUATION OF RISK-BASED MICROBIOLOGICAL CRITERIA FOR ""CAMPYLOBACTER"" IN BROILER MEAT"|http://www.jstor.org/stable/43826427|43826427|2015-09-01|2015|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
We develop a two-sector monetary model with a centralized and decentralized market. Activities in the centralized market resemble those in a standard New Keynesian economy with price rigidities. In the decentralized market agents engage in bilateral exchanges for which money is essential. This paper is the first to formally estimate such a model, evaluate its fit based on postwar US data, and assess its money demand properties. Steady-state welfare calculations reveal that the distortions created by the monetary friction may be of similar magnitude as the distortions created by the New Keynesian friction.|Sticky Prices versus Monetary Frictions: An Estimation of Policy Trade-offs|http://www.jstor.org/stable/41237132|41237132|2011-01-01|2011|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Business', 'Economics']
We use cumulants to derive Bayesian credible intervals for wavelet regression estimates. The first four cumulants of the posterior distribution of the estimates are expressed in terms of the observed data and integer powers of the mother wavelet functions. These powers are closely approximated by linear combinations of wavelet scaling functions at an appropriate finer scale. Hence, a suitable modification of the discrete wavelet transform allows the posterior cumulants to be found efficiently for any given data set. Johnson transformations then yield the credible intervals themselves. Simulations show that these intervals have good coverage rates, even when the underlying function is inhomogeneous, where standard methods fail. In the case where the curve is smooth, the performance of our intervals remains competitive with established nonparametric regression methods.|Posterior Probability Intervals for Wavelet Thresholding|http://www.jstor.org/stable/3088795|3088795|2002-01-01|2002|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Applied mathematics', 'Mathematics - Mathematical analysis']|['Science & Mathematics', 'Statistics']
The Singing-Ground Survey (SGS) is a primary source of information on population change for American woodcock (Scolopax minor). We analyzed the SGS using a hierarchical log-linear model and compared the estimates of change and annual indices of abundance to a route regression analysis of SGS data. We also grouped SGS routes into Bird Conservation Regions (BCRs) and estimated population change and annual indices using BCRs within states and provinces as strata. Based on the hierarchical model-based estimates, we concluded that woodcock populations were declining in North America between 1968 and 2006 (trend = -0.9%/yr, 95% credible interval: -1.2, -0.5). Singing-Ground Survey results are generally similar between analytical approaches, but the hierarchical model has several important advantages over the route regression. Hierarchical models better accommodate changes in survey efficiency over time and space by treating strata, years, and observers as random effects in the context of a log-linear model, providing trend estimates that are derived directly from the annual indices. We also conducted a hierarchical model analysis of woodcock data from the Christmas Bird Count and the North American Breeding Bird Survey. All surveys showed general consistency in patterns of population change, but the SGS had the shortest credible intervals. We suggest that population management and conservation planning for woodcock involving interpretation of the SGS use estimates provided by the hierarchical model.|A Hierarchical Model for Estimating Change in American Woodcock Populations|http://www.jstor.org/stable/25097520|25097520|2008-01-01|2008|['eng']|['Physical sciences - Earth sciences', 'Biological sciences - Ecology', 'Physical sciences - Astronomy']|['Science & Mathematics', 'Biological Sciences', 'Zoology']
We propose a novel approach to cross-lingual language model and translation lexicon adaptation for statistical machine translation (SMT) based on bilingual latent semantic analysis. Bilingual LSA enables latent topic distributions to be efficiently transferred across languages by enforcing a one-to-one topic correspondence during training. Using the proposed bilingual LSA framework, model adaptation can be performed by, first, inferring the topic posterior distribution of the source text and then applying the inferred distribution to an n-gram language model of the target language and translation lexicon via marginal adaptation. The background phrase table is enhanced with the additional phrase scores computed using the adapted translation lexicon. The proposed framework also features rapid bootstrapping of LSA models for new languages based on a source LSA model of another language. Our approach is evaluated on the Chinese-English MTO6 test set using the medium-scale SMT system and the GALE SMT system measured in BLEU and NIST scores. Improvement in both scores is observed on both systems when the adapted language model and the adapted translation lexicon are applied individually. When the adapted language model and the adapted translation lexicon are applied simultaneously, the gain is additive. At the 95% confidence interval of the unadapted baseline system, the gain in both scores is statistically significant using the medium-scale SMT system, while the gain in the NIST score is statistically significant using the GALE SMT system.|Bilingual LSA-Based Adaptation for Statistical Machine Translation|http://www.jstor.org/stable/30219115|30219115|2007-12-01|2007|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Linguistics', 'Computer Science', 'Social Sciences']
The literature on potential outcomes has shown that traditional methods for characterizing surrogate endpoints in clinical trials based only on observed quantities can fail to capture causal relationships between treatments, surrogates, and outcomes. Building on the potential-outcomes formulation of a principal surrogate, we introduce a Bayesian method to estimate the causal effect predictiveness (CEP) surface and quantify a candidate surrogate's utility for reliably predicting clinical outcomes. In considering the full joint distribution of all potentially observable quantities, our Bayesian approach has the following features. First, our approach illuminates implicit assumptions embedded in previously-used estimation strategies that have been shown to result in poor performance. Second, our approach provides tools for making explicit and scientifically-interpretable assumptions regarding associations about which observed data are not informative. Through simulations based on an HIV vaccine trial, we found that the Bayesian approach can produce estimates of the CEP surface with improved performance compared to previous methods. Third, our approach can extend principal-surrogate estimation beyond the previously considered setting of a vaccine trial where the candidate surrogate is constant in one arm of the study. We illustrate this extension through an application to an AIDS therapy trial where the candidate surrogate varies in both treatment arms.|A Bayesian Approach to Improved Estimation of Causal Effect Predictiveness for a Principal Surrogate Endpoint|http://www.jstor.org/stable/23270610|23270610|2012-09-01|2012|['eng']|['Biological sciences - Biology']|['Science and Mathematics', 'Statistics']
Whole genome prediction (WGP) modeling and genome-wide association (GWA) analyses are big data issues in agricultural quantitative genetics. Both areas require meaningful input from the statistical scholarly community in order to further improve the accuracy of prediction of genetic merit and inference on putative causal variants as well as improving the computational efficiency of existing methods and algorithms. These concerns have become increasingly critical as new sequencing technologies will only exacerbate current model dimensionality problems. We focus primarily on mixed model and hierarchical Bayesian analyses which have been most commonly pursued by animal and plant breeders for WGP thus far. We draw attention to our observation that many such previous analyses have not carefully inferred upon hyperparameters defined at the top levels of the Bayesian model hierarchy, but simply arbitrarily specify their values. We also reassess previous discussions on WGP model dimensionality, believing that useful data augmentation schemes utilized in various Markov Chain Monte Carlo (MCMC) schemes have led to a general misunderstanding that heavy-tailed or variable selection-based WGP models may be highly parameterized relative to more standard mixed model representations. Computational efficiency is addressed with respect to MCMC and competitive, albeit approximate, alternatives. Furthermore, GWA analyses are reassessed, encouraging a greater reliance on shrinkage-based inferences based on critically chosen priors, instead of potentially nonreproducible fixed effects P value-based inference.|Statistical and Computational Challenges in Whole Genome Prediction and Genome-Wide Association Analyses for Plant and Animal Breeding|http://www.jstor.org/stable/26451849|26451849|2015-12-01|2015|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Agriculture', 'Statistics']
The Challenger disaster evoked a national resolve to reduce the risk of future space flight tragedies. A part of that resolve was to estimate the present risks as accurately as possible. This article illustrates and compares several such ways of incorporating past experience into the process of estimating such risks. Failure data from five military solid rocket programs, as well as pre- and post-Challenger space shuttle data, are used in selected Bayes, empirical Bayes, and Bayes empirical Bayes methods. The derived posterior distributions are used to estimate the required probability of catastrophic failure of the solid rocket boosters on the space shuttle for the Galileo mission, which occurred in October 1989. The point estimates are in fairly close agreement, while the credibility intervals differ substantially. We estimate the probability of catastrophic failure to be 1/60 with 90% credibility interval of 1/578 to 1/21.|The Risk of Catastrophic Failure of the Solid Rocket Boosters on the Space Shuttle|http://www.jstor.org/stable/2684410|2684410|1992-02-01|1992|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Data from a classical Pareto distribution are to be used to make inferences about the inequality and precision parameters. In addition, it is desired to predict the behavior of further observations from the distribution. Three typical data configurations are considered (iid and two types of censoring). Dependent conjugate prior analyses are reviewed and are compared with an analysis involving independent priors for the inequality and precision parameters. It is argued that mathematical tractability should be, perhaps, a minor consideration in the choice of priors. A comparative example is included.|Bayesian Estimation and Prediction for Pareto Data|http://www.jstor.org/stable/2290086|2290086|1989-12-01|1989|['eng']|['Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
In this article, the authors propose a Bayesian method for estimating disaggregate choice models using aggregate data. Compared with existing methods, the advantage of the proposed method is that it allows for the analysis of microlevel consumer dynamic behavior, such as the impact of purchase history on current brand choice, when only aggregate-level data are available. The essence of this approach is to simulate latent choice data that are consistent with the observed aggregate data. When the augmented choice data are made available in each iteration of the Markov chain Monte Carlo algorithm, the dynamics of consumer buying behavior can be explicitly modeled. The authors first demonstrate the validity of the method with a series of simulations and then apply the method to an actual store-level data set of consumer purchases of refrigerated orange juice. The authors find a significant amount of dynamics in consumer buying behavior. The proposed method is useful for managers to understand better the consumer purchase dynamics and brand price competition when they have access to aggregate data only.|Estimating Disaggregate Models Using Aggregate Data through Augmentation of Individual Choice|http://www.jstor.org/stable/30162506|30162506|2007-11-01|2007|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Business', 'Business & Economics Collection', 'Marketing & Advertising']
"This article develops a general method for assessing the influence of model assumptions in a Bayesian analysis. We assume that model choices are indexed by a hyperparameter with some given initial choice. We use the term ""model"" to encompass both the sampling model and the prior distribution. We wish to assess the effect of changing the hyperparameter away from the initial choice. We are performing a sensitivity analysis, with the hyperparameter defining our perturbations. We use the Kullback-Leibler divergence to measure the difference between posteriors corresponding to different choices of the hyperparameter. We also measure the change in priors. If small changes in the priors lead to large changes in posteriors, the choice of hyperparameter is influential. The second-order difference in the Kullback-Leibler divergence is expressed by Fisher information matrices. The relative change in posteriors compared with priors may be summarized by the relative eigenvalue of the posterior and prior Fisher information matrices. The corresponding eigenvector indicates which aspects of the perturbation hyperparameter are most influential. Examples considered are the choice of conjugate prior in regression, case weights in regression, and the choice of Dirichlet prior for multinomials."|Local Model Influence|http://www.jstor.org/stable/2289932|2289932|1989-06-01|1989|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
In a situation where several hundred new music albums are released each month, producing sales forecasts in a reliable and consistent manner is a rather difficult and cumbersome task. The purpose of this study is to obtain sales forecasts for a new album before it is introduced. We develop a hierarchical Bayesian model based on a logistic diffusion process. It allows for the generalization of various adoption patterns out of discrete data and can be applied in a situation where the eventual number of adopters is unknown. Using sales of previous albums along with information known prior to the launch of a new album, the model constructs informed priors, yielding prelaunch sales forecasts, which are out-of-sample predictions. In the context of new product forecasting before introduction, the information we have is limited to the relevant background characteristics of a new album. Knowing only the general attributes of a new album, the meta-analytic approach proposed here provides an informed prior on the dynamics of duration, the effects of marketing variables, and the unknown market potential. As new data become available, weekly sales forecasts and market size (number of eventual adopters) are revised and updated. We illustrate our approach using weekly sales data of albums that appeared in Billboard's Top 200 albums chart from January 1994 to December 1995.|A Bayesian Model for Prelaunch Sales Forecasting of Recorded Music|http://www.jstor.org/stable/822513|822513|2003-02-01|2003|['eng']|['Information science - Informetrics']|['Business', 'Business & Economics Collection', 'Management & Organizational Behavior']
"Selection models are appropriate when a datum c enters the sample only with probability or weight w(x). It is typically assumed that the weight function w is monotone, but the precise functional form of the weight function is often unknown. In this article, the Dirichlet process prior, centered on a parametric form, is used as a prior distribution on the weight function. This allows for incorporation of knowledge about the weight function, without restricting it to be of some particular functional form. By introducing latent variables related to the selection mechanism, computation via Gibbs sampling can be implemented in the case where the total number of selected and unselected observations, N, is known. When N is unknown, a reversible jump Markov chain sampler is needed to carry out the computations. An important difficulty that can be thought of as ""practical nonidentifiability"" is revealed, even for selection models in which the weight functions are theoretically identifiable. The proposed solution to this problem depends on the existence of prior knowledge concerning the effective range of the weight function."|Semiparametric Bayesian Analysis of Selection Models|http://www.jstor.org/stable/3085908|3085908|2001-12-01|2001|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
When a candidate model for data is nonidentifiable, conventional wisdom dictates that the model must be simplified somehow so as to gain identifiability. We explore two scenarios involving mismeasured variables where, in fact, model expansion, as opposed to model contraction, might be used to obtain identifiability. We compare the merits of model contraction and model expansion. We also investigate whether it is necessarily a good idea to alter the model for the sake of identifiability. In particular, estimators obtained from identifiable models are compared to those obtained from nonidentifiable models in tandem with crude prior distributions. Both asymptotic theory and simulations with Markov chain Monte Carlo-based estimators are used to draw comparisons. A technical point which arises is that the asymptotic behavior of a posterior mean from a nonidentifiable model can be investigated using standard asymptotic theory, once the posterior mean is described in terms of the identifiable part of the model only.|On Model Expansion, Model Contraction, Identifiability and Prior Information: Two Illustrative Scenarios Involving Mismeasured Variables [with Comments and Rejoinder]|http://www.jstor.org/stable/20061166|20061166|2005-05-01|2005|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science and Mathematics', 'Statistics']
The Arnason--Schwarz model is usually used for estimating survival and movement probabilities of animal populations from capture-recapture data. The missing data structure of this capture-recapture model is exhibited and summarised via a directed graph representation. Taking advantage of this structure we implement a Gibbs sampling algorithm from which Bayesian estimates and credible intervals for survival and movement probabilities are derived. Convergence of the algorithm is proved using a duality principle. We illustrate our approach through a real example.|Bayesian Estimation of Movement and Survival Probabilities from Capture- Recapture Data|http://www.jstor.org/stable/2337343|2337343|1995-12-01|1995|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
"In the recent years several alternative Bayes Factors have been introduced in order to handle the problem of the extreme sensitivity of the Bayes Factor (BF) to the priors of the models under comparison in model selection or hypothesis testing problems. In particular, the impossibility of using the Bayes Factor with standard noninformative priors has led to introduce new automatic criteria as the Intrinsic Bayes Factors (IBFs) and the Fractional Bayes Factor (FBF). As pointed out by De Santis-Spezzaferri (1995), the use of IBFs and of the FBF seems to be appealing also in robust Bayesian analyses, when the priors of the parameters of the models vary in large classes of distributions, containing, in the limiting case, improper priors. In this paper we study the behaviour of the BF and of the FBF in a problem of comparing two hierarchical models. We assume the exchangeability of the parameters and introduce a class of distributions at the third stage of the hierarchy of the ""biggest"" model. In this context, the use of the FBF seems to avoid the problems of lack of robustness of the BF, providing an alternative to the use of the BF itself."|Comparing Hierarchical Models Using Bayes Factor and Fractional Bayes Factor: A Robust Analysis|http://www.jstor.org/stable/4355925|4355925|1996-01-01|1996|['eng']|['Mathematics - Mathematical logic']|['Mathematics', 'Science & Mathematics', 'Statistics']
The proposed smooth blockwise iterative thresholding estimator (SBITE) is a model selection technique defined as a fixed point reached by iterating a likelihood gradient-based thresholding function. The smooth James-Stein thresholding function has two regularization parameters λ and ν, and a smoothness parameter s. It enjoys smoothness like ridge regression and selects variables like lasso. Focusing on Gaussian regression, we show that SBITE is uniquely defined, and that its Stein unbiased risk estimate is a smooth function of λ and ν, for better selection of the two regularization parameters. We perform a Monte Carlo simulation to investigate the predictive and oracle properties of this smooth version of adaptive lasso. The motivation is a gravitational wave burst detection problem from several concomitant time series. A nonparametric wavelet-based estimator is developed to combine information from all captors by block-thresholding multiresolution coefficients. We study how the smoothness parameter s tempers the erraticity of the risk estimate, and derives a universal threshold, an information criterion, and an oracle inequality in this canonical setting.|Smooth Blockwise Iterative Thresholding: A Smooth Fixed Point Estimator Based on the Likelihood's Block Gradient|http://www.jstor.org/stable/23239612|23239612|2012-06-01|2012|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Consider the ranking of genes using data from replicated microarray time course experiments, where there are multiple biological conditions, and the genes of interest are those whose temporal profiles differ across conditions. We derive a multisample multivariate empirical Bayes' statistic for ranking genes in the order of differential expression, from both longitudinal and cross-sectional replicated developmental microarray time course data. Our longitudinal multisample model assumes that time course replicates are independent and identically distributed multivariate normal vectors. On the other hand, we construct a cross-sectional model using a normal regression framework with any appropriate basis for the design matrices. In both cases, we use natural conjugate priors in our empirical Bayes' setting which guarantee closed form solutions for the posterior odds. The simulations and two case studies using published worm and mouse microarray time course datasets indicate that the proposed approaches perform satisfactorily.|On Gene Ranking Using Replicated Microarray Time Course Data|http://www.jstor.org/stable/25502242|25502242|2009-03-01|2009|['eng']|['Applied sciences - Engineering', 'Biological sciences - Biology']|['Science & Mathematics', 'Statistics']
We consider the high energy physics unfolding problem where the goal is to estimate the spectrum of elementary particles given observations distorted by the limited resolution of a particle detector. This important statistical inverse problem arising in data analysis at the Large Hadron Collider at CERN consists in estimating the intensity function of an indirectly observed Poisson point process. Unfolding typically proceeds in two steps: one first produces a regularized point estimate of the unknown intensity and then uses the variability of this estimator to form frequentisi confidence intervals that quantify the uncertainty of the solution. In this paper, we propose forming the point estimate using empirical Bayes estimation which enables a data-driven choice of the regularization strength through marginal maximum likelihood estimation. Observing that neither Bayesian credible intervals nor standard bootstrap confidence intervals succeed in achieving good frequentist coverage in this problem due to the inherent bias of the regularized point estimate, we introduce an iteratively bias-corrected bootstrap technique for constructing improved confidence intervals. We show using simulations that this enables us to achieve nearly nominal frequentist coverage with only a modest increase in interval length. The proposed methodology is applied to unfolding the Z boson invariant mass spectrum as measured in the CMS experiment at the Large Hadron Collider.|STATISTICAL UNFOLDING OF ELEMENTARY PARTICLE SPECTRA: EMPIRICAL BAYES ESTIMATION AND BIAS-CORRECTED UNCERTAINTY QUANTIFICATION|http://www.jstor.org/stable/43826438|43826438|2015-09-01|2015|['eng']|['Mathematics - Applied mathematics']|['Mathematics', 'Science & Mathematics', 'Statistics']
Discriminant analysis is an effective tool for the classification of experimental units into groups. When the number of variables is much larger than the number of observations it is necessary to include a dimension reduction procedure in the inferential process. Here we present a typical example from chemometrics that deals with the classification of different types of food into species via near infrared spectroscopy. We take a nonparametric approach by modeling the functional predictors via wavelet transforms and then apply discriminant analysis in the wavelet domain. We consider a Bayesian conjugate normal discriminant model, either linear or quadratic, that avoids independence assumptions among the wavelet coefficients. We introduce latent binary indicators for the selection of the discriminatory wavelet coefficients and propose prior formulations that use Markov random tree (MRT) priors to map scale-location connections among wavelets coefficients. We conduct posterior inference via MCMC methods, we show performances on our case study on food authenticity, and compare results to several other procedures.|BAYESIAN WAVELET-BASED CURVE CLASSIFICATION VIA DISCRIMINANT ANALYSIS WITH MARKOV RANDOM TREE PRIORS|http://www.jstor.org/stable/24310021|24310021|2012-04-01|2012|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science and Mathematics', 'Statistics']
In this article we describe methods for obtaining the predictive distributions of outcome gains in the framework of a standard latent variable selection model. Although most previous work has focused on estimation of mean treatment parameters as the method for characterizing outcome gains from program participation, we show how the entire distributions associated with these gains can be obtained in certain situations. Although the out-of-sample outcome gain distributions depend on an unidentified parameter, we use the results of Koop and Poirier to show that learning can take place about this parameter through information contained in the identified parameters via a positive definiteness restriction on the covariance matrix. In cases where this type of learning is not highly informative, the spread of the predictive distributions depends more critically on the prior. We show both theoretically and in extensive generated data experiments how learning occurs, and delineate the sensitivity of our results to the prior specifications. We relate our analysis to three treatment parameters widely used in the evaluation literature--the average treatment effect, the effect of treatment on the treated, and the local average treatment effect--and show how one might approach estimation of the predictive distributions associated with these outcome gains rather than simply the estimation of mean effects. We apply these techniques to predict the effect of literacy on the weekly wages of a sample of New Jersey child laborers in 1903.|On the Predictive Distributions of Outcome Gains in the Presence of an Unidentified Parameter|http://www.jstor.org/stable/1392461|1392461|2003-04-01|2003|['eng']|['Mathematics - Applied mathematics', 'Philosophy - Logic']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
A commonly used paradigm in modeling count data is to assume that individual counts are generated from a Binomial distribution, with probabilities varying between individuals according to a Beta distribution. The marginal distribution of the counts is then Beta-Binomial. Bradlow, Hardie, and Fader (2002, p. 189) make use of polynomial expansions to simplify Bayesian computations with Negative-Binomial distributed data. This article exploits similar expansions to facilitate Bayesian inference with data from the Beta-Binomial model. This has great application and computational importance to many problems, as previous research has resorted to computationally intensive numerical integration or Markov chain Monte Carlo techniques.|Bayesian Inference for the Beta-Binomial Distribution via Polynomial Expansions|http://www.jstor.org/stable/1391134|1391134|2002-03-01|2002|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Computer Science', 'Statistics']
In certain biomedical studies, one may anticipate changes in the shape of a response distribution across the levels of an ordinal predictor. For instance, in toxicology studies, skewness and modality might change as dose increases. To address this issue, we propose a Bayesian nonparametric method for testing for distribution changes across an ordinal predictor. Using a dynamic mixture of Dirichlet processes, we allow the response distribution to change flexibly at each level of the predictor. In addition, by assigning mixture priors to the hyperparameters, we can obtain posterior probabilities of no effect of the predictor and identify the lowest dose level for which there is an appreciable change in distribution. The method also provides a natural framework for performing tests across multiple outcomes. We apply our method to data from a genotoxicity experiment. /// Dans certaines études biomédicales, on peut s'attendre à une influence des différents niveaux d'un facteur ordinal sur la forme même de la distribution de la réponse. Par exemple, dans des études de toxicologie, ce sont le coefficient d'asymétrie ou le nombre de modes qui sont à même d'être modifiés au fur et à mesure que la dose augmente. Pour aborder ce problème, nous proposons une méthode bayésienne non paramétrique testant l'effet d'un facteur prédictif ordinal sur la distribution de la réponse. En utilisant un mélange dynamique de processus de Dirichlet, nous permettons à la distribution de la réponse de s'adapter avec souplesse à chaque niveau du facteur prédictif. En outre, en assignant des distributions a priori aux hyperparamètres du mélange dynamique, nous pouvons déterminer les probabilités a posteriori d'une absence d'effet du facteur prédictif ainsi que la plus petit dose associée à un changement appréciable de la distribution. Cette technique fournit également un cadre naturel permettant de réaliser des tests lorsqu'il y a plusieurs variables réponses. Nous appliquons notre méthode à des données issues d'un essai de génotoxicité.|Nonparametric Bayes Testing of Changes in a Response Distribution with an Ordinal Predictor|http://www.jstor.org/stable/25502077|25502077|2008-06-01|2008|['eng']|['Mathematics - Mathematical logic', 'Applied sciences - Engineering', 'Health sciences - Health and wellness']|['Science & Mathematics', 'Statistics']
Estimation of the expectation of a multivariate normal random vector is considered. The components of the mean vector are assumed to be exchangeable. This information is modeled using a hierarchical prior with independent symmetric stable law at the first stage level. It is shown that the first stage Bayes estimator has an analytic expression and that it is robust with respect to the presence of outlying observations.|Protection against Outliers Using a Symmetric Stable Law Prior|http://www.jstor.org/stable/4355922|4355922|1996-01-01|1996|['eng']|['Mathematics - Mathematical logic']|['Mathematics', 'Science & Mathematics', 'Statistics']
We propose methods for Bayesian inference for a new class of semiparametric survival models with a cure fraction. Specifically, we propose a semiparametric cure rate model with a smoothing parameter that controls the degree of parametricity in the right tail of the survival distribution. We show that such a parameter is crucial for these kinds of models and can have an impact on the posterior estimates. Several novel properties of the proposed model are derived. In addition, we propose a class of improper noninformative priors based on this model and examine the properties of the implied posterior. Also, a class of informative priors based on historical data is proposed and its theoretical properties are investigated. A case study involving a melanoma clinical trial is discussed in detail to demonstrate the proposed methodology.|Bayesian Semiparametric Models for Survival Data with a Cure Fraction|http://www.jstor.org/stable/3068343|3068343|2001-06-01|2001|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
Bayesian inference and model comparisons are easily performed quite accurately using Gibbs sampling, even if (1) the likelihood is analytically intractable and (2) nonstandard prior probability density functions (pdfs) are required. In this study Bayesian model comparisons are performed among five competing theories of endogenous protection. Tariff and nontariff barrier data from 1983 between the United States and five OECD partner countries-Japan, France, Germany, Italy, and the United Kingdom-are used in the analysis. Posterior odds based on two priors show special-interest models to be more likely than other models in determining U.S. protection.|Comparing Theories of Endogenous Protection: Bayesian Comparison of Tobit Models using Gibbs Sampling Output|http://www.jstor.org/stable/2646735|2646735|1998-02-01|1998|['eng']|['Mathematics - Mathematical logic']|['Business', 'Business & Economics Collection', 'Economics']
Observations, model evaluations and expert judgements are combined to make predictions of snow velocity in large chute experiments. Different experimental variables, namely the environmental conditions snow density and snow surface temperature, affect all aspects of this inference. We show how the effect of these two variables can be incorporated in our judgements regarding the uncertain parameters of the physical model, the discrepancy between the physical model and reality and the observation error. We adopt a Bayes linear approach to avoid the necessity of fully probabilistic belief specifications and demonstrate visual tools for statistical validation. Our results represent an important first step in improving the specification of uncertainty in model-based avalanche hazard mapping.|Predicting snow velocity in large chute flows under different environmental conditions|http://www.jstor.org/stable/40925425|40925425|2010-11-01|2010|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
When we use simulation to estimate the performance of a stochastic system, the simulation often contains input models that were estimated from real-world data; therefore, there is both simulation and input uncertainty in the performance estimates. In this paper, we provide a method to measure the overall uncertainty while simultaneously reducing the influence of simulation estimation error due to output variability. To reach this goal, a Bayesian framework is introduced. We use a Bayesian posterior for the input-model parameters, conditional on the real-world data, to quantify the input-parameter uncertainty; we propagate this uncertainty to the output mean using a Gaussian process posterior distribution for the simulation response as a function of the input-model parameters, conditional on a set of simulation experiments. We summarize overall uncertainty via a credible interval for the mean. Our framework is fully Bayesian, makes more effective use of the simulation budget than other Bayesian approaches in the stochastic simulation literature, and is supported with both theoretical analysis and an empirical study. We also make clear how to interpret our credible interval and why it is distinctly different from the confidence intervals for input uncertainty obtained in other papers.|A Bayesian Framework for Quantifying Uncertainty in Stochastic Simulation|http://www.jstor.org/stable/24540567|24540567|2014-11-01|2014|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Business & Economics', 'Business']
Although alternatives have long been available, adding a constant to the cells of a contingency table remains common in practice, especially to avoid problems from zero counts or small cells. There is a certain illogic to the practice, however: It injects unfounded prior information about nuisance parameters. As a result, it can induce a form of Simpson's paradox, producing a point estimate of the target parameter that is outside the interval bounded by the maximum likelihood estimate from the observed table and the null value of the parameter implied by the constants. Furthermore, it can increase apparent evidence against the null hypothesis, even if both the observed data and the added constants fit this null. The paradox can be seen as arising from default independence priors or additive penalty functions. It can be avoided with simple modifications of constants, or by relevant reparameterization. More generally, noncollapsibility over the prior and likelihood may signal a problem with the prior specification when the prior does not penalize the target parameter directly.|Simpson's Paradox From Adding Constants in Contingency Tables as an Example of Bayesian Noncollapsibility|http://www.jstor.org/stable/23020212|23020212|2010-11-01|2010|['eng']|['Mathematics - Applied mathematics', 'Philosophy - Logic']|['Science & Mathematics', 'Statistics']
This paper investigates the use of different priors to improve the inflation forecasting performance of BVAR models with Litterman's prior. A Quasi-Bayesian method, with several different priors, is applied to a VAR model of simulated data as well as to the Australian economy from 1978:Q2 to 2006:Q4. A novel feature with this paper is the use of g-prior in the BVAR models to alleviate poor estimation of drift parameters of Traditional BVAR models. Some results are as follows: (1) In the Quasi-Bayesian framework, BVAR models with Normal–Wishart prior provide the most accurate forecasts of Australian inflation; (2) Generally in the parsimonious models, the BVAR with g-prior performs better than BVAR with Litterman's prior; (3) In simulated data, the BVAR model with g-prior produces more accurate forecasts of driftless variable in the long-run horizons (first and second year forecast horizons).|ALTERNATIVE BVAR MODELS FOR FORECASTING INFLATION|http://www.jstor.org/stable/41318017|41318017|2011-03-01|2011|['eng']|['Applied sciences - Engineering']|['Business & Economics', 'Business', 'Economics']
We discuss the problem of model criticism, with emphasis on developing summary diagnostic measures. We approach model criticism by identifying possible troublesome features of the currently entertained model, embedding the model in an elaborated model, and measuring the value of elaborating. This requires three elements: a model elaboration, a prior distribution, and a utility function. Each triplet generates a different diagnostic measure. We focus primarily on the measure given by a Kullback-Leibler divergence between the marginal prior and posterior distributions on the elaboration parameter. We also develop a linearized version of this diagnostic and use it to show that our procedure is related to other tools commonly used for model diagnostics, such as Bayes factors and the score function. One attraction of this approach is that it allows model criticism to be performed jointly with parameter inference and prediction. Also, this diagnostic approach aims at maintaining an exploratory nature to the criticism process, while affording feasibility of implementation. In this article we present the general outlook and discuss general families of elaborations for use in practice; the exponential connection elaboration plays a key role. We then describe model elaborations for use in diagnosing: departures from normality, goodness of fit in generalized linear models, and variable selection in regression and outlier detection. We illustrate our approach with two applications.|Diagnostic Measures for Model Criticism|http://www.jstor.org/stable/2291670|2291670|1996-06-01|1996|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
In a panel setting, we analyse the speed of (beta) convergence of (cause-specific) mortality and life expectancy at birth in EU countries between 1995 and 2009. Our contribution is threefold. First, in contrast to earlier literature, we allow the convergence rate to vary, and thereby uncover significant differences in the speed of convergence across time and regions. Second, we control for spatial correlations across regions. Third, we estimate convergence among regions, rather than countries, and thereby highlight noteworthy variations within a country. Although we find (beta) convergence on average, we also identify significant differences in the catching-up process across both time and regions. Moreover, we use the coefficient of variation to measure the dynamics of dispersion levels of mortality and life expectancy (sigma convergence) and, surprisingly, find no reduction, on average, in dispersion levels. Consequently, if the reduction of dispersion is the ultimate measure of convergence, then, to the best of our knowledge, our study is the first that shows a lack of convergence in health across EU regions.|Health inequalities in the European Union: an empirical analysis of the dynamics of regional differences|http://www.jstor.org/stable/24774048|24774048|2015-06-01|2015|['eng']|['Physical sciences - Astronomy']|['Business & Economics', 'Business', 'European Studies', 'Economics', 'Area Studies']
With the development of MCMC methods, Bayesian methods play a more and more important role in model selection and statistical prediction. However, the sensitivity of the methods to prior distributions has caused much difficulty to users. In the context of multiple linear regression, we propose an automatic prior setting, in which there is no parameter to be specified by users. Under the prior setting, we show that sampling from the posterior distribution is approximately equivalent to sampling from a Boltzmann distribution defined on Cp values. The numerical results show that the Bayesian model averaging procedure resulted from the automatic prior settin provides a significant improvement in predictive performance over other two procedures proposed in the literature. The procedure is extended to the problem of Bayesian curve fitting with regression splines. Evolutionary Monte Carlo is used to sample from the posterior distributions.|AUTOMATIC BAYESIAN MODEL AVERAGING FOR LINEAR REGRESSION AND APPLICATIONS IN BAYESIAN CURVE FITTING|http://www.jstor.org/stable/24306895|24306895|2001-10-01|2001|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Mathematics', 'Science and Mathematics', 'Statistics']
A method introduced by Arjas &amp; Gasbarra (1994) and later modified by Arjas &amp; Heikkinen (1997) for the non-parametric Bayesian estimation of an intensity on the real line is generalized to cover spatial processes. The method is based on a model approximation where the approximating intensities have the structure of a piecewise constant function. Random step functions on the plane are generated using Voronoi tessellations of random point patterns. Smoothing between nearby intensity values is applied by means of a Markov random field prior in the spirit of Bayesian image analysis. The performance of the method is illustrated in examples with both real and simulated data.|Non-Parametric Bayesian Estimation of a Spatial Poisson Intensity|http://www.jstor.org/stable/4616514|4616514|1998-09-01|1998|['eng']|['Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
"The paper provides a systematic comparison of the Eurosystem, the U.S. Federal Reserve, and the Bank of Japan. These monetary authorities exhibit somewhat different status and tasks, which reflect different historical conditions and national characteristics. However, widespread changes in central banking practices in the direction of greater independence and increased transparency, as well as changes in the economic and financial environment over the past 15-20 years, have contributed to reduce the differences among these three world's principal monetary authorities. A comparison based on simple ""over-the-counter"" policy reaction functions shows no striking differences in terms of monetary policy implementation."|The Eurosystem, the U.S. Federal Reserve, and the Bank of Japan: Similarities and Differences|http://www.jstor.org/stable/4494320|4494320|2007-10-01|2007|['eng']|['Economics - Economic disciplines', 'Business - Business operations', 'Business - Accountancy']|['Business & Economics', 'Business', 'Economics', 'Finance']
"The popularity of the sport of auto racing is increasing rapidly, but its fans remain less interested in statistics than the fans of other sports. In this article, we propose a new class of models for permutations that closely resembles the behavior of auto racing results. We pose the model in a Bayesian hierarchical framework, which permits hierarchical specification and fully hierarchical estimation of interaction terms. We demonstrate the methodology using several rich datasets that consist of repeated rankings for a collection of drivers. Our models can potentially identify individuals racing in ""minor league"" divisions who have higher potential for competitive performance at higher levels. We also present evidence that one of the sport's more controversial figures, Jeff Gordon, is a statistically dominant figure."|Hierarchical Models for Permutations: Analysis of Auto Racing Results|http://www.jstor.org/stable/30045236|30045236|2003-06-01|2003|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
We derive a unified statistical method with which one can produce substantially improved definitions and estimates of almost any feature of two-party electoral systems that can be defined based on district vote shares. Our single method enables one to calculate more efficient estimates, with more trustworthy assessments of their uncertainty, than each of the separate multifarious existing measures of partisan bias, electoral responsiveness, seats-votes curves, expected or predicted vote in each district in a legislature, the probability that a given party will win the seat in each district, the proportion of incumbents or others who will lose their seats, the proportion of women or minority candidates to be elected, the incumbency advantage and other causal effects, the likely effects on the electoral system and district votes of proposed electoral reforms such as term limitations, campaign spending limits, and drawing majority-minority districts, and numerous others. To illustrate, we estimate the partisan bias and electoral responsiveness of the U.S. House of Representatives since 1900 and evaluate the fairness of competing redistricting plans for the 1992 Ohio state legislature.|A Unified Method of Evaluating Electoral Systems and Redistricting Plans|http://www.jstor.org/stable/2111417|2111417|1994-05-01|1994|['eng']|['Information science - Information analysis', 'Economics - Economic disciplines']|['Political Science', 'American Studies', 'Social Sciences', 'Area Studies']
Families of probability distributions which arise naturally as parameter likelihoods in conjugate prior distributions for exponential families are identified, described and their relevance to computational issues in Bayes hierarchical models noted.|Conjugate Likelihood Distributions|http://www.jstor.org/stable/4616270|4616270|1993-01-01|1993|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
The paper proposes Metropolis adjusted Langevin and Hamiltonian Monte Carlo sampling methods defined on the Riemann manifold to resolve the shortcomings of existing Monte Carlo algorithms when sampling from target densities that may be high dimensional and exhibit strong correlations. The methods provide fully automated adaptation mechanisms that circumvent the costly pilot runs that are required to tune proposal densities for Metropolis-Hastings or indeed Hamiltonian Monte Carlo and Metropolis adjusted Langevin algorithms. This allows for highly efficient sampling even in very high dimensions where different scalings may be required for the transient and stationary phases of the Markov chain. The methodology proposed exploits the Riemann geometry of the parameter space of statistical models and thus automatically adapts to the local structure when simulating paths across this manifold, providing highly efficient convergence and exploration of the target density. The performance of these Riemann manifold Monte Carlo methods is rigorously assessed by performing inference on logistic regression models, log-Gaussian Cox point processes, stochastic volatility models and Bayesian estimation of dynamic systems described by non-linear differential equations. Substantial improvements in the time-normalized effective sample size are reported when compared with alternative sampling approaches. MATLAB code that is available from www.ucl.ac.uk/statistics/research/rmhmc allows replication of all the results reported.|Riemann manifold Langevin and Hamiltonian Monte Carlo methods|http://www.jstor.org/stable/41057430|41057430|2011-03-01|2011|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Orthogonal series estimators of univariate densities are proposed that are derived from and motivated by kernel estimators optimal in Whittle's (1958) sense. A preliminary fit of the data from within a one or two parameter class of densities plays the role of a prior mean density. The ratio of the true density and the prior mean density is assumed to have a series expansion in terms of functions orthogonal with respect to the prior mean density. Coefficients of terms of the series are given a joint prior distribution according to which they are independent, with zero means.|Univariate Density Estimation by Orthogonal Series|http://www.jstor.org/stable/2335903|2335903|1978-12-01|1978|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Some competitions involve teams comprising different numbers of players. For informal games, such as the popular 'pub quiz', we argue that teams with fewer players are at a disadvantage. This paper investigates the properties of these games and develops several methods for allocating handicaps within such competitions, so that the competitions may be considered fair, based on a simple Bernoulli model for correctly answering questions and assuming exchangeability of participants. We recommend a natural conjugate prior subjective handicapping rule; with this rule handicaps may be set beforehand given the judgement of the quiz setter regarding the difficulty of the questions. We also describe a posterior rule that provides improved accuracy but is calculated after the quiz is complete. Finally, the paper considers modifications of the proposed rules to cope with multiple-choice questions and progressive quizzes.|On the Development of Decision Rules for Bar Quiz Handicapping|http://www.jstor.org/stable/20202215|20202215|2008-10-01|2008|['eng']|['Mathematics - Applied mathematics']|['Business', 'Business & Economics Collection']
This paper outlines a multiple imputation method for handling missing data in designed longitudinal studies. A random coefficients model is developed to accommodate incomplete multivariate continuous longitudinal data. Multivariate repeated measures are jointly modeled; specifically, an i.i.d. normal model is assumed for time-independent variables and a hierarchical random coefficients model is assumed for time-dependent variables in a regression model conditional on the time-independent variables and time, with heterogeneous error variances across variables and time points. Gibbs sampling is used to draw model parameters and for imputations of missing observations. An application to data from a study of startle reactions illustrates the model. A simulation study compares the multiple imputation procedure to the weighting approach of Robins, Rotnitzky, and Zhao (1995, Journal of the American Statistical Association 90, 106-121) that can be used to address similar data structures.|Multiple Imputation and Posterior Simulation for Multivariate Missing Data in Longitudinal Studies|http://www.jstor.org/stable/2677050|2677050|2000-12-01|2000|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
This paper considers some Bayesian design problems in quantal response analysis. An experimenter must choose a set of dose levels and number of independent observations to take at these levels, subject to a total sample size, in order to estimate some characteristic φ, e.g., ED50, of a tolerance distribution Fθ, where θ is the vector of unknown parameters. It is shown for the logistic model that the sampling variability of the posterior variance of φ is such that the predicted posterior variance alone is an undesirable criterion for design selection. A family of penalty functions is introduced that penalizes any excess in the posterior variance over the expected or predicted variance and protects against unexpected outcomes. The goal is to find a design that avoids experimental results with little information, at the expense of a small sacrifice in the Bayes risk. Numerical results indicate that the chance of an extreme posterior variance can be reduced by sacrificing a small amount of posterior risk.|Bayesian Design for Dose-Response Curves with Penalized Risk|http://www.jstor.org/stable/2533495|2533495|1997-12-01|1997|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
"Replication of experiments is common in applied research. However, systematic studies of the goals and motivations of a ""replication"" are rare. As a consequence, there does not seem to be a precise notion of what a ""success"" when replicating means. This article discusses some of the possible goals for replication; this leads to different (but precise) notions of ""success"" when replicating. Bayesian hierarchical models allow for a flexible and explicit incorporation of the assumed relationship among the experiments. Bayesian predictive distributions are a natural tool to compute the probability of the replication being successful, and hence to design the replication so that the probability of success is high enough. Derivations are exemplified with data coming from a noncentral t distribution."|"Bayesian Design of ""Successful"" Replications"|http://www.jstor.org/stable/3087300|3087300|2002-08-01|2002|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Background: Analytic methods commonly used in epidemiology do not account for spatial correlation between observations. In regression analyses, this omission can bias parameter estimates and yield incorrect standard error estimates. We present a Bayesian hierarchical model (BHM) approach that accounts for spatial correlation, and illustrate its strengths and weaknesses by applying this modeling approach to data on Wuchereria bancrofti infection in Haiti. Methods: A program to eliminate lymphatic filariasis in Haiti assessed prevalence of W. bancrofti infection in 57 schools across Leogane Commune. We analyzed the spatial pattern in the prevalence data using semi-variograms and correlograms. We then modeled the data using (1) standard logistic regression (GLM); (2) non-Bayesian logistic generalized linear mixed models (GLMMs) with school-specific nonspatial random effects; (3) BHMs with school-specific nonspatial random effects; and (4) BHMs with spatial random effects. Results: An exponential semi-variogram with an effective range of 2.15 km best fit the data. GLMM and nonspatial BHM point estimates were comparable and also were generally similar with the marginal GLM point estimates. In contrast, compared with the nonspatial mixed model results, spatial BHM point estimates were markedly attenuated. Discussion: The clear spatial pattern evident in the Haitian W. bancrofti prevalence data and the observation that point estimates and standard errors differed depending on the modeling approach indicate that it is important to account for residual spatial correlation in analyses of W. bancrofti infection data. Bayesian hierarchical models provide a flexible, readily implementable approach to modeling spatially correlated data. However, our results also illustrate that spatial smoothing must be applied with care.|Residual Spatial Correlation between Geographically Referenced Observations: A Bayesian Hierarchical Modeling Approach|http://www.jstor.org/stable/20486091|20486091|2005-07-01|2005|['eng']|['Physical sciences - Astronomy']|['Health Sciences', 'Medicine and Allied Health']
The aim of this paper is to assess whether modeling structural change can help improving the accuracy of macroeconomic forecasts. We conduct a simulated real-time out-of-sample exercise using a time-varying coefficients vector autoregression (VAR) with stochastic volatility to predict the inflation rate, unemployment rate and interest rate in the USA. The model generates accurate predictions for the three variables. In particular, the forecasts of inflation are much more accurate than those obtained with any other competing model, including fixed coefficients VARs, time-varying autoregressions and the naïve random walk model. The results hold true also after the mid 1980s, a period in which forecasting inflation was particularly hard.|MACROECONOMIC FORECASTING AND STRUCTURAL CHANGE|http://www.jstor.org/stable/23355996|23355996|2013-01-01|2013|['eng']|['Physical sciences - Astronomy']|['Business', 'Business & Economics Collection', 'Economics']
We propose a new methodology that does not assume a prior specification of the statistical properties of the measurement errors and treats all sources as noisy measures of some underlying true value. The unobservable true value can be represented as a weighted average of all available measures, using weights that must be specified a priori unless there has been a truth audit. The Census Bureau's Survey of Income and Program Participation (SIPP) survey jobs are linked to Social Security Administration earnings data, creating two potential annual earnings observations. The reliability statistics for both sources are quite similar except for cases where the SIPP used imputations for some missing monthly earnings reports.|ESTIMATING MEASUREMENT ERROR IN ANNUAL JOB EARNINGS: A COMPARISON OF SURVEY AND ADMINISTRATIVE DATA|http://www.jstor.org/stable/43554841|43554841|2013-12-01|2013|['eng']|['Physical sciences - Astronomy', 'Information science - Informetrics']|['Business & Economics', 'Science & Mathematics', 'Business', 'Statistics', 'Economics']
In this article, we propose a Bayesian approach to dose-response assessment and the assessment of synergy between two combined agents. We consider the case of an in vitro ovarian cancer research study aimed at investigating the antiproliferative activities of four agents, alone and paired, in two human ovarian cancer cell lines. In this article, independent dose-response experiments were repeated three times. Each experiment included replicates at investigated dose levels including control (no drug). We have developed a Bayesian hierarchical nonlinear regression model that accounts for variability between experiments, variability within experiments (i.e., replicates), and variability in the observed responses of the controls. We use Markov chain Monte Carlo to fit the model to the data and carry out posterior inference on quantities of interest (e.g., median inhibitory concentration IC₄₀). In addition, we have developed a method, based on Loewe additivity, that allows one to assess the presence of synergy with honest accounting of uncertainty. Extensive simulation studies show that our proposed approach is more reliable in declaring synergy compared to current standard analyses such as the median-effect principle/combination index method (Chou and Talalay, 1984, Advances in Enzyme Regulation 22, 27-55), which ignore important sources of variability and uncertainty.|A Bayesian Approach to Dose–Response Assessment and Synergy and Its Application to In Vitro Dose–Response Studies|http://www.jstor.org/stable/40962525|40962525|2010-12-01|2010|['eng']|['Applied sciences - Engineering', 'Biological sciences - Biology', 'Health sciences - Health and wellness']|['Science & Mathematics', 'Statistics']
"In the case of the exponential family of distributions it is well known that the use of a prior distribution belonging to the natural conjugate family substantially simplifies the analysis whilst often being realistic in applications. The present paper explores the related idea of a conjugate family of utilities, and generally the notion of choosing a utility structure that is suitably ""matched"" to the probability structure and, at the same time, realistic in application."|A Class of Utility Functions|http://www.jstor.org/stable/2957990|2957990|1976-01-01|1976|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical analysis']|['Science & Mathematics', 'Statistics']
The nominal response model (NRM) was proposed by Bock [Psychometrika 37 (1972) 29–51] in order to improve the latent trait (ability) estimation in multiple choice tests with nominal items. When the item parameters are known, expectation a posteriori or maximum a posteriori methods are commonly employed to estimate the latent traits, considering a standard symmetric normal distribution as the latent traits prior density. However, when this item set is presented to a new group of examinees, it is not only necessary to estimate their latent traits but also the population parameters of this group. This article has two main purposes: first, to develop a Monte Carlo Markov Chain algorithm to estimate both latent traits and population parameters concurrently. This algorithm comprises the Metropolis–Hastings within Gibbs sampling algorithm (MHWGS) proposed by Patz and Junker [Journal of Educational and Behavioral Statistics 24 (1999b) 346–366]. Second, to compare, in the latent trait recovering, the performance of this method with three other methods: maximum likelihood, expectation a posteriori and maximum a posteriori. The comparisons were performed by varying the total number of items (NI), the number of categories and the values of the mean and the variance of the latent trait distribution. The results showed that MHWGS outperforms the other methods concerning the latent traits estimation as well as it recoveries properly the population parameters. Furthermore, we found that NI accounts for the highest percentage of the variability in the accuracy of latent trait estimation.|An estimation method for latent traits and population parameters in Nominal Response Model|http://www.jstor.org/stable/26358961|26358961|2010-01-01|2010|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Mathematics', 'Statistics']
"Markov chain Monte Carlo (MCMC) methods make possible the use of flexible Bayesian models that would otherwise be computationally infeasible. In recent years, a great variety of such applications have been described in the literature. Applied statisticians who are new to these methods may have several questions and concerns, however: How much effort and expertise are needed to design and use a Markov chain sampler? How much confidence can one have in the answers that MCMC produces? How does the use of MCMC affect the rest of the model-building process? At the Joint Statistical Meetings in August, 1996, a panel of experienced MCMC users discussed these and other issues, as well as various ""tricks of the trade"". This article is an edited recreation of that discussion. Its purpose is to offer advice and guidance to novice users of MCMC-and to not-so-novice users as well. Topics include building confidence in simulation results, methods for speeding and assessing convergence, estimating standard errors, identification of models for which good MCMC algorithms exist, and the current state of software development."|Markov Chain Monte Carlo in Practice: A Roundtable Discussion|http://www.jstor.org/stable/2685466|2685466|1998-05-01|1998|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Heavy long-lasting rainfall can trigger earthquake swarms. We are interested in the specific shape of lagged rain influence on the occurrence of earthquakes at different depths at Mount Hochstaufen, Bavaria. We present a novel penalty structure for interpretable and flexible estimates of lag coefficients based on spline representations. We provide an easy-to-use implementation of our flexible distributed lag approach that can be used directly in the established R package mgcv for estimation of generalized additive models. This allows our approach to be immediately included in complex additive models for generalized responses even in hierarchical or longitudinal data settings, making use of established stable and well-tested inference algorithms. The benefit of flexible distributed lag modelling is shown in a detailed simulation study.|Flexible distributed lags for modelling earthquake data|http://www.jstor.org/stable/24771900|24771900|2015-02-01|2015|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
This paper provides a comprehensive evaluation of the short-horizon predictive ability of economic fundamentals and forward premiums on monthly exchange-rate returns in a framework that allows for volatility timing. We implement Bayesian methods for estimation and ranking of a set of empirical exchange rate models, and construct combined forecasts based on Bayesian model averaging. More importantly, we assess the economic value of the in-sample and out-of-sample forecasting power of the empirical models, and find two key results: (1) a risk-averse investor will pay a high performance fee to switch from a dynamic portfolio strategy based on the random walk model to one that conditions on the forward premium with stochastic volatility innovations and (2) strategies based on combined forecasts yield large economic gains over the random walk benchmark. These two results are robust to reasonably high transaction costs.|An Economic Evaluation of Empirical Exchange Rate Models|http://www.jstor.org/stable/40247668|40247668|2009-09-01|2009|['eng']|['Applied sciences - Engineering']|['Business & Economics', 'Business', 'Finance']
Capture-recapture models were developed to estimate survival using data arising from marking and monitoring wild animals over time. Variation in survival may be explained by incorporating relevant covariates. We propose nonparametric and semiparametric regression methods for estimating survival in capture-recapture models. A fully Bayesian approach using Markov chain Monte Carlo simulations was employed to estimate the model parameters. The work is illustrated by a study of Snow petrels, in which survival probabilities are expressed as nonlinear functions of a climate covariate, using data from a 40-year study on marked individuals, nesting at Petrels Island, Terre Adélie.|Semiparametric Regression in Capture-Recapture Modeling|http://www.jstor.org/stable/4124576|4124576|2006-09-01|2006|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
This paper develops methods for automatic selection of variables in Bayesian vector autoregressions (VARs) using the Gibbs sampler. In particular, I provide computationally efficient algorithms for stochastic variable selection in generic linear and nonlinear models, as well as models of large dimensions. The performance of the proposed variable selection method is assessed in forecasting three major macroeconomic time series of the UK economy. Data-based restrictions of VAR coefficients can help improve upon their unrestricted counterparts in forecasting, and in many cases they compare favorably to shrinkage estimators.|VAR FORECASTING USING BAYESIAN VARIABLE SELECTION|http://www.jstor.org/stable/23355915|23355915|2013-03-01|2013|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Business', 'Business & Economics Collection', 'Economics']
Polyploidy is an important speciation mechanism, particularly in land plants. Allopolyploid species are formed after hybridization between otherwise intersterile parental species. Recent theoretical progress has led to successful implementation of species tree models that take population genetic parameters into account. However, these models have not included allopolyploid hybridization and the special problems imposed when species trees of allopolyploids are inferred. Here, 2 new models for the statistical inference of the evolutionary history of allopolyploids are evaluated using simulations and demonstrated on 2 empirical data sets. It is assumed that there has been a single hybridization event between 2 diploid species resulting in a genomic allotetraploid. The evolutionary history can be represented as a species network or as a multilabeled species tree, in which some pairs of tips are labeled with the same species. In one of the models (AlloppMUL), the multilabeled species tree is inferred directly. This is the simplest model and the most widely applicable, since fewer assumptions are made. The second model (AlloppNET) incorporates the hybridization event explicitly which means that fewer parameters need to be estimated. Both models are implemented in the BEAST framework. Simulations show that both models are useful and that AlloppNET is more accurate if the assumptions it is based on are valid. The models are demonstrated on previously analyzed data from the genera Pachycladon (Brassicaceae) and Silene (Caryophyllaceae).|Statistical Inference of Allopolyploid Species Networks in the Presence of Incomplete Lineage Sorting|http://www.jstor.org/stable/23485205|23485205|2013-05-01|2013|['eng']|['Biological sciences - Biology']|['Biological Sciences', 'Ecology & Evolutionary Biology', 'Science and Mathematics']
Payphones, parking meters, and vending machines illustrate the modern business practice of substituting machinery for manpower. They do not eliminate manual labor completely, because full coin-boxes still must be replaced and vending machines still must be stocked. Deciding when to replace a coin-box is important, with unequal losses resulting from underestimation and overestimation. This article derives optimal methodology for this problem by incorporating collection history and specifying common prior distributions over average daily fill rate and standard deviation at each box. The approach is implemented and analyzed on collection records from 11,308 pay phones over a large geographical region. When the loss from overestimation is 19 times that from underestimation, our methods outperform the one in current use at least 69.9% of the time, translating into average potential collection-cost reductions exceeding 21%.|Pay Phones, Parking Meters, Vending Machines, and Optimal Bayesian Decisions on Collection Times|http://www.jstor.org/stable/2670286|2670286|2001-06-01|2001|['eng']|['Applied sciences - Engineering', 'Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
Firms are challenged to improve the effectiveness of cross-selling campaigns. The authors propose a customer-response model that recognizes the evolvement of customer demand for various products; the possible multifaceted roles of cross-selling solicitations for promotion, advertising, and education; and customer heterogeneous preference for communication channels. They formulate cross-selling campaigns as solutions to a stochastic dynamic programming problem in which the firm's goal is to maximize the long-term profit of its existing customers while taking into account the development of customer demand over time and the multistage role of cross-selling promotion. The model yields optimal cross-selling strategies for how to introduce the right product to the right customer at the right time using the right communication channel. Applying the model to panel data with cross-selling solicitations provided by a national bank, the authors demonstrate that households have different preferences and responsiveness to cross-selling solicitations. In addition to generating immediate sales, cross-selling solicitations also help households move faster along the financial continuum (educational role) and build up goodwill (advertising role). A decomposition analysis shows that the educational effect (83%) largely dominates the advertising effect (15%) and instantaneous promotional effect (2%). The cross-selling solicitations resulting from the proposed framework are more customized and dynamic and improve immediate response rate by 56%, long-term response rate by 149%, and long-term profit by 177%.|Cross-Selling the Right Product to the Right Customer at the Right Time|http://www.jstor.org/stable/23033447|23033447|2011-08-01|2011|['eng']|['Business - Business operations', 'Applied sciences - Engineering', 'Information science - Data products']|['Business', 'Business & Economics Collection', 'Marketing & Advertising']
The retrieval of wind vectors from satellite scatterometer observations is a non-linear inverse problem. A common approach to solving inverse problems is to adopt a Bayesian framework and to infer the posterior distribution of the parameters of interest given the observations by using a likelihood model relating the observations to the parameters, and a prior distribution over the parameters. We show how Gaussian process priors can be used efficiently with a variety of likelihood models, using local forward (observation) models and direct inverse models for the scatterometer. We present an enhanced Markov chain Monte Carlo method to sample from the resulting multimodal posterior distribution. We go on to show how the computational complexity of the inference can be controlled by using a sparse, sequential Bayes algorithm for estimation with Gaussian processes. This helps to overcome the most serious barrier to the use of probabilistic, Gaussian process methods in remote sensing inverse problems, which is the prohibitively large size of the data sets. We contrast the sampling results with the approximations that are found by using the sparse, sequential Bayes algorithm.|Bayesian Analysis of the Scatterometer Wind Retrieval Inverse Problem: Some New Approaches|http://www.jstor.org/stable/3647498|3647498|2004-01-01|2004|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Published exactly seventy years ago, Jeffreys's Theory of Probability (1939) has had a unique impact on the Bayesian community and is now considered to be one of the main classics in Bayesian Statistics as well as the initiator of the objective Bayes school. In particular, its advances on the derivation of noninformative priors as well as on the scaling of Bayes factors have had a lasting impact on the field. However, the book reflects the characteristics of the time, especially in terms of mathematical rigor. In this paper we point out the fundamental aspects of this reference work, especially the thorough coverage of testing problems and the construction of both estimation and testing noninformative priors based on functional divergences. Our major aim here is to help modern readers in navigating in this difficult text and in concentrating on passages that are still relevant today.|Harold Jeffreys's Theory of Probability Revisited|http://www.jstor.org/stable/25681291|25681291|2009-05-01|2009|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
It is common to assess disability of stroke patients using standardized scales, such as the Rankin Stroke Outcome Scale (RS) and the Barthel Index (BI). The RS, which was designed for applications to stroke, is based on assessing directly the global conditions of a patient. The BI, which was designed for more general applications, is based on a series of questions about the patient's ability to carry out 10 basic activities of daily living. Because both scales are commonly used, but few studies use both, translating between scales is important in gaining an overall understanding of the efficacy of alternative treatments, and in developing prognostic models that combine several datasets. The objective of our analysis is to provide a tool for translating between BI and RS. Specifically, we estimate the conditional probability distributions of each given the other. Subjects consisted of 459 individuals who sustained a stroke and who were recruited for the Kansas City Stroke Study from 1995 to 1998. We assessed patients with BI and RS measures 1, 3, and 6 months after stroke. In addition, we included data from the Framingham study, in the form of a table cross-classifying patients by RS and coarsely aggregated BI. Our statistical estimation approach is motivated by several goals: (a) overcoming the difficulty presented by the fact that our two sources report data at different resolutions; (b) smoothing the empirical counts to provide estimates of probabilities in regions of the table that are sparsely populated; (c) avoiding estimates that would conflict with medical knowledge about the relationship between the two measures; and (d) estimating the relationship between RS and BI at three months after the stroke, while borrowing strength from measurements made at 1 month and 6 months. We address these issues via a Bayesian analysis combining data augmentation and constrained semiparametric inference. Our results provide the basis for comparing and integrating the results of clinical trials using different disability measures, and integrating clinical trials results into a comprehensive decision model for the assessment of long-term implications and cost-effectiveness of stroke prevention and acute treatment interventions. In addition, our results indicate that the degree of agreement between the two measures is less strong than commonly reported, and emphasize the importance of trial designs that include multiple assessments of outcome.|Cross-Calibration of Stroke Disability Measures: Bayesian Analysis of Longitudinal Ordinal Categorical Data Using Negative Dependence|http://www.jstor.org/stable/30045235|30045235|2003-06-01|2003|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
A formal Bayesian analysis of a mixture model usually leads to intractable calculations, since the posterior distribution takes into account all the partitions of the sample. We present approximation methods which evaluate the posterior distribution and Bayes estimators by Gibbs sampling, relying on the missing data structure of the mixture model. The data augmentation method is shown to converge geometrically, since a duality principle transfers properties from the discrete missing data chain to the parameters. The fully conditional Gibbs alternative is shown to be ergodic and geometric convergence is established in the normal case. We also consider non-informative approximations associated with improper priors, assuming that the sample corresponds exactly to a k-component mixture.|Estimation of Finite Mixture Distributions through Bayesian Sampling|http://www.jstor.org/stable/2345907|2345907|1994-01-01|1994|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
In industrial hygiene, a worker's exposure to chemical, physical, and biological agents is increasingly being modeled using deterministic physical models that study exposures near and farther away from a contaminant source. However, predicting exposure in the workplace is challenging and simply regressing on a physical model may prove ineffective due to biases and extraneous variability. A further complication is that data from the workplace are usually misaligned. This means that not all timepoints measure concentrations near and far from the source. We recognize these challenges and outline a flexible Bayesian hierarchical framework to synthesize the physical model with the field data. We reckon that the physical model, by itself, is inadequate for enhanced inferential and predictive performance and deploy (multivariate) Gaussian processes to capture uncertainties and associations. We propose rich covariance structures for multiple outcomes using latent stochastic processes. This article has supplementary material available online.|Bayesian Modeling for Physical Processes in Industrial Hygiene Using Misaligned Workplace Data|http://www.jstor.org/stable/24586972|24586972|2014-05-01|2014|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics', 'Philosophy - Logic']|['Science & Mathematics', 'Statistics']
We analyze the convergence rate of a simplified version of a popular Gibbs sampling method used for statistical discovery of gene regulatory binding motifs in DNA sequences. This sampler satisfies a very strong form of ergodicity (uniform). However, we show that, due to multimodality of the posterior distribution, the rate of convergence often decreases exponentially as a function of the length of the DNA sequence. Specifically, we show that this occurs whenever there is more than one true repeating pattern in the data. In practice there are typically multiple such patterns in biological data, the goal being to detect the most well-conserved and frequently-occurring of these. Our findings match empirical results, in which the motif-discovery Gibbs sampler has exhibited such poor convergence that it is used only for finding modes of the posterior distribution (candidate motifs) rather than for obtaining samples from that distribution. Ours are some of the first meaningful bounds on the convergence rate of a Markov chain method for sampling from a multimodal posterior distribution, as a function of statistical quantities like the number of observations.|CONVERGENCE RATE OF MARKOV CHAIN METHODS FOR GENOMIC MOTIF DISCOVERY|http://www.jstor.org/stable/41806600|41806600|2013-02-01|2013|['eng']|['Mathematics - Mathematical logic']|['Science and Mathematics', 'Statistics']
To analyze and project age-specific mortality or morbidity rates ageperiod-cohort (APC) models are very popular. Bayesian approaches facilitate estimation and improve predictions by assigning smoothing priors to age, period and cohort effects. Adjustments for overdispersion are straightforward using additional random effects. When rates are further stratified, for example, by countries, multivariate APC models can be used, where differences of stratum-specific effects are interpretable as log relative risks. Here, we incorporate correlated stratum-specific smoothing priors and correlated overdispersion parameters into the multivariate APC model, and use Markov chain Monte Carlo and integrated nested Laplace approximations for inference. Compared to a model without correlation, the new approach may lead to more precise relative risk estimates, as shown in an application to chronic obstructive pulmonary disease mortality in three regions of England and Wales. Furthermore, the imputation of missing data for one particular stratum may be improved, since the new approach takes advantage of the remaining strata if the corresponding observations are available there. This is shown in an application to female mortality in Denmark, Sweden and Norway from the 20th century, where we treat for each country in turn either the first or second half of the observations as missing and then impute the omitted data. The projections are compared to those obtained from a univariate APC model and an extended Lee-Carter demographic forecasting approach using the proper Dawid-Sebastiani scoring rule.|ESTIMATION AND EXTRAPOLATION OF TIME TRENDS IN REGISTRY DATA—BORROWING STRENGTH FROM RELATED POPULATIONS|http://www.jstor.org/stable/41713451|41713451|2012-03-01|2012|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
The illegal ivory trade recently intensified to the highest levels ever reported. Policing this trafficking has been hampered by the inability to reliably determine geographic origin of contraband ivory. Ivory can be smuggled across multiple international borders and along numerous trade routes, making poaching hotspots and potential trade routes difficult to identify. This fluidity also makes it difficult to refute a country's denial of poaching problems. We extend an innovative DNA assignment method to determine the geographic origin(s) of large elephant ivory seizures. A Voronoi tessellation method is used that utilizes genetic similarities across tusks to simultaneously infer the origin of multiple samples that could have one or more common origin(s). We show that this joint analysis performs better than sample-by-sample methods in assigning sample clusters of known origin. The joint method is then used to infer the geographic origin of the largest ivory seizure since the 1989 ivory trade ban. Wildlife authorities initially suspected that this ivory came from multiple locations across forest and savanna Africa. However, we show that the ivory was entirely from savanna elephants, most probably originating from a narrow east-to-west band of southern Africa, centered on Zambia. These findings enabled law enforcement to focus their investigation to a smaller area and fewer trade routes and led to changes within the Zambian government to improve antipoaching efforts. Such outcomes demonstrate the potential of genetic analyses to help combat the expanding wildlife trade by identifying origin(s) of large seizures of contraband ivory. Broader applications to wildlife trade are discussed.|Using DNA to Track the Origin of the Largest Ivory Seizure since the 1989 Trade Ban|http://www.jstor.org/stable/25426810|25426810|2007-03-06|2007|['eng']|['Applied sciences - Engineering', 'Physical sciences - Astronomy']|['Science & Mathematics', 'Biological Sciences', 'General Science']
For the problem of variable selection in generalized linear models, we develop various adaptive Bayesian criteria. Using a hierarchical mixture setup for model uncertainty, combined with an integrated Laplace approximation, we derive Empirical Bayes and Fully Bayes criteria that can be computed easily and quickly. The performance of these criteria is assessed via simulation and compared to other criteria such as AIC and BIC on normal, logistic and Poisson regression model classes. A Fully Bayes criterion based on a restricted region hyperprior seems to be the most promising. Finally, our criteria are illustrated and compared with competitors on a data example.|ADAPTIVE BAYESIAN CRITERIA IN VARIABLE SELECTION FOR GENERALIZED LINEAR MODELS|http://www.jstor.org/stable/24307736|24307736|2007-04-01|2007|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics']|['Mathematics', 'Science and Mathematics', 'Statistics']
Statistics has struggled for nearly a century over the issue of whether the Bayesian or frequentist paradigm is superior. This debate is far from over and, indeed, should continue, since there are fundamental philosophical and pedagogical issues at stake. At the methodological level, however, the debate has become considerably muted, with the recognition that each approach has a great deal to contribute to statistical practice and each is actually essential for full development of the other approach. In this article, we embark upon a rather idiosyncratic walk through some of these issues.|The Interplay of Bayesian and Frequentist Analysis|http://www.jstor.org/stable/4144373|4144373|2004-02-01|2004|['eng']|['Mathematics - Mathematical logic']|['Science and Mathematics', 'Statistics']
The nature of the effect of media advertising on brand choice is investigated in two product categories in analyses that combine household scanner panel data with media exposure information. Alternative model specifications are tested in which advertising is assumed to directly affect brand utility, model error variance, and brand consideration. We find strong support for advertising effects on choice through an indirect route of consideration set formation that does not directly affect brand utility. Implications for media buying and advertising effects are explored.|The Effect of Media Advertising on Brand Consideration and Choice|http://www.jstor.org/stable/23012323|23012323|2011-01-01|2011|['eng']|['Mathematics - Applied mathematics', 'Philosophy - Logic', 'Linguistics - Language', 'Mathematics - Mathematical logic']|['Business', 'Business & Economics Collection', 'Marketing & Advertising']
When several simple regression models are assumed to have similar slopes, empirical Bayes methods can efficiently process tis vague information by estimating the hyperparameters of a conjugate prior. The shrinkage estimators we obtain are shown to be minimax and, furthermore, dominate usual confidence regions in terms of coverage probability. /// RÉSUMÉ. -- Lorsque plusieurs équations de régression simple sont supposées avoir une pente similaire, la meilleure méthode tirant profit de cette information est l'approche bayésienne empirique, qui imite l'approche bayésienne en estimant les paramètres de la loi a priori. Les estimateurs ainsi obtenus sont minimax et, de plus, les régions de confiance qui s'en déduisent dominent les régions usuelles (au sens où elles ont une plus grande probabilité de couverture).|Point Estimation and Confidence Set Estimation in a Parallelism Model: An Empirical Bayes Approach|http://www.jstor.org/stable/20075835|20075835|1991-07-01|1991|['fre']||['Business & Economics', 'Mathematics', 'Science & Mathematics', 'Economics', 'Statistics']
Phylogenetic modeling is computationally challenging and most phylogeny models fit a single phylogeny to a single set of molecular sequences. Individual phylogenetic analyses are typically performed independently using publicly available software that fits a computationally intensive Bayesian model using Markov chain Monte Carlo (MCMC) simulation. We develop a Bayesian hierarchical semiparametric regression model to combine multiple phylogenetic analyses of HIV-1 nucleotide sequences and estimate parameters of interest within and across analyses. We use a mixture of Dirichlet processes as a prior for the parameters to relax inappropriate parametric assumptions and to ensure the prior distribution for the parameters is continuous. We use several reweighting algorithms for combining completed MCMC analyses to shrink parameter estimates while adjusting for data set-specific covariates. This avoids constructing a large complex model involving all the original data, which would be computationally challenging and would require rewriting the existing stand-alone software.|A Hierarchical Semiparametric Regression Model for Combining HIV-1 Phylogenetic Analyses Using Iterative Reweighting Algorithms|http://www.jstor.org/stable/4541405|4541405|2007-09-01|2007|['eng']|['Biological sciences - Biology', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
We consider density estimation when the variable of interest is subject to heteroscedastic measurement error. The density is assumed to have a smooth but unknown functional form that we model with a penalized mixture of B-splines. We treat the situation in which multiple mismeasured observations of each variable of interest are observed for at least some of the subjects, and the measurement error is assumed to be additive and normal. The measurement error variance function is modeled with a second penalized mixture of B-splines. The article's main contributions are to address the effects of heteroscedastic measurement error effectively, explain the biases caused by ignoring heteroscedasticity, and present an equivalent kernel for a spline-based density estimator. Derivation of the equivalent kernel may be of independent interest. We use small-sigma asymptotics to approximate the biases incurred by assuming that the measurement error is homoscedastic when it actually is heteroscedastic. The biases incurred by misspecifying heteroscedastic measurement error as homoscedastic can be substantial. We fit the model using Bayesian methods and apply it to an example from nutritional epidemiology and a simulation experiment.|Density Estimation in the Presence of Heteroscedastic Measurement Error|http://www.jstor.org/stable/27640095|27640095|2008-06-01|2008|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
In this paper a flexible multiple regime GARCH(1, 1)-type model is developed to describe the sign and size asymmetries and intermittent dynamics in financial volatility. The results of the paper are important to other nonlinear GARCH models. The proposed model nests some of the previous specifications found in the literature and has the following advantages. First, contrary to most of the previous models, more than two limiting regimes are possible, and the number of regimes is determined by a simple sequence of tests that circumvents identification problems that are usually found in nonlinear time series models. The second advantage is that the novel stationarity restriction on the parameters is relatively weak, thereby allowing for rich dynamics. It is shown that the model may have explosive regimes but can still be strictly stationary and ergodic. A simulation experiment shows that the proposed model can generate series with high kurtosis and low first-order autocorrelation of the squared observations and exhibit the so-called Taylor effect, even with Gaussian errors. Estimation of the parameters is addressed, and the asymptotic properties of the quasi-maximum likelihood estimator are derived under weak conditions. A Monte-Carlo experiment is designed to evaluate the finite-sample properties of the sequence of tests. Empirical examples are also considered.|Modeling Multiple Regimes in Financial Volatility with a Flexible Coefficient GARCH(1, 1) Model|http://www.jstor.org/stable/20532434|20532434|2009-02-01|2009|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Business', 'Economics']
"Bayesian statistics have made great strides in recent years, developing a class of methods for estimation and inference via stochastic simulation known as Markov Chain Monte Carlo (MCMC) methods. MCMC constitutes a revolution in statistical practice with effects beginning to be felt in the social sciences: models long consigned to the ""too hard"" basket are now within reach of quantitative researchers. I review the statistical pedigree of MCMC and the underlying statistical concepts. I demonstrate some of the strengths and weaknesses of MCMC and offer practical suggestions for using MCMC in social-science settings. Simple, illustrative examples include a probit model of voter turnout and a linear regression for time-series data with autoregressive disturbances. I conclude with a more challenging application, a multinomial probit model, to showcase the power of MCMC methods."|Estimation and Inference via Bayesian Simulation: An Introduction to Markov Chain Monte Carlo|http://www.jstor.org/stable/2669318|2669318|2000-04-01|2000|['eng']|['Mathematics - Applied mathematics']|['Political Science', 'American Studies', 'Social Sciences', 'Area Studies']
Estimation of the parameters of the Rasch model, a one-parameter item response model, is considered when both the item parameters and the ability parameters are considered random quantities. It is assumed that the item parameters are drawn from a N (γ, τ 2) distribution, and the abilities are drawn from a N(0, σ 2) distribution. A variation of the EM algorithm is used to find approximate maximum likelihood estimates of γ, τ, and σ. A second approach assumes that the difficulty parameters are drawn from a uniform distribution over part of the real line. Real and simulated data sets are discussed for illustration.|Estimation for the Rasch Model When Both Ability and Difficulty Parameters Are Random|http://www.jstor.org/stable/1164629|1164629|1987-04-01|1987|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Education', 'Statistics', 'Social Sciences']
This article takes up methods for Bayesian inference in a linear model in which the disturbances are independent and have identical Student-t distributions. It exploits the equivalence of the Student-t distribution and an appropriate scale mixture of normals, and uses a Gibbs sampler to perform the computations. The new method is applied to some well-known macroeconomic time series. It is found that posterior odds ratios favour the independent Student-t linear model over the normal linear model, and that the posterior odds ratio in favour of difference stationarity over trend stationarity is often substantially less in the favoured Student-t models.|Bayesian Treatment of the Independent Student-t Linear Model|http://www.jstor.org/stable/2285073|2285073|1993-12-01|1993|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Business & Economics', 'Business', 'Economics']
We investigate the application of two topic models, latent Dirichlet allocation (LDA) and the correlated topic model (CTM), to market basket analysis. Topic models measure the association between observed purchases and underlying latent activities of shoppers by conceiving each basket as random mixture of latent activities. We explain the structure of the two topic models used. We discuss estimation of LDA models by blocked Gibbs sampling. In addition, we show how to evaluate the performance of topic models on estimation and holdout data. In the empirical study, we analyse a total of 18,000 purchases made at a medium-sized supermarket which refer to 60 product categories. The LDA model performs better than the CTM in terms of log likelihood values. Latent activities inferred by these models are intuitive and interpretable, e.g., related to shopping of beverages or personal care, to baking or to an inclination towards luxury food. To illustrate the managerial relevance of estimated topic models we sketch the core of a recommender system which ranks purchase probabilities of other product categories conditional on the basket of a shopper.|Linking Multi-Category Purchases to Latent Activities of Shoppers|http://www.jstor.org/stable/26426741|26426741|2014-01-01|2014|['eng']|['Biological sciences - Biochemistry']|['Marketing & Advertising', 'Business & Economics']
Reversible jump Metropolis-Hastings updating schemes can be used to analyse continuous-time latent models, sometimes known as state space models or hidden Markov models. We consider models where the observed process X can be represented as a stochastic differential equation and where the latent process D is a continuous-time Markov chain. We develop Markov chain Monte Carlo methods for analysing both Markov and non-Markov versions of these models. As an illustration of how these methods can be used in practice we analyse data from the New York Mercantile Exchange oil market. In addition, we analyse data generated by a process that has linear and mean reverting states.|Markov Chain Monte Carlo Methods for Switching Diffusion Models|http://www.jstor.org/stable/2673481|2673481|2001-06-01|2001|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
In response-surface methodology, when the data are fitted using a quadratic model, it is important to make inference about the eigenvalues of the matrix of pure and mixed second-order coefficients, since they contain information on the nature of the stationary point and the shape of the surface. In this article, we propose a Bayesian simulation-based approach to explore the behavior of the posterior distributions of these eigenvalues. Highest posterior density (HPD) intervals for the ordered eigenvalues are then computed and their empirical coverage probabilities are evaluated. A user-friendly software tool has been developed to get the kernel density plots of these simulated posterior distributions and to obtain the corresponding HPD intervals. It is provided online as supplementary materials to this article.|On the Nature of the Stationary Point of a Quadratic Response Surface: A Bayesian Simulation-Based Approach|http://www.jstor.org/stable/24590345|24590345|2013-02-01|2013|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
In this article, we automatically create two large and richly annotated data sets for studying the English dative alternation. With an intrinsic and an extrinsic evaluation, we address the question of whether such data sets that are obtained and enriched automatically are suitable for linguistic research, even if they contain errors. The extrinsic evaluation consists of building logistic regression models with these data sets. We conclude that the automatic approach for detecting instances of the dative alternation still needs human intervention, but that it is indeed possible to annotate the instances with features that are syntactic, semantic and discourse-related in nature. Only the automatic classification of the concreteness of nouns is problematic.|Evaluating automatic annotation: automatically detecting and enriching instances of the dative alternation|http://www.jstor.org/stable/23325373|23325373|2012-10-01|2012|['eng']|['Linguistics - Grammar']|['Linguistics', 'Social Sciences']
Copy number variants (CNVs) and single nucleotide polymorphisms (SNPs) coexist throughout the human genome and jointly contribute to phenotypic variations. Thus, it is desirable to consider both types of variants, as characterized by allele-specific copy numbers (ASCNs), in association studies of complex human diseases. Current SNP genotyping technologies capture the CNV and SNP information simultaneously via fluorescent intensity measurements. The common practice of calling ASCNs from the intensity measurements and then using the ASCN calls in downstream association analysis has important limitations. First, the association tests are prone to false-positive findings when differential measurement errors between cases and controls arise from differences in DNA quality or handling. Second, the uncertainties in the ASCN calls are ignored. We present a general framework for the integrated analysis of CNVs and SNPs, including the analysis of total copy numbers as a special case. Our approach combines the ASCN calling and the association analysis into a single step while allowing for differential measurement errors. We construct likelihood functions that properly account for case-control sampling and measurement errors. We establish the asymptotic properties of the maximum likelihood estimators and develop EM algorithms to implement the corresponding inference procedures. The advantages of the proposed methods over the existing ones are demonstrated through realistic simulation studies and an application to a genome-wide association study of schizophrenia. Extensions to next-generation sequencing data are discussed.|A Likelihood-Based Framework for Association Analysis of Allele-Specific Copy Numbers|http://www.jstor.org/stable/24247389|24247389|2014-12-01|2014|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
This paper considers general linear models for Gaussian geostatistical data with multi-dimensional separable correlation functions involving multiple parameters. We derive various objective priors, such as the Jeffreys-rule, independence Jeffreys, and usual and exact reference priors for the model parameters. In addition, we relax and simplify the assumptions in Paulo (2005) for the propriety of the posteriors in the general setup. We show that the frequentist coverage of posterior credible intervals for a function of range parameters do not depend on the regression coefficient or error variance. These objective priors and a proper flat prior based on ML estimates are compared by examining the frequentist coverage of equal-tailed Bayesian credible intervals. An illustrative example is given from the field of complex computer model validations. Cet article s'intéresse aux modèles linéaires généraux pour des données géostatistiques gaussiennes comportant des fonctions de corrélation multidimensionnelles séparables à paramètres multiples. Les auteurs obtiennent diverses lois a priori objectives, comme la loi de Jeffreys, la loi d'indépendance de Jeffreys et les lois a priori de référence classiques et exactes pour les paramètres du modèle. De plus, ils assouplissent et simplifient les hypothèses formulées par Paulo (2005) à propos des lois a posteriori impropres dans un contexte général. Ils montrent que, pour une fonction des paramètres d'échelle, la couverture fréquentiste des intervalles de crédibilité ne dépend pas du coefficient de régression ou de la variance de l'erreur. Ces lois a priori objectives et une loi a priori intégrable à plateaux basée sur l'estimateur au maximum de vraisemblance sont comparées en examinant le taux fréquentiste de couverture des intervalles de crédibilité bilatéraux symétriques. Ces résultats sont illustrés à l'aide d'un exemple issu du domaine de la validation de modèles informatiques complexes.|Objective Bayesian analysis of spatial models with separable correlation functions|http://www.jstor.org/stable/43186200|43186200|2013-09-01|2013|['eng']|['Mathematics - Mathematical objects']|['Science and Mathematics', 'Statistics']
In the study of earthquakes, several aspects of the underlying physical process, such as the time non-stationarity of the process, are not yet well understood, because we lack clear indications about its evolution in time. Taking as our point of departure the theory that the seismic process evolves in phases with different activity patterns, we have attempted to identify these phases through the variations in the interevent time probability distribution within the framework of the multiple-changepoint problem. In a nonparametric Bayesian setting, the distribution under examination has been considered a random realization from a mixture of Dirichlet processes, the parameter of which is proportional to a generalized gamma distribution. In this way we could avoid making precise assumptions about the functional form of the distribution. The number and location in time of the phases are unknown and are estimated at the same time as the interevent time distributions. We have analysed the sequence of main shocks that occurred in Irpinia, a particularly active area in southern Italy: the method consistently identifies changepoints at times when strong stress releases were recorded. The estimation problem can be solved by stochastic simulation methods based on Markov chains, the implementation of which is improved, in this case, by the good analytical properties of the Dirichlet process.|Analysing the Interevent Time Distribution to Identify Seismicity Phases: A Bayesian Nonparametric Approach to the Multiple-Changepoint Problem|http://www.jstor.org/stable/2680787|2680787|2000-01-01|2000|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
In this article, we develop a Bayesian framework for parameter estimation of a computationally expensive dynamic epidemic model using time series epidemic data. Specifically, we work with a model for A/H1N1 influenza, which is implemented as a deterministic computer simulator, taking as input the underlying epidemic parameters and calculating the corresponding time series of reported infections. To obtain Bayesian inference for the epidemic parameters, the simulator is embedded in the likelihood for the reported epidemic data. However, the simulator is computationally slow, making it impractical to use in Bayesian estimation where a large number of simulator runs is required. We propose an efficient approximation to the simulator using an emulator, a statistical model that combines a Gaussian process (GP) prior for the output function of the simulator with a dynamic linear model (DLM) for its evolution through time. This modeling framework is both flexible and tractable, resulting in efficient posterior inference through Markov chain Monte Carlo (MCMC). The proposed dynamic emulator is then used in a calibration procedure to obtain posterior inference for the parameters of the influenza epidemic.|Bayesian Emulation and Calibration of a Dynamic Epidemic Model for A/H1N1 Influenza|http://www.jstor.org/stable/24247380|24247380|2014-12-01|2014|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
This paper considers the application of Bayesian techniques for simultaneous estimation to the specification of regression weights for selection tests used in various technical training courses in the Marine Corps. Results of a method for m-group regression developed by Molenaar and Lewis (1979) suggest that common weights for training courses belonging to certain general categories are justified in many cases. However, such commonality of regression weights does not appear to hold for all courses in these categories-weights for some training courses remain distinct even after the application of the simultaneous estimation procedure. Thus, a hypothesis of complete generalization of the predictor-criterion relationships across training courses in a given category would only be retained for a carefully selected subset of courses and not for all groups included in the analysis.|Simultaneous Estimation of Regression Functions for Marine Corps Technical Training Specialties|http://www.jstor.org/stable/1164700|1164700|1986-12-01|1986|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Education', 'Statistics', 'Social Sciences']
In this article, a new deterministic approximation method for Bayesian computation, known as design of experiments-based interpolation technique (Dolt), is proposed. The method works by sampling points from the parameter space using an experimental design and then fitting a kriging model to interpolate the unnormalized posterior. The approximated posterior density is a weighted average of normal densities, and therefore, most of the posterior quantities can be easily computed. Dolt is a general computing technique that is easy to implement and can be applied to many complex Bayesian problems. Moreover, it does not suffer from the curse of dimensionality as much as some quadrature methods. It can work using fewer posterior evaluations, which is a great advantage over the Monte Carlo and Markov chain Monte Carlo methods, especially when dealing with computationally expensive posteriors. This article has supplementary material that is available online.|Bayesian Computation Using Design of Experiments-Based Interpolation Technique|http://www.jstor.org/stable/41714886|41714886|2012-08-01|2012|['eng']|['Mathematics - Applied mathematics']|['Science and Mathematics', 'Statistics']
"The increasing availability of longitudinal student achievement data has heightened interest among researchers, educators and policy makers in using these data to evaluate educational inputs, as well as for school and possibly teacher accountability. Researchers have developed elaborate ""value-added models"" of these longitudinal data to estimate the effects of educational inputs (e.g., teachers or schools) on student achievement while using prior achievement to adjust for nonrandom assignment of students to schools and classes. A challenge to such modeling efforts is the extensive numbers of students with incomplete records and the tendency for those students to be lower achieving. These conditions create the potential for results to be sensitive to violations of the assumption that data are missing at random, which is commonly used when estimating model parameters. The current study extends recent value-added modeling approaches for longitudinal student achievement data Lockwood et al. [J. Educ. Behav. Statist. 32 (2007) 125-150] to allow data to be missing not at random via random effects selection and pattern mixture models, and applies those methods to data from a large urban school district to estimate effects of elementary school mathematics teachers. We find that allowing the data to be missing not at random has little impact on estimated teacher effects. The robustness of estimated teacher effects to the missing data assumptions appears to result from both the relatively small impact of model specification on estimated student effects compared with the large variability in teacher effects and the downweighting of scores from students with incomplete data."|MISSING DATA IN VALUE-ADDED MODELING OF TEACHER EFFECTS|http://www.jstor.org/stable/23024905|23024905|2011-06-01|2011|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
In the estimation of cell probabilities from a 2 × 2 table, a prior distribution is developed that can reflect prior beliefs about the cross-classification structure in the table. The posterior means using this prior shrink the classical estimates towards the association structure specified a priori by the user. Closed-form approximations to the posterior means and credible regions are developed in the special case where the two variables of the table are believed independent.|Bayesian Estimation Methods for 2 × 2 Contingency Tables Using Mixtures of Dirichlet Distributions|http://www.jstor.org/stable/2288141|2288141|1983-09-01|1983|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Latent class models (LCMs) are used increasingly for addressing a broad variety of problems, including sparse modeling of multivariate and longitudinal data, model-based clustering, and flexible inferences on predictor effects. Typical frequentist LCMs require estimation of a single finite number of classes, which does not increase with the sample size, and have a well-known sensitivity to parametric assumptions on the distributions within a class. Bayesian nonparametric methods have been developed to allow an infinite number of classes in the general population, with the number represented in a sample increasing with sample size. In this article, we propose a new nonparametric Bayes model that allows predictors to flexibly impact the allocation to latent classes, while limiting sensitivity to parametric assumptions by allowing class-specific distributions to be unknown subject to a stochastic ordering constraint. An efficient MCMC algorithm is developed for posterior computation. The methods are validated using simulation studies and applied to the problem of ranking medical procedures in terms of the distribution of patient morbidity.|Nonparametric Bayes Stochastically Ordered Latent Class Models|http://www.jstor.org/stable/23427549|23427549|2011-09-01|2011|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Estimates of the under-five mortality rate (U5MR) are used to track progress in reducing child mortality and to evaluate countries' performance related to Millennium Development Goal 4. However, for the great majority of developing countries without well-functioning vital registration systems, estimating the U5MR is challenging due to limited data availability and data quality issues. We describe a Bayesian penalized B-spline regression model for assessing levels and trends in the U5MR for all countries in the world, whereby biases in data series are estimated through the inclusion of a multilevel model to improve upon the limitations of current methods. B-spline smoothing parameters are also estimated through a multilevel model. Improved spline extrapolations are obtained through logarithmic pooling of the posterior predictive distribution of country-specific changes in spline coefficients with observed changes on the global level. The proposed model is able to flexibly capture changes in U5MR over time, gives point estimates and credible intervals reflecting potential biases in data series and performs reasonably well in out-of-sample validation exercises. It has been accepted by the United Nations Inter-agency Group for Child Mortality Estimation to generate estimates for all member countries.|GLOBAL ESTIMATION OF CHILD MORTALITY USING A BAYESIAN B-SPLINE BIAS-REDUCTION MODEL|http://www.jstor.org/stable/24522377|24522377|2014-12-01|2014|['eng']|['Physical sciences - Astronomy']|['Mathematics', 'Science & Mathematics', 'Statistics']
We use an economic approach of Mendel to derive new bivariate exponential lifetime distributions. Features distinguishing this approach from the existing ones are (1) it makes use of the principle of indifference; (2) our parameter of interest is a measurable function of observable quantities; (3) the assessment of the probability measure for random lifetimes is performed by assessing that for random lifetime costs with a change of variables; and (4) characterization properties other than the bivariate loss-of-memory property are used to construct distributions. For the infinite population case, our distributions correspond to mixtures of existing bivariate exponential distributions such as the Freund distribution, the Marshall-Olkin distribution, and the Friday-Patil distribution. Moreover, a family of natural conjugate priors for Bayesian Freund (-type) bivariate exponential distributions is discussed.|The Construction of New Bivariate Exponential Distributions from a Bayesian Perspective|http://www.jstor.org/stable/2290932|2290932|1994-09-01|1994|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
The general principles of Bayesian data analysis imply that models for survey responses should be constructed conditional on all variables that affect the probability of inclusion and nonresponse, which are also the variables used in survey weighting and clustering. However, such models can quickly become very complicated, with potentially thousands of poststratification cells. It is then a challenge to develop general families of multilevel probability models that yield reasonable Bayesian inferences. We discuss in the context of several ongoing public health and social surveys. This work is currently open-ended, and we conclude with thoughts on how research could proceed to solve these problems.|Struggles with Survey Weighting and Regression Modeling|http://www.jstor.org/stable/27645813|27645813|2007-05-01|2007|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
This article provides an efficient algorithm for generating a random matrix according to a Wishart distribution, but with eigenvalues constrained to be less than a given vector of positive values. The procedure of Odell and Feiveson provides a guide, but the modifications here ensure that the diagonal elements of a candidate matrix are less than the corresponding elements of the constraint vector, thus greatly improving the chances that the matrix will be acceptable. The Normal hierarchical model with vector outcomes and the multivariate random effects model provide motivating applications.|Simulation from Wishart Distributions with Eigenvalue Constraints|http://www.jstor.org/stable/1390660|1390660|2000-06-01|2000|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Computer Science', 'Statistics']
This article compares three binary Markov random fields (MRFs) which are popular Bayesian priors for spatial smoothing. These are the Ising prior and two priors based on latent Gaussian MRFs. Concern is given to the selection of a suitable Markov chain Monte Carlo (MCMC) sampling scheme for each prior. The properties of the three priors and sampling schemes are investigated in the context of three empirical examples. The first is a simulated dataset, the second involves a confocal fluorescence microscopy dataset, while the third is based on the analysis of functional magnetic resonance imaging (fMRI) data. In the case of the Ising prior, single site and multi-site Swendsen-Wang sampling schemes are both considered. The single site scheme is shown to work consistently well, while it is shown that the Swendsen-Wang algorithm can have convergence problems. The sampling schemes for the priors are extended to generate the smoothing parameters, so that estimation becomes fully automatic. Although this works well, it is found that for highly contiguous images fixing smoothing parameters to very high values can improve results by injecting additional prior information concerning the level of contiguity in the image. The relative properties of the three binary MRFs are investigated, and it is shown how the Ising prior in particular defines sharp edges and encourages clustering. In addition, one of the latent Gaussian MRF priors is shown to be unable to distinguish between higher levels of smoothing. In the context of the fMRI example we also undertake a simulation study.|Estimation of Binary Markov Random Fields Using Markov Chain Monte Carlo|http://www.jstor.org/stable/27594172|27594172|2006-03-01|2006|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Computer Science', 'Statistics']
Quantile regression estimates the relationship between covariates and the τth quantile of the response distribution, rather than the mean. We present a Bayesian quantile regression model for count data and apply it in the field of environmental epidemiology, which is an area in which quantile regression is yet to be used. Our methods are applied to a new study of the relationship between long-term exposure to air pollution and respiratory hospital admissions in Scotland. We observe a decreasing relationship between pollution and the τth quantile of the response distribution, with a relative risk ranging between 1.023 and 1.070.|Bayesian quantile regression for count data with application to environmental epidemiology|http://www.jstor.org/stable/40925434|40925434|2010-11-01|2010|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Some areas of recent development and current interest in time series are noted, with some discussion of Bayesian modelling efforts motivated by substantial practical problems. The areas include non-linear auto-regressive time series modelling, measurement error structures in state-space modelling of time series, and issues of timing uncertainties and time deformations. Some discussion of the needs and opportunities for work on non/semi-parametric models and robustness issues is given in each context.|Modelling and Robustness Issues in Bayesian Time Series Analysis [With Discussion and Rejoinder]|http://www.jstor.org/stable/4355920|4355920|1996-01-01|1996|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Mathematics', 'Science & Mathematics', 'Statistics']
Recurrent event data arises in several areas, such as, biometrics, criminology, demography, industrial reliability and production. In this kind of data it is reasonable to presume that there is dependency among the observations related to the same subject. Such dependency can be modelled by allowing a random effect, usually called frailty term, in the modelling. In this paper the recurrent event problem is treated under the homogeneous Poisson process approach, but with a frailty term. A Bayesian inference procedure based on Markov Chain Monte Carlo Methods is developed. The methodology is illustrated on a real data set.|Bayesian modelling for multivariate lifetime data with a homogeneous Poisson process with a frailty term|http://www.jstor.org/stable/43601037|43601037|2004-06-01|2004|['eng']|['Mathematics - Mathematical objects']|['Science & Mathematics', 'Mathematics', 'Statistics']
Log-linear models have been shown to be useful for smoothing contingency tables when categorical outcomes are subject to nonignorable nonresponse. A log-linear model can be fit to an augmented data table that includes an indicator variable designating whether subjects are respondents or nonrespondents. Maximum likelihood estimates calculated from the augmented data table are known to suffer from instability due to boundary solutions. Park and Brown (1994, Journal of the American Statistical Association 89, 44-52) and Park (1998, Biometrics 54, 1579-1590) developed empirical Bayes models that tend to smooth estimates away from the boundary. In those approaches, estimates for nonrespondents were calculated using an EM algorithm by maximizing a posterior distribution. As an extension of their earlier work, we develop a Bayesian hierarchical model that incorporates a log-linear model in the prior specification. In addition, due to uncertainty in the variable selection process associated with just one log-linear model, we simultaneously consider a finite number of models using a stochastic search variable selection (SSVS) procedure due to George and McCulloch (1997, Statistica Sinica 7, 339-373). The integration of the SSVS procedure into a Markov chain Monte Carlo (MCMC) sampler is straightforward, and leads to estimates of cell frequencies for the nonrespondents that are averages resulting from several log-linear models. The methods are demonstrated with a data example involving serum creatinine levels of patients who survived renal transplants. A simulation study is conducted to investigate properties of the model.|A Bayesian Hierarchical Model for Categorical Data with Nonignorable Nonresponse|http://www.jstor.org/stable/3695328|3695328|2003-12-01|2003|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Interval-censored data occur in survival analysis when the survival time of each patient is only known to be within an interval and these censoring intervals differ from patient to patient. For such data, we present some Bayesian discretized semiparametric models, incorporating proportional and nonproportional hazards structures, along with associated statistical analyses and tools for model selection using sampling-based methods. The scope of these methodologies is illustrated through a reanalysis of a breast cancer data set (Finkelstein, 1986, Biometrics 42, 845-854) to test whether the effect of covariate on survival changes over time.|Bayesian Analysis and Model Selection for Interval-Censored Survival Data|http://www.jstor.org/stable/2533810|2533810|1999-06-01|1999|['eng']|['Applied sciences - Engineering', 'Mathematics - Mathematical analysis']|['Science & Mathematics', 'Statistics']
In this paper, we use covariates and an indication of sampling effort in an autologistic model to improve predictions of probability of presence for lattice data. The model is applied to sampled data where only a small proportion of the available sites have been observed. We adopt a Bayesian set-up and develop a Gibbs sampling estimation procedure. In four examples based on simulated data, we show that the autologistic model with covariates improves predictions compared with the simple logistic regression model and the basic autologistic model (without covariates). Software to implement the methodology is available at no cost from StatLib.|An Improved Model for Spatially Correlated Binary Responses|http://www.jstor.org/stable/1400634|1400634|2000-03-01|2000|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Agriculture', 'Statistics']
For longitudinal data, the modeling of a correlation matrix R can be a difficult statistical task due to both the positive definite and the unit diagonal constraints. Because the number of parameters increases quadratically in the dimension, it is often useful to consider a sparse parameterization. We introduce a pair of prior distributions on the set of correlation matrices for longitudinal data through the partial autocorrelations (PACs), which vary independently over (-1,1). The first prior shrinks each of the PACs toward zero with increasingly aggressive shrinkage in lag. The second prior (a selection prior) is a mixture of a zero point mass and a continuous component for each PAC, allowing for a sparse representation. The structure implied under our priors is readily interpretable for time-ordered responses because each zero PAC implies a conditional independence relationship in the distribution of the data. Selection priors on the PACs provide a computationally attractive alternative to selection on the elements of R or R⁻¹ for ordered data. These priors allow for data-dependent shrinkage/selection under an intuitive parameterization in an unconstrained setting. The proposed priors are compared to standard methods through a simulation study and illustrated using a multivariate probit data example. Supplemental materials for this article (appendix, data, and R code) are available online.|Sparsity Inducing Prior Distributions for Correlation Matrices of Longitudinal Data|http://www.jstor.org/stable/43304793|43304793|2014-12-01|2014|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Computer Science', 'Statistics']
In meteorology, the traditional approach to forecasting employs deterministic models mimicking atmospheric dynamics. Forecast uncertainty due to partial knowledge of the initial conditions is tackled by ensemble predictions systems. Probabilistic forecasting is a relatively new approach which may properly account for all sources of uncertainty. We propose a hierarchical Bayesian model which develops this idea and makes it possible to deal with ensemble predictions systems with non-identifiable members by using a suitable definition of the second level of the model. An application to Italian small-scale temperature data is shown.|A Bayesian hierarchical approach to ensemble weather forecasting|http://www.jstor.org/stable/40783139|40783139|2010-05-01|2010|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
A random-walk Metropolis sampler is geometrically ergodic if its equilibrium density is super-exponentially light and satisfies a curvature condition [Stochastic Process. Appl. 85 (2000) 341-361]. Many applications, including Bayesian analysis with conjugate priors of logistic and Poisson regression and of log-linear models for categorical data result in posterior distributions that are not super-exponentially light. We show how to apply the change-ofvariable formula for diffeomorphisms to obtain new densities that do satisfy the conditions for geometric ergodicity. Sampling the new variable and mapping the results back to the old gives a geometrically ergodic sampler for the original variable. This method of obtaining geometric ergodicity has very wide applicability.|VARIABLE TRANSFORMATION TO OBTAIN GEOMETRIC ERGODICITY IN THE RANDOM-WALK METROPOLIS ALGORITHM|http://www.jstor.org/stable/41806566|41806566|2012-12-01|2012|['eng']|['Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
We discuss a general approach to dynamic sparsity modeling in multivariate time series analysis. Timevarying parameters are linked to latent processes that are thresholded to induce zero values adaptively, providing natural mechanisms for dynamic variable inclusion/selection. We discuss Bayesian model specification, analysis and prediction in dynamic regressions, time-varying vector autoregressions, and multivariate volatility models using latent thresholding. Application to a topical macroeconomic time series problem illustrates some of the benefits of the approach in terms of statistical and economic interpretations as well as improved predictions. Supplementary materials for this article are available online.|Bayesian Analysis of Latent Threshold Dynamic Models|http://www.jstor.org/stable/43701602|43701602|2013-04-01|2013|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
The use of hierarchical models in statistical applications, and for educational data, is a promising but still underutilized approach. However, because these models are more complicated than many standard methods, it is important that we, as users and developers, not rush to use them before we understand them. We emphasize here, in support of the views on hierarchical models expressed in the 3 preceding papers by Draper, by Rogosa and Saner, and by de Leeuw and Kreft, the need to not diminish hard thinking about data and iterative model checking when fitting hierarchical models, the need for more and better software, the need to test methods to assure their proper calibration, and the need to produce supporting materials to aid analysts and users of hierarchical modeling methods.|Hierarchical Models for Educational Data: An Overview|http://www.jstor.org/stable/1165356|1165356|1995-07-01|1995|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Education', 'Statistics', 'Social Sciences']
Identification, ranking and selecting hazardous traffic accident locations from a group under consideration is a fundamental goal for traffic safety researchers. Few methods exist that can quantitatively, accurately and easily discriminate between sites that commonly have small and variable observation count periods. One method that embodies all these advantages is the hierarchical Bayesian model, the method proposed in this paper. The particular hierarchical Bayesian approach that we use incorporates expert knowledge about accident sites as a group believed a priori to be exchangeable, the Poisson assumption and a conjugate gamma prior. We then propose three natural strategies for ranking and selecting the most hazardous subgroup of accident locations. Also presented is an especially useful procedure that gives the probability that each particular site is worst and by how much it is worst. All proposed strategies are illustrated using previously published fatality accident data from 35 sites in Auckland, New Zealand.|Ranking and Selecting Motor Vehicle Accident Sites by Using a Hierarchical Bayesian Model|http://www.jstor.org/stable/2988566|2988566|1997-01-01|1997|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
Estimation of finite population means is considered when samples are collected using a stratified sampling design. Finite populations for different strata are assumed to be realizations from different superpopulations. The true means of the observations lie on a regression surface with random intercepts for different strata. The true sampling variances are also different and random for different strata. The strata are connected through two common prior distributions, one for the intercepts and another for the sampling variances for all the strata. The model is appropriate in two important survey situations. First, it can be applied to repeated surveys where the physical characteristics of the sampling units change slowly over time. Second, the model is appropriate in small-area estimation problems where a very few samples are available for any particular area. Empirical Bayes estimators of the finite population means are shown to be asymptotically optimal in the sense of Robbins. The proposed empirical Bayes estimators are also compared to the classical regression estimators in terms of the relative savings loss due to Efron and Morris. A measure of variability of the proposed empirical Bayes estimator is considered based on bootstrap samples. This measure of variability incorporates all sources of variations due to the estimation of various model parameters. A numerical study is conducted to evaluate the performance of the proposed empirical Bayes estimator compared to rival estimators.|Empirical Bayes Estimation of Finite Population Means from Complex Surveys|http://www.jstor.org/stable/2965426|2965426|1997-12-01|1997|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Structural equation models are widely used in marketing and psychometric literature to model relationships between unobserved constructs and manifest variables and to control for measurement error. Most applications of structural equation models assume that data come from a homogeneous population. This assumption may be unrealistic, as individuals are likely to be heterogeneous in their perceptions and evaluations of unobserved constructs. In addition, individuals may exhibit different measurement reliabilities. It is well-known in statistical literature that failure to account for unobserved sources of individual differences can result in misleading inferences and incorrect conclusions. We develop a hierarchical Bayesian framework for modeling general forms of heterogeneity in partially recursive structural equation models. Our framework elucidates the motivations for accommodating heterogeneity and illustrates theoretically the types of misleading inferences that can result when unobserved heterogeneity is ignored. We describe in detail the choices that researchers can make in incorporating different forms of measurement and structural heterogeneity. Current random-coefficient models in psychometric literature can accommodate heterogeneity solely in mean structures. We extend these models by allowing for heterogeneity both in mean and covariance structures. Specifically, in addition to heterogeneity in measurement intercepts and factor means, we account for heterogeneity in factor covariance structure, measurement error, and structural parameters. Models such as random-coefficient factor analysis, random-coefficient second-order factor analysis, and random-coefficient, partially recursive simultaneous equation models are special cases of our proposed framework. We also develop Markov Chain Monte Carlo (MCMC) procedures to perform Bayesian inference in partially recursive, random-coefficient structural equation models. These procedures provide individual-specific estimates of the factor scores, structural coefficients, and other model parameters. We illustrate our approach using two applications. The first application illustrates our methods on synthetic data, whereas the second application uses consumer satisfaction data involving measurements on satisfaction, expectation disconfirmation, and performance variables obtained from a panel of subjects. Our results from the synthetic data application show that our Bayesian procedures perform well in recovering the true parameters. More importantly, we find that models that ignore heterogeneity can yield a severely distorted picture of the nature of associations among variables and can therefore generate misleading inferences. Specifically, we find that ignoring heterogeneity can result in inflated estimates of measurement reliability, wrong signs of factor covariances, and can yield attenuated model fit and standard errors. The results from the consumer satisfaction study show that individuals vary both in means and covariances and indicate that conventional psychometric methods are not appropriate for our data. In addition, we find that heterogeneous models outperform the standard structural equation model in predictive ability. Managerially, we show how one can use the individual-level factor scores and structural parameter estimates from the Bayesian approach to perform quadrant analysis and refine marketing policy (e.g., develop a one-on-one marketing policy). The framework introduced in this paper and the inference procedures we describe should be of interest to researchers in a wide range of disciplines in which measurement error and unobserved heterogeneity are problematic. In particular, our approach is suitable for studies in which panel data or multiple observations are available for a given set of respondents or objects (e.g., firms, organizations, markets). At a practical level, our procedures can be used by managers and other policymakers to customize marketing activities or policies. Future research should extend our procedures to deal with the general nonrecursive structural equation model and to handle binary and ordinal data situations.|A Hierarchical Bayesian Methodology for Treating Heterogeneity in Structural Equation Models|http://www.jstor.org/stable/193265|193265|2000-10-01|2000|['eng']|['Applied sciences - Engineering']|['Marketing & Advertising', 'Business & Economics', 'Business']
"Bayesian analysis for a simple but widely applied dynamic programming model is obtained. The setting is the prototypal job-search model. The general case of wage and duration data, with potential censoring, is studied. The optimality condition implied by the dynamic programming setup is fully imposed. The posterior distribution reveals a ""ridge"" reflecting the characteristic nonstandard nature of the inference problem. Marginal distributions and moments are obtained in a canonical parameterization after a suitable approximation. The adequacy of the approximation is easily assessed. Simulation is applied to study alternative parameterizations and prior robustness and to facilitate prior elicitations. Finally, we illustrate the applicability of our methods by giving posterior distributions for the elasticities of unemployment durations and reemployment wages with respect to unemployment income. Our analysis is easy to implement and all computations are simple to perform."|Bayesian Analysis of the Prototypal Search Model|http://www.jstor.org/stable/1392573|1392573|1998-04-01|1998|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
"We employ an unobserved components model to disentangle the long-term trend from cyclical movements in the price of internationally traded crude oil using data from 1861 to 2010. The in-sample estimation of the model identifies a deterministic quadratic trend and two types of cycles, with the short cycle having a period of 6 years and the long cycle of 29 years. Compared to the large amplitude of the cycles, the growth rate of the long-term trend is small. The out-of-sample forecasting performance of various competing models is compared to that of a ""no change"" random walk forecast. While the random walk forecast tends to be the most accurate at shorter horizons, it is outperformed by the trend-cycle models at horizons longer than one year. The results provide evidence of predictability in the price of crude oil at long horizons."|Small Trends and Big Cycles in Crude Oil Prices|http://www.jstor.org/stable/24695013|24695013|2015-01-01|2015|['eng']|['Physical sciences - Astronomy']|['Business & Economics', 'Business', 'Economics']
"A mixed-Dirichlet prior was previously used to model the hypotheses of ""independence"" and ""dependence"" in contingency tables, thus leading to a Bayesian test for independence. Each Dirichlet has a main hyperparameter κ and the mixing is attained by assuming a hyperprior for κ. This hyperparameter can be regarded as a flattening or shrinking constant. We here review the method, generalize it and check the robustness and sensitivity with respect to variations in the hyperpriors and in their hyperhyperparameters. The hyperpriors examined included generalized log-Students with various numbers of degrees of freedom ν. When ν is as large as 15 this hyperprior approximates a log-normal distribution and when ν = 1 it is a log-Cauchy. Our experiments caused us to recommend the log-Cauchy hyperprior (or of course any distribution that closely approximates it). The user needs to judge values for the upper and lower quartiles, or any two quantiles, of κ, but we find that the outcome is robust with respect to fairly wide variations in these judgments."|"The Robustness and Sensitivity of the Mixed-Dirichlet Bayesian Test for ""Independence"" in Contingency Tables"|http://www.jstor.org/stable/2241333|2241333|1987-06-01|1987|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
A Bayesian estimation of a regime-switching threshold asymmetric GARCH model is proposed. The specification is based on a Markov-switching model with Student-t innovations and K separate GJR(1,1) processes whose asymmetries are located at free non-positive threshold parameters. The model aims at determining whether or not: (i) structural breaks are present within the volatility dynamics; (ii) asymmetries (leverage effects) are present, and are different between regimes and (iii) the threshold parameters (locations of bad news) are similar between regimes. A novel MCMC scheme is proposed which allows for a fully automatic Bayesian estimation of the model. The presence of two distinct volatility regimes is shown in an empirical application to the Swiss Market Index log-returns. The posterior results indicate no differences with regards to the asymmetries and their thresholds when comparing highly volatile periods with the milder ones. Comparisons with a single-regime specification indicates a better in-sample fit and a better forecasting performance for the Markov-switching model.|Bayesian estimation of a Markov-switching threshold asymmetric GARCH model with Student-t innovations|http://www.jstor.org/stable/23116668|23116668|2009-01-01|2009|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Business & Economics', 'Business', 'Economics']
We propose a flexible class of models based on scale mixture of uniform distributions to construct shrinkage priors for covariance matrix estimation. This new class of priors enjoys a number of advantages over the traditional scale mixture of normal priors, including its simplicity and flexibility in characterizing the prior density. We also exhibit a simple, easy to implement Gibbs sampler for posterior simulation, which leads to efficient estimation in high-dimensional problems. We first discuss the theory and computational details of this new approach and then extend the basic model to a new class of multivariate conditional autoregressive models for analyzing multivariate areal data. The proposed spatial model flexibly characterizes both the spatial and the outcome correlation structures at an appealing computational cost. Examples consisting of both synthetic and real-world data show the utility of this new framework in terms of robust estimation as well as improved predictive performance. Supplementary materials are available online.|On a Class of Shrinkage Priors for Covariance Matrix Estimation|http://www.jstor.org/stable/43304855|43304855|2013-09-01|2013|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Computer Science', 'Statistics']
This article builds on the work of Greenidge, employing structural time series model (STSM) to explain and forecast quarterly tourist flows from Barbados' primary source markets — the USA, the United Kingdom, Canada and CARICOM — for the period 1996:1—2007:4. Results show that the structure and nature of the individual time series components have evolved significantly since the work of Greenidge. Of particular interest, arrivals from the main source markets appear to be less income sensitive. The study also investigates the predictive power of STSM. A seasonal naïve model is used for benchmark comparison purposes. We find that STSM outperforms the seasonal naïve model in its both multivariate and univariate form.|Modelling and forecasting tourist flows to Barbados using structural time series models|http://www.jstor.org/stable/23745508|23745508|2010-01-01|2010|['eng']|['Applied sciences - Imaging']|['Management & Organizational Behavior', 'Business & Economics', 'Business']
Particulate matter (PM) has been linked to a range of serious cardiovascular and respiratory health problems, including premature mortality. The main objective of our research is to quantify uncertainties about the impacts of fine PM exposure on mortality. We develop a multivariate spatial regression model for the estimation of the risk of mortality associated with fine PM and its components across all counties in the conterminous United States. We characterize different sources of uncertainty in the data and model the spatial structure of the mortality data and the speciated fine PM. We consider a flexible Bayesian hierarchical model for a space-time series of counts (mortality) by constructing a likelihood-based version of a generalized Poisson regression model that combines methods for point-level misaligned data and change of support regression. Our results seem to suggest an increase by a factor of two in the risk of mortality due to fine particles with respect to coarse particles. Our study also shows that in the Western United States, the nitrate and crustal components of the speciated fine PM seem to have more impact on mortality than the other components. On the other hand, in the Eastern United States, sulfate and ammonium explain most of the fine PM effect.|Spatial Association between Speciated Fine Particles and Mortality|http://www.jstor.org/stable/4124596|4124596|2006-09-01|2006|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
We introduce easy-to-implement, regression-based methods for predicting quarterly real economic activity that use daily financial data and rely on forecast combinations of mixed data sampling (MIDAS) regressions. We also extract a novel small set of daily financial factors from a large panel of about 1000 daily financial assets. Our analysis is designed to elucidate the value of daily financial information and provide real-time forecast updates of the current (nowcasting) and future quarters of real GDP growth.|Should Macroeconomic Forecasters Use Daily Financial Data and How?|http://www.jstor.org/stable/43701607|43701607|2013-04-01|2013|['eng']|['Applied sciences - Engineering', 'Information science - Informetrics']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
In this article, we propose an improvement on the sequential updating and greedy search (SUGS) algorithm for fast fitting of Dirichlet process mixture models. The SUGS algorithm provides a means for very fast approximate Bayesian inference for mixture data which is particularly of use when dataseis are so large that many standard Markov chain Monte Carlo (MCMC) algorithms cannot be applied efficiently, or take a prohibitively long time to converge. In particular, these ideas are used to initially interrogate the data, and to refine models such that one can potentially apply exact data analysis later on. SUGS relies upon sequentially allocating data to clusters and proceeding with an update of the posterior on the subsequent allocations and parameters which assumes this allocation is correct. Our modification softens this approach, by providing a probability distribution over allocations, with a similar computational cost; this approach has an interpretation as a variational Bayes procedure and hence we term it variational SUGS (VSUGS). It is shown in simulated examples that VSUGS can outperform, in terms of density estimation and classification, a version of the SUGS algorithm in many scenarios. In addition, we present a data analysis for flow cytometry data, and SNP data via a three-class Dirichlet process mixture model, illustrating the apparent improvement over the original SUGS algorithm.|A Sequential Algorithm for Fast Fitting of Dirichlet Process Mixture Models|http://www.jstor.org/stable/43304802|43304802|2014-12-01|2014|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Computer Science', 'Statistics']
Factor analysis is over a century old, but it is still problematic to choose the number of factors for a given data set. We provide a systematic review of current methods and then introduce a method based on bi-cross-validation, using randomly held-out submatrices of the data to choose the optimal number of factors. We find it performs better than many existing methods especially when both the number of variables and the sample size are large and some of the factors are relatively weak. Our performance criterion is based on recovery of an underlying signal, equal to the product of the usual factor and loading matrices. Like previous comparisons, our work is simulation based. Recent advances in random matrix theory provide principled choices for the number of factors when the noise is homoscedastic, but not for the heteroscedastic case. The simulations we chose are designed using guidance from random matrix theory. In particular, we include factors which are asymptotically too small to detect, factors large enough to detect but not large enough to improve the estimate, and two classes of factors (weak and strong) large enough to be useful. We also find that a form of early stopping regularization improves the recovery of the signal matrix.|Bi-Cross-Validation for Factor Analysis|http://www.jstor.org/stable/24780836|24780836|2016-02-01|2016|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
A Bayesian approach to covariance estimation and spatial prediction based on flexible variogram models is introduced. In particular, we consider black-box kriging models. These variogram models do not require restrictive assumptions on the functional shape of the variogram; furthermore, they can handle quite naturally non isotropic random fields. The proposed Bayesian approach does not require the computation of an empirical variogram estimator, thus avoiding the arbitrariness implied in the construction of the empirical variogram itself. Moreover, it provides a complete assessment of the uncertainty in the variogram estimation. The advantages of this approach are illustrated via simulation studies and by application to a well known benchmark dataset.|A Bayesian Approach to Spatial Prediction With Flexible Variogram Models|http://www.jstor.org/stable/23238854|23238854|2012-06-01|2012|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Agriculture', 'Statistics']
We investigate the relationships between Dirichlet process (DP) based models and allocation models for a variable number of components, based on exchangeable distributions. It is shown that the DP partition distribution is a limiting case of a Dirichlet-multinomial allocation model. Comparisons of posterior performance of DP and allocation models are made in the Bayesian paradigm and illustrated in the context of univariate mixture models. It is shown in particular that the unbalancedness of the allocation distribution, present in the prior DP model, persists a posteriori. Exploiting the model connections, a new MCMC sampler for general DP based models is introduced, which uses split/merge moves in a reversible jump framework. Performance of this new sampler relative to that of some traditional samplers for DP processes is then explored.|Modelling Heterogeneity with and without the Dirichlet Process|http://www.jstor.org/stable/4616664|4616664|2001-06-01|2001|['eng']|['Physical sciences - Chemistry', 'Applied sciences - Engineering', 'Mathematics - Mathematical logic', 'Mathematics - Mathematical analysis']|['Science & Mathematics', 'Statistics']
Spatially explicit data layers of tree species assemblages, referred to as forest types or forest type groups, are a key component in large-scale assessments of forest sustainability, biodiversity, timber biomass, carbon sinks and forest health monitoring. This paper explores the utility of coupling georeferenced national forest inventory (NFI) data with readily available and spatially complete environmental predictor variables through spatially-varying multinomial logistic regression models to predict forest type groups across large forested landscapes. These models exploit underlying spatial associations within the NFI plot array and the spatially-varying impact of predictor variables to improve the accuracy of forest type group predictions. The richness of these models incurs onerous computational burdens and we discuss dimension reducing spatial processes that retain the richness in modeling. We illustrate using NFI data from Michigan, USA, where we provide a comprehensive analysis of this large study area and demonstrate improved prediction with associated measures of uncertainty.|Hierarchical Spatial Models for Predicting Tree Species Assemblages across Large Domains|http://www.jstor.org/stable/30242877|30242877|2009-09-01|2009|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
Analysing the use of marijuana is challenging in part because there is no widely accepted single measure of individual use. Similarly, there is no single response variable that effectively captures attitudes toward its social and moral acceptability. One approach is to view the joint distribution of multiple use and attitude indicators as a mixture of latent classes. Pooling items from the annual 'Monitoring the future' surveys of American high school seniors from 1977 to 2001, we find that marijuana use and attitudes are well summarized by a four-class model. Secular trends in class prevalences over this period reveal major shifts in use and attitudes. Applying a multinomial logistic model to the latent response, we investigate how class membership relates to demographic and life style factors, political beliefs and religiosity over time. Inferences about the parameters of the latent class logistic model are obtained by a combination of maximum likelihood and Bayesian techniques.|Latent Class Logistic Regression: Application to Marijuana Use and Attitudes among High School Seniors|http://www.jstor.org/stable/3877397|3877397|2006-01-01|2006|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Predicting vegetation shifts under climate change is a challenging endeavor, given the complex interactions between biotic and abiotic variables that influence demographic rates. To determine how current trends and variation in climate change affect seedling establishment, we analyzed demographic responses to spatiotemporal variation to temperature and soil moisture in the southern Appalachian Mountains. We monitored seedling establishment for 10 years in five plots located along an elevational gradient of five dominant tree species: Acer rubrum, Betula spp., Liriodendron tulipifera, Nyssa sylvatica, and Quercus rubra. A hierarchical Bayes model allowed us to incorporate different sources of information, observation errors, and the inherent variability of the establishment process. From our analysis, spring temperatures and heterogeneity in soil moisture emerge as key drivers, and they act through nonlinear population demographic processes. We found that all species benefited from warmer springs, in particular the species found on dry slopes, N. sylvatica, and those dominant at higher elevations, Betula spp. and Q. rubra. This last species also benefited from dry environments. Conversely, L. tulipifera, which is abundant on mesic sites, experienced highest establishment rates at high moisture. The mechanisms behind these results may differ among species. Higher temperatures are apparently more important for some, while dry conditions and reduced pathogenic attacks on their seeds and new seedlings have a large impact for others. Our results suggest that only communities found at higher elevations are in danger of regional extinction when their habitats disappear given the current climatic trends. We conclude that the recruitment dynamics of the communities where these species are dominant could be affected by minor changes in climate in ways that cannot be predicted using only climate envelopes, which use different variables and miss the nonlinearities.|Exploiting Temporal Variability to Understand Tree Recruitment Response to Climate Change|http://www.jstor.org/stable/27646080|27646080|2007-05-01|2007|['eng']|['Biological sciences - Ecology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
This paper presents the Bayesian analysis of a semiparametric regression model that consists of parametric and nonparametric components. The nonparametric component is represented with a Fourier series where the Fourier coefficients are assumed a priori to have zero means and to decay to 0 in probability at either algebraic or geometric rates. The rate of decay controls the smoothness of the response function. The posterior analysis automatically selects the amount of smoothing that is coherent with the model and data. Posterior probabilities of the parametric and semiparametric models provide a method for testing the parametric model against a non-specific alternative. The Bayes estimator's mean integrated squared error compares favourably with the theoretically optimal estimator for kernel regression.|Bayesian Inference for Semiparametric Regression Using a Fourier Representation|http://www.jstor.org/stable/2680710|2680710|1999-01-01|1999|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
We present a Bayesian method of ion channel analysis and apply it to a simulated data set. An alternating renewal process prior is assigned to the signal, and an autoregressive process is fitted to the noise. After choosing model hyperconstants to yield 'uninformative' priors on the parameters, the joint posterior distribution is computed by using the reversible jump Markov chain Monte Carlo method. A novel form of simulated tempering is used to improve the mixing of the original sampler.|A Bayesian Restoration of an Ion Channel Signal|http://www.jstor.org/stable/2680739|2680739|1999-01-01|1999|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
This article is motivated by the difficulty of applying standard simulation techniques when identification constraints or theoretical considerations induce covariance restrictions in multivariate models. To deal with this difficulty, we build upon a decomposition of positive definite matrices and show that it leads to straightforward Markov chain Monte Carlo samplers for restricted covariance matrices. We introduce the approach by reviewing results for multivariate Gaussian models without restrictions, where standard conjugate priors on the elements of the decomposition induce the usual Wishart distribution on the precision matrix and vice versa. The unrestricted case provides guidance for constructing efficient Metropolis—Hastings and accept-reject Metropolis—Hastings samplers in more complex settings, and we describe in detain how simulation can be performed under several important constraints. The proposed approach is illustrated in a simulation study and two applications in economics. Supplemental materials for this article (appendixes, data, and computer code) are available online.|MCMC Estimation of Restricted Covariance Matrices|http://www.jstor.org/stable/25651255|25651255|2009-06-01|2009|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Mathematical objects']|['Science & Mathematics', 'Computer Science', 'Statistics']
We consider the problem of directly extracting high-level shape information from images of scenes involving faces. The approach adopted owes much to the work of Grenander and colleagues at Brown University on pattern analysis and involves designing stochastic deformable templates for objects in the underlying image scenes. A wide range of realistic object poses can be captured by imposing a prior probability distribution over the space of allowable deformations. We show how hierarchical models can be used to organize the prior information into a coherent structure. Markov chain Monte Carlo methods are exploited to recover the deformation given observed image data.|Bayesian Faces via Hierarchical Template Modeling|http://www.jstor.org/stable/2290978|2290978|1994-12-01|1994|['eng']|['Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
Many hormones are secreted into the circulatory system in a pulsatile manner and are cleared exponentially. The most common method of analyzing these systems is to deconvolve the hormone concentration into a secretion function and a clearance function. Accurate estimation of the model parameters depends on the number and location of the secretion pulses. To date, deconvolution analysis assumes the number and approximate location of these pulses are known a priori. In this article, we present a novel Bayesian approach to deconvolution that jointly models the number of pulses along with all other model parameters. Our method stochastically searches for the secretion pulses. This is accomplished by viewing the set of parameters that define the pulses as a point process. Pulses are determined by a birth-death process which is embedded in Markov chain Monte Carlo algorithm. This idea originated with Stephens (2000, Annals of Statistics 28, 40-74) in the context of finite mixture model density estimation, where the number of mixture components is unknown. There are several advantages that our model enjoys over the traditional frequentist approaches. These advantages are highlighted with four datasets consisting of serum concentration levels of luteinizing hormone obtained from ovariectomized ewes.|Bayesian Deconvolution Analysis of Pulsatile Hormone Concentration Profiles|http://www.jstor.org/stable/3695441|3695441|2003-09-01|2003|['eng']|['Biological sciences - Biochemistry']|['Science & Mathematics', 'Statistics']
1. Effective resource management ultimately influences vital rates of fecundity and survival for target species. Meta-analysis can be used to combine results from multiple demographic studies replicated in time and space to obtain estimates of vital rates as well as metrics of population growth. 2. Workshop formats were used to conduct meta-analyses of mark-recapture experiments on spotted owls Strix occidentalis in the western USA. The implied motivation for demographic studies of spotted owls has been that changes in vital rates and population growth, λ, reflect the success of conservation strategies, but how to interpret results may not be obvious. Demographic analysis is of little practical utility until vital rates can be linked to management. In the case of spotted owls, future meta-analyses must focus on co-variation between vital rates and habitat variables, and experiments will be necessary. 3. Sensitivity of population growth to variation in vital rates is central to demographic analysis, but results must be interpreted cautiously because these sensitivities are not likely to identify the vital rates most responsible for variation in population size, and cannot reveal which vital rates will be most responsive to conservation investments. 4. Difficulties in documenting dispersal seriously compromised estimates of juvenile survival and thereby biased estimates of λ pm from a projection matrix, a problem that was resolved in later workshops by estimating λ RJS directly using a reparameterized Jolly-Seber mark-recapture method. 5. Several sources of bias for estimates of vital rates and λ were reviewed. Bias exists in meta-analysis estimates of λ combined over spatial replicates because λ is a non-linear function of vital rates. Bias also exists in estimates of average population growth where λ, varies over time. This problem can be reduced by calculating the geometric mean of λ. Research to measure biases associated with the estimation of vital rates and the selection of study areas will be necessary to validate meta-analyses of demography for spotted owls. 6. Synthesis and applications. Meta-analysis is ideally suited to studies of the demography of long-lived species because of the large areas involved, high costs for each individual study, and multiple jurisdictions within which the organisms occur. Mixed models selected using information-theoretic approaches provide a powerful way to combine research results from several studies in a meta-analysis.|Demographic Meta-Analysis: Synthesizing Vital Rates for Spotted Owls|http://www.jstor.org/stable/3505937|3505937|2005-02-01|2005|['eng']|['Biological sciences - Biogeography', 'Biological sciences - Biology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
"External information, such as prior information or expert opinions, can play an important role in the design, analysis and interpretation of clinical trials. However, little attention has been devoted thus far to incorporating external information in clinical trials with binary outcomes, perhaps due to the perception that binary outcomes can be treated as normally-distributed outcomes by using normal approximations. In this paper we show that these two types of clinical trials could behave differently, and that special care is needed for the analysis of clinical trials with binary outcomes. In particular, we first examine a simple but commonly used univariate Bayesian approach and observe a technical flaw. We then study the full Bayesian approach using different beta priors and a new frequentist approach based on the notion of confidence distribution (CD). These approaches are illustrated and compared using data from clinical studies and simulations. The full Bayesian approach is theoretically sound, but surprisingly, under skewed prior distributions, the estimate derived from the marginal posterior distribution may not fall between those from the marginal prior and the likelihood of clinical trial data. This counterintuitive phenomenon, which we call the ""discrepant posterior phenomenon,"" does not occur in the CD approach. The CD approach is also computationally simpler and can be applied directly to any prior distribution, symmetric or skewed."|INCORPORATING EXTERNAL INFORMATION IN ANALYSES OF CLINICAL TRIALS WITH BINARY OUTCOMES|http://www.jstor.org/stable/23566514|23566514|2013-03-01|2013|['eng']|['Mathematics - Mathematical logic', 'Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
This paper develops nonparametric estimation for discrete choice models based on the mixed multinomial logit (MMNL) model. It has been shown that MMNL models encompass all discrete choice models derived under the assumption of random utility maximization, subject to the identification of an unknown distribution G. Noting the mixture model description of the MMNL, we employ a Bayesian nonparametric approach, using nonparametric priors on the unknown mixing distribution G, to estimate choice probabilities. We provide an important theoretical support for the use of the proposed methodology by investigating consistency of the posterior distribution for a general nonparametric prior on the mixing distribution. Consistency is defined according to an L₁-type distance on the space of choice probabilities and is achieved by extending to a regression model framework a recent approach to strong consistency based on the summability of square roots of prior probabilities. Moving to estimation, slightly different techniques for non-panel and panel data models are discussed. For practical implementation, we describe efficient and relatively easy-to-use blocked Gibbs sampling procedures. These procedures are based on approximations of the random probability measure by classes of finite stick-breaking processes. A simulation study is also performed to investigate the performance of the proposed methods.|Bayesian nonparametric estimation and consistency of mixed multinomial logit choice models|http://www.jstor.org/stable/25735008|25735008|2010-08-01|2010|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Mathematics', 'Statistics']
"Consumers make multicategory decisions in a variety of contexts such as choice of multiple categories during a shopping trip or mail-order purchasing. The choice of one category may affect the selection of another category due to the complementary nature (e.g., cake mix and cake frosting) of the two categories. Alternatively, two categories may co-occur in a shopping basket not because they are complementary but because of similar purchase cycles (e.g., beer and diapers) or because of a host of other unobserved factors. While complementarity gives managers some control over consumers' buying behavior (e.g., a change in the price of cake mix could change the purchase probability of cake frosting), co-occurrence or co-incidence is less controllable. Other factors that may affect multi-category choice may be (unobserved) household preferences or (observed) household demographics. We also argue that not accounting for these three factors simultaneously could lead to erroneous inferences. We then develop a conceptual framework that incorporates complementarity, co-incidence and heterogeneity (both observed and unobserved) as the factors that could lead to multicategory choice. We then translate this framework into a model of multicategory choice. Our model is based on random utility theory and allows for simultaneous, interdependent choice of many items. This model, the multivariate probit model, is implemented in a Hierarchical Bayes framework. The hierarchy consists of three levels. The first level captures the choice of items for the shopping basket during a shopping trip. The second level captures differences across households and the third level specifies the priors for the unknown parameters. We generalize some recent advances in Markov chain Monte Carlo methods in order to estimate the model. Specifically, we use a substitution sampler which incorporates techniques such as the Metropolis Hit-and-Run algorithm and the Gibbs Sampler. The model is estimated on four categories (cake mix, cake frosting, fabric detergent and fabric softener) using multicategory panel data. The results disentangle the complementarity and co-incidence effects. The complementarity results show that pricing and promotional changes in one category affect purchase incidence in related product categories. In general, the cross-price and cross-promotion effects are smaller than the own-price and own-promotions effects. The cross-effects are also asymmetric across pairs of categories, i.e., related category pairs may be characterized as having a ""primary"" and a ""secondary"" category. Thus these results provide a more complete description of the effects of promotional changes by examining them both within and across categories. The co-incidence results show the extent of the relationship between categories that arises from uncontrollable and unobserved factors. These results are useful since they provide insights into a general structure of dependence relationships across categories. The heterogeneity results show that observed demographic factors such as family size influence the intrinsic category preference of households. Larger family sizes also tend to make households more price sensitive for both the primary and secondary categories. We find that price sensitivities across categories are not highly correlated at the household level. We also find some evidence that intrinsic preferences for cake mix and cake frosting are more closely related than preferences for fabric detergent and fabric softener. We compare our model with a series of null models using both estimation and holdout samples. We show that both complementarity and co-incidence play a significant role in predicting multicategory choice. We also show how many single-category models used in conjunction may not be good predictors of joint choice. Our results are likely to be of interest to retailers and manufacturers trying to optimize pricing and promotion strategies across many categories as well as in designing micromarketing strategies. We illustrate some of these benefits by carrying out an analysis which shows that the ""true"" impact of complementarity and co-incidence on profitability is significant in a retail setting. Our model can also be applied to other domains. The combination of item interdependence and individual household level estimates may be of particular interest to database marketers in building customized ""cross-selling"" strategies in the direct mail and financial service industries."|"The ""Shopping Basket"": A Model for Multicategory Purchase Incidence Decisions"|http://www.jstor.org/stable/193211|193211|1999-01-01|1999|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering', 'Information science - Coding theory']|['Marketing & Advertising', 'Business & Economics', 'Business']
"The author suggests that for most researchers whose training has been exclusively in classical statistics, the use of the Bayesian approach will require considerable retooling. He observes that the ""technical details of the Bayesian approach are formidable..."" and will require studying textbooks, applications (one of which is included as an appendix), evaluating computer packages, and consulting colleagues."|The Bayesian Approach to Research in Economic Education|http://www.jstor.org/stable/1182271|1182271|1986-01-01|1986|['eng']|['Information science - Informetrics', 'Social sciences - Psychology']|['Business & Economics', 'Education', 'Economics', 'Social Sciences']
Bayesian models provide a structure for studying collections of parameters such are considered in the investigation of communities, ecosystems, and landscapes. This structure allows for improved estimation of individual parameters by considering them in the context of a group of related parameters. Individual estimates are differntially adjusted toward in overall mean, with the magnitude of their adjustment based on their precision. Consequently, Bayesian estimation allows for a more reliable ranking of parameters and, in particular, a more credible identification of extreme values from a collection of estimates. In Bayesian models, individual parameters are regarded as values sampled from a specified probability distribution, called a prior. The requirements that the prior be known is often regarded as an unattractive feature of Bayesian analysis and may be the reason Bayesian analyses are not frequently applied in ecological studies. Empirical Bayes methods provide an alternative approach that incorporates the structural advantages of Bayesian models while requirng a less stringent specification of prior knowledge. Empirical Bayes methods require only that the prior be in a certain family of distributions, indexed by hyperparameters that can be estimated from the available data. This structur is of interest per se, in addition to its value in allowing for improved estimation of individual parameters; for example, hypothese regarding the existence of distinct subgroups in a collection of paramet ers can be considered under the empirical Bayes framework by allowing the hyperparameters to vary among subgroups. We describe the empirical Bayes approach in application to estimation of proportions, using data obtained in a community-wide study Brown-headed Cowbird paratism rates for illustration. Empirical Bayes estimates identify those species for which there is the greatest evidence of extreme parasitism rates. Subgroup analysis of our data on cowbird parasitism rates indicates that parasitisms rates for neotropical migrants as a group are no greater than those of resident/short-distance migrant in this forest community. Our data and analyses demonstrate that the parasitism rates for certain neotropical migrant species (Wood Thrush and Rose-breasted Grosbeak) are remarkably low while those for others (Ovenbird and Red-eyed Vireo) are remarkably high.|Empirical Bayes Estimation of Proportions with Application to Cowbird Parasitism Rates|http://www.jstor.org/stable/2265751|2265751|1996-12-01|1996|['eng']|['Applied sciences - Engineering']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
Statistical methods for analyzing disease incidence and mortality data over time and geographical regions have gained considerable interest in recent years due to increasing concerns of public health, health disparity and legitimate resource allocation. Trend analysis of cancer incidence and mortality rates is essential for subsequent public health investigations. For example, the National Cancer Institute provides software for fitting statistical models to track changes in cancer curves. Currently available models for detecting trend changes over time are designed for a single curve. When multiple curves are available, current methods could be applied multiple times, however, this may not be efficient in the statistical sense. This paper proposes a statistical model that allows concurrent change-point estimation and grouping for multiple curves while maintaining local variabilities. The Bayesian analysis is carried out by eliciting a Dirichlet process prior on the relevant functional space to model change-points. Improper priors are elicited and the resulting posterior is shown to be valid and proper. The age-adjusted lung cancer mortality rates of U.S. states are analyzed to detect change-points and rates of change as well as clusters of states that share similar trends over time. The procedure is also compared with an approach that group states according to a penalized likelihood criterion.|CLUSTERING CURVES BASED ON CHANGE POINT ANALYSIS : A NONPARAMETRIC BAYESIAN APPROACH|http://www.jstor.org/stable/24311040|24311040|2015-04-01|2015|['eng']|['Information science - Coding theory', 'Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
"Statistical analysis of mobility tables has long played a pivotal role in comparative stratification research. This article proposes a shrinkage estimator of the log-odds ratio for comparing mobility tables. Building on an empirical Bayes framework, the shrinkage estimator improves estimation efficiency by ""borrowing strength"" across multiple tables while placing no restrictions on the pattern of association within tables. Numerical simulation shows that the shrinkage estimator outperforms the usual maximum likelihood estimator (MLE) in both the total squared error and the correlation with the true values. Moreover, the benefits of the shrinkage estimator relative to the MLE depend on both the variation in the true log-odds ratio and the variation in sample size among mobility regimes. To illustrate the effects of shrinkage, the author contrasts the shrinkage estimates with the usual estimates for the mobility data assembled by Hazelrigg and Garnier for 16 countries in the 1960s and 1970s. For mobility tables with more than two categories, the shrinkage estimates of log-odds ratios can also be used to calculate summary measures of association that are based on aggregations of log-odds ratios. Specifically, the author constructs an adjusted estimator of the Altham index and, with a set of calibrated simulations, demonstrates its usefulness in enhancing both the precision of individual estimates and the accuracy of cross-table comparisons. Finally, using two real data sets, the author shows that in gauging the overall degree of social fluidity, the adjusted estimator of the Altham index agrees more closely with results from the Unidiff model than does the direct estimator of the Altham index."|SHRINKAGE ESTIMATION OF LOGODDS RATIOS FOR COMPARING MOBILITY TABLES|http://www.jstor.org/stable/44289462|44289462|2015-08-01|2015|['eng']|['Mathematics - Applied mathematics']|['Sociology', 'Social Sciences']
This article models the common density of an exchangeable sequence of observations by a generalization of the process derived from a logistic transform of a Gaussian process. The support of the logistic normal includes all distributions that are absolutely continuous with respect to the dominating measure of the observations. The logistic-normal family is closed in the prior to posterior Bayes analysis, with the observations entering the posterior distribution through the covariance function of the Gaussian process. The covariance of the Gaussian process plays the role of a smoothing kernel. Three features of the model provide a flexible structure for computing the predictive density: (a) The mean of the Gaussian process corresponds to the prior mean of the random density. (b) The prior variance of the Gaussian process controls the influence of the data in the posterior process. As the variance increases, the predictive density has greater fidelity to the data. (c) The prior covariance of the Gaussian process controls the smoothness of its sample paths and the amount of pooling of the sample information. For iid observations the empirical distribution function (edf) is a sufficient statistic for all inference. Since the human eye finds it difficult to distinguish important features of distribution functions, their densities often are plotted instead. Unfortunately, the edf does not possess a density function with respect to Lebesgue measure; consequently, many techniques have been proposed to smooth the edf so that its modification does possess a proper density. From the subjective, Bayesian perspective, the data are iid given a common but unspecified density. Beliefs about the unknown density are modeled through probability statements. When the density has a known functional form, the prior distribution concerns the density's parameters, which describe important, unknown features of the density. In this article the density is not constrained to a functional form, so it becomes the parameter of interest. Its posterior distribution becomes the mechanism for smoothing the edf so that the density estimator (the posterior mean) evaluated at a point can use nearby data.|The Logistic Normal Distribution for Bayesian, Nonparametric, Predictive Densities|http://www.jstor.org/stable/2288870|2288870|1988-06-01|1988|['eng']|['Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
It is critical to estimate control-by-noise interactions in robust parameter design. This can be achieved by using a cross array, which is a cross product of a design for control factors and another design for noise factors. However, the total run size of such arrays can be prohibitively large. To reduce the run size, single arrays are proposed in the literature, where a modified effect hierarchy principle is used for the optimal selection of the arrays. In this article, we argue that effect hierarchy principle should not be altered for achieving the robustness objective of the experiment. We propose a Bayesian approach to develop single arrays which incorporate the importance of control-by-noise interactions without altering the effect hierarchy. The approach is very general and places no restrictions on the number of runs or levels or type of factors or type of designs. A modified exchange algorithm is proposed for finding the optimal single arrays. MATLAB code for implementing the algorithm is available as supplemental material in the online version of this article on the Technometrics web site. We also explain how to design experiments with internal noise factors, a topic that has received scant attention in the literature. The advantages of the proposed approach are illustrated using several examples.|Bayesian Optimal Single Arrays for Robust Parameter Design|http://www.jstor.org/stable/40586620|40586620|2009-08-01|2009|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Knowledge of organisms' growth rates and ages at sexual maturity is important for conservation efforts and a wide variety of studies in ecology and evolutionary biology. However, these life history parameters may be difficult to obtain from natural populations: individuals encountered may be of unknown age, information on age at sexual maturity may be uncertain and interval-censored, and growth data may include both individual heterogeneity and measurement errors. We analyzed mark–recapture data for Red-backed Salamanders (Plethodon cinereus) to compare sex-specific growth rates and ages at sexual maturity. Aging of individuals was made possible by the use of a von Bertalanffy model of growth, complemented with models for interval-censored and imperfect observations at sexual maturation. Individual heterogeneity in growth was modeled through the use of Gamma processes. Our analysis indicates that female P. cinereus mature earlier and grow more quickly than males, growing to nearly identical asymptotic size distributions as males.|Individual Heterogeneity in Growth and Age at Sexual Maturity: A Gamma Process Analysis of Capture–Mark–Recapture Data|http://www.jstor.org/stable/26451837|26451837|2015-09-01|2015|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Agriculture', 'Statistics']
The current framework for U.S. Environmental Protection Agency regulation of water quality in community drinking water supplies produces sequential rules for either single contaminants or small groups of similar contaminants. For both substantive and pragmatic reasons, some water industry experts have advocated the development of a more holistic regulatory process in which rules are promulgated less frequently but for larger contaminant classes. Such a framework would require the expansion of existing regulatory evaluation technologies to account for joint occurrence distributions of multiple contaminants. This article presents an analysis, using two national contaminant databases, of the joint distributions of seven contaminants (arsenic, nitrate, uranium, manganese, magnesium, calcium, and sulfate) in community water system source waters. Inferences are based on a flexible Bayesian hierarchical modeling structure with numerous features desirable for empirical exploration of multicontaminant regulations, including the simultaneous estimation of spatial heterogeneity in contaminant levels and covariations among contaminants, applicability to sparse data collected over a large spatial scale, and coherent assimilation of information provided by censored observations. The model is used to estimate a family of joint distributions for the contaminants indexed by water system characteristics, with empirically appropriate complexity given the resolution of the available data. The resulting distributions provide insights about the nature of, and uncertainty about, contaminant co-occurrence patterns, quantify the impact on national assessments of jointly modeling the contaminants, and facilitate identification of critical classes of water systems where uncertainty is highest.|Analysis of Contaminant Co-Occurrence in Community Water Systems|http://www.jstor.org/stable/27590352|27590352|2004-03-01|2004|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
In this article, we show that the production, aggregation, and estimation theories that lead to a stochastic-coefficients production function are more general than those leading to a fixed-coefficients production function. Estimates of productivity changes implied by these two types of Cobb-Douglas production functions are compared for several U.S. manufacturing sectors for the period 1955-1982. All of the estimates of the stochastic-coefficients Cobb-Douglas function have the right algebraic sign for all industries considered, whereas some of the estimates of its fixed-coefficients counterpart have the wrong sign.|Productivity Analysis of U.S. Manufacturing Using a Stochastic-Coefficients Production Function|http://www.jstor.org/stable/1391886|1391886|1988-07-01|1988|['eng']|['Mathematics - Mathematical logic']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
When fitting wavelet based models, shrinkage of the empirical wavelet coefficients is an effective tool for denoising the data. This article outlines a Bayesian approach to shrinkage, obtained by placing priors on the wavelet coefficients. The prior for each coefficient consists of a mixture of two normal distributions with different standard deviations. The simple and intuitive form of prior allows us to propose automatic choices of prior parameters. These parameters are chosen adaptively according to the resolution level of the coefficients, typically shrinking high resolution (frequency) coefficients more heavily. Assuming a good estimate of the background noise level, we obtain closed form expressions for the posterior means and variances of the unknown wavelet coefficients. The latter may be used to assess uncertainty in the reconstruction. Several examples are used to illustrate the method, and comparisons are made with other shrinkage methods.|Adaptive Bayesian Wavelet Shrinkage|http://www.jstor.org/stable/2965411|2965411|1997-12-01|1997|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
This article proposes a nonlinear block shrinkage method in the wavelet domain for estimating an unknown function in the presence of Gaussian noise. This shrinkage uses an empirical Bayesian blocking approach that accounts for the sparseness of the representation of the unknown function. The modeling is accomplished by using a mixture of two normal-inverse gamma (NIG) distributions as a joint prior on wavelet coefficients and noise variance in each block at a particular resolution level. This method results in an explicit and readily implementable weighted sum of shrinkage rules. An automatic, level-dependent choice for the model hyperparameters, that leads to amplitude-scale invariant solutions, is also suggested. Finally, the performance of the proposed method, BBS (Bayesian block shrinkage), is illustrated on the battery of standard test functions and compared to some existing block-wavelet denoising methods.|Wavelet Bayesian Block Shrinkage via Mixtures of Normal-Inverse Gamma Priors|http://www.jstor.org/stable/1391182|1391182|2004-06-01|2004|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Computer Science', 'Statistics']
The p-beauty contest is a multi-player number guessing game that is widely used to study strategic behavior. Using new data from a speciallydesigned web experiment, we examine the evidence in favor of a popular class of behavioral economic models called k-step thinking models. After fitting a custom Bayesian spline model to the experimental data, we estimate that the proportion of players who could be using a k-step thinking strategy is approximately 25%.|A BAYESIAN HIERARCHICAL MODEL FOR INFERRING PLAYER STRATEGY TYPES IN A NUMBER GUESSING GAME|http://www.jstor.org/stable/43826429|43826429|2015-09-01|2015|['eng']|['Mathematics - Mathematical logic']|['Mathematics', 'Science & Mathematics', 'Statistics']
We describe adaptive Markov chain Monte Carlo (MCMC) methods for sampling posterior distributions arising from Bayesian variable selection problems. Point-mass mixture priors are commonly used in Bayesian variable selection problems in regression. However, for generalized linear and nonlinear models where the conditional densities cannot be obtained directly, the resulting mixture posterior may be difficult to sample using standard MCMC methods due to multimodality. We introduce an adaptive MCMC scheme that automatically tunes the parameters of a family of mixture proposal distributions during simulation. The resulting chain adapts to sample efficiently from multimodal target distributions. For variable selection problems point-mass components are included in the mixture, and the associated weights adapt to approximate marginal posterior variable inclusion probabilities, while the remaining components approximate the posterior over nonzero values. The resulting sampler transitions efficiently between models, performing parameter estimation and variable selection simultaneously. Ergodicity and convergence are guaranteed by limiting the adaptation based on recent theoretical results. The algorithm is demonstrated on a logistic regression model, a sparse kernel regression, and a random field model from statistical biophysics; in each case the adaptive algorithm dramatically outperforms traditional MH algorithms. Supplementary materials for this article are available online.|Adaptive Markov Chain Monte Carlo for Bayesian Variable Selection|http://www.jstor.org/stable/43304856|43304856|2013-09-01|2013|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Computer Science', 'Statistics']
Constrained parameter problems arise in a wide variety of applications, including bioassay, actuarial graduation, ordinal categorical data, response surfaces, reliability development testing, and variance component models. Truncated data problems arise naturally in survival and failure time studies, ordinal data models, and categorical data studies aimed at uncovering underlying continuous distributions. In many applications both parameter constraints and data truncation are present. The statistical literature on such problems is very extensive, reflecting both the problems' widespread occurrence in applications and the methodological challenges that they pose. However, it is striking that so little of this applied and theoretical literature involves a parametric Bayesian perspective. From a technical viewpoint, this perhaps is not difficult to understand. The fundamental tool for Bayesian calculations in typical realistic models is (multidimensional) numerical integration, which often is problematic in unconstrained contexts and can be well-nigh impossible for the kinds of constrained problems we consider. In this article we show that Bayesian calculations can be implemented routinely for constrained parameter and truncated data problems by means of the Gibbs sampler. Specific models discussed include constrained multinormal parameters, constrained linear model parameters, ordered parameters in experimental family models, data and order restricted parameters from exponential distributions, straight line regression with censoring and bivariate grouped data models. Analysis of data sets illustrating the first two of these settings is provided.|Bayesian Analysis of Constrained Parameter and Truncated Data Problems Using Gibbs Sampling|http://www.jstor.org/stable/2290286|2290286|1992-06-01|1992|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
A Bayesian hierarchical model is developed for count data with spatial and temporal correlations as well as excessive zeros, uneven sampling intensities, and inference on missing spots. Our contribution is to develop a model on zero-inflated count data that provides flexibility in modeling spatial patterns in a dynamic manner and also improves the computational efficiency via dimension reduction. The proposed methodology is of particular importance for studying species presence and abundance in the field of ecological sciences. The proposed model is employed in the analysis of the survey data by the Northeast Fisheries Sciences Center (NEFSC) for estimation and prediction of the Atlantic cod in the Gulf of Maine - Georges Bank region. Model comparisons based on the deviance information criterion and the log predictive score show the improvement by the proposed spatial-temporal model.|BAYESIAN SPATIAL-TEMPORAL MODELING OF ECOLOGICAL ZERO-INFLATED COUNT DATA|http://www.jstor.org/stable/24311011|24311011|2015-01-01|2015|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
We present a new joint longitudinal and survival model aimed at estimating the association between the risk of an event and the change in and history of a biomarker that is repeatedly measured over time. We use cubic B-splines models for the longitudinal component that lend themselves to straight-forward formulations of the slope and integral of the trajectory of the biomarker. The model is applied to data collected in a long term followup study of HIV infected infants in Uganda. Estimation is carried out using MCMC methods. We also explore using the deviance information criteria, the conditional predictive ordinate and ROC curves for model selection and evaluation.|Assessing the Association between Trends in a Biomarker and Risk of Event with an Application in Pediatric HIV/AIDS|http://www.jstor.org/stable/30242882|30242882|2009-09-01|2009|['eng']|['Mathematics - Mathematical objects']|['Mathematics', 'Science & Mathematics', 'Statistics']
In this paper we provide a general framework for constructing test statistics for parameters in time series models using bootstrap methods. We consider parametric and nonparametric bootstrap tests for linear restrictions on the parameters of linear time series models with nonstationary components. A study into the efficiency of bootstrap tests in cases where the parameters lie on the boundary of the parameter space is undertaken. We set up a Monte Carlo study to show that bootstrap tests for a particular class of time series models have desirable properties with respect to size and power. An illustration is given for a series of electrical energy consumption in the northeast region of Brazil.|BOOTSTRAP TESTS WHEN PARAMETERS OF NONSTATIONARY TIME SERIES MODELS LIE ON THE BOUNDARY OF THE PARAMETER SPACE|http://www.jstor.org/stable/43601402|43601402|1999-06-01|1999|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Science & Mathematics', 'Mathematics', 'Statistics']
The authors propose methods for Bayesian inference for generalized linear models with missing covariate data. They specify a parametric distribution for the covariates that is written as a sequence of one-dimensional conditional distributions. They propose an informative class of joint prior distributions for the regression coefficients and the parameters arising from the covariate distributions. They examine the properties of the proposed prior and resulting posterior distributions. They also present a Bayesian criterion for comparing various models, and a calibration is derived for it. A detailed simulation is conducted and two real data sets are examined to demonstrate the methodology. /// Les auteurs proposent des méthodes d'inférence bayésienne adaptées aux modèles linéaires généralisés pour les situations où les valeurs de certaines covariables sont manquantes au hasard. La loi paramétrique choisie pour les covariables s'exprime comme succession de lois conditionnelles univariées. Une classe de lois a priori informative est suggérée pour les coefficients de régression et pour les paramètres liés aux lois des covariables. Les auteurs examinent les propriétés de ces lois a priori et des lois a posteriori qui en découlent. Ils présentent aussi un critère bayésien pour la comparaison des différents modèles et montrent comment le calibrer. Une simulation détaillée et l'examen de deux ensembles de données réelles illustrent l'approche proposée.|Bayesian Methods for Generalized Linear Models with Covariates Missing at Random|http://www.jstor.org/stable/3315865|3315865|2002-03-01|2002|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
The importance of mixed effects model is well known for the small area estimation problem. Gaussian distributions are commonly used for the small area specific random effects. The mixing distribution is used to 'borrow strength' from the other small areas and the Gaussian mixing distribution is mathematically convenient for inference purposes. However, this choice places a strong assumption about the shape of the mixing distribution that may not be valid. In this article, we focus on misspecification in unit specific small area model with random intercept commonly known as type II model. We propose a Gaussian mixture with an unknown number of components to model the prior distribution of the random intercept in a hierarchical Bayesian framework.|Modelling Small Area Effects Using Mixture of Gaussians|http://www.jstor.org/stable/25053291|25053291|2003-08-01|2003|['eng']|['Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
"Empirical Bayes concepts are implemented in a simultaneous analysis of a system of mixed linear models having linked and serially correlated random effects. Emphasis is placed on the estimation of the random effects and exploration of the relationships between them. Application is made to the investigation of several series of laboratory assay data that were observed during overlapping time intervals and were therefore subjected to common systematic errors, or ""daily effects."" The motivation for this work was the need to investigate methods of adjustment for such daily effects, and to estimate the degree to which concurrently run series are impacted in common. Attention is given to the construction of confidence intervals for daily effects. Tractable methods are proposed that yield approximately correct coverage for large samples. Although derived within a Bayes-empirical Bayes framework, these intervals are somewhat similar to intervals constructed by the method of Kackar and Harville. Implementation of Type III bootstrap confidence intervals is also discussed. The expectation maximization (EM) algorithm provides a natural parameter estimation method because of its intimate relationship with the estimation of the posterior distribution of the unobservable effects. The E step is shown to be essentially equivalent to Kalman smoothing of daily sums of residuals within each of the linear models of the system, while the M step admits a complete decoupling of the system into its individual components, followed by standard least squares calculations."|Empirical Bayes Analysis for Systems of Mixed Models with Linked Autocorrelated Random Effects|http://www.jstor.org/stable/2290518|2290518|1991-12-01|1991|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Markov chain Monte Carlo (MCMC) algorithms, such as the Gibbs sampler, have provided a Bayesian inference machine in image analysis and in other areas of spatial statistics for several years, founded on the pioneering ideas of Ulf Grenander. More recently, the observation that hyperparameters can be included as part of the updating schedule and the fact that almost any multivariate distribution is equivalently a Markov random field has opened the way to the use of MCMC in general Bayesian computation. In this paper, we trace the early development of MCMC in Bayesian inference, review some recent computational progress in statistical physics, based on the introduction of auxiliary variables, and discuss its current and future relevance in Bayesian applications. We briefly describe a simple MCMC implementation for the Bayesian analysis of agricultural field experiments, with which we have some practical experience.|Spatial Statistics and Bayesian Computation|http://www.jstor.org/stable/2346064|2346064|1993-01-01|1993|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
In the USA, the Centers for Medicare and Medicaid Services use 30-day readmission, following hospitalization, as a proxy outcome to monitor quality of care. These efforts generally focus on treatable health conditions, such as pneumonia and heart failure. Expanding quality-of-care systems to monitor conditions for which treatment options are limited or non-existent, such as pancreatic cancer, is challenging because of the non-trivial force of mortality; 30-day mortality for pancreatic cancer is approximately 30%. In the statistical literature, data that arise when the observation of the time to some non-terminal event is subject to some terminal event are referred to as 'semicompeting risks data'. Given such data, scientific interest may lie in at least one of three areas: estimation or inference for regression parameters, characterization of dependence between the two events and prediction given a covariate profile. Existing statistical methods focus almost exclusively on the first of these; methods are sparse or non-existent, however, when interest lies with understanding dependence and performing prediction. We propose a Bayesian semiparametric regression framework for analysing semicompeting risks data that permits the simultaneous investigation of all three of the aforementioned scientific goals. Characterization of the induced posterior and posterior predictive distributions is achieved via an efficient Metropolis–Hastings–Green algorithm, which has been implemented in an R package. The framework proposed is applied to data on 16051 individuals who were diagnosed with pancreatic cancer between 2005 and 2008, obtained from Medicare part A. We found that increased risk for readmission is associated with a high comorbidity index, a long hospital stay at initial hospitalization, non-white race, being male and discharge to home care.|Bayesian semiparametric analysis of semicompeting risks data: investigating hospital readmission after a pancreatic cancer diagnosis|http://www.jstor.org/stable/24771893|24771893|2015-02-01|2015|['eng']|['Applied sciences - Engineering', 'Health sciences - Health and wellness', 'Biological sciences - Biology', 'Health sciences - Medical specialties']|['Science & Mathematics', 'Statistics']
This article describles a Bayesian statistical analysis of long-term changes in the depth of the ocean's mixed layer. The data are thermal profiles recorded by ships. For these data, there is no good sampling model and thus no obvious likelihood function. Our approach is to elicit posterior distributions for training data directly from the expert. We then infer the likelihood function and use it on large datasets.|Subjective Likelihood for the Assessment of Trends in the Ocean's Mixed-Layer Depth [With Comments, Rejionder]|http://www.jstor.org/stable/27639924|27639924|2007-09-01|2007|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Mixtures of Polya tree models provide a flexible alternative when a parametric model may only hold approximately. I provide computational strategies for obtaining full semiparametric inference for mixtures of finite Polya tree models given a standard parameterization, including models that would be troublesome to fit using Dirichlet process mixtures. Recommendations are put forth on choosing the level of a finite Polya tree, and model comparison is discussed. Several examples demonstrate the utility of finite Polya tree modeling, including data fit to generalized linear mixed models and several survival models.|Inference for Mixtures of Finite Polya Tree Models|http://www.jstor.org/stable/27639772|27639772|2006-12-01|2006|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Diagnostic checking of the specification of time series models is normally carried out using the innovations--that is, the one-step-ahead prediction errors. In an unobserved-components model, other sets of residuals are available. These auxiliary residuals are estimators of the disturbances associated with the unobserved components. They can often yield information that is less apparent from the innovations, but they suffer from the disadvantage that they are serially correlated even in a correctly specified model with known parameters. This article shows how the properties of the auxiliary residuals may be obtained, how they are related to each other and to the innovations, and how they can be used to construct test statistics. Applications are presented showing how residuals can be used to detect and distinguish between outliers and structural change.|Diagnostic Checking of Unobserved-Components Time Series Models|http://www.jstor.org/stable/1391813|1391813|1992-10-01|1992|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
Two coupled Gibbs sampler chains, both with invariant probability density π, are run in parallel so that the chains are negatively correlated. We define an asymptotically unbiased estimator of the π-expectation E(f(X)) which achieves significant variance reduction with respect to the usual Gibbs sampler at comparable computational cost. The variance of the estimator based on the new algorithm is always smaller than the variance of a single Gibbs sampler chain, if π is attractive and f is monotone nondecreasing in all components of X. For nonattractive targets π, our results are not complete: The new antithetic algorithm outperforms the standard Gibbs sampler when π is a multivariate normal density or the Ising model. More generally, nonrigorous arguments and numerical experiments support the usefulness of the antithetically coupled Gibbs samplers also for other nonattractive models. In our experiments the variance is reduced to at least a third and the efficiency also improves significantly.|Antithetic Coupling of Two Gibbs Sampler Chains|http://www.jstor.org/stable/2673957|2673957|2000-08-01|2000|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics', 'Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
We introduce a Bayesian multiple regression tree model to characterize relationships between physico-chemical properties of nanoparticles and their in-vitro toxicity over multiple doses and times of exposure. Unlike conventional models that rely on data summaries, our model solves the low sample size issue and avoids arbitrary loss of information by combining all measurements from a general exposure experiment across doses, times of exposure, and replicates. The proposed technique integrates Bayesian trees for modeling threshold effects and interactions, and penalized B-splines for dose- and time-response surface smoothing. The resulting posterior distribution is sampled by Markov Chain Monte Carlo. This method allows for inference on a number of quantities of potential interest to substantive nanotoxicology, such as the importance of physico-chemical properties and their marginal effect on toxicity. We illustrate the application of our method to the analysis of a library of 24 nano metal oxides.|A BAYESIAN REGRESSION TREE APPROACH TO IDENTIFY THE EFFECT OF NANOPARTICLES' PROPERTIES ON TOXICITY PROFILES|http://www.jstor.org/stable/24522423|24522423|2015-03-01|2015|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
We consider a multivariate lognormal frailty model for correlated exchangeable failure time data, where the marginal lifetimes have conditional Weibull distributions. We discuss Bayesian statistical methods to fit this model to experimental data with varying cluster sizes. The Bayesian inferential approach arises naturally from the hierarchical structure of the frailty model. In contrast, implementation of the maximum likelihood approach encounters practical difficulties. The methodology is illustrated with the analyses of three datasets.|Multivariate Frailty Models for Exchangeable Survival Data with Covariates|http://www.jstor.org/stable/25471212|25471212|2006-08-01|2006|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Statistical identification of isochore structure, the variation in large-scale GC composition (proportion of DNA bases that are G or as opposed to A or T), of mammalian genomes is a necessary requirement for understanding both the evolution of base composition and the many genomic features such as mutation and recombination rates, which covary with base composition. We have developed a Bayesian method for isochore analysis that we demonstrate to be more accurate than the commonly used binary segmentation approach implemented within the program IsoFinder. The method accounts for both fine-scale and large-scale structure. We adapt direct simulation methods to allow for iid samples from the posterior distribution of our model, and provide an accurate approximation to this that can analyze data from a chromosome in a matter of seconds. We apply our method to human chromosome 1. The resulting estimate of how GC content varies across this region is shown to be a better predictor of local recombination rates than IsoFinder, and we are able to detect regions consistent with the classic definition of isochores that cover 85% of the chromosome. We also show a measure of relative GC content to be particularly predictive of local recombination rates.|Bayesian Analysis of Isochores|http://www.jstor.org/stable/40591905|40591905|2009-03-01|2009|['eng']|['Biological sciences - Biology']|['Science & Mathematics', 'Statistics']
A smoothness priors modeling of time series with trends and seasonalities is shown. An observed time series is decomposed into local polynomial trend, seasonal, globally stationary autoregressive and observation error components. Each component is characterized by an unknown variance-white noise perturbed difference equation constraint. The constraints or Bayesian smoothness priors are expressed in state space model form. Trading day factors are also incorporated in the model. A Kalman predictor yields the likelihood for the unknown variances (hyperparameters). Likelihoods are computed for different constraint order models in different subsets of constraint equation model classes. Akaike's minimum AIC procedure is used to select the best model fitted to the data within and between the alternative model classes. Smoothing is achieved by using a fixed-interval smoother algorithm. Examples are shown.|A Smoothness Priors-State Space Modeling of Time Series with Trend and Seasonality|http://www.jstor.org/stable/2288279|2288279|1984-06-01|1984|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
A novel fully automatic Bayesian procedure for variable selection in normal regression models is proposed. The procedure uses the posterior probabilities of the models to drive a stochastic search. The posterior probabilities are computed using intrinsic priors, which can be considered default priors for model selection problems; that is, they are derived from the model structure and are free from tuning parameters. Thus they can be seen as objective priors for variable selection. The stochastic search is based on a Metropolis-Hastings algorithm with a stationary distribution proportional to the model posterior probabilities. The procedure is illustrated on both simulated and real examples.|Objective Bayesian Variable Selection|http://www.jstor.org/stable/30047446|30047446|2006-03-01|2006|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Here I propose a convergence diagnostic for Markov chain Monte Carlo (MCMC) algorithms based on couplings of a Markov chain with an auxiliary chain that is periodically restarted from a fixed parameter value. The diagnostic provides a mechanism for estimating the specific constants governing the rate of convergence of geometrically and uniformly ergodic chains, and provides a lower bound on the effective sample size of a MCMC run. It also provides a simple procedure for obtaining what is, with high probability, an independent sample from the stationary distribution.|A Coupling-Regeneration Scheme for Diagnosing Convergence in Markov Chain Monte Carlo Algorithms|http://www.jstor.org/stable/2669620|2669620|1998-03-01|1998|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
This article seeks to improve understanding of cross-country patterns of economic growth. It adopts a stochastic production-frontier model that allows for the decomposition of output change into input, efficiency, and technical change. The production frontier is assumed to depend on effective inputs rather than measured inputs. We develop a model in which effective inputs depend on observed factor use and a correction term that depends on variables such as education. A further extension over related work is our use of a production frontier that varies over regional country groups. Empirical results indicate that both these extensions are very important.|Modeling the Sources of Output Growth in a Panel of Countries|http://www.jstor.org/stable/1392262|1392262|2000-07-01|2000|['eng']|['Economics - Economic policy', 'Applied sciences - Engineering']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
This paper describes the creation of a fine-grained named entity annotation scheme and corpus for Dutch, and experiments on automatic main type and subtype named entity recognition. We give an overview of existing named entity annotation schemes, and motivate our own, which describes six main types (persons, organizations, locations, products, events and miscellaneous named entities) and finer-grained information on subtypes and metonymic usage. This was applied to a one-million-word subset of the Dutch SoNaR reference corpus. The classifier for main type named entities achieves a micro-averaged F-score of 84.91 %, and is publicly available, along with the corpus and annotations.|Fine-grained Dutch named entity recognition|http://www.jstor.org/stable/24710414|24710414|2014-04-01|2014|['eng']|['Applied sciences - Engineering']|['Linguistics', 'Social Sciences']
Neurophysiologists investigating mechanisms underlying neural responses to stimuli have, in recent years, developed substantial interest in modelling certain types of neural response data by using simple mixture distributions. Techniques of mixture deconvolution using likelihood-based techniques have become popular. This paper reports on novel Bayesian approaches using (uncertain) mixtures of (uncertain numbers of) noise distributions to model data measuring maximum levels of evoked neural responses following various levels of electrical stimulus of nerve tissue. We discuss some of the key scientific issues, including physiological hypotheses of 'quantal' levels of neuronal transmissions, together with technical aspects of data analysis, modelling and the use of prior information in addressing these issues within an appropriate Bayesian framework. Illustration of neural response deconvolution analysis using this approach is presented.|Deconvolution of Mixtures in Analysis of Neural Synaptic Transmission|http://www.jstor.org/stable/2348930|2348930|1994-01-01|1994|['eng']|['Applied sciences - Engineering', 'Biological sciences - Biology']|['Science & Mathematics', 'Statistics']
We develop a structural demand model that endogenously captures the effect of out-of-stocks on customer choice by simulating a time-varying set of available alternatives. Our estimation method uses store-level data on sales and partial information on product availability. Our model allows for flexible substitution patterns, which are based on utility maximization principles and can accommodate categorical and continuous product characteristics. The methodology can be applied to data from multiple markets and in categories with a relatively large number of alternatives, slow-moving products, and frequent out-of-stocks (unlike many existing approaches). In addition, we illustrate how the model can be used to assist the decisions of a store manager in two ways. First, we show how to quantify the lost sales induced by out-of-stock products. Second, we provide insights on the financial consequences of out-of-stocks and suggest price promotion policies that can be used to help mitigate their negative economic impact, which run counter to simple commonly used heuristics.|Structural Estimation of the Effect of Out-of-Stocks|http://www.jstor.org/stable/40785250|40785250|2010-07-01|2010|['eng']|['Applied sciences - Engineering', 'Information science - Coding theory']|['Management & Organizational Behavior', 'Business & Economics', 'Business']
We develop algorithms for performing semiparametric regression analysis in real time, with data processed as it is collected and made immediately available via modern telecommunications technologies. Our definition of semiparametric regression is quite broad and includes, as special cases, generalized linear mixed models, generalized additive models, geostatistical models, wavelet nonparametric regression models and their various combinations. Fast updating of regression fits is achieved by couching semiparametric regression into a Bayesian hierarchical model or, equivalently, graphical model framework and employing online mean field variational ideas. An Internet site attached to this article, realtime-semiparametnc-regression.net, illustrates the methodology for continually arriving stock market, real estate, and airline data. Flexible real-time analyses based on increasingly ubiquitous streaming data sources stand to benefit. This article has online supplementary material.|Real-Time Semiparametric Regression|http://www.jstor.org/stable/43304913|43304913|2014-09-01|2014|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Computer Science', 'Statistics']
We derive a new class of priors for the variance component in the Fay-Herriot model, a mixed regression model widely used in small area estimation. This class includes the well-known uniform or superharmonic prior. Through simulation we illustrate the use of our class of priors.|A New Class of Average Moment Matching Priors|http://www.jstor.org/stable/20441480|20441480|2008-06-01|2008|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
A Bayesian method for segmenting weed and crop textures is described and implemented. The work forms part of a project to identify weeds and crops in images so that selective crop spraying can be carried out. An image is subdivided into blocks and each block is modelled as a single texture. The number of different textures in the image is assumed unknown. A hierarchical Bayesian procedure is used where the texture labels have a Potts model (colour Ising Markov random field) prior and the pixels within a block are distributed according to a Gaussian Markov random field, with the parameters dependent on the type of texture. We simulate from the posterior distribution by using a reversible jump Metropolis-Hastings algorithm, where the number of different texture components is allowed to vary. The methodology is applied to a simulated image and then we carry out texture segmentation on the weed and crop images that motivated the work.|Bayesian Texture Segmentation of Weed and Crop Images Using Reversible Jump Markov Chain Monte Carlo Methods|http://www.jstor.org/stable/3592630|3592630|2003-01-01|2003|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
We present several Markov chain Monte Carlo simulation methods that have been widely used in recent years in econometrics and statistics. Among these is the Gibbs sampler, which has been of particular interest to econometricians. Although the paper summarizes some of the relevant theoretical literature, its emphasis is on the presentation and explanation of applications to important models that are studied in econometrics. We include a discussion of some implementation issues, the use of the methods in connection with the EM algorithm, and how the methods can be helpful in model specification questions. Many of the applications of these methods are of particular interest to Bayesians, but we also point out ways in which frequentist statisticians may find the techniques useful.|Markov Chain Monte Carlo Simulation Methods in Econometrics|http://www.jstor.org/stable/3532527|3532527|1996-08-01|1996|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Business', 'Business & Economics Collection', 'Economics']
This paper considers Bayesian nonparametric estimation of conditional densities by countable mixtures of location-scale densities with covariate dependent mixing probabilities. The mixing probabilities are modeled in two ways. First, we consider finite covariate dependent mixture models, in which the mixing probabilities are proportional to a product of a constant and a kernel and a prior on the number of mixture components is specified. Second, we consider kernel stick-breaking processes for modeling the mixing probabilities. We show that the posterior in these two models is weakly and strongly consistent for a large class of data-generating processes. A simulation study conducted in the paper demonstrates that the models can perform well in small samples.|POSTERIOR CONSISTENCY IN CONDITIONAL DENSITY ESTIMATION BY COVARIATE DEPENDENT MIXTURES|http://www.jstor.org/stable/24534557|24534557|2014-06-01|2014|['eng']|['Mathematics - Mathematical logic']|['Business & Economics', 'Business', 'Economics']
The authors consider the problem of simultaneous transformation and variable selection for linear regression. They propose a fully Bayesian solution to the problem, which allows averaging over all models considered including transformations of the response and predictors. The authors use the Box—Cox family of transformations to transform the response and each predictor. To deal with the change of scale induced by the transformations, the authors propose to focus on new quantities rather than the estimated regression coefficients. These quantities, referred to as generalized regression coefficients, have a similar interpretation to the usual regression coefficients on the original scale of the data, but do not depend on the transformations. This allows probabilistic statements about the size of the effect associated with each variable, on the original scale of the data. In addition to variable and transformation selection, there is also uncertainty involved in the identification of outliers in regression. Thus, the authors also propose a more robust model to account for such outliers based on a t-distribution with unknown degrees of freedom. Parameter estimation is carried out using an efficient Markov chain Monte Carlo algorithm, which permits moves around the space of all possible models. Using three real data sets and a simulated study, the authors show that there is considerable uncertainty about variable selection, choice of transformation, and outlier identification, and that there is advantage in dealing with all three simultaneously. Nous considérons le problème de la selection de transformations et de variables pour la régression linéaire. Nous proposons une approche Bayesienne à ce problème qui nous permet de faire la moyenne de tous les modèles considerés y compris les transformations de type Box-Cox de la résponse et des prédicteurs. Pour prendre en considération le changement d'unité induit par les transformations, nous proposons d'examiner et d'estimer de nouvelles quantités à la place des coéfficients de regréssion. Ces quantités nouvelles, que nous appellons coefficients de régressions generalisés, peuvent être interpretés comme les coéfficients de régression dans l'unité originale des données, et ne dependent donc pas des transformations sélectionées. En particulier, cela nous permet de faire de l'inference sur la taille des effets associés avec chaque variable, et ce, dans l'unité original des données. En plus des transformations, nous considérons aussi le problème de la détection de valeurs abérrantes, ainsi que l'incertitude associée à cette détection. Pour modéliser ces données abérrantes, nous utilisons une loi de t avec un nombre de degrés de liberté inconnu. L'estimation des paramêtres est faite en utilisant un algorithm MCMC efficace qui nous permet de traverser l'espace constitué de tous les modèles possibles. En utilisant trois jeux de données réelles ainsi que des données simulées, nous montrons que l'incertitude associée au choix de variables, de transformations et de données abérrantes est considérable, et qu'il est important que les trois sélections soient considérées en meme temps.|Bayesian robust transformation and variable selection: a unified approach|http://www.jstor.org/stable/25653485|25653485|2009-09-01|2009|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
A Bayesian reference analysis of the cointegrated vector autoregression is presented based on a new prior distribution. Among other properties, it is shown that this prior distribution distributes its probability mass uniformly over all cointegration spaces for a given cointegration rank and is invariant to the choice of normalizing variables for the cointegration vectors. Several methods for computing the posterior distribution of the number of cointegrating relations and distribution of the model parameters for a given number of relations are proposed, including an efficient Gibbs sampling approach where all inferences are determined from the same posterior sample. Simulated data are used to illustrate the procedures and for discussing the well-known issue of local nonidentification.|Bayesian Reference Analysis of Cointegration|http://www.jstor.org/stable/3533468|3533468|2005-04-01|2005|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Mathematical objects']|['Business', 'Business & Economics Collection', 'Economics']
A class of model-based filters for extracting trends and cycles in economic time series is presented. These lowpass and bandpass filters are derived in a mutually consistent manner as the joint solution to a signal extraction problem in an unobserved-components model. The resulting trends and cycles are computed in finite samples using the Kalman filter and associated smoother. The filters form a class which is a generalization of the class of Butterworth filters, widely used in engineering. They are very flexible and have the important property of allowing relatively smooth cycles to be extracted from economic time series. Perfectly sharp, or ideal, bandpass filters emerge as a limiting case. Applying the method to quarterly series on U.S. investment and GDP shows a clearly defined cycle.|General Model-Based Filters for Extracting Cycles and Trends in Economic Time Series|http://www.jstor.org/stable/3211576|3211576|2003-05-01|2003|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical analysis']|['Business', 'Business & Economics Collection', 'Economics']
The paper presents a novel evolutionary technique constructed as an alternative of the standard support vector machines architecture. The approach adopts the learning strategy of the latter but aims to simplify and generalize its training, by offering a transparent substitute to the initial black-box. Contrary to the canonical technique, the evolutionary approach can at all times explicitly acquire the coefficients of the decision function, without any further constraints. Moreover, in order to converge, the evolutionary method does not require the positive (semi-) definition properties for kernels within nonlinear learning. Several potential structures, enhancements and additions are proposed, tested and confirmed using available benchmarking test problems. Computational results show the validity of the new approach in terms of runtime, prediction accuracy and flexibility.|Support Vector Machine Learning with an Evolutionary Engine|http://www.jstor.org/stable/40206837|40206837|2009-08-01|2009|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Business & Economics', 'Business']
We propose a Bayesian stochastic search approach to selecting restrictions on multivariate regression models where the errors exhibit deterministic or stochastic conditional volatilities. We develop a Markov chain Monte Carlo (MCMC) algorithm that generates posterior restrictions on the regression coefficients and Cholesky decompositions of the covariance matrix of the errors. Numerical simulations with artificially generated data show that the proposed method is effective in selecting the data-generating model restrictions and improving the forecasting performance of the model. Applying the method to daily foreign exchange rate data, we conduct stochastic search on a VAR model with stochastic conditional volatilities.|Selection of Multivariate Stochastic Volatility Models via Bayesian Stochastic Search|http://www.jstor.org/stable/23243801|23243801|2011-07-01|2011|['eng']|['Applied sciences - Engineering']|['Business', 'Business & Economics Collection', 'Economics', 'Science and Mathematics', 'Statistics']
Researchers have recently introduced a finite mixture Bayesian regression model to simultaneously identify consumer market segments (heterogeneity) and determine how such segments differ with respect to active regression coefficients (variable selection). This article introduces three extensions of this model to incorporate managerial restrictions (constraints). The authors demonstrate with synthetic data that the new constrained finite mixture Bayesian regression models can be used to identify and represent several constrained heterogeneous response patterns commonly encountered in practice. In addition, they show that the proposed models are more robust against multicollinearity than traditional methods. Finally, to illustrate the proposed models' usefulness, the authors apply the proposed constrained models in the context of a service quality (SERVPERF) survey of National Insurance Company's customers.|Implementing Managerial Constraints in Model-Based Segmentation: Extensions of Kim, Fong, and DeSarbo (2012) with an Application to Heterogeneous Perceptions of Service Quality|http://www.jstor.org/stable/42002793|42002793|2013-10-01|2013|['eng']|['Applied sciences - Systems science', 'Information science - Informetrics']|['Business', 'Business & Economics Collection', 'Marketing & Advertising']
Many species show fission-fusion group dynamics because it has clear advantages for flexibly exploiting heterogeneous environments. However, the mechanisms by which these dynamics arise are not well known. We used a hierarchical Bayesian model to disentangle the different influences on spider monkey (Ateles geoffroyi) individual fissions and fusions, including the three dimensions of fission-fusion dynamics (subgroup size, dispersion, and composition). Furthermore, we considered the influences of other individuals also leaving or joining a subgroup at the same time. We found that the most important influence on individual fissions and fusions is whether other individuals are also doing the same. Subgroup size and dispersion did not have clear effects on the probability that an individual fissioned or fusioned, while individuals tended to leave subgroups that were biased toward the opposite sex and to join subgroups that were biased toward their own sex. The networks constructed by the interindividual influences during fissions and fusions were cohesive and did not show assortativity by sex or by degree. Individuals had a similar degree in both networks and each was influenced by a different set of individuals, suggesting a high fluidity in the social networks. We suggest that these networks reflect the way in which information about the environment flows as individuals follow one another during fissions and fusions.|Unraveling fission-fusion dynamics: how subgroup properties and dyadic interactions influence individual decisions|http://www.jstor.org/stable/43599483|43599483|2014-08-01|2014|['eng']|['Biological sciences - Biology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
This paper discusses the problem of estimating marginal likelihoods for mixture and Markov switching model. Estimation is based on the method of bridge sampling (Meng and Wong 1996; Statistica Sinica 11, 552–86.) where Markov Chain Monte Carlo (MCMC) draws from the posterior density are combined with an i.i.d. sample from an importance density. The importance density is constructed in an unsupervised manner from the MCMC draws using a mixture of complete data posteriors. Whereas the importance sampling estimator as well as the reciprocal importance sampling estimator are sensitive to the tail behaviour of the importance density, we demonstrate that the bridge sampling estimator is far more robust. Our case studies range from computing marginal likelihoods for a mixture of multivariate normal distributions, testing for the inhomogeneity of a discrete time Poisson process, to testing for the presence of Markov switching and order selection in the MSAR model.|Estimating marginal likelihoods for mixture and Markov switching models using bridge sampling techniques|http://www.jstor.org/stable/23115004|23115004|2004-01-01|2004|['eng']|['Applied sciences - Engineering']|['Business & Economics', 'Business', 'Economics']
Our general subject is model determination methods and their use in the prediction of economic time series. The methods suggested are Bayesian in spirit but they can be justified by classical as well as Bayesian arguments. The main part of the paper is concerned with model determination, forecast evaluation, and the construction of evolving sequences of models that can adapt in dimension and form (including the way in which any nonstationarity in the data is modelled) as new characteristics in the data become evident. The paper continues some recent work on Bayesian asymptotics by the author and Werner Ploberger (1995), develops embedding techniques for vector martingales that justify the role of a class of exponential densities in model selection and forecast evaluation, and implements the modelling ideas in a multivariate regression framework that includes Bayesian vector autoregressions (BVAR's) and reduced rank regressions (RRR's). It is shown how the theory in the paper can be used: (i) to construct optimized BVAR's with data-determined hyperparameters; (ii) to compare models such as BVAR's, optimized BVAR's, and RRR's; (iii) to perform joint order selection of cointegrating rank, lag length, and trend degree in a VAR; and (iv) to discard data that may be irrelevant and thereby reset the initial conditions of a model.|Econometric Model Determination|http://www.jstor.org/stable/2171845|2171845|1996-07-01|1996|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics']|['Business & Economics', 'Mathematics', 'Science & Mathematics', 'Business', 'Economics']
Despite its potential pitfalls, ecological inference is an unavoidable part of some quantitative settings, including US voting rights litigation. In such applications, the analyst will typically encounter two-way tables with more than two rows and columns. Although several ecological inference methods are currently available for 2 x 2 tables, there are fewer options for analysing general R x C tables, and virtually none that model counts as opposed to fractions. We propose a count R x C method that respects the bounds deterministically, that allows for complex relationships between internal cell quantities, that is easily extensible and that results from transparent assumptions. We study the method via simulation, and then apply it to an example that is drawn from the state of Texas relevant to recent redistricting litigation there.|R x C Ecological Inference: Bounds, Correlations, Flexibility and Transparency of Assumptions|http://www.jstor.org/stable/30136741|30136741|2009-01-01|2009|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Queuing theory models systems which provide services to customers whose arrival times and service requirements are random. The main quantities of interest in a queuing system are not usually the parameters governing the queue but observable quantities such as the number of customers in the queue or the time that a customer spends in the system. The Bayesian approach is thus ideally suited to handle the inferential aims for a queuing situation. In this paper the prediction of several quantities in an $M/M/1$ queue in equilibrium is addressed. Special care is devoted to the selection of an appropriate family of prior distributions and a brand-new family is introduced to generalize the natural conjugate distribution: the Gauss hypergeometric family of distributions. The results are illustrated with a real life example.|Prior Assessments for Prediction in Queues|http://www.jstor.org/stable/2348939|2348939|1994-01-01|1994|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
The bayesian (or integrated likelihood) approach to statistical modelling and analysis proceeds by representing all uncertainties in the form of probability distributions. Learning from new data is accomplished by application of Bayes's Theorem, the latter providing a joint probability description of uncertainty for all model unknowns. To pass from this joint probability distribution to a collection of marginal summary inferences for specified interesting individual (or subsets of) unknowns, requires appropriate integration of the joint distribution. In all but simple stylized problems, these (typically high-dimensional) integrations will have to be performed numerically. This need for efficient simultaneous calculation of potentially many numerical integrals poses novel computational problems. Developments over the past decade are reviewed, including adaptive quadrature, adaptive Monte Carlo, and a variant of a Markov chain simulation procedure known as the Gibbs sampler.|Bayesian Computational Methods|http://www.jstor.org/stable/53988|53988|1991-12-15|1991|['eng']|['Mathematics - Applied mathematics']|['General Science', 'Mathematics', 'Science and Mathematics']
This paper develops new econometric methods to infer hospital quality in a model with discrete dependent variables and nonrandom selection. Mortality rates in patient discharge records are widely used to infer hospital quality. However, hospital admission is not random and some hospitals may attract patients with greater unobserved severity of illness than others. In this situation the assumption of random admission leads to spurious inference about hospital quality. This study controls for hospital selection using a model in which distance between the patient's residence and alternative hospitals are key exogenous variables. Bayesian inference in this model is feasible using a Markov chain Monte Carlo posterior simulator, and attaches posterior probabilities to quality comparisons between individual hospitals and groups of hospitals. The study uses data on 74,848 Medicare patients admitted to 114 hospitals in Los Angeles County from 1989 through 1992 with a diagnosis of pneumonia. It finds the smallest and largest hospitals to be of the highest quality. There is strong evidence of dependence between the unobserved severity of illness and the assignment of patients to hospitals, whereby patients with a high unobserved severity of illness are disproportionately admitted to high quality hospitals. Consequently a conventional probit model leads to inferences about quality that are markedly different from those in this study's selection model.|Bayesian Inference for Hospital Quality in a Selection Model|http://www.jstor.org/stable/1555495|1555495|2003-07-01|2003|['eng']|['Applied sciences - Engineering']|['Business & Economics', 'Mathematics', 'Science & Mathematics', 'Business', 'Economics']
The distribution of genetic variation among populations is conveniently measured by Wright's FST, which is a scaled variance taking on values in [0,1]. For certain types of genetic markers and for single-nucleotide polymorphisms (SNPs) in particular, it is reasonable to presume that allelic differences at most loci are selectively neutral. For such loci, the distribution of genetic variation among populations is determined by the size of local populations, the pattern and rate of migration among those populations, and the rate of mutation. Because the demographic parameters (population sizes and migration rates) are common across all autosomal loci, locus-specific estimates of FST will depart from a common distribution only for loci with unusually high or low rates of mutation or for loci that are closely associated with genomic regions having a relationship with fitness. Thus, loci that are statistical outliers showing significantly more among-population differentiation than others may mark genomic regions subject to diversifying selection among the sample populations. Similarly, statistical outliers showing significantly less differentiation among populations than others may mark genomic regions subject to stabilizing selection across the sample populations. We propose several Bayesian hierarchical models to estimate locus-specific effects on FST, and we apply these models to single nucleotide polymorphism data from the HapMap project. Because loci that are physically associated with one another are likely to show similar patterns of variation, we introduce conditional autoregressive models to incorporate the local correlation among loci for high-resolution genomic data. We estimate the posterior distributions of model parameters using Markov chain Monte Carlo (MCMC) simulations. Model comparison using several criteria, including deviance information criterion (DIC) and pseudomarginal likelihood (LPML), reveals that a model with locus-and population-specific effects is superior to other models for the data used in the analysis. To detect statistical outliers we propose an approach that measures divergence between the posterior distributions of locus-specific effects and the common FST with the Kullback-Leibler divergence measure. We calibrate this measure by comparing values with those produced from the divergence between a biased and a fair coin. We conduct a simulation study to illustrate the performance of our approach for detecting loci subject to stabilizing/divergent selection, and we apply the proposed models to low-and high-resolution SNP data from the HapMap project. Model comparison using DIC and LPML reveals that conditional autoregressive (CAR) models are superior to alternative models for the high-resolution data. For both low-and high-resolution data, we identify statistical outliers that are associated with known genes.|A Bayesian Hierarchical Model for Analysis of Single-Nucleotide Polymorphisms Diversity in Multilocus, Multipopulation Samples|http://www.jstor.org/stable/40591906|40591906|2009-03-01|2009|['eng']|['Applied sciences - Engineering', 'Biological sciences - Biology']|['Science & Mathematics', 'Statistics']
"We construct an empirical Bayes (EB) prediction interval for the finite population mean of a small area when data are available from many similar small areas. We assume that the individuals of the population of the ith area are a random sample from a normal distribution with mean μi and variance $\sigma ^{2}_{i}$. Then, given $\sigma ^{2}_{i}$, the μi are independently distributed with each μi having a normal distribution with mean θ and variance $\sigma ^{2}_{i} \tau$, and the $\sigma ^{2}_{i}$ are a random sample from an inverse gamma distribution with index η and scale (η − 1)δ. First, assuming θ,τ,δ and η are fixed and known, we obtain the highest posterior density (HDP) interval for the finite population mean of the lth area. Second, we obtain the EB interval by ""substituting"" point estimators for the fixed and unknown parameters θ,τ,δ and η into the HPD interval, and a two-stage procedure is used to partially account for underestimation of variability. Asymptotic properties (as ℓ → ∞) of the EB interval are obtained by comparing its center, width and coverage probability with those of HPD interval. Finally, by using a small-scale numerical study, we assess the asymptotic properties of the proposed EB interval, and we show that the EB interval is a good approximation to the HPD interval for moderate values of ℓ."|AN EMPIRICAL BAYES PREDICTION INTERVAL FOR THE FINITE POPULATION MEAN OF A SMALL AREA|http://www.jstor.org/stable/24306587|24306587|1999-04-01|1999|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Mathematics', 'Science and Mathematics', 'Statistics']
Valuations of entrepreneurial companies are only observed occasionally, albeit more frequently for well-performing companies. Consequently, estimators of risk and return must correct for sample selection to obtain consistent estimates. We develop a general model of dynamic sample selection and estimate it using data from venture capital investments in entrepreneurial companies. Our selection correction leads to markedly lower intercepts and higher estimates of risks compared to previous studies. The methodology is generally applicable to estimating risk and return in illiquid markets with endogenous trading.|Risk and Return Characteristics of Venture Capital-Backed Entrepreneurial Companies|http://www.jstor.org/stable/40865574|40865574|2010-10-01|2010|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Business & Economics', 'Business', 'Finance']
Many statistical problems involve data from thousands of parallel cases. Each case has some associated effect size, and most cases will have no effect. It is often important to estimate the effect size and the local or tail-area false discovery rate for each case. Most current methods do this separately, and most are designed for normal data. This paper uses an empirical Bayes mixture model approach to estimate both quantities together for exponential family data. The proposed method yields simple, interpretable models that can still be used nonparametrically. It can also estimate an empirical null and incorporate it fully into the model. The method outperforms existing effect size and false discovery rate estimation procedures in normal data simulations; it nearly acheives the Bayes error for effect size estimation. The method is implemented in an R package (mixfdr), freely available from CRAN.|AN EMPIRICAL BAYES MIXTURE METHOD FOR EFFECT SIZE AND FALSE DISCOVERY RATE ESTIMATION|http://www.jstor.org/stable/27801593|27801593|2010-03-01|2010|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
Linear mixed effects models are highly flexible in handling a broad range of data types and are therefore widely used in applications. A key part in the analysis of data is model selection, which often aims to choose a parsimonious model with other desirable properties from a possibly very large set of candidate statistical models. Over the last 5-10 years the literature on model selection in linear mixed models has grown extremely rapidly. The problem is much more complicated than in linear regression because selection on the covariance structure is not straightforward due to computational issues and boundary problems arising from positive semidefinite constraints on covariance matrices. To obtain a better understanding of the available methods, their properties and the relationships between them, we review a large body of literature on linear mixed model selection. We arrange, implement, discuss and compare model selection methods based on four major approaches: information criteria such as AIC or BIC, shrinkage methods based on penalized loss functions such as LASSO, the Fence procedure and Bayesian techniques.|Model Selection in Linear Mixed Models|http://www.jstor.org/stable/43288485|43288485|2013-05-01|2013|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
Dynamic factor models are becoming increasingly popular in empirical macroeconomics due to their ability to cope with large datasets. Dynamic stochastic general equilibrium (DSGE) models, on the other hand, are suitable for the analysis of policy interventions from a methodical point of view. In this article, we provide a Bayesian method to combine the statistically rich specification of the former with the conceptual advantages of the latter by using information from a DSGE model to form a prior belief about parameters in the dynamic factor model. Because the method establishes a connection between observed data and economic theory and at the same time incorporates information from a large dataset, our setting is useful to study the effects of policy interventions on a large number of observed variables. An application of the method to U.S. data shows that a moderate weight of the DSGE prior is optimal and that the model performs well in terms of forecasting. We then analyze the impact of monetary shocks on both the factors and selected series using a DSGE-based identification of these shocks. Supplementary materials for this article are available online.|Structural Dynamic Factor Analysis Using Prior Information From Macroeconomic Theory|http://www.jstor.org/stable/43701601|43701601|2013-04-01|2013|['eng']|['Physical sciences - Astronomy']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
The two-stage random-effects model (Harville (1977), Laird and Ware (1982)) offers a powerful and flexible tool for the analysis of longitudinal data. This method assumes individual response may be modeled as the sum of an overall population effect, a random individual deviation, and random error. When individuals are nested within families or companies, this source of variability should also be considered. Building a model for hearing loss for minimally noise-exposed workers with data compiled from multiple sources motivated extending the two-stage model to include a nested random effect. Computational methods to compute population effects, random effects, and variance components in this more general setting using the EM algorithm are given.|EM FOR LONGITUDINAL DATA FROM MULTIPLE STUDIES|http://www.jstor.org/stable/24305555|24305555|1995-01-01|1995|['eng']|['Mathematics - Applied mathematics']|['Mathematics', 'Science and Mathematics', 'Statistics']
Most phase II screening designs available in the literature consider one treatment at a time. Each study is considered in isolation. We propose a more systematic decision-making approach to the phase II screening process. The sequential design allows for more efficiency and greater learning about treatments. The approach incorporates a Bayesian hierarchical model that allows combining information across several related studies in a formal way and improves estimation in small data sets by borrowing strength from other treatments. The design incorporates a utility function that includes sampling costs and possible future payoff. Computer simulations show that this method has high probability of discarding treatments with low success rates and moving treatments with high success rates to phase III trial. /// Dans la littérature, la plupart des plans de screening en phase 2 considèrent un traitement à la fois. Chaque étude est considérée isolément. Nous proposons une approche décisionnelle plus systématique pour le processus de screening de phase 2. Le plan séquentiel permet une plus grande efficacité et un meilleur apprentissage sur les traitements. L'approche incorpore un modèle Bayesien hiérarchique qui permet d'une façon formelle la combinaison d'information entre différentes études reliées entre elles, et améliore l'estimation dans de petits échantillons en exploitant l'information venant des autres traitements. Le plan incorpore une fonction d'utilité qui prend en compte le coût d'échantillonnage et le possible retour sur investissement. Des simulations par ordinateur montrent que cette méthode a une forte probabilité d'écarter des traitements à faible taux de succès et de faire évoluer en phase 3 des traitements à fort taux de succès.|Bayesian Optimal Design for Phase II Screening Trials|http://www.jstor.org/stable/25502147|25502147|2008-09-01|2008|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
The Bayesian vector autoregression (BVAR) employment-forecasting approach is generalized using data for the state of Georgia. This study advances previous regional BVAR approaches by (a) incorporating regional input-output coefficients instead of national coefficients, (b) using the coefficients both to specify the prior means in one model and to weight the variances of a Minnesota-type prior in a second model, and (c) including final-demand effects and links to national and world economies. Out-of-sample forecasts produced by the generalized BVAR models are compared to forecasts produced from an autoregressive model, an unconstrained VAR model, and a Minnesota BVAR model.|Generalizing the Bayesian Vector Autoregression Approach for Regional Interindustry Employment Forecasting|http://www.jstor.org/stable/1392016|1392016|1998-01-01|1998|['eng']|['Applied sciences - Engineering']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
Bayesian clinical trial designs offer the possibility of a substantially reduced sample size, increased statistical power, and reductions in cost and ethical hazard. However when prior and current information conflict, Bayesian methods can lead to higher than expected type I error, as well as the possibility of a costlier and lengthier trial. This motivates an investigation of the feasibility of hierarchical Bayesian methods for incorporating historical data that are adaptively robust to prior information that reveals itself to be inconsistent with the accumulating experimental data. In this article, we present several models that allow for the commensurability of the information in the historical and current data to determine how much historical information is used. A primary tool is elaborating the traditional power prior approach based upon a measure of commensurability for Gaussian data. We compare the frequentist performance of several methods using simulations, and close with an example of a colon cancer trial that illustrates a linear models extension of our adaptive borrowing approach. Our proposed methods produce more precise estimates of the model parameters, in particular, conferring statistical significance to the observed reduction in tumor size for the experimental regimen as compared to the control regimen.|Hierarchical Commensurate and Power Prior Models for Adaptive Incorporation of Historical Information in Clinical Trials|http://www.jstor.org/stable/41242553|41242553|2011-09-01|2011|['eng']|['Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
Dirichlet process (DP) priors are a popular choice for semiparametric Bayesian random effect models. The fact that the DP prior implies a non-zero mean for the random effect distribution creates an identifiability problem that complicates the interpretation of, and inference for, the fixed effects that are paired with the random effects. Similarly, the interpretation of, and inference for, the variance components of the random effects also becomes a challenge. We propose an adjustment of conventional inference using a post-processing technique based on an analytic evaluation of the moments of the random moments of the DP. The adjustment for the moments of the DP can be conveniently incorporated into Markov chain Monte Carlo simulations at essentially no additional computational cost. We conduct simulation studies to evaluate the performance of the proposed inference procedure in both a linear mixed model and a logistic linear mixed effect model. We illustrate the method by applying it to a prostate specific antigen dataset. We provide an R function that allows one to implement the proposed adjustment in a post-processing step of posterior simulation output, without any change to the posterior simulation itself.|CENTER-ADJUSTED INFERENCE FOR A NONPARAMETRIC BAYESIAN RANDOM EFFECT DISTRIBUTION|http://www.jstor.org/stable/24309560|24309560|2011-07-01|2011|['eng']|['Mathematics - Applied mathematics']|['Mathematics', 'Science and Mathematics', 'Statistics']
Methods for handling missing data depend strongly on the mechanism that generated the missing values, such as missing completely at random (MCAR) or missing at random (MAR), as well as other distributional and modeling assumptions at various stages. It is well known that the resulting estimates and tests may be sensitive to these assumptions as well as to outlying observations. In this paper, we introduce various perturbations to modeling assumptions and individual observations, and then develop a formal sensitivity analysis to assess these perturbations in the Bayesian analysis of statistical models with missing data. We develop a geometric framework, called the Bayesian perturbation manifold, to characterize the intrinsic structure of these perturbations. We propose several intrinsic influence measures to perform sensitivity analysis and quantify the effect of various perturbations to statistical models. We use the proposed sensitivity analysis procedure to systematically investigate the tenability of the non-ignorable missing at random (MNAR) assumption. Simulation studies are conducted to evaluate our methods, and a dataset is analyzed to illustrate the use of our diagnostic measures.|BAYESIAN SENSITIVITY ANALYSIS OF STATISTICAL MODELS WITH MISSING DATA|http://www.jstor.org/stable/24310881|24310881|2014-04-01|2014|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Applied mathematics']|['Mathematics', 'Science & Mathematics', 'Statistics']
In a case-cohort design a random sample from the study cohort, referred as a subcohort, and all the cases outside the subcohort are selected for collecting extra covariate data. The union of the selected subcohort and all cases are referred as the case-cohort set. Such a design is generally employed when the collection of information on an extra covariate for the study cohort is expensive. An advantage of the case-cohort design over more traditional case-control and the nested case-control designs is that it provides a set of controls which can be used for multiple end-points, in which case there is information on some covariates and event follow-up for the whole study cohort. Here, we propose a Bayesian approach to analyse such a case-cohort design as a cohort design with incomplete data on the extra covariate. We construct likelihood expressions when multiple end-points are of interest simultaneously and propose a Bayesian data augmentation method to estimate the model parameters. A simulation study is carried out to illustrate the method and the results are compared with the complete cohort analysis.|Bayesian Inference from Case-Cohort Data with Multiple End-Points|http://www.jstor.org/stable/4616906|4616906|2006-03-01|2006|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Observed spatial patterns in natural systems may result from processes acting across multiple spatial and temporal scales. Although spatially explicit data on processes that generate ecological patterns, such as the distribution of disease over a landscape, are frequently unavailable, information about the scales over which processes operate can be used to understand the link between pattern and process. Our goal was to identify scales of mule deer (Odocoileus hemionus) movement and mixing that exerted the greatest influence on the spatial pattern of chronic wasting disease (CWD) in northcentral Colorado, USA. We hypothesized that three scales of mixing (individual, winter subpopulation, or summer subpopulation) might control spatial variation in disease prevalence. We developed a fully Bayesian hierarchical model to compare the strength of evidence for each mixing scale. We found strong evidence that the finest mixing scale corresponded best to the spatial distribution of CWD infection. There was also evidence that land ownership and habitat use play a role in exacerbating the disease, along with the known effects of sex and age. Our analysis demonstrates how information on the scales of spatial processes that generate observed patterns can be used to gain insight when process data are sparse or unavailable.|Linking Chronic Wasting Disease to Mule Deer Movement Scales: A Hierarchical Bayesian Approach|http://www.jstor.org/stable/40061719|40061719|2006-06-01|2006|['eng']|['Biological sciences - Biology', 'Applied sciences - Engineering']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
Empirical researchers studying party systems often struggle with the question of how to count parties. Indexes of party system fragmentation used to address this problem (e.g., the effective number of parties) have a fundamental shortcoming: since the same index value may represent very different party systems, they are impossible to interpret and may lead to erroneous inference. We offer a novel approach to this problem: instead of focusing on index measures, we develop a model that predicts the entire distribution of party vote-shares and, thus, does not require any index measure. First, a model of party counts predicts the number of parties. Second, a set of multivariate t models predicts party vote-shares. Compared to the standard index-based approach, our approach helps to avoid inferential errors and, in addition, yields a much richer set of insights into the variation of party systems. For illustration, we apply the model on two data sets. Our analyses call into question the conclusions one would arrive at by the index-based approach. Software is provided to implement the proposed model.|A Statistical Model for Party-Systems Analysis|http://www.jstor.org/stable/23260174|23260174|2012-04-01|2012|['eng']|['Mathematics - Applied mathematics']|['Political Science', 'Social Sciences']
This article reviews Markov chain methods for sampling from the posterior distribution of a Dirichlet process mixture model and presents two new classes of methods. One new approach is to make Metropolis-Hastings updates of the indicators specifying which mixture component is associated with each observation, perhaps supplemented with a partial form of Gibbs sampling. The other new approach extends Gibbs sampling for these indicators by using a set of auxiliary parameters. These methods are simple to implement and are more efficient than previous ways of handling general Dirichlet process mixture models with non-conjugate priors.|Markov Chain Sampling Methods for Dirichlet Process Mixture Models|http://www.jstor.org/stable/1390653|1390653|2000-06-01|2000|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Science & Mathematics', 'Computer Science', 'Statistics']
The law school validity studies are primarily concerned with the prediction of first-year average in law school from Law School Aptitude Test score and undergraduate grade point average. Traditionally, a separate admitting equation is estimated in each law school by the method of least squares based on data from students who attended the law school in recent years. These least squares equations can fluctuate rather widely from year to year. This study employs empirical Bayes techniques to obtain admitting equations that are better than the least squares admitting equations in two ways: for each law school, the empirical Bayes admitting equations are more stable in time than the least squares admitting equations; and the empirical Bayes admitting equations predict student performance more accurately than the least squares admitting equations.|Using Empirical Bayes Techniques in the Law School Validity Studies|http://www.jstor.org/stable/2287162|2287162|1980-12-01|1980|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
Standard practice when analyzing data from different types of experiments is to treat data from each type separately. By borrowing strength across multiple sources, an integrated analysis can produce better results. Careful adjustments must be made to incorporate the systematic differences among various experiments. Toward this end, some Bayesian hierarchical Gaussian process models are proposed. The heterogeneity among different sources is accounted for by performing flexible location and scale adjustments. The approach tends to produce prediction closer to that from the high-accuracy experiment. The Bayesian computations are aided by the use of Markov chain Monte Carlo and sample average approximation algorithms. The proposed method is illustrated with two examples, one with detailed and approximate finite elements simulations for mechanical material design and the other with physical and computer experiments for modeling a food processor.|Bayesian Hierarchical Modeling for Integrating Low-Accuracy and High-Accuracy Experiments|http://www.jstor.org/stable/25471459|25471459|2008-05-01|2008|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
This article examines the intergenerational transmission of income risk. Do risky parents have risky kids? Income volatility—a proxy for income risk—is not observed directly; instead, it must be estimated with substantial error from the time series variability of income. I characterize an income process with individual-specific volatility parameters and estimate the joint distribution of volatility parameters for fathers and for their adult sons. In data from the Panel Study of Income Dynamics, fathers with higher income volatility have sons with higher income volatility. This finding is correlated with, but far from fully explained by, the intergenerational transmission of risk tolerance and of the propensity for self-employment.|The Intergenerational Transmission of Income Volatility: Is Riskiness Inherited?|http://www.jstor.org/stable/23243803|23243803|2011-07-01|2011|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical objects']|['Business', 'Business & Economics Collection', 'Economics', 'Science and Mathematics', 'Statistics']
In this paper the currency-substitution model is tested under the German hyperinflation using several expectations-formation mechanisms. The maximum-likelihood estimates of the currency-substitution model reveal that extrapolative and adaptive expectations seem to have been predominant and that there was a significant degree of currency substitution. The results also reveal that expectation was destabilizing and that it was not possible to distinguish between the effects of the expected change in the exchange rate and the expected inflation.|Testing the Currency-Substitution Model under the German Hyperinflation|http://www.jstor.org/stable/41794733|41794733|1999-01-01|1999|['eng']|['Philosophy - Logic', 'Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Business & Economics Collection', 'Economics']
In this paper we consider reserving and pricing methodologies for a pensions-type contract with a simple form of guaranteed annuity option. We consider only unit-linked contracts, but our methodologies and, to some extent, our numerical results would apply also to with-profits contracts. The Report of the Annuity Guarantees Working Party (Bolton et al, 1997), presented the results of a very interesting survey, as at the end of 1996, of life assurance companies offering guaranteed annuity options. There was no consensus at that time among the companies on how to reserve for such options. The Report discussed several approaches to reserving, but concluded that it was unable to recommend a single approach. This paper is an attempt to fill that gap. We investigate two approaches to reserving and pricing. In the first sections of the paper we consider quantile, and conditional tail expectation, reserves. The methodology we adopt here is very close to that proposed by the Maturity Guarantees Working Party in its Report to the profession (Ford et al, 1980). We show how these policies could have been reserved for in 1985, and what would have been the outcome of using the proposed method. In a later section we consider the feasibility of using option pricing methodology to dynamically hedge a guaranteed annuity option. It is shown that this is possible within the context of the model we propose, but we submit that, in practical terms, dynamic hedging is not a complete solution to the problem since suitable tradeable assets do not in practice exist. Finally, we describe several enhancements to our models and methodology, which would make them even more realistic, though generally they would have the effect of increasing the required contingency reserves|RESERVING, PRICING AND HEDGING FOR POLICIES WITH GUARANTEED ANNUITY OPTIONS|http://www.jstor.org/stable/41141596|41141596|2003-01-01|2003|['eng']|['Economics - Economic disciplines']|['Business', 'Business & Economics Collection', 'Economics', 'Finance']
1. Geographic gradients in population dynamics may occur because of spatial variation in resources that affect the deterministic components of the dynamics (i.e. carrying capacity, the specific growth rate at small densities or the strength of density regulation) or because of spatial variation in the effects of environmental stochasticity. To evaluate these, we used a hierarchical Bayesian approach to estimate parameters characterizing deterministic components and stochastic influences on population dynamics of eight species of ducks (mallard, northern pintail, blue-winged teal, gadwall, northern shoveler, American wigeon, canvasback and redhead (Anas platyrhynchos, A. acuta, A. discors, A. strepera, A. clypeata, A. americana, Aythya valisineria and Ay. americana, respectively) breeding in the North American prairies, and then tested whether these parameters varied latitudinally. 2. We also examined the influence of temporal variation in the availability of wetlands, spring temperature and winter precipitation on population dynamics to determine whether geographical gradients in population dynamics were related to large-scale variation in environmental effects. Population variability, as measured by the variance of the population fluctuations around the carrying capacity K, decreased with latitude for all species except canvasback. This decrease in population variability was caused by a combination of latitudinal gradients in the strength of density dependence, carrying capacity and process variance, for which details varied by species. 3. The effects of environmental covariates on population dynamics also varied latitudinally, particularly for mallard, northern pintail and northern shoveler. However, the proportion of the process variance explained by environmental covariates, with the exception of mallard, tended to be small. 4. Thus, geographical gradients in population dynamics of prairie ducks resulted from latitudinal gradients in both deterministic and stochastic components, and were likely influenced by spatial differences in the distribution of wetland types and shapes, agricultural practices and dispersal processes. 5. These results suggest that future management of these species could be improved by implementing harvest models that account explicity for spatial variation in density effects and environmental stochasticity on population abundance.|Geographical Gradients in the Population Dynamics of North American Prairie Ducks|http://www.jstor.org/stable/20143263|20143263|2008-09-01|2008|['eng']|['Biological sciences - Ecology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
We introduce a novel stochastic process that we term the multivariate beta process. The process is defined for modelling-dependent random probabilities and has beta marginal distributions. We use this process to define a probability model for a family of unknown distributions indexed by covariates. The marginal model for each distribution is a Polya tree prior. An important feature of the proposed prior is the easy centring of the nonparametric model around any parametric regression model. We use the model to implement nonparametric inference for survival distributions. The nonparametric model that we introduce can be adopted to extend the support of prior distributions for parametric regression models.|The multivariate beta process and an extension of the Polya tree model|http://www.jstor.org/stable/29777162|29777162|2011-03-01|2011|['eng']|['Mathematics - Mathematical logic']|['Science and Mathematics', 'Statistics']
Brad Efron's paper has inspired a return to the ideas behind Bayes, frequency and empirical Bayes. The latter preferably would not be limited to exchangeable models for the data and hyperparameters. Parallels are revealed between microarray analyses and profiling of hospitals, with advances suggesting more decision modeling for gene identification also. Then good multilevel and empirical Bayes models for random effects should be sought when regression toward the mean is anticipated.|Comment: Microarrays, Empirical Bayes and the Two-Groups Model|http://www.jstor.org/stable/27645874|27645874|2008-02-01|2008|['eng']|['Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
This expository paper intends to give educational researchers suitable access to a method for investigating the stability of their dichotomous decisions under changes of assumptions. Inferences and decisions based on statistical models typically involve model assumptions, such as normality and independence of observations. In statistical decision theory, one also has to specify loss functions, and in a Bayesian analysis a prior distribution has to be specified. In the case of dichotomous decisions (passing or failing a student, choosing between two teaching methods, rejecting or retaining a hypothesis), the total set of all such assumptions/specifications for which the decision would have been the same is the robustness region (section 2). Inspection of this (data-dependent) region is a form of sensitivity analysis which may lead to improved decision making. Section 1 discusses earlier forms of sensitivity or robustness analysis, both data-dependent and a priori. Examples of robustness regions deal with mastery decisions (sections 3 and 4), evaluation of a teaching experiment (section 5), and aptitude treatment interaction (section 6).|Robustness Regions for Dichotomous Decisions|http://www.jstor.org/stable/1164874|1164874|1981-10-01|1981|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Education', 'Statistics', 'Social Sciences']
Storm surge, the onshore rush of sea water caused by the high winds and low pressure associated with a hurricane, can compound the effects of inland flooding caused by rainfall, leading to loss of property and loss of life for residents of coastal areas. Numerical ocean models are essential for creating storm surge forecasts for coastal areas. These models are driven primarily by the surface wind forcings. Currently, the gridded wind fields used by ocean models are specified by deterministic formulas that are based on the central pressure and location of the storm center. While these equations incorporate important physical knowledge about the structure of hurricane surface wind fields, they cannot always capture the asymmetric and dynamic nature of a hurricane. A new Bayesian multivariate spatial statistical modeling framework is introduced combining data with physical knowledge about the wind fields to improve the estimation of the wind vectors. Many spatial models assume the data follow a Gaussian distribution. However, this may be overly-restrictive for wind fields data which often display erratic behavior, such as sudden changes in time or space. In this paper we develop a semiparametric multivariate spatial model for these data. Our model builds on the stick-breaking prior, which is frequently used in Bayesian modeling to capture uncertainty in the parametric form of an outcome. The stick-breaking prior is extended to the spatial setting by assigning each location a different, unknown distribution, and smoothing the distributions in space with a series of kernel functions. This semiparametric spatial model is shown to improve prediction compared to usual Bayesian Kriging methods for the wind field of Hurricane Ivan.|"A Multivariate Semiparametric Bayesian Spatial Modeling Framework for
              Hurricane Surface Wind Fields"|http://www.jstor.org/stable/4537431|4537431|2007-06-01|2007|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Applied mathematics']|['Mathematics', 'Science & Mathematics', 'Statistics']
Inference for parameters associated with small geographical areas or domains of study requires considerable care because the subpopulation sample sizes are usually very small. Since sample survey data are usually clustered, hierarchical models are often appropriate. However, the customary hierarchical models may specify more exchangeability than is warranted. Thus, we propose an alternative model that is more flexible. We consider the case of a set of multiple linear regressions, one for each subpopulation. The objective is to make inference about one or more regression coefficients, β̲i. We derive the posterior mean and variance of β̲i, and obtain simplified versions of these moments by using reference-type prior distributions. We use a set of numerical examples to contrast our method with the more conventional hierarchical analysis, and to exhibit the large gains in precision that are possible.|METHODOLOGY FOR POOLING SUBPOPULATION REGRESSIONS WHEN SAMPLE SIZES ARE SMALL AND THERE IS UNCERTAINTY ABOUT WHICH SUBPOPULATION ARE SIMILAR|http://www.jstor.org/stable/24306588|24306588|1999-04-01|1999|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Mathematics', 'Science and Mathematics', 'Statistics']
The recent RNA-seq technology is an attractive method to study gene expression. One of the most important goals in RNA-seq data analysis is to detect genes differentially expressed across treatments. Although several statistical methods have been published, there are no theoretical justifications for whether these methods are optimal or how to search for the optimal test. Furthermore, most proposed tests are designed for testing whether the mean expression levels are exactly the same or not across treatments, whereas sometimes, biologists are interested in detecting genes with expression changes larger than a certain threshold. Another issue with current methods is that the false discovery rate (FDR) control is not well studied. In this manuscript, we propose a test to address all the above issues. Under model assumptions, we derive an optimal test that achieves the maximum of average power among those that control FDR at the same level. We also provide an approximated version, the approximated most average powerful (AMAP) test, for practical implementation. The proposed method allows for testing null hypotheses that are much more general than the ones most previous studies have considered, and it leads to a natural way of controlling the FDR. Through simulation studies, we show that our test has a higher power than other methods, including the widely-used edgeR, DESeq, and baySeq methods, as well as better FDR control than two other FDR control procedures commonly used in practice. For demonstration, we also apply the proposed method to a real RNA-seq dataset obtained from maize.|An Optimal Test with Maximum Average Power While Controlling FDR with Application to RNA-Seq Data|http://www.jstor.org/stable/24538125|24538125|2013-09-01|2013|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
This article shows how the pooled-variance two-sample t statistic arises from a Bayesian formulation of the two-sided point null testing problem, with emphasis on teaching. We identify a reasonable and useful prior giving a closed-form Bayes factor that can be written in terms of the distribution of the two-sample t statistic under the null and alternative hypotheses, respectively. This provides a Bayesian motivation for the two-sample t statistic, which has heretofore been buried as a special case of more complex linear models, or given only roughly via analytic or Monte Carlo approximations. The resulting formulation of the Bayesian test is easy to apply in practice, and also easy to teach in an introductory course that emphasizes Bayesian methods. The priors are easy to use and simple to elicit, and the posterior probabilities are easily computed using available software, in some cases using spreadsheets.|The Bayesian Two-Sample t Test|http://www.jstor.org/stable/27643674|27643674|2005-08-01|2005|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
With the expansion of Medicare, increasing attention has been paid to the behavior of elderly persons in choosing health insurance. This article investigates how the elderly use plan attributes to screen their Medicare health plans to simplify a complicated choice situation. The proposed model extends the conventional random utility models by considering a screening stage. Bayesian estimation is implemented, and the results based on Medicare data show that the elderly are likely to screen according to premium, prescription drug coverage, and vision coverage. These attributes have nonlinear effects on plan choice that cannot be captured by conventional models. This article has supplementary material online.|Medicare Health Plan Choices of the Elderly: A Choice-With-Screening Model|http://www.jstor.org/stable/41810030|41810030|2012-01-01|2012|['eng']|['Health sciences - Health and wellness', 'Mathematics - Applied mathematics']|['Business', 'Business & Economics Collection', 'Economics', 'Science and Mathematics', 'Statistics']
For Hypothesis Testing and Model Selection, the Bayesian approach is attracting considerable attention. The reasons for this attention include: (i) it yields posterior probabilities of the models (and not simply accept-reject rules); (ii) it is a predictive approach; and (iii) it automatically incorporates the principle of scientific parsimony. Until recently, obtaining such benefits through the Bayesian approach required elicitation of proper subjective prior distributions, or the use of approximations (such as BIC) of questionable generality. In Berger and Pericchi (1996), the Intrinsic Bayes Factor Strategy was introduced, and shown to be an automatic default method corresponding to an actual (and sensible) Bayesian analysis. In particular, it was shown that the Intrinsic Bayes Factor yields an answer which is asymptotically equivalent to the use of a specific (and reasonable) proper prior distribution, called the Intrinsic Prior. Indeed, the IBF method can also be thought of as a method for constructing default proper priors appropriate for model comparisons. In this paper we study an implementation of the IBF strategy called the Median IBF. This seems to be a simple and very generally applicable IBF, which works well for nested or non-nested models, and even for small or moderate sample sizes; some of these situations can cause difficulties for other versions of IBFs.|Accurate and Stable Bayesian Model Selection: The Median Intrinsic Bayes Factor|http://www.jstor.org/stable/25053019|25053019|1998-04-01|1998|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science and Mathematics', 'Statistics']
Analogical reasoning depends fundamentally on the ability to learn and generalize about relations between objects. We develop an approach to relational learning which, given a set of pairs of objects S = {A⁽¹⁾: B⁽¹⁾, A⁽¹⁾: B⁽²⁾,...,A (N) : B (N) }, measures how well other pairs A: B fit in with the set S. Our work addresses the following question: is the relation between objects A and B analogous to those relations found in S? Such questions are particularly relevant in information retrieval, where an investigator might want to search for analogous pairs of objects that match the query set of interest. There are many ways in which objects can be related, making the task of measuring analogies very challenging. Our approach combines a similarity measure on function spaces with Bayesian analysis to produce a ranking. It requires data containing features of the objects of interest and a link matrix specifying which relationships exist; no further attributes of such relationships are necessary. We illustrate the potential of our method on text analysis and information networks. An application on discovering functional interactions between pairs of proteins is discussed in detail, where we show that our approach can work in practice even if a small set of protein pairs is provided.|RANKING RELATIONS USING ANALOGIES IN BIOLOGICAL AND INFORMATION NETWORKS|http://www.jstor.org/stable/29765523|29765523|2010-06-01|2010|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
"The completion of the Panamanian Isthmus is one of the greatest natural experiments in evolution, sending multiple species pairs from a broad range of taxonomic groups on independent evolutionary trajectories. The resulting transisthmian sister species have been used as model systems for examining consequences that accompany cessation of gene flow in formerly panmictic populations. However, variance in pairwise genetic distances of these ""geminates"" often exceeds expectations, seemingly conflicting with the assumption that separation of populations was contemporaneous with the final closure of the Isthmus. Multilocus datasets and coalescent-based analytical methods can be used to estimate divergence times while accounting for variance in gene divergence that predates isolation, thus removing the need to invoke unequal divergence times. Here we present results from Bayesian analyses of sequence data from seven nuclear and one mitochondrial marker in eight transisthmian species pairs in the snapping shrimp genus Alpheus. Divergence times in two species pairs were shown to occur much earlier than the Isthmus final closure, but much of the variance in pairwise genetic distances from cytochrome oxidase I (COI) was explained when ancestral polymorphisms were accounted for. Results illustrate how coalescent approaches may be more appropriate for dating recent divergences than for estimating ancient speciation events."|A Multilocus Test of Simultaneous Divergence across the Isthmus of Panama Using Snapping Shrimp in the Genus Alpheus|http://www.jstor.org/stable/25483601|25483601|2009-02-01|2009|['eng']|['Biological sciences - Biology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
Unbiased, label-free proteomics is becoming a powerful technique for measuring protein expression in almost any biological sample. The output of these measurements after preprocessing is a collection of features and their associated intensities for each sample. Subsets of features within the data are from the same peptide, subsets of peptides are from the same protein, and subsets of proteins are in the same biological pathways, therefore, there is the potential for very complex and informative correlational structure inherent in these data. Recent attempts to utilize this data often focus on the identification of single features that are associated with a particular phenotype that is relevant to the experiment. However, to date, there have been no published approaches that directly model what we know to be multiple different levels of correlation structure. Here we present a hierarchical Bayesian model which is specifically designed to model such correlation structure in unbiased, label-free proteomics. This model utilizes partial identification information from peptide sequencing and database lookup as well as the observed correlation in the data to appropriately compress features into latent proteins and to estimate their correlation structure. We demonstrate the effectiveness of the model using artificial/benchmark data and in the context of a series of proteomics measurements of blood plasma from a collection of volunteers who were infected with two different strains of viral influenza.|LATENT PROTEIN TREES|http://www.jstor.org/stable/23566409|23566409|2013-06-01|2013|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science and Mathematics', 'Statistics']
International migration data in Europe are collected by individual countries with separate collection systems and designs. As a result, reported data are inconsistent in availability, definition, and quality. In this article, we propose a Bayesian model to overcome the limitations of the various data sources. The focus is on estimating recent international migration flows among 31 countries in the European Union and European Free Trade Association from 2002 to 2008, using data collated by Eurostat. We also incorporate covariate information and information provided by experts on the effects of undercount, measurement, and accuracy of data collection systems. The methodology is integrated and produces a synthetic database with measures of uncertainty for international migration flows and other model parameters. Supplementary materials for this article are available online.|Integrated Modeling of European Migration|http://www.jstor.org/stable/24246865|24246865|2013-09-01|2013|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
Hospital profiling involves a comparison of a health care provider's structure, processes of care, or outcomes to a standard, often in the form of a report card. Given the ubiquity of report cards and similar consumer ratings in contemporary American culture, it is notable that these are a relatively recent phenomenon in health care. Prior to the 1986 release of Medicare hospital outcome data, little such information was publicly available. We review the historical evolution of hospital profiling with special emphasis on outcomes; present a detailed history of cardiac surgery report cards, the paradigm for modern provider profiling; discuss the potential unintended negative consequences of public report cards; and describe various statistical methodologies for quantifying the relative performance of cardiac surgery programs. Outstanding statistical issues are also described.|Statistical and Clinical Aspects of Hospital Outcomes Profiling|http://www.jstor.org/stable/27645821|27645821|2007-05-01|2007|['eng']|['Health sciences - Medical treatment', 'Health sciences - Medical specialties']|['Science and Mathematics', 'Statistics']
This paper is motivated by the recent interest in the use of Bayesian VARs for forecasting, even in cases where the number of dependent variables is large. In such cases factor methods have been traditionally used, but recent work using a particular prior suggests that Bayesian VAR methods can forecast better. In this paper, we consider a range of alternative priors which have been used with small VARs, discuss the issues which arise when they are used with medium and large VARs and examine their forecast performance using a US macroeconomic dataset containing 168 variables. We find that Bayesian VARs do tend to forecast better than factor methods and provide an extensive comparison of the strengths and weaknesses of various approaches. Typically, we find that the simple Minnesota prior forecasts well in medium and large VARs, which makes this prior attractive relative to computationally more demanding alternatives. Our empirical results show the importance of using forecast metrics based on the entire predictive density, instead of relying solely on those based on point forecasts.|FORECASTING WITH MEDIUM AND LARGE BAYESIAN VARs|http://www.jstor.org/stable/23355914|23355914|2013-03-01|2013|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics']|['Business', 'Business & Economics Collection', 'Economics']
This article investigates possible ideological differences between circuits of the U.S. Courts of Appeals. It looks at the distribution of three-judge panel ideologies on the circuits and at differences in decisionmaking patterns, testing several theoretical approaches to circuit differences: the attitudinalist approach, arguing that different judicial ideologies account for intercircuit differences; historical-institutionalist approaches that argue that circuit norms lead to differences in the proportion of conservative decisions and in the effects of judicial ideologies; and the rational-choice institutionalist argument that overall circuit preferences constrain three-judge panel decisions through the en banc process. Using a multilevel logit model, the study finds some support for the attitudinalist and historical-institutionalist accounts of circuit differences. It also finds that intercircuit ideological differences contribute comparatively little to the prediction of appeals court outcomes.|Comparing Circuits: Are Some U.S. Courts of Appeals More Liberal or Conservative Than Others?|http://www.jstor.org/stable/23011962|23011962|2011-03-01|2011|['eng']|['Philosophy - Applied philosophy']|['Law', 'History', 'Political Science', 'Sociology', 'History', 'Social Sciences', 'Law']
We define a class of tessellation models based on perturbing or deforming standard tessellations such as the Voronoi tessellation. We show how distributions over this class of 'deformed' tessellations can be used to define prior distributions for models based on tessellations, and how inference for such models can be carried out using Markov chain Monte Carlo methods; stability properties of the algorithms are investigated. Our approach applies not only to fixed dimension problems, but also to variable dimension problems, in which the number of cells in the tessellation is unknown. We illustrate our methods with two real examples. The first relates to reconstructing animal territories, represented by the individual cells of a tessellation, from observation of an inhomogeneous Poisson point process. The second example involves the analysis of an image of a cross-section through a sample of metal, with the tessellation modelling the micro-crystalline structure of the metal.|Bayesian Analysis of Deformed Tessellation Models|http://www.jstor.org/stable/1428270|1428270|2003-03-01|2003|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
We discuss the development of dynamic factor models for multivariate financial time series, and the incorporation of stochastic volatility components for latent factor processes. Bayesian inference and computation is developed and explored in a study of the dynamic factor structure of daily spot exchange rates for a selection of international currencies. The models are direct generalizations of univariate stochastic volatility models and represent specific varieties of models recently discussed in the growing multivariate stochastic volatility literature. We discuss model fitting based on retrospective data and sequential analysis for forward filtering and short-term forecasting. Analyses are compared with results from the much simpler method of dynamic variance-matrix discounting that, for over a decade, has been a standard approach in applied financial econometrics. We study these models in analysis, forecasting, and sequential portfolio allocation for a selected set of international exchange-rate-return time series. Our goals are to understand a range of modeling questions arising in using these factor models and to explore empirical performance in portfolio construction relative to discount approaches. We report on our experiences and conclude with comments about the practical utility of structured factor models and on future potential model extensions.|Bayesian Dynamic Factor Models and Portfolio Allocation|http://www.jstor.org/stable/1392266|1392266|2000-07-01|2000|['eng']|['Applied sciences - Engineering']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
Prentice &amp; Pyke (1979) established that the maximum likelihood estimate of an odds ratio in a case-control study is the same as would be found by fitting a logistic regression; in other words, for this specific target the incorrect prospective model is inferentially equivalent to the correct retrospective model. Similar results have been obtained for other models, and conditions have also been identified under which the corresponding Bayesian property holds, namely that the posterior distribution of the odds ratio is the same whether it is computed using the prospective or the retrospective likelihood. In this article we demonstrate how these results follow directly from certain parameter independence properties of the models and priors, and identify prior laws that support such reverse analysis, for both standard and stratified designs.|Retrospective-prospective symmetry in the likelihood and Bayesian analysis of case-control studies|http://www.jstor.org/stable/43305603|43305603|2014-03-01|2014|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
A Bayesian analysis for factorial experiments is presented, using finite mixture distributions to model the main effects and interactions. This allows both estimation and an analogue of hypothesis testing in a posterior analysis using a single prior specification. A detailed formulation based on this approach is provided for the case of the two-way model with replication, allowing interactions. Issues in formulating a suitable prior are discussed in detail, and, in the context of two illustrative applications, we discuss implementation, presentation of posterior distributions, sensitivity and performance of the Markov chain Monte Carlo methods that are used.|Bayesian Analysis of Factorial Experiments by Mixture Modelling|http://www.jstor.org/stable/2673558|2673558|2000-03-01|2000|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
1. Intraspecific aggregation at a single spatial scale can promote the coexistence of competitors. This paper demonstrates how this same mechanism can be applied to the many systems that are patchy at two scales, with patches nested within `superpatches'. 2. Data are presented from a field study showing that insects living in rotting fruits have aggregated distributions in the fruits under a single tree, and that the mean density and degree of aggregation varies significantly among trees. Observations in this system motivate the following models. 3. A model of competition has been developed between two species which explicitly represents spatial variation at two scales. By integrating the probability distributions for each scale, the marginal distributions of competitors over all patches can be found and used to calculate coexistence criteria. This model assumes global movement of the competitors. 4. Although spatial variation at a single scale may not be sufficient for coexistence, the total variation over all patches can allow coexistence. Variation in mean densities among superpatches and variation in the degree of aggregation among superpatches both promote coexistence, but act in different ways. 5. A second model of competition between two species is described which incorporates the effects of limited movement among superpatches. Limited movement among superpatches generally promotes coexistence, and also leads to correlations among aggregation and the mean densities of competitors.|Integrating Nested Spatial Scales: Implications for the Coexistence of Competitors on a Patchy Resource|http://www.jstor.org/stable/2647306|2647306|1999-01-01|1999|['eng']|['Biological sciences - Biology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
In considering the inference about the unknown proportion of a Bernoulli process, it is shown that the choices involved in the frequentist approach are equivalent, from a Bayesian point of view, to the choice of a particular ignorance prior within a restricted ignorance zone. This link sheds light on the nature of both kinds of choices, and on undesirable properties that go with null variance data.|Bayesian Interpretation of Frequentist Procedures for a Bernoulli Process|http://www.jstor.org/stable/2685036|2685036|1996-02-01|1996|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Money laundering is a process designed to conceal the true origin of funds that were originally derived from illegal activities. Because money laundering often involves criminal activities, financial institutions have the responsibility to detect and report it to the appropriate government agencies in a timely manner. But the huge number of transactions occurring each day make detecting money laundering difficult. The usual approach adopted by financial institutions is to extract some summary statistics from the transaction history and conduct a thorough and time-consuming investigation on those suspicious accounts. In this article we propose an active learning through sequential design method for prioritization to improve the process of money laundering detection. The method uses a combination of stochastic approximation and D-optimal designs to judiciously select the accounts for investigation. The sequential nature of the method helps identify the optimal prioritization criterion with minimal time and effort. A case study with real banking data demonstrates the performance of the proposed method. A simulation study shows the method's efficiency and accuracy, as well as its robustness to model assumptions.|Active Learning Through Sequential Design, With Applications to Detection of Money Laundering|http://www.jstor.org/stable/40592268|40592268|2009-09-01|2009|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
We introduce Generalized Multilevel Functional Linear Models (GMFLMs), a novel statistical framework for regression models where exposure has a multilevel functional structure. We show that GMFLMs are, in fact, generalized multilevel mixed models. Thus, GMFLMs can be analyzed using the mixed effects inferential machinery and can be generalized within a well-researched statistical framework. We propose and compare two methods for inference: (1) a two-stage frequentist approach; and (2) a joint Bayesian analysis. Our methods are motivated by and applied to the Sleep Heart Health Study, the largest community cohort study of sleep. However, our methods are general and easy to apply to a wide spectrum of emerging biological and medical datasets. Supplemental materials for this article are available online.|Generalized Multilevel Functional Regression|http://www.jstor.org/stable/40592361|40592361|2009-12-01|2009|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Hierarchical linear and generalized linear models can be fit using Gibbs samplers and Metropolis algorithms; these models, however, often have many parameters, and convergence of the seemingly most natural Gibbs and Metropolis algorithms can sometimes be slow. We examine solutions that involve reparameterization and over-parameterization. We begin with parameter expansion using working parameters, a strategy developed for the EM algorithm. This strategy can lead to algorithms that are much less susceptible to becoming stuck near zero values of the variance parameters than are more standard algorithms. Second, we consider a simple rotation of the regression coefficients based on an estimate of their posterior covariance matrix. This leads to a Gibbs algorithm based on updating the transformed parameters one at a time or a Metropolis algorithm with vector jumps; either of these algorithms can perform much better (in terms of total CPU time) than the two standard algorithms: one-at-a-time updating of untransformed parameters or vector updating using a linear regression at each step. We present an innovative evaluation of the algorithms in terms of how quickly they can get away from remote areas of parameter space, along with some more standard evaluation of computation and convergence speeds. We illustrate our methods with examples from our applied work. Our ultimate goal is to develop a fast and reliable method for fitting a hierarchical linear model as easily as one can now fit a nonhierarchical model, and to increase understanding of Gibbs samplers for hierarchical models in general.|Using Redundant Parameterizations to Fit Hierarchical Models|http://www.jstor.org/stable/27594294|27594294|2008-03-01|2008|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Computer Science', 'Statistics']
A programme for fostering the growth of victims and potential victims of child abuse is evaluated via a longitudinal and cross-sectional study. A Bayesian analysis of the data is presented. Some linear hypotheses were of interest to the experimenters. As is usually the case, the experimenters were not necessarily interested in whether the hypotheses were exactly true, but rather in how close they were to being true. To address these concerns, some Bayesian measures of the degree of departure from a linear hypothesis are considered. These measures are in the same spirit as the distance measure considered by Geisser (1967) in the linear discrimination problem. The problem of eliciting prior information from the experimenters is considered.|A Residential Programme for Treating Victims of Child Abuse|http://www.jstor.org/stable/2987603|2987603|1983-03-01|1983|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Traditional approaches for process optimization start by fitting a model and then optimizing the model to obtain optimal operating settings. These methods do not account for any uncertainty in the parameters of the model or in the form of the model. Bayesian approaches have been proposed recently to account for the uncertainty on the parameters of the model, assuming that the model form is known. This article presents a Bayesian predictive approach to process optimization that accounts for the uncertainty in the model form, also accounting for the uncertainty of the parameters given each potential model. We propose optimizing the model-averaged posterior predictive density of the response where the weighted average is taken using the model posterior probabilities as weights. The resulting model-robust optimization is illustrated with two experiments from the literature, one involving a mixture experiment and the other involving a small composite design.|Model-Robust Process Optimization Using Bayesian Model Averaging|http://www.jstor.org/stable/25470977|25470977|2005-05-01|2005|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
This paper investigates how individuals' product choices are influenced by the product choices of their connected others and how the influence mechanism may differ for fashion- versus technology-related products. We conduct a novel field experiment to induce and observe choice interdependence in a closed social network. In our experiment, we conceptualize individuals' choices to be driven by multiattribute utilities, and we measure their initial attribute preferences prior to observing their choice interdependence and collecting network information. These design elements help alleviate concerns in identifying social interaction effects from other confounds. Given that we have complete information on choices and their sequence, we use a discrete-time Markov chain model. Nonetheless, we also use a Markov random field (MRF) model as an alternative when the information on choice sequence is missing. We find significant social interaction effects. Our findings show that whereas experts exert asymmetrically greater influence on a technology-related product, popular individuals exert greater influence on a fashion-related product. In addition, we find choices made by early decision makers to be more influential than choices made later for the technology-related product. Finally, using the MRF with snapshot data can also provide good out-of-sample predictions for a technology-related product.|Modeling Choice Interdependence in a Social Network|http://www.jstor.org/stable/24545003|24545003|2013-11-01|2013|['eng']|['Mathematics - Applied mathematics']|['Marketing & Advertising', 'Business & Economics', 'Business']
Utilizing the notion of matching predictives as in Berger and Pericchi, we show that for the conjugate family of prior distributions in the normal linear model, the symmetric Kullback-Leibler divergence between two particular predictive densities is minimized when the prior hyperparameters are taken to be those corresponding to the predictive priors proposed in Ibrahim and Laud and Laud and Ibrahim. The main application for this result is for Bayesian variable selection.|On Properties of Predictive Priors in Linear Models|http://www.jstor.org/stable/2685901|2685901|1997-11-01|1997|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
We discuss inference for data with repeated measurements at multiple levels. The motivating example is data with blood counts from cancer patients undergoing multiple cycles of chemotherapy, with days nested within cycles. Some inference questions relate to repeated measurements over days within cycle, while other questions are concerned with the dependence across cycles. When the desired inference relates to both levels of repetition, it becomes important to reflect the data structure in the model. We develop a semiparametric Bayesian modeling approach, restricting attention to two levels of repeated measurements. For the top-level longitudinal sampling model we use random effects to introduce the desired dependence across repeated measurements. We use a nonparametric prior for the random effects distribution. Inference about dependence across second-level repetition is implemented by the clustering implied in the nonparametric random effects model. Practical use of the model requires that the posterior distribution on the latent random effects be reasonably precise. /// Nous discutons de l'inférence pour des données de mesures répétées à plusieurs niveaux. L'exemple qui motive ce travail consiste en des données de comptage sanguin pour des patients subissant une chimiothérapie comportant plusieurs cycles, avec les jours emboîtés dans les cycles. Certaines questions d'inférence sont reliées aux mesures répétées à l'intérieur d'un cycle tandis que d'autres concernent la dépendance entre cycles. Si l'inférence désirée se rapporte à tous les niveaux de répétition, il devient important de refléter la structure des données dans le modèle. Nous développons une approche bayésienne semi-paramétrique en restreignant l'attention aux deux niveaux de mesures répétées. Pour le niveau le plus haut de l'échantillonnage longitudinal nous utilisons des effets aléatoires pour introduire la dépendance désirée entre les mesures répétées. Nous utilisons un a priori non paramétrique pour la distribution des effets aléatoires. L'inférence sur le second niveau de répétitions est rendue possible par l'effet de groupes impliqué dans le modèle à effets aléatoires non paramétriques. L'utilisation pratique du modèle requiert que la distribution a posteriori sur les effets aléatoires latents soit raisonnablement précise.|Semiparametric Bayesian Inference for Multilevel Repeated Measurement Data|http://www.jstor.org/stable/4541324|4541324|2007-03-01|2007|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Dementia is characterized by accelerated cognitive decline before and after diagnosis as compared to normal aging. It has been known that cognitive impairment occurs long before the diagnosis of dementia. For individuals who develop dementia, it is important to determine the time when the rate of cognitive decline begins to accelerate and the subsequent gap time to dementia diagnosis. For normal aging individuals, it is also useful to understand the trajectory of cognitive function until their death. A Bayesian change-point model is proposed to fit the trajectory of cognitive function for individuals who develop dementia. In real life, people in older ages are subject to two competing risks, e.g., dementia and dementia-free death. Because the majority of people do not develop dementia, a mixture model is used for survival data with competing risks, which consists of dementia onset time after the change point of cognitive function decline for demented individuals and death time for nondemented individuals. The cognitive trajectories and the survival process are modeled jointly and the parameters are estimated using the Markov chain Monte Carlo method. Using data from the Honolulu Asia Aging Study, we show the trajectories of cognitive function and the effect of education, apolipoprotein E 4 genotype, and hypertension on cognitive decline and the risk of dementia.|<strong>Joint Modeling for Cognitive Trajectory and Risk of Dementia in the Presence of Death</strong>|http://www.jstor.org/stable/40663178|40663178|2010-03-01|2010|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
The multivariate regression model is considered with p regressors. A latent vector with p binary entries serves to identify one of two types of regression coefficients: those close to 0 and those not. Specializing our general distributional setting to the linear model with Gaussian errors and using natural conjugate prior distributions, we derive the marginal posterior distribution of the binary latent vector. Fast algorithms aid its direct computation, and in high dimensions these are supplemented by a Markov chain Monte Carlo approach to sampling from the known posterior distribution. Problems with hundreds of regressor variables become quite feasible. We give a simple method of assigning the hyperparameters of the prior distribution. The posterior predictive distribution is derived and the approach illustrated on compositional analysis of data involving three sugars with 160 near infra-red absorbances as regressors.|Multivariate Bayesian Variable Selection and Prediction|http://www.jstor.org/stable/2985935|2985935|1998-01-01|1998|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Current statistical biogeographical analysis methods are limited in the ways ecology can be related to the processes of diversification and geographical range evolution, requiring conflation of geography and ecology, and/or assuming ecologies that are uniform across all lineages and invariant in time. This precludes the possibility of studying a broad class of macroevolutionary biogeographical theories that relate geographical and species histories through lineage-specific ecological and evolutionary dynamics, such as taxon cycle theory. Here we present a new model that generates phylogenies under a complex of superpositioned geographical range evolution, trait evolution, and diversification processes that can communicate with each other. We present a likelihood-free method of inference under our model using discriminant analysis of principal components of summary statistics calculated on phylogenies, with the discriminant functions trained on data generated by simulations under our model. This approach of model selection by classification of empirical data with respect to data generated under training models is shown to be efficient, robust, and performs well over a broad range of parameter space defined by the relative rates of dispersal, trait evolution, and diversification processes. We apply our method to a case study of the taxon cycle, that is testing for habitat and trophic level constraints in the dispersal regimes of the Wallacean avifaunal radiation.|Machine Learning Biogeographic Processes from Biotic Patterns: A New Trait-Dependent Dispersal and Diversification Model with Model Choice By Simulation-Trained Discriminant Analysis|http://www.jstor.org/stable/44028773|44028773|2016-05-01|2016|['eng']|['Biological sciences - Biology', 'Applied sciences - Engineering']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
This article presents Bayesian inference for exponential mixtures, including the choice of a noninformative prior based on a location-scale reparameterization of the mixture. Adapted control sheets are proposed for studying the convergence of the associated Gibbs sampler. They exhibit a strong lack of stability in the allocations of the observations to the different components of the mixture. The setup is extended to the case when the number of components in the mixture is unknown and a reversible jump MCMC technique is implemented. The results are illustrated on simulations and a real dataset.|MCMC Control Spreadsheets for Exponential Mixture Estimation|http://www.jstor.org/stable/1390638|1390638|1999-06-01|1999|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Science & Mathematics', 'Computer Science', 'Statistics']
In the Bayesian approach to model selection and hypothesis testing, the Bayes factor plays a central role. However, the Bayes factor is very sensitive to prior distributions of parameters. This is a problem especially in the presence of weak prior information on the parameters of the models. The most radical consequence of this fact is that the Bayes factor is undetermined when improper priors are used. Nonetheless, extending the non-informative approach of Bayesian analysis to model selection/testing procedures is important both from a theoretical and an applied viewpoint. The need to develop automatic and robust methods for model comparison has led to the introduction of several alternative Bayes factors. In this paper we review one of these methods: the fractional Bayes factor (O'Hagan, 1995). We discuss general properties of the method, such as consistency and coherence. Furthermore, in addition to the original, essentially asymptotic justifications of the fractional Bayes factor, we provide further finite-sample motivations for its use. Connections and comparisons to other automatic methods are discussed and several issues of robustness with respect to priors and data are considered. Finally, we focus on some open problems in the fractional Bayes factor approach, and outline some possible answers and directions for future research. /// Dans l'approche Bayesienne relative à la sélection d'un model et à la vérification d'une hypothèse, le facteur de Bayes joue une rôle fondamental. Toutefois le facteur de Bayes est très sensible aux distributions à priori des paramètres. Ceci constitue un problème surtout en présence d'une faible information à priori en ce qui concerne les paramètres des models. La conséquence la plus radical de ce fait est que le facteur de Bayes est undeterminé quand les distributions à priori non informatives sont utilisées. Cepandant, il est important d'élargir l'approche non informative de l'analyse Bayesienne à l'effet soit de déterminer la sélection d'un model que de vérifier une hypothèse. La necessité de développer des méthodes automatiques et robustes pour la comparaison des models, a amené à l'introduction des plusieurs facteurs de Bayes alternatifs. Cette étude prend en consideration les resultats principaux relatifs à une de ces methodes, à savoir le facteur de Bayes fractionnaire. Nous analysons les caracteristique générales de cette methode telles que sa consistance et sa cohérence. De plus en sus des justifications asyntotiques données à l'origine au facteur fractionnaire de Bayes nous apportons d'autres raisons qui demontrent le bien fondé de son utilisation dans le domaine d'un échantillonage fini. Nous prenons aussi en consideration par comparaison d'autres methodes automatiques et nous examinons d'autres caracteristiques telles que la robustesse par rapport aux les distributions à priori et aux données. En conclusion, nous attirons l'attention sur certains problèmes non encore resolus et proposons des solutions qui peuvent ètre explorées d'avantage.|Methods for Default and Robust Bayesian Model Comparison: The Fractional Bayes Factor Approach|http://www.jstor.org/stable/1403706|1403706|1999-12-01|1999|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
We propose a class of double hierarchical generalized linear models in which random effects can be specified for both the mean and dispersion. Heteroscedasticity between clusters can be modelled by introducing random effects in the dispersion model, as is heterogeneity between clusters in the mean model. This class will, among other things, enable models with heavy-tailed distributions to be explored, providing robust estimation against outliers. The h-likelihood provides a unified framework for this new class of models and gives a single algorithm for fitting all members of the class. This algorithm does not require quadrature or prior probabilities.|Double Hierarchical Generalized Linear Models|http://www.jstor.org/stable/3592661|3592661|2006-01-01|2006|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
In this paper we put forward a Bayesian model averaging method aimed at performing inference under model uncertainty in the presence of potential spatial autocorrelation. The method uses spatial filtering in order to account for uncertainty in spatial linkages. Our procedure is applied to a dataset of income per capita growth and 50 potential determinants for 255 NUTS-2 European regions. We show that ignoring uncertainty in the type of spatial weight matrix can have an important effect on the estimates of the parameters attached to the model covariates. After integrating out the uncertainty implied by the choice of regressors and spatial links, human capital investments and transitional dynamics related to income convergence appear as the most robust determinants of growth at the regional level in Europe. Our results imply that a quantitatively important part of the income convergence process in Europe is influenced by spatially correlated growth spillovers.|SPATIAL FILTERING, MODEL UNCERTAINTY AND THE SPEED OF INCOME CONVERGENCE IN EUROPE|http://www.jstor.org/stable/43907532|43907532|2013-06-01|2013|['eng']|['Mathematics - Mathematical objects']|['Business & Economics', 'Business', 'Economics']
We give an approach for using flow information from a system of wells to characterize hydrologic properties of an aquifer. In particular, we consider experiments where an impulse of tracer fluid is injected along with the water at the input wells and its concentration is recorded over time at the uptake wells. We focus on characterizing the spatially varying permeability field, which is a key attribute of the aquifer for determining flow paths and rates for a given flow experiment. As is standard for estimation from such flow data, we use complicated subsurface flow code that simulates the fluid flow through the aquifer for a particular well configuration and aquifer specification, in particular the permeability field over a grid. The solution to this ill-posed problem requires that some regularity conditions be imposed on the permeability field. Typically, this regularity is accomplished by specifying a stationary Gaussian process model for the permeability field. Here we use an intrinsically stationary Markov random field, which compares favorably to Gaussian process models and offers some additional flexibility and computational advantages. Our interest in quantifying uncertainty leads us to take a Bayesian approach, using Markov chain Monte Carlo for exploring the high-dimensional posterior distribution. We demonstrate our approach with several examples. We also note that the methodology is general and is not specific to hydrology applications.|Markov Random Field Models for High-Dimensional Parameters in Simulations of Fluid Flow in Porous Media|http://www.jstor.org/stable/1270487|1270487|2002-08-01|2002|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering', 'Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
Two physiologically based pharmacokinetic models for trichloroethylene (TCE) in mice and humans were calibrated with new toxicokinetic data sets. Calibration is an important step in model development, essential to a legitimate use of models for research or regulatory purposes. A Bayesian statistical framework was used to combine prior information about the model parameters with the data likelihood to yield posterior parameter distributions. For mice, these distributions represent uncertainty. For humans, the use of a population statistical model yielded estimates of both variability and uncertainty in human toxicokinetics of TCE. After adjustment of the models by Markov chain Monte Carlo sampling, the mouse model agreed with a large part of the data. Yet, some data on secondary metabolites were not fit well. The posterior parameter distributions obtained for mice were quite narrow (coefficient of variation [CV] of about 10 or 20%), but these CVs might be underestimated because of the incomplete fit of the model. The data fit, for humans, was better than for mice. Yet, some improvement of the model is needed to correctly describe trichloroethanol concentrations over long time periods. Posterior uncertainties about the population means corresponded to 10-20% CV. In terms of human population variability, volumes and flows varied across subject by approximately 20% CV. The variability was somewhat higher for partition coefficients (between 30 and 40%) and much higher for the metabolic parameters (standard deviations representing about a factor of 2). Finally, the analysis points to differences between human males and females in the toxicokinetics of TCE. The significance of these differences in terms of risk remains to be investigated.|Statistical Analysis of Fisher et al. PBPK Model of Trichloroethylene Kinetics|http://www.jstor.org/stable/4619456|4619456|2000-05-01|2000|['eng']|['Applied sciences - Engineering']|['Medicine & Allied Health', 'Health Sciences']
This paper demonstrates how Bayesian methods of predictive inference can be applied to the game of snooker to develop optimal strategies when choosing between different object-balls and pockets. Allowing for random variation on the circle and calculating predictive distributions for subsequent cue-ball and object-ball directions, it is possible to evaluate the probability of successfully potting a given object-ball in a stated pocket. If there is a choice between several shots, the corresponding probabilities can be combined with appropriate loss functions to determine which shot has the greatest expected gain. A discussion of computational difficulties and possible extensions is included.|Stochastic Snooker|http://www.jstor.org/stable/2348142|2348142|1994-01-01|1994|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Understanding the dynamics of climate change in its full richness requires the knowledge of long temperature time series. Although long-term, widely distributed temperature observations are not available, there are other forms of data, known as climate proxies, that can have a statistical relationship with temperatures and have been used to infer temperatures in the past before direct measurements. We propose a Bayesian hierarchical model to reconstruct past temperatures that integrates information from different sources, such as proxies with different temporal resolution and forcings acting as the external drivers of large scale temperature evolution. Additionally, this method allows us to quantify the uncertainty of the reconstruction in a rigorous manner. The reconstruction method is assessed, using a global climate model as the true climate system and with synthetic proxy data derived from the simulation. The target is to reconstruct Northern Hemisphere temperature from proxies that mimic the sampling and errors from tree ring measurements, pollen indices, and borehole temperatures. The forcing series used as covariates are solar irradiance, volcanic aerosols, and greenhouse gas concentrations. The Bayesian model was successful in integrating these different sources of information in creating a coherent reconstruction. Within the context of this numerical testbed, a statistical process model that includes the external forcings can improve the quality of a hemispheric reconstruction when long time scale proxy information is not available. This article has supplementary material online.|The Value of Multiproxy Reconstruction of Past Climate|http://www.jstor.org/stable/27920111|27920111|2010-09-01|2010|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
A number of procedures have been proposed to attack different inference problems for data drawn from a survey with a complex sample design (i.e., a design that entails unequal weighting). Most procedures either are based on finite-population assumptions or require the specification of an explicit model using a superpopulation rationale. Herein we propose some relatively simple approximate procedures that are based on a superpopulation model. They provide valid variance estimators, test statistics, and confidence intervals that allow for sample design effects as expressed by design weights and other weights. The procedures do not rely on conditioning on model elements such as covariates to adjust for design effects. Instead, we obtain estimators by rescaling sample weights to sum to the equivalent sample size (equal to sample size divided by design effect). Using weighted estimators for superpopulation models, we obtain approximations to confidence bounds on the mean for simple sampling situations as well as for cluster sampling, post-stratification, and stratified sampling. We also obtain approximate tests of hypotheses for one-way analysis of variance and k × 2 homogeneity testing. For all of these, further refinements based on the concept of equivalent degrees of freedom are provided. Additionally, a general method for determining and using poststratification weights is described and illustrated. The procedures in this article are better justified than the common expedient of making proportional adjustments so that the weights add to the sample size.|"""Equivalent Sample Size"" and ""Equivalent Degrees of Freedom"" Refinements for Inference Using Survey Weights Under Superpopulation Models"|http://www.jstor.org/stable/2290269|2290269|1992-06-01|1992|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Estimation of a smooth function is considered when observations on this function added with Gaussian errors are observed. The problem is formulated as a general linear model, and a hierarchical Bayesian approach is then used to study it. Credible bands are also developed for the function. Sensitivity analysis is conducted to determine the influence of the choice of priors on hyperparameters. Finally, the methodology is illustrated using real and simulated examples where it is compared with classical cubic splines. It is also shown that our approach provides a Bayesian solution to some problems in discrete time series. /// Nous étudierons le lissage d'une fonction lorsque les observations de cette fonction sont sujettes à des erreurs gaussiennes. Le problème sera formulé à l'aide d'un modèle linéaire et nous utiliserons l'approche bayesienne hiérarchique pour l'étudier. De plus nous développerons des bandes de crédibilité pour le lissage. Une analyse de sensibilité sera faite pour déterminer l'influence sur le lissage de la densité a priori sur les hyperparamètres. Pour conclure, nous illustrerons cette nouvelle méthodologie à l'aide de données réelles et d'une simulation; nous comparerons les résultats obtenus avec ceux fournis par les splines cubiques. Il sera aussi montré que cette approche fournit une solution bayesienne à quelques problèmes en séries chronologiques.|Hierarchical Bayesian Curve Fitting and Smoothing|http://www.jstor.org/stable/3315573|3315573|1992-03-01|1992|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
We use a time-varying factor-augmented VAR to investigate changes in the transmission mechanism of economic shocks in the UK. Our estimates demonstrate the importance of time variation and suggest that monetary policy shocks had a bigger impact on inflation, equity prices and the exchange rate during the inflation targeting period. Changes in the transmission of policy shocks to bond yields point to more efficient management of long-run inflation expectations. Finally, we investigate responses of disaggregated prices, with the median becoming more negative and cross-sectional patterns consistent with a decrease in the role of cost channels.|WHAT LIES BENEATH? A TIME-VARYING FAVAR MODEL FOR THE UK TRANSMISSION MECHANISM|http://www.jstor.org/stable/42919325|42919325|2014-05-01|2014|['eng']|['Physical sciences - Astronomy']|['Business', 'Business & Economics Collection', 'Economics']
Analysts fitting a hierarchical Bayesian model must specify the distribution of heterogeneity. There are several distributions to choose from, including the multivariate normal, mixture of normals, Dirichlet processes priors, and so forth. Although significant progress has been made, estimating the models and obtaining measures for model selection remain ongoing areas of research for more flexible distributions of heterogeneity. As a result, the multivariate normal remains the default choice for many researchers and software packages. This article proposes model-checking statistics that signal the adequacy of the multivariate normal assumption for the distribution of heterogeneity; these methods do not require the analyst to fit alternative models. The authors use posterior predictive model checking to determine whether a discrepancy exists between the individual-level parameters and those implied by the assumed distribution of heterogeneity. In simulated and real data sets, the results show that these statistics are useful for identifying when the multivariate normal distribution is adequate, when there is a departure in the tails of the distribution, and when a multimodal distribution of heterogeneity may be more appropriate.|Posterior Predictive Model Checking: An Application to Multivariate Normal Heterogeneity|http://www.jstor.org/stable/20751551|20751551|2010-10-01|2010|['eng']|['Applied sciences - Engineering']|['Business & Economics', 'Marketing & Advertising', 'Business']
For the problem of estimating a sparse sequence of coefficients of a parametric or non-parametric generalized linear model, posterior mode estimation with a Subbotin(λ, v) prior achieves thresholding and therefore model selection when v Є [0,1] for a class of likelihood functions. The proposed estimator also offers a continuum between the (forward/backward) best subset estimator (v = 0), its approximate convexification called lasso (v = 1) and ridge regression (v = 2). Rather than fixing v, selecting the two hyperparameters λ and v adds flexibility for a better fit, provided both are well selected from the data. Considering first the canonical Gaussian model, we generalize the Stein unbiased risk estimate, SURE (λ, v), to the situation where the thresholding function is not almost differentiable (i.e. v&lt;1). We then propose a more general selection of λ and v by deriving an information criterion that can be employed for instance for the lasso or wavelet smoothing. We investigate some asymptotic properties in parametric and non-parametric settings. Simulations and applications to real data show excellent performance.|Adaptive Posterior Mode Estimation of a Sparse Sequence for Model Selection|http://www.jstor.org/stable/41000341|41000341|2009-12-01|2009|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
"Recent work on Bayesian inference of disease mapping models discusses the advantages of the fully Bayesian (FB) approach over its empirical Bayes (EB) counterpart, suggesting that FB posterior standard deviations of small-area relative risks are more reflective of the uncertainty associated with the relative risk estimation than counterparts based on EB inference, since the latter fail to account for the variability in the estimation of the hyperparameters. In this article, an EB bootstrap methodology for relative risk inference with accurate parametric EB confidence intervals is developed, illustrated, and contrasted with the hyperprior Bayes. We elucidate the close connection between the EB bootstrap methodology and hyperprior Bayes, present a comparison between FB inference via hybrid Markov chain Monte Carlo and EB inference via penalized quasi-likelihood, and illustrate the ability of parametric bootstrap procedures to adjust for the undercoverage in the ""naive"" EB interval estimates. We discuss the important roles that FB and EB methods play in risk inference, map interpretation, and real-life applications. The work is motivated by a recent analysis of small-area infant mortality rates in the province of British Columbia in Canada."|Estimation in Bayesian Disease Mapping|http://www.jstor.org/stable/3695465|3695465|2004-12-01|2004|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Increasingly, political science researchers are turning to Markov chain Monte Carlo methods to solve inferential problems with complex models and problematic data. This is an enormously powerful set of tools based on replacing difficult or impossible analytical work with simulated empirical draws from the distributions of interest. Although practitioners are generally aware of the importance of convergence of the Markov chain, many are not fully aware of the difficulties in fully assessing convergence across multiple dimensions. In most applied circumstances, every parameter dimension must be converged for the others to converge. The usual culprit is slow mixing of the Markov chain and therefore slow convergence towards the target distribution. This work demonstrates the partial convergence problem for the two dominant algorithms and illustrates these issues with empirical examples.|Is Partial-Dimension Convergence a Problem for Inferences from MCMC Algorithms?|http://www.jstor.org/stable/25791926|25791926|2008-04-01|2008|['eng']|['Mathematics - Mathematical logic']|['Political Science', 'Social Sciences']
This study compares the performance of a recently proposed multiprocess mixture model and a random-walk time-varying parameter (TVP) model, using the interest rate--weekly money relationship for illustrative purposes. For the case of this relationship, which is subject to regime shifts and outliers, the mixture model performs well and the latter model performs poorly. This finding is of general interest, since investigators often adopt random-walk TVP models to accommodate potential regime shifts in regression relationships. The TVP estimation procedure is unlikely to find abrupt shifts, since the estimate of parameter variance is based on the entire data sample. In the face of rapid discontinuous shifts in the parameters, this variance estimate is unrepresentative of the variability during periods of abrupt shift or transient observations.|A Comparison of Time-Varying Parameter and Multiprocess Mixture Models in the Case of Money-Supply Announcements|http://www.jstor.org/stable/1391678|1391678|1992-04-01|1992|['eng']|['Physical sciences - Astronomy', 'Mathematics - Applied mathematics']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
Tropospheric ozone is one of six criteria pollutants regulated by the US EPA, and has been linked to respiratory and cardiovascular endpoints and adverse effects on vegetation and ecosystems. Regional photochemical models have been developed to study the impacts of emission reductions on ozone levels. The standard approach is to run the deterministic model under new emission levels and attribute the change in ozone concentration to the emission control strategy. However, running the deterministic model requires substantial computing time, and this approach does not provide a measure of uncertainty for the change in ozone levels. Recently, a reduced form model (RFM) has been proposed to approximate the complex model as a simple function of a few relevant inputs. In this paper, we develop a new statistical approach to make full use of the RFM to study the effects of various control strategies on the probability and magnitude of extreme ozone events. We fuse the model output with monitoring data to calibrate the RFM by modeling the conditional distribution of monitoring data given the RFM using a combination of flexible semiparametric quantile regression for the center of the distribution where data are abundant and a parametric extreme value distribution for the tail where data are sparse. Selected parameters in the conditional distribution are allowed to vary by the RFM value and the spatial location. Also, due to the simplicity of the RFM, we are able to embed the RFM in our Bayesian hierarchical framework to obtain a full posterior for the model input parameters, and propagate this uncertainty to the estimation of the effects of the control strategies. We use the new framework to evaluate three potential control strategies, and find that reducing mobile-source emissions has a larger impact than reducing point-source emissions or a combination of several emission sources.|EXTREME VALUE ANALYSIS FOR EVALUATING OZONE CONTROL STRATEGIES|http://www.jstor.org/stable/23566411|23566411|2013-06-01|2013|['eng']|['Physical sciences - Astronomy']|['Mathematics', 'Science and Mathematics', 'Statistics']
"Technological advances in genotyping have given rise to hypothesis-based association studies of increasing scope. As a result, the scientific hypotheses addressed by these studies have become more complex and more difficult to address using existing analytic methodologies. Obstacles to analysis include inference in the face of multiple comparisons, complications arising from correlations among the SNPs (single nucleotide polymorphisms), choice of their genetic parametrization and missing data. In this paper we present an efficient Bayesian model search strategy that searches over the space of genetic markers and their genetic parametrization. The resulting method for Multilevel Inference of SNP Associations, MISA, allows computation of multilevel posterior probabilities and Bayes factors at the global, gene and SNP level, with the prior distribution on SNP inclusion in the model providing an intrinsic multiplicity correction. We use simulated data sets to characterize MISA's statistical power, and show that MISA has higher power to detect association than standard procedures. Using data from the North Carolina Ovarian Cancer Study (NCOCS), MISA identifies variants that were not identified by standard methods and have been externally ""validated"" in independent studies. We examine sensitivity of the NCOCS results to prior choice and method for imputing missing data. MISA is available in an R package on CRAN."|BAYESIAN MODEL SEARCH AND MULTILEVEL INFERENCE FOR SNP ASSOCIATION STUDIES|http://www.jstor.org/stable/29765557|29765557|2010-09-01|2010|['eng']|['Applied sciences - Engineering', 'Biological sciences - Biology']|['Mathematics', 'Science & Mathematics', 'Statistics']
The statistics literature on functional data analysis focuses primarily on flexible black-box approaches, which are designed to allow individual curves to have essentially any shape while characterizing variability. Such methods typically cannot incorporate mechanistic information, which is commonly expressed in terms of differential equations. Motivated by studies of muscle activation, we propose a nonparametric Bayesian approach that takes into account mechanistic understanding of muscle physiology. A novel class of hierarchical Gaussian processes is defined that favors curves consistent with differential equations defined on motor, damper, spring systems. A Gibbs sampler is proposed to sample from the posterior distribution and applied to a study of rats exposed to noninjurious muscle activation protocols. Although motivated by muscle force data, a parallel approach can be used to include mechanistic information in broad functional data analysis applications.|Mechanistic Hierarchical Gaussian Processes|http://www.jstor.org/stable/24247419|24247419|2014-09-01|2014|['eng']|['Mathematics - Applied mathematics', 'Biological sciences - Biology', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
In this article, we demonstrate a statistical method for fitting the parameters of a sophisticated network and epidemic model to disease data. The pattern of contacts between hosts is described by a class of dyadic independence exponential-family random graph models (ERGMs), whereas the transmission process that runs over the network is modeled as a stochastic susceptible-exposed-infectious-removed (SEIR) epidemic. We fit these models to very detailed data from the 1861 measles outbreak in Hagelloch, Germany. The network models include parameters for all recorded host covariates including age, sex, household, and classroom membership and household location whereas the SEIR epidemic model has exponentially distributed transmission times with gamma-distributed latent and infective periods. This approach allows us to make meaningful statements about the structure of the population—separate from the transmission process—as well as to provide estimates of various biological quantities of interest, such as the effective reproductive number, R. Using reversible jump Markov chain Monte Carlo, we produce samples from the joint posterior distribution of all the parameters of this model—the network, transmission tree, network parameters, and SEIR parameters—and perform Bayesian model selection to find the best-fitting network model. We compare our results with those of previous analyses and show that the ERGM network model better fits the data than a Bernoulli network model previously used. We also provide a software package, written in R, that performs this type of analysis.|A Network-based Analysis of the 1861 Hagelloch Measles Data|http://www.jstor.org/stable/23270593|23270593|2012-09-01|2012|['eng']|['Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
This article investigates the application of the three-parameter, Coale-McNeil marriage model and some related hyperparameterized specifications to data on the first marriage patterns of American women. Because the model is parametric, it can be used to estimate the parameters of the marriage process for cohorts that have yet to complete their first marriage experience. Empirical evidence from three surveys is reported on the ability of the model to replicate and project observed marriage behavior. The results indicate that the model can be a useful tool for analyzing cohort marriage data and that recent cohorts are showing relatively strong proclivities to both delay and forego marriage. Consistent with earlier work, the results also indicate that education is a powerful covariate of the timing of first marriage and that race is a powerful covariate of its incidence.|Modeling American Marriage Patterns|http://www.jstor.org/stable/2289597|2289597|1990-12-01|1990|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
We apply a linear Bayesian model to seismic tomography, a high-dimensional inverse problem in geophysics. The objective is to estimate the three-dimensional structure of the earth's interior from data measured at its surface. Since this typically involves estimating thousands of unknowns or more, it has always been treated as a linear(ized) optimization problem. Here we present a Bayesian hierarchical model to estimate the joint distribution of earth structural and earthquake source parameters. An ellipsoidal spatial prior allows to accommodate the layered nature of the earth's mantle. With our efficient algorithm we can sample the posterior distributions for large-scale linear inverse problems and provide precise uncertainty quantification in terms of parameter distributions and credible intervals given the data. We apply the method to a full-fledged tomography problem, an inversion for upper-mantle structure under western North America that involves more than 11,000 parameters. In studies on simulated and real data, we show that our approach retrieves the major structures of the earth's interior as well as classical least-squares minimization, while additionally providing uncertainty assessments.|A BAYESIAN LINEAR MODEL FOR THE HIGH-DIMENSIONAL INVERSE PROBLEM OF SEISMIC TOMOGRAPHY|http://www.jstor.org/stable/23566426|23566426|2013-06-01|2013|['eng']|['Physical sciences - Astronomy']|['Mathematics', 'Science & Mathematics', 'Statistics']
Learning the properties of an image associated with human gaze placement is important both for understanding how biological systems explore the environment and for computer vision applications. There is a large literature on quantitative eye movement models that seeks to predict fixations from images (sometimes termed “saliency” prediction). A major problem known to the field is that existing model comparison metrics give inconsistent results, causing confusion. We argue that the primary reason for these inconsistencies is because different metrics and models use different definitions of what a “saliency map” entails. For example, some metrics expect a model to account for image-independent central fixation bias whereas others will penalize a model that does. Here we bring saliency evaluation into the domain of information by framing fixation prediction models probabilistically and calculating information gain. We jointly optimize the scale, the center bias, and spatial blurring of all models within this framework. Evaluating existing metrics on these rephrased models produces almost perfect agreement in model rankings across the metrics. Model performance is separated from center bias and spatial blurring, avoiding the confounding of these factors in model comparison. We additionally provide a method to show where and how models fail to capture information in the fixations on the pixel level. These methods are readily extended to spatiotemporal models of fixation scanpaths, and we provide a software package to facilitate their use.|Information-theoretic model comparison unifies saliency metrics|http://www.jstor.org/stable/26466280|26466280|2015-12-29|2015|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Biological Sciences', 'General Science']
This paper compares two methods for undertaking likelihood-based inference in dynamic equilibrium economies: a sequential Monte Carlo filter and the Kalman filter. The sequential Monte Carlo filter exploits the nonlinear structure of the economy and evaluates the likelihood function of the model by simulation methods. The Kalman filter estimates a linearization of the economy around the steady state. We report two main results. First, both for simulated and for real data, the sequential Monte Carlo filter delivers a substantially better fit of the model to the data as measured by the marginal likelihood. This is true even for a nearly linear case. Second, the differences in terms of point estimates, although relatively small in absolute values, have important effects on the moments of the model. We conclude that the nonlinear filter is a superior procedure for taking models to the data.|Estimating Dynamic Equilibrium Economies: Linear versus Nonlinear Likelihood|http://www.jstor.org/stable/25146404|25146404|2005-12-01|2005|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Business & Economics', 'Business', 'Economics']
Generalized linear mixed models with semiparametric random effects are useful in a wide variety of Bayesian applications. When the random effects arise from a mixture of Dirichlet process (MDP) model with normal base measure, Gibbs sampling algorithms based on the Pólya urn scheme are often used to simulate posterior draws in conjugate models (essentially, linear regression models and models for binary outcomes). In the nonconjugate case, some common problems associated with existing simulation algorithms include convergence and mixing difficulties. This article proposes an algorithm for MDP models with exponential family likelihoods and normal base measures. The algorithm proceeds by making a Laplace approximation to the likelihood function, thereby matching the proposal with that of the Gibbs sampler. The proposal is accepted or rejected via a Metropolis—Hastings step. For conjugate MDP models, the algorithm is identical to the Gibbs sampler. The performance of the technique is investigated using a Poisson regression model with semiparametric random effects. The algorithm performs efficiently and reliably, even in problems where large-sample results do not guarantee the success of the Laplace approximation. This is demonstrated by a simulation study where most of the count data consist of small numbers. The technique is associated with substantial benefits relative to existing methods, both in terms of convergence properties and computational cost.|Posterior Simulation in the Generalized Linear Mixed Model with Semiparametric Random Effects|http://www.jstor.org/stable/27594314|27594314|2008-06-01|2008|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Computer Science', 'Statistics']
Single-transect methods of shoreline change prediction are unparsimonious, i.e., they tend to overfit data by using more parameters than necessary because they assume that both signal and noise at adjacent transects are independent. Here we introduce some new methods that reduce overfitting by expressing change rate as a linear sum of basis functions. In the method of IC-binning, the basis functions are boxcars—an information criterion is used to assign contiguous alongshore locations into bins within which change rate is constant; the resulting rate is discontinuous but may be useful for beach management. In the polynomial method, the basis functions are polynomials in alongshore distance, and the change rate varies continuously along the beach. In the eigenbeaches method, the basis functions are the principal components of the matrix of shorelines. To choose the number of basis functions in each method, and to compare methods with each other, we use an information criterion. We apply these new methods to shoreline change on Maui Island, Hawaii, briefly here, and in more detail in a companion paper. The polynomial method works best for short beaches with rates that vary slowly in the alongshore direction while eigenbeaches works best for shorelines that are long, or have rates that vary rapidly in the alongshore direction. The Schwarz information criterion and the AICu version of the Akaike information criterion performed well in tests on real data and noisy synthetic data.|Toward Parsimony in Shoreline Change Prediction (I): Basis Function Methods|http://www.jstor.org/stable/27698329|27698329|2009-03-01|2009|['eng']|['Mathematics - Applied mathematics']|['Ecology & Evolutionary Biology', 'Aquatic Sciences', 'Science & Mathematics', 'Biological Sciences']
The (maximum) penalized-likelihood method of probability density estimation and bump-hunting is improved and exemplified by applications to scattering and chondrite data. We show how the hyperparameter in the method can be satisfactorily estimated by using statistics of goodness of fit. A Fourier expansion is found to be usually more expeditious than a Hermite expansion but a compromise is useful. The best fit to the scattering data has 13 bumps, all of which are evaluated by the Bayesian interpretation of the method. Eight bumps are well supported. The result for the chondrite data suggests that it is trimodal and confirms that there are (at least) three kinds of chondrite.|Density Estimation and Bump-Hunting by the Penalized Likelihood Method Exemplified by Scattering and Meteorite Data|http://www.jstor.org/stable/2287377|2287377|1980-03-01|1980|['eng']|['Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
This paper examines the cyclical and seasonal relationships between Japanese industrial production and oil imports using structural time series analysis. The time series are decomposed into trend, cyclical, seasonal and irregular components. Results of causality testing reveal that there is causal influence from the seasonal component of industrial production to the seasonal component of oil imports, but no such relationship between the cyclical components. These results are compared and reconciled with those obtained from seasonal error correction modelling.|THE EFFECT OF CYCLICAL AND SEASONAL VARIATION IN INDUSTRIAL PRODUCTION ON OIL IMPORTS: A STRUCTURAL TIME SERIES STUDY OF THE JAPANESE CASE|http://www.jstor.org/stable/43296069|43296069|1996-12-01|1996|['eng']|['Mathematics - Mathematical logic', 'Philosophy - Logic']|['Business & Economics', 'Economics']
We attempt to recover an n-dimensional vector observed in white noise, where n is large and the vector is known to be sparse, but the degree of sparsity is unknown. We consider three different ways of defining sparsity of a vector: using the fraction of nonzero terms; imposing power-law decay bounds on the ordered entries; and controlling the $ell_{p}$ norm for p small. We obtain a procedure which is asymptotically minimax for $\ell ^{r}$ loss, simultaneously throughout a range of such sparsity classes. The optimal procedure is a data-adaptive thresholding scheme, driven by control of the false discovery rate (FDR). FDR control is a relatively recent innovation in simultaneous testing, ensuring that at most a certain expected fraction of the rejected null hypotheses will correspond to false rejections. In our treatment, the FDR control parameter $q_{n}$ also plays a determining role in asymptotic minimaxity. If $q={\rm lim}\ q_{n}\in [0,1/2]$ and also $q_{n}&gt;\gamma /{\rm log}(n)$, we get sharp asymptotic minimaxity, simultaneously, over a wide range of sparse parameter spaces and loss functions. On the other hand, $q={\rm lim}\ q_{n}\in (1/2,1]$ forces the risk to exceed the minimax risk by a factor growing with q. To our knowledge, this relation between ideas in simultaneous inference and asymptotic decision theory is new. Our work provides a new perspective on a class of model selection rules which has been introduced recently by several authors. These new rules impose complexity penalization of the form 2 · log(potential model size/actual model sizes). We exhibit a close connection with FDR-controlling procedures under stringent control of the false discovery rate.|Special Invited Lecture: Adapting to Unknown Sparsity by Controlling the False Discovery Rate|http://www.jstor.org/stable/25463431|25463431|2006-04-01|2006|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
A generalized hierarchical Markov model for sequences that contain length-restricted features is introduced. This model is motivated by the recent development of high-density tiling array data for determining genomic elements of functional importance. Due to length constraints on certain features of interest, as well as variability in probe behavior, usual hidden Markov-type models are not always applicable. A robust Bayesian framework that can incorporate length constraints, probe variability, and bias is developed. Moreover, a novel recursion-based Monte Carlo algorithm is proposed to estimate the parameters and impute hidden states under length constraints. Application of this methodology to yeast chromosomal arrays demonstrate substantial improvement over currently existing methods in terms of sensitivity as well as biological interpretability.|Generalized Hierarchical Markov Models for the Discovery of Length-Constrained Sequence Features from Genome Tiling Arrays|http://www.jstor.org/stable/4541412|4541412|2007-09-01|2007|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
This paper uses data from a randomized evaluation of Head Start to answer two questions: (i) How much do short-run causal effects vary across Head Start centers? and (ii) Do observed inputs explain this variation? I find that the cross-center standard deviation of cognitive effects is 0.18 test score standard deviations, which is larger than typical estimates of variation in teacher or school effectiveness. Centers offering full-day service and home visiting are more effective, while centers that draw more children from center-based preschool have smaller effects. Other key inputs, including the High/Scope curriculum, teacher education, and class size are not correlated with Head Start effectiveness.|Inputs in the Production of Early Childhood Human Capital: Evidence from Head Start|http://www.jstor.org/stable/24739060|24739060|2015-10-01|2015|['eng']|['Education - Formal education']|['Business & Economics', 'Business', 'Economics']
This article provides new tools for the evaluation of dynamic stochastic general equilibrium (DSGE) models and applies them to a large-scale new Keynesian model. We approximate the DSGE model by a vector autoregression, and then systematically relax the implied cross-equation restrictions and document how the model fit changes. We also compare the DSGE model's impulse responses to structural shocks with those obtained after relaxing its restrictions. We find that the degree of misspecification in this largescale DSGE model is no longer so large as to prevent its use in day-to-day policy analysis, yet is not small enough to be ignored.|On the Fit of New Keynesian Models|http://www.jstor.org/stable/27638915|27638915|2007-04-01|2007|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
Parsimonious modelling of the within-subject covariance structure while heeding its positive-definiteness is of great importance in the analysis of longitudinal data. Using the Cholesky decomposition and the ensuing unconstrained and statistically meaningful reparameterisation, we provide a convenient and intuitive framework for developing conditionally conjugate prior distributions for covariance matrices and show their connections with generalised inverse Wishart priors. Our priors offer many advantages with regard to elicitation, positive definiteness, computations using Gibbs sampling, shrinking covariances toward a particular structure with considerable flexibility, and modelling covariances using covariates. Bayesian estimation methods are developed and the results are compared using two simulation studies. These simulations suggest simpler and more suitable priors for the covariance structure of longitudinal data.|Bayesian Analysis of Covariance Matrices and Dynamic Models for Longitudinal Data|http://www.jstor.org/stable/4140601|4140601|2002-09-01|2002|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
"We have developed Bayesian Markov chain Monte Carlo (MCMC) methods for inferences of continuous-time models with stochastic volatility and infinite-activity Lévy jumps using discretely sampled data. Simulation studies show that (i) our methods provide accurate joint identification of diffusion, stochastic volatility, and Lévy jumps, and (ii) the affine jump-diffusion (AJD) models fail to adequately approximate the behavior of infinite-activity jumps. In particular, the AJD models fail to capture the ""infinitely many"" small Lévy jumps, which are too big for Brownian motion to model and too small for compound Poisson process to capture. Empirical studies show that infinite-activity Lévy jumps are essential for modeling the S&amp;P 500 index returns."|A Bayesian Analysis of Return Dynamics with Lévy Jumps|http://www.jstor.org/stable/40056885|40056885|2008-09-01|2008|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Business', 'Finance']
Many multivariate Gaussian models can conveniently be split into independent, block-wise problems. Common settings where this situation arises are balanced ANOVA models, balanced longitudinal models, and certain block-wise shrinkage estimators in nonparametric regression estimation involving orthogonal bases such as Fourier or wavelet bases. It is well known that the standard, least squares estimate in multidimensional Gaussian models can often be improved through the use of minimax shrinkage estimators or related Bayes estimators. In the following we show that the traditional estimators constructed via independent shrinkage can be improved in terms of their squared-error risk, and we provide improved minimax estimators. An alternate class of block-wise shrinkage estimators is also considered, and fairly precise conditions are given that characterize when these estimators are admissible or quasi-admissible. These results can also be applied to the classical Stein-Lindley estimator that shrinks toward an overall mean. It is shown how this estimator can be improved by introducing additional shrinkage.|ESTIMATORS FOR GAUSSIAN MODELS HAVING A BLOCK-WISE STRUCTURE|http://www.jstor.org/stable/24308936|24308936|2009-07-01|2009|['eng']|['Mathematics - Mathematical logic']|['Mathematics', 'Science & Mathematics', 'Statistics']
Synthetic magnetic resonance imaging involves the estimation, based on a set of measured images with noise, of three basic physical quantities that are nonlinearly related to the observations. The methods currently available for this ill-conditioned inverse problem either do not provide sufficiently accurate estimates or require time-consuming data collection. We formulate this nonlinear problem in a Bayesian framework, taking into account knowledge about the physics of the magnetic resonance imaging experiment, statistical properties of the experimental noise, and prior information about the underlying physical quantities, modelled by a suitable Markov random field. A new multilayer Markov random field is proposed. Inference is drawn by means of Markov chain Monte Carlo methods or iterated conditional modes. Some examples are included to demonstrate how synthetic magnetic resonance imaging by this approach can be performed in an accurate and reliable way.|A Bayesian Approach to Synthetic Magnetic Resonance Imaging|http://www.jstor.org/stable/2337404|2337404|1995-06-01|1995|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Debate exists over how to incorporate information from multipartite sequence data in phylogenetic analyses. Strict combined-data approaches argue for concatenation of all partitions and estimation of one evolutionary history, maximizing the explanatory power of the data. Consensus/independence approaches endorse a two-step procedure where partitions are analyzed independently and then a consensus is determined from the multiple results. Mixtures across the model space of a strict combined-data approach and a priori independent parameters are popular methods to integrate these methods. We propose an alternative middle ground by constructing a Bayesian hierarchical phylogenetic model. Our hierarchical framework enables researchers to pool information across data partitions to improve estimate precision in individual partitions while permitting estimation and testing of tendencies in across-partition quantities. Such across-partition quantities include the distribution from which individual topologies relating the sequences within a partition are drawn. We propose standard hierarchical priors on continuous evolutionary parameters across partitions, while the structure on topologies varies depending on the research problem. We illustrate our model with three examples. We first explore the evolutionary history of the guinea pig (Cavia porcellus) using alignments of 13 mitochondrial genes. The hierarchical model returns substantially more precise continuous parameter estimates than an independent parameter approach without losing the salient features of the data. Second, we analyze the frequency of horizontal gene transfer using 50 prokaryotic genes. We assume an unknown species-level topology and allow individual gene topologies to differ from this with a small estimable probability. Simultaneously inferring the species and individual gene topologies returns a transfer frequency of 17%. We also examine HIV sequences longitudinally sampled from HIV+ patients. We ask whether posttreatment development of CCR5 coreceptor virus represents concerted evolution from middisease CXCR4 virus or reemergence of initial infecting CCR5 virus. The hierarchical model pools partitions from multiple unrelated patients by assuming that the topology for each patient is drawn from a multinomial distribution with unknown probabilities. Preliminary results suggest evolution and not reemergence.|Hierarchical Phylogenetic Models for Analyzing Multipartite Sequence Data|http://www.jstor.org/stable/3651067|3651067|2003-10-01|2003|['eng']|['Biological sciences - Biology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
1. Metabolic scaling theory predicts that diameter growth rates of tree species are related to tree diameter by a universal scaling law. This model has been criticised because it ignores the influence of competition for resources such as light on the scaling of demographic rates with size. 2. We here test whether scaling exponents of abundant tropical tree species comply with the prediction of metabolic scaling theory and evaluate whether the scaling of growth with size depends on light availability. Light reaching each individual tree was estimated from yearly vertical censuses of canopy density, and a hierarchical Bayesian approach allowed quantifying confidence intervals for scaling exponents and accounting for different sources of error. 3. We found no universal scaling relationship, and 50—70% of the species had scaling exponents that significantly differed from the predicted value of 1/3. As would be expected if competition for light were important, scaling exponents were &gt; 1/3 for the majority of species when all trees were combined. However, the community average of scaling exponents was not significantly different from the predicted value of 1/3 when only considering individuals that grew under highlight conditions. 4. These results support the hypothesis that the prediction of metabolic ecology for the scaling of tree growth with size is only valid when competition for light is unimportant.|Testing metabolic theory with models of tree growth that include light competition|http://www.jstor.org/stable/23258570|23258570|2012-06-01|2012|['eng']|['Physical sciences - Astronomy']|['Biological Sciences', 'Ecology & Evolutionary Biology', 'Science and Mathematics']
In this paper the first order polynomial dynamic model is considered introducing a nonparametric specification of the error terms, using mixtures of Dirichlet processes. In order to make inference about the relevant parameters of the model the Gibbs Sampling approach is used. The approach is suitable to cope with features like outliers and changes in level, both for prediction and detection purposes, showing some characteristics of robustness due to the memory of the processes. An example is shown using artificial data.|Nonparametric Specication of Error Terms in Dynamic Models|http://www.jstor.org/stable/4355924|4355924|1996-01-01|1996|['eng']|['Physical sciences - Astronomy']|['Mathematics', 'Science & Mathematics', 'Statistics']
ABSTRACT: We developed a species distribution model of Cryptotympana facialis in Japan in order to investigate (1) the relationship between climate change and the northward shift of this species, (2) the existence of potential habitats in northern areas, and (3) the possibility of further northward shifts in the future. The distribution of C. facialis can be explained by life-history-related climate factors, including egg-hatching probability based on the sum of effective temperature, total precipitation during the rainy season, topographic slope, and the proportions of forest and urban areas, using an intrinsic Gaussian conditional autoregressive (CAR) model. The changes in potential habitat for C. facialis under climate change were projected using predicted climate conditions for 2070. In the parameter estimates of the CAR model, hatching probability, precipitation, and urban area were positive factors, while slope and forest area were negative factors. The fixed effects of the CAR model showed that more potential habitats exist in the north than in the current range of the species in western Japan. Moreover, our projection showed areas of suitable habitat increasing under all climate change scenarios. The current distribution of C. facialis is not in a state of equilibrium, possibly due to its low speed of dispersal. The distribution of C. facialis will expand to northern areas without climate warming, but climate warming will increase the amount of potential habitat. We emphasize the importance of considering lifehistory-related climatic factors, non-climatic factors, and spatial autocorrelation when modeling species distributions under climate change.|Climate change and the northward shift of <em>Cryptotympana facialis</em> in Japan:|http://www.jstor.org/stable/24896564|24896564|2016-03-23|2016|['eng']|['Biological sciences - Biology']|['Science & Mathematics', 'Environmental Science']
In this paper we propose a Bayesian nonparametric model for clustering partial ranking data. We start by developing a Bayesian nonparametric extension of the popular Plackett–Luce choice model that can handle an infinite number of choice items. Our framework is based on the theory of random atomic measures, with the prior specified by a completely random measure. We characterise the posterior distribution given data, and derive a simple and effective Gibbs sampler for posterior simulation. We then develop a Dirichlet process mixture extension of our model and apply it to investigate the clustering of preferences for college degree programmes amongst Irish secondary school graduates. The existence of clusters of applicants who have similar preferences for degree programmes is established and we determine that subject matter and geographical location of the third level institution characterise these clusters.|BAYESIAN NONPARAMETRIC PLACKETT–LUCE MODELS FOR THE ANALYSIS OF PREFERENCES FOR COLLEGE DEGREE PROGRAMMES|http://www.jstor.org/stable/24522091|24522091|2014-06-01|2014|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical objects']|['Mathematics', 'Science & Mathematics', 'Statistics']
The epidemiology of human campylobacteriosis is complex but in recent years understanding of this disease has advanced considerably. Despite being a major public health concern in many countries, the presence of multiple hosts, genotypes and transmission pathways has made it difficult to identify and quantify the determinants of human infection and disease. This has delayed the development of successful intervention programmes for this disease in many countries including New Zealand, a country with a comparatively high, yet until recently poorly understood, rate of notified disease. This study investigated the epidemiology of Campylobacter jejuni at the genotype-level over a 3-year period between 2005 and 2008 using multilocus sequence typing. By combining epidemiological surveillance and population genetics, a dominant, internationally rare strain of C. jejuni (ST474) was identified, and most human cases (65-7%) were found to be caused by only seven different genotypes. Source association of genotypes was used to identify risk factors at the genotype-level through multivariable logistic regression and a spatial model. Poultry-associated cases were more likely to be found in urban areas compared to rural areas. In particular young children in rural areas had a higher risk of infection with ruminant strains than their urban counterparts. These findings provide important information for the implementation of pathway-specific control strategies.|Molecular and spatial epidemiology of human campylobacteriosis: source association and genotype-related risk factors|http://www.jstor.org/stable/40801195|40801195|2010-10-01|2010|['eng']|['Health sciences - Medical conditions']|['Health Sciences', 'Medicine and Allied Health']
Data augmentation, sometimes known as the method of auxiliary variables, is a powerful tool for constructing optimisation and simulation algorithms. In the context of optimisation, Meng &amp; van Dyk (1997, 1998) reported several successes of the `working parameter' approach for constructing efficient data-augmentation schemes for fast and simple EM-type algorithms. This paper investigates the use of working parameters in the context of Markov chain Monte Carlo, in particular in the context of Tanner &amp; Wong's (1987) data augmentation algorithm, via a theoretical study of two working-parameter approaches, the conditional augmentation approach and the marginal augmentation approach. Posterior sampling under the univariate t model is used as a running example, which particularly illustrates how the marginal augmentation approach obtains a fast-mixing positive recurrent Markov chain by first constructing a nonpositive recurrent Markov chain in a larger space.|Seeking Efficient Data Augmentation Schemes via Conditional and Marginal Augmentation|http://www.jstor.org/stable/2673513|2673513|1999-06-01|1999|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
The marketing manager faces several dilemmas when analyzing multivariate frequency data. If the choice is to analyze a series of two-dimensional condensed tables, the interrelationships between those factors not in the table will be lost and biased inferences can result. If the decision is to analyze the complete multiway table, many of the cells may be sparse. The authors address the issue of how best to handle sparse-cell values in the context of a marketing data set relating store choice behavior to a number of shopper-specific variables. A simple new approach to this problem, which utilizes loglinear modeling techniques, is developed and contrasted with alternative remedies. The results of the comparative analysis show the proposed approach performs well, especially in the correct classification of seemingly unclassifiable shoppers.|Analyzing Qualitative Predictors with Too Few Data: An Alternative Approach to Handling Sparse-Cell Values|http://www.jstor.org/stable/3151314|3151314|1981-02-01|1981|['eng']|['Mathematics - Mathematical logic']|['Business', 'Business & Economics Collection', 'Marketing & Advertising']
We propose a model-based vulnerability index of the population from Uruguay to vector-borne diseases. We have available measurements of a set of variables in the census tract level of the 19 Departmental capitals of Uruguay. In particular, we propose an index that combines different sources of information via a set of micro-environmental indicators and geographical location in the country. Our index is based on a new class of spatially hierarchical factor models that explicitly account for the different levels of hierarchy in the country, such as census tracts within the city level, and cities in the country level. We compare our approach with that obtained when data are aggregated in the city level. We show that our proposal outperforms current and standard approaches, which fail to properly account for discrepancies in the region sizes, for example, number of census tracts. We also show that data aggregation can seriously affect the estimation of the cities vulnerability rankings under benchmark models.|MEASURING THE VULNERABILITY OF THE URUGUAYAN POPULATION TO VECTOR-BORNE DISEASES VIA SPATIALLY HIERARCHICAL FACTOR MODELS|http://www.jstor.org/stable/41713450|41713450|2012-03-01|2012|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical objects']|['Mathematics', 'Science & Mathematics', 'Statistics']
We study the asymptotic behaviour of the posterior distribution in a mixture model when the number of components in the mixture is larger than the true number of components: a situation which is commonly referred to as an overfitted mixture. We prove in particular that quite generally the posterior distribution has a stable and interesting behaviour, since it tends to empty the extra components. This stability is achieved under some restriction on the prior, which can be used as a guideline for choosing the prior. Some simulations are presented to illustrate this behaviour.|Asymptotic behaviour of the posterior distribution in overfitted mixture models|http://www.jstor.org/stable/41262270|41262270|2011-11-01|2011|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Bayesian analysis of system failure data from engineering applications under a competing risks framework is considered when the cause of failure may not have been exactly identified but has only been narrowed down to a subset of all potential risks. In statistical literature, such data are termed masked failure data. In addition to masking, failure times could be right censored owing to the removal of prototypes at a prespecified time or could be interval censored in the case of periodically acquired readings. In this setting, a general Bayesian formulation is investigated that includes most commonly used parametric lifetime distributions and that is sufficiently flexible to handle complex forms of censoring. The methodology is illustrated in two engineering applications with a special focus on model comparison issues.|Bayesian Analysis of Competing Risks with Partially Masked Cause of Failure|http://www.jstor.org/stable/3592633|3592633|2003-01-01|2003|['eng']|['Information science - Coding theory', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
In this article we present a Bayesian hierarchical modeling approach for imaging genetics, where the interest lies in linking brain connectivity across multiple individuals to their genetic information. We have available data from a functional magnetic resonance imaging (fMRl) study on schizophrenia. Our goals are to identify brain regions of interest (ROIs) with discriminating activation patterns between schizophrenic patients and healthy controls, and to relate the ROIs' activations with available genetic information from single nucleotide polymorphisms (SNPs) on the subjects. For this task, we develop a hierarchical mixture model that includes several innovative characteristics: it incorporates the selection of ROIs that discriminate the subjects into separate groups; it allows the mixture components to depend on selected covariates; it includes prior models that capture structural dependencies among the ROIs. Applied to the schizophrenia dataset, the model leads to the simultaneous selection of a set of discriminatory ROIs and the relevant SNPs, together with the reconstruction of the correlation structure of the selected regions. To the best of our knowledge, our work represents the first attempt at a rigorous modeling strategy for imaging genetics data that incorporates all such features.|An Integrative Bayesian Modeling Approach to Imaging Genetics|http://www.jstor.org/stable/24246871|24246871|2013-09-01|2013|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Tropospheric ozone is one of the six criteria pollutants regulated by the United States Environmental Protection Agency under the Clean Air Act and has been linked with several adverse health effects, including mortality. Due to the strong dependence on weather conditions, ozone may be sensitive to climate change and there is great interest in studying the potential effect of climate change on ozone, and how this change may affect public health. In this paper we develop a Bayesian spatial model to predict ozone under different meteorological conditions, and use this model to study spatial and temporal trends and to forecast ozone concentrations under different climate scenarios. We develop a spatial quantile regression model that does not assume normality and allows the covariates to affect the entire conditional distribution, rather than just the mean. The conditional distribution is allowed to vary from site-to-site and is smoothed with a spatial prior.For extremely large datasets our model is computationally infeasible, and we develop an approximate method. We apply the approximate version of our model to summer ozone from 1997-2005 in the Eastern U.S., and use deterministic climate models to project ozone under future climate conditions. Our analysis suggests that holding all other factors fixed, an increase in daily average temperature will lead to the largest increase in ozone in the Industrial Midwest and Northeast.|Bayesian Spatial Quantile Regression|http://www.jstor.org/stable/41415529|41415529|2011-03-01|2011|['eng']|['Physical sciences - Astronomy']|['Science and Mathematics', 'Statistics']
In the modelling of longitudinal data from several groups, appropriate handling of the dependence structure is of central importance. Standard methods include specifying a single covariance matrix for all groups or independently estimating the covariance matrix for each group without regard to the others, but when these model assumptions are incorrect, these techniques can lead to biased mean effects or loss of efficiency, respectively. Thus, it is desirable to develop methods for simultaneously estimating the covariance matrix for each group that will borrow strength across groups in a way that is ultimately informed by the data. In addition, for several groups with covariance matrices of even medium dimension, it is difficult to manually select a single best parametric model among the huge number of possibilities given by incorporating structural zeros and/or commonality of individual parameters across groups. In this paper we develop a family of nonparametric priors using the matrix stick-breaking process of Dunson et al. (2008) that seeks to accomplish this task by parameterizing the covariance matrices in terms of their modified Cholesky decompositions (Pourahmadi, 1999). We establish some theoretical properties of these priors, examine their effectiveness via a simulation study, and illustrate the priors using data from a longitudinal clinical trial.|A nonparametric prior for simultaneous covariance estimation|http://www.jstor.org/stable/43304541|43304541|2013-03-01|2013|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
"We present a Bayesian mark—recapture method for explicity communicating uncertainty about the size of a closed population where capture probabilities vary across both individuals and sampling occasions. Heterogeneity is modeled hierarchically using a continuous logistic-Normal model to specify the capture probabilities for both individuals that are captured on at least one occasion and individuals that are never captured and so remain undetected. Inference about how many undetected individuals to include in the model is accomplished through a Bayesian model selection procedure using MCMC, applied to a product space of possible models for different numbers of undetected individuals. Setting the estimation problem in a fixed dimensional parameter space enables the model selection procedure to be performed using the freely available WinBUGS software. The outcome of inference is a full ""posterior"" probability distribution for the population size parameter. We demonstrate this method through an example involving real mark—recapture data."|Mark: Recapture with Occasion and Individual Effects: Abundance Estimation through Bayesian Model Selection in a Fixed Dimensional Parameter Space|http://www.jstor.org/stable/27595564|27595564|2005-09-01|2005|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Agriculture', 'Statistics']
"Bayes factors against various hypotheses of independence are proposed for contingency tables and for multidimensional contingency tables. The priors assumed for the nonnull hypothesis are linear combinations of symmetric Dirichlet distributions as in some work of 1965 and later. The results can be used also for probability estimation. The evidence concerning independence, provided by the marginal totals alone, is evaluated, and preliminary numerical calculations suggest it is small. The possibility of applying the Bayes/non-Bayes synthesis is proposed because it was found useful for an analogous problem for multinomial distributions. As a spinoff, approximate formulae are suggested for enumerating ""arrays"" in two and more dimensions."|On the Application of Symmetric Dirichlet Distributions and their Mixtures to Contingency Tables|http://www.jstor.org/stable/2958586|2958586|1976-11-01|1976|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
We review applications of Gibbs sampling in medicine, involving longitudinal, spatial, covariate measurement and survival models. Applications in immunology, pharmacology, transplantation, cancer screening, industrial epidemiology and genetic epidemiology are discussed.|Modelling Complexity: Applications of Gibbs Sampling in Medicine|http://www.jstor.org/stable/2346065|2346065|1993-01-01|1993|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
A general class of statistical models for a univariate response variable is presented which we call the generalized additive model for location, scale and shape (GAMLSS). The model assumes independent observations of the response variable y given the parameters, the explanatory variables and the values of the random effects. The distribution for the response variable in the GAMLSS can be selected from a very general family of distributions including highly skew or kurtotic continuous and discrete distributions. The systematic part of the model is expanded to allow modelling not only of the mean (or location) but also of the other parameters of the distribution of y, as parametric and/or additive nonparametric (smooth) functions of explanatory variables and/or random-effects terms. Maximum (penalized) likelihood estimation is used to fit the (non)parametric models. A Newton-Raphson or Fisher scoring algorithm is used to maximize the (penalized) likelihood. The additive terms in the model are fitted by using a backfitting algorithm. Censored data are easily incorporated into the framework. Five data sets from different fields of application are analysed to emphasize the generality of the GAMLSS class of models.|Generalized Additive Models for Location, Scale and Shape|http://www.jstor.org/stable/3592732|3592732|2005-01-01|2005|['eng']|['Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
"Colonization-competition trade-offs represent a stabilizing mechanism that is thought to maintain diversity of forest trees. If so, then early-successional species should benefit from high capacity to colonize new sites, and late-successional species should be good competitors. Tests of this hypothesis in forests have been precluded by an inability to estimate the many factors that contribute to seed production and dispersal, particularly the many types of stochasticity that contribute to fecundity data. We develop a hierarchical Bayes modeling structure, and we use it to estimate fecundity schedules from the two types of data that ecologists typically collect, including seed-trap counts and observations of tree status. The posterior density is obtained using Markov-chain Monte Carlo techniques. The flexible structure yields estimates of size and covariate effects on seed production, variability associated with population heterogeneity, and interannual stochasticity (variability and serial autocorrelation), sex ratio, and dispersal. It admits the errors in data associated with the ability to accurately recognize tree status and process misspecification. We estimate year-by-year seed-production rates for all individuals in each of nine sample stands from two regions and up to 11 years. A rich characterization of differences among species and relationships among individuals allows evaluation of a number of hypotheses related to masting, effective population sizes, and location and covariate effects. It demonstrates large bias in previous methods. We focus on implications for colonization-competition and a related hypothesis, the successional niche-trade-offs in the capacity to exploit high resource availability in early successional environments vs. the capacity to survive low-resource conditions late in succession. Contrary to predictions of trade-off hypotheses, we find no relationship between successional status and fecundity, dispersal, or expected arrivals at distant sites. Results suggest a mechanism for maintenance of diversity that may be more general than colonization-competition and successional niches. High variability and strong individual effects (variability within populations) generate massive stochasticity in recruitment that, when combined with ""storage,"" may provide a stabilizing mechanism. The storage effect stabilizes diversity when species differences ensure that responses to stochasticity are not highly correlated among species. Process variability and individual effects mean that many species have the advantage at different times and places even in the absence of ""deterministic"" trade-offs. Not only does colonization vary among species, but also individual behavior is highly stochastic and weakly correlated among members of the same population. Although these factors are the dominant sources of variability in data sets (substantially larger than the deterministic relationships typically examined), they have not been not included in the models that ecologists have used to evaluate mechanisms of species coexistence (e.g., even individual-based models lack random individual effects). Recognition of the mechanisms of coexistence requires not only heuristic models that capture the principal sources of stochasticity, but also data-modeling techniques that allow for their estimation."|Fecundity of Trees and the Colonization-Competition Hypothesis|http://www.jstor.org/stable/4539065|4539065|2004-08-01|2004|['eng']|['Biological sciences - Biology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
1. Nesting birds can be vulnerable to predation. Wildlife managers sometimes manipulate predator communities to enhance avian productivity and abundance. Managers need to know the predation risk from different predator species responsible for nest failures to maximize success. This issue is especially important when considering reductions in only a part of the predator community in complex ecosystems. 2. We conducted a 7-year crossover experiment at four study sites to examine the effect of mesomammalian predator control on nest success of northern bobwhite Colinus virginianus in the southeastern USA. Nests were monitored using 24-h near-infrared video. We hypothesized that nest failures caused by different predator guilds may not be independent and may lead to compensation by other predators as one predator guild was reduced. 3. We compared levels of bobwhite nest predation by mesomammals, snakes and other predators in years with and without mesomammal control. 4. Control of mesomammal predators reduced the levels of mesomammal nest predation, but predation levels by snakes and other predators increased such that total nest mortality was not reduced. Nest mortality among predator groups was best described as compensatory, and total nest mortality differed among sites. 5. Synthesis and applications. Our findings suggest that reductions in predation risk from one predator guild can be compensated by an increased risk from other predators in complex ecosystems. Predator removal within one group may not translate to additive increases in overall nest success, but rather results in shifts in the identity of predators responsible for nest failures. Management efforts focused on manipulating predator communities to enhance avian reproduction are encouraged to examine cause-specific nest fates to determine the effectiveness of predator reduction programmes.|Predator reduction results in compensatory shifts in losses of avian ground nests|http://www.jstor.org/stable/23259063|23259063|2012-06-01|2012|['eng']|['Biological sciences - Ecology']|['Biological Sciences', 'Ecology & Evolutionary Biology', 'Science and Mathematics']
Accurate weather benefit many key societal functions and activities, including agriculture, transportation, recreation, and basic human and infrastructural safety. Over the past two decades, ensembles of numerical weather prediction models have been developed, in which multiple estimates of the current state of the atmosphere are used to generate probabilistic forecasts for future weather events. However, ensemble systems are uncalibrated and biased, and thus need to be statistically postprocessed. Bayesian model averaging (BMA) is a preferred way of doing this. Particularly for quantitative precipitation, biases and calibration errors depend critically on local terrain features. We introduce a geostatistical approach to modeling locally varying BMA parameters, as opposed to the extant method that holds parameters constant across the forecast domain. Degeneracies caused by enduring dry periods are overcome by Bayesian regularization and Laplace approximations. The new approach, called geostatistical model averaging (GMA), was applied to 48-hour-ahead forecasts of daily precipitation accumulation over the North American Pacific Northwest, using the eight-member University of Washington Mesoscale Ensemble. GMA had better aggregate and local calibration than the extant technique, and was sharper on average.|Geostatistical Model Averaging for Locally Calibrated Probabilistic Quantitative Precipitation Forecasting|http://www.jstor.org/stable/23239538|23239538|2011-12-01|2011|['eng']|['Physical sciences - Astronomy', 'Biological sciences - Biogeography']|['Science & Mathematics', 'Statistics']
Using data from the National Longitudinal Survey of Youth (NLSY) we introduce and estimate various Bayesian hierarchical models that investigate the nature of unobserved heterogeneity in returns to schooling. We consider a variety of possible forms for the heterogeneity, some motivated by previous theoretical and empirical work and some new ones, and let the data decide among the competing specifications. Empirical results indicate that heterogeneity is present in returns to education. Furthermore, we find strong evidence that the heterogeneity follows a continuous rather than a discrete distribution, and that bivariate normality provides a very reasonable description of individual-level heterogeneity in intercepts and returns to schooling.|Learning about Heterogeneity in Returns to Schooling|http://www.jstor.org/stable/25146329|25146329|2004-11-01|2004|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Business', 'Economics']
We conduct a case-study evaluating the source-specific effects of particulate matter on respiratory function. Using a structural equation approach, we assess the effect of different receptor models on the estimated source-specific effects for univariate respiratory response. Furthermore, we extend the structural equation model by placing a factor analysis model on the response to represent the measured respiratory responses in terms of underlying respiratory patterns. We estimate the particulate matter source-specific effects on respiratory rate, accentuated normal breathing and airway irritation and find a strong increase in airway irritation that is associated with exposure to motor vehicle particulate matter.|Statistical Methods to Evaluate Health Effects Associated with Major Sources of Air Pollution: A Case-Study of Breathing Patterns during Exposure to Concentrated Boston Air Particles|http://www.jstor.org/stable/20492609|20492609|2008-01-01|2008|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
We propose a nonparametric Bayes wavelet model for clustering of functional data. The wavelet-based methodology is aimed at the resolution of generic global and local features during clustering and is suitable for clustering high dimensional data. Based on the Dirichlet process, the nonparametric Bayes model extends the scope of traditional Bayes wavelet methods to functional clustering and allows the elicitation of prior belief about the regularity of the functions and the number of clusters by suitably mixing the Dirichlet processes. Posterior inference is carried out by Gibbs sampling with conjugate priors, which makes the computation straightforward. We use simulated as well as real data sets to illustrate the suitability of the approach over other alternatives.|Functional Clustering by Bayesian Wavelet Methods|http://www.jstor.org/stable/3647571|3647571|2006-01-01|2006|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
The processes influencing animal movement and resource selection are complex and varied. Past efforts to model behavioral changes over time used Bayesian statistical models with variable parameter space, such as reversible-jump Markov chain Monte Carlo approaches, which are computationally demanding and inaccessible to many practitioners. We present a continuous-time discrete-space (CTDS) model of animal movement that can be fit using standard generalized linear modeling (GLM) methods. This CTDS approach allows for the joint modeling of location-based as well as directional drivers of movement. Changing behavior over time is modeled using a varying-coefficient framework which maintains the computational simplicity of a GLM approach, and variable selection is accomplished using a group lasso penalty. We apply our approach to a study of two mountain lions (Puma concolor) in Colorado, USA.|CONTINUOUS-TIME DISCRETE-SPACE MODELS FOR ANIMAL MOVEMENT|http://www.jstor.org/stable/24522414|24522414|2015-03-01|2015|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
A vector stochastic process $X_{t}$ may be decomposed in to its expectation $\xi _{t}$ and a residual process $v_{t}$. A linear dynamic model is defined by a set of dynamic linear relations constraining the $\xi _{t}$'s given some conditioning variables and by the distribution of the $v_{t}$ process. This paper presents a strategy for the specification of this class of models providing computable posterior distributions for a suitable class of prior measures. Some conditional independence properties characterizing exogeneity conditions through global or sequential cuts, innovation property or non causality relations are studied and are shown to allow reductions by conditioning of the model. /// Un processus stochastique vectoriel $x_{t}$ peut être décomposé en son espérance $\xi _{t}$ et un résidu $v_{t}$. Un modèle linéaire dynamique est défini par un ensemble de relations dynamiques contraignant les $\xi _{t}$'s pour des variables conditionnantes données et par la loi du processus $v_{t}$. Cet article présente une stratégie pour la spécification de cette classe de modèles fournissant les lois a posteriori calculables pour une classe de lois a priori convenable. On montre que le modèle peut être réduit en utilisant quelques propriétés d'indépendance conditionnelle caractérisant les conditions d'exogénéité par des coupes globales ou séquentielles, la propriété d'innovation ou des relations de non causalité.|Dynamic Error-in-Variables Models and Limited Information Analysis|http://www.jstor.org/stable/20075658|20075658|1987-04-01|1987|['eng']|['Mathematics - Mathematical logic']|['Business & Economics', 'Mathematics', 'Science & Mathematics', 'Economics', 'Statistics']
We present new methodology to extend hidden Markov models to the spatial domain, and use this class of models to analyze spatial heterogeneity of count data on a rare phenomenon. This situation occurs commonly in many domains of application, particularly in disease mapping. We assume that the counts follow a Poisson model at the lowest level of the hierarchy, and introduce a finite-mixture model for the Poisson rates at the next level. The novelty lies in the model for allocation to the mixture components, which follows a spatially correlated process, the Potts model, and in treating the number of components of the spatial mixture as unknown. Inference is performed in a Bayesian framework using reversible jump Markov chain Monte Carlo. The model introduced can be viewed as a Bayesian semiparametric approach to specifying flexible spatial distribution in hierarchical models. Performance of the model and comparison with an alternative well-known Markov random field specification for the Poisson rates are demonstrated on synthetic datasets. We show that our allocation model avoids the problem of oversmoothing in cases where the underlying rates exhibit discontinuities, while giving equally good results in cases of smooth gradient-like or highly autocorrelated rates. The methodology is illustrated on an epidemiologic application to data on a rare cancer in France.|Hidden Markov Models and Disease Mapping|http://www.jstor.org/stable/3085830|3085830|2002-12-01|2002|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
A methodology is proposed to efficiently model a random set via a multistage hierarchical Bayesian model. We define a NonOverlapping Random Disk Model (NORDM), which is similar in spirit to the well-known Poisson-Boolean model. This model is formulated in a conditional setting that facilitates Bayesian sampling of important parameters in the model. This framework can accommodate any object, not just those with disk shapes, although the model can be easily extended to include any known compact convex set instead of the disc (e.g., polygons or ellipses). We further propose a growth model that is conceptually simple and allows straightforward estimation of parameters, without the need for tedious calculations of hitting or inclusion probabilities. The model is applied to severe storm cell development as obtained from weather radar.|A Bayesian Hierarchical Nonoverlapping Random Disc Growth Model|http://www.jstor.org/stable/40591917|40591917|2009-03-01|2009|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
We present an adaptive Bayesian method for dose-finding in phase I/II clinical trials based on trade-offs between the probabilities of treatment efficacy and toxicity. The method accommodates either trinary or bivariate binary outcomes, as well as efficacy probabilities that possibly are nonmonotone in dose. Doses are selected for successive patient cohorts based on a set of efficacy-toxicity trade-off contours that partition the two-dimensional outcome probability domain. Priors are established by solving for hyperparameters that optimize the fit of the model to elicited mean outcome probabilities. For trinary outcomes, the new algorithm is compared to the method of Thall and Russell (1998, Biometrics 54, 251-264) by application to a trial of rapid treatment for ischemic stroke. The bivariate binary outcome case is illustrated by a trial of graft-versus-host disease treatment in allogeneic bone marrow transplantation. Computer simulations show that, under a wide rage of dose-outcome scenarios, the new method has high probabilities of making correct decisions and treats most patients at doses with desirable efficacy-toxicity trade-offs.|Dose-Finding Based on Efficacy-Toxicity Trade-Offs|http://www.jstor.org/stable/3695390|3695390|2004-09-01|2004|['eng']|['Health sciences - Health and wellness', 'Information science - Coding theory']|['Science & Mathematics', 'Statistics']
"Let X be a nonnegative discrete random variable distributed according to an exponential family with natural parameter θ ∈ Θ. Subject to some regularity we characterize conjugate prior measures on Θ through the property of linear posterior expectation of the mean parameter of X : E{E(X|θ)|X = x} = ax + b. We also delineate some necessary conditions for the hyperparameters a and b, and find a necessary and sufficient condition that 0 &lt; a &lt; 1. Besides the power series distribution with parameter space bounded above (for example, the negative binomial distribution and the logarithmic series distribution) and the Poisson distribution, we apply the result to the log-zeta distribution and all hyper-Poisson distributions."|CHARACTERIZATION OF CONJUGATE PRIORS FOR DISCRETE EXPONENTIAL FAMILIES|http://www.jstor.org/stable/24306869|24306869|2001-04-01|2001|['eng']|['Mathematics - Mathematical logic']|['Mathematics', 'Science and Mathematics', 'Statistics']
This article considers deterministic computer experiments with real-valued tuning parameters which determine the accuracy of the numerical algorithm. A prominent example is finite-element analysis with its mesh density as the tuning parameter. The aim of this work is to integrate computer outputs with different tuning parameters. Novel nonstationary Gaussian process models are proposed to establish a framework consistent with the results in numerical analysis. Numerical studies show the advantages of the proposed method over existing methods. The methodology is illustrated with a problem in casting simulation. Supplementary material for this article is available online.|Surrogate Modeling of Computer Experiments With Different Mesh Densities|http://www.jstor.org/stable/24587247|24587247|2014-08-01|2014|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Dimension reduction is central to an analysis of data with many predictors. Sufficient dimension reduction aims to identify the smallest possible number of linear combinations of the predictors, called the sufficient predictors, that retain all of the information in the predictors about the response distribution. In this article, we propose a Bayesian solution for sufficient dimension reduction. We directly model the response density in terms of the sufficient predictors using a finite mixture model. This approach is computationally efficient and offers a unified framework to handle categorical predictors, missing predictors, and Bayesian variable selection. We illustrate the method using both a simulation study and an analysis of an HIV data set.|Sufficient Dimension Reduction via Bayesian Mixture Modeling|http://www.jstor.org/stable/41242537|41242537|2011-09-01|2011|['eng']|['Mathematics - Mathematical objects']|['Science and Mathematics', 'Statistics']
Destruction and fatalities from recent tornado outbreaks in North America have raised considerable concerns regarding their climatic and geographic variability. However, regional characterization of tornado activity in relation to large-scale climatic processes remains highly uncertain. Here, a novel Bayesian hierarchical framework is developed for elucidating the spatiotemporal variability of the factors underlying tornado occurrence in North America. It is demonstrated that regional variability of tornado activity can be characterized using a hierarchical parameterization of convective available potential energy, storm relative helicity, and vertical wind shear quantities. It is shown that the spatial variability of tornado occurrence during the warm summer season can be explained by convective available potential energy and storm relative helicity alone, while vertical wind shear is clearly better at capturing the spatial variability of the cool season tornado activity. The results suggest that the Bayesian hierarchical modeling approach is effective for understanding the regional tornadic environment and in forming the basis for establishing tornado prognostic tools in North America.|Predicting the Climatology of Tornado Occurrences in North America with a Bayesian Hierarchical Modeling Framework|http://www.jstor.org/stable/26385370|26385370|2016-03-01|2016|['eng']|['Physical sciences - Astronomy', 'Environmental studies - Atmospheric sciences']|
The vast amount of biological knowledge accumulated over the years has allowed researchers to identify various biochemical interactions and define different families of pathways. There is an increased interest in identifying pathways and pathway elements involved in particular biological processes. Drug discovery efforts, for example, are focused on identifying biomarkers as well as pathways related to a disease. We propose a Bayesian model that addresses this question by incorporating information on pathways and gene networks in the analysis of DNA microarray data. Such information is used to define pathway summaries, specify prior distributions, and structure the MCMC moves to fit the model. We illustrate the method with an application to gene expression data with censored survival outcomes. In addition to identifying markers that would have been missed otherwise and improving prediction accuracy, the integration of existing biological knowledge into the analysis provides a better understanding of underlying molecular processes.|INCORPORATING BIOLOGICAL INFORMATION INTO LINEAR MODELS: A BAYESIAN APPROACH TO THE SELECTION OF PATHWAYS AND GENES|http://www.jstor.org/stable/23069361|23069361|2011-09-01|2011|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
The Sloan digital sky survey is an extremely large astronomical survey that is conducted with the intention of mapping more than a quarter of the sky. Among the data that it is generating are spectroscopic and photometric measurements, both containing information about the red shift of galaxies. The former are precise and easy to interpret but expensive to gather; the latter are far cheaper but correspondingly more difficult to interpret. Recently, Csabai and co-workers have described various calibration techniques aiming to predict red shift from photometric measurements. We investigate what a structured Bayesian approach to the problem can add. In particular, we are interested in providing uncertainty bounds that are associated with the underlying red shifts and the classifications of the galaxies. We find that quite a generic statistical modelling approach, using for the most part standard model ingredients, can compete with much more specific custom-made and highly tuned techniques that are already available in the astronomical literature.|A Bayesian Hierarchical Model for Photometric Red Shifts|http://www.jstor.org/stable/20492620|20492620|2008-09-01|2008|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
We study Bayesian models and methods for analysing network traffic counts in problems of inference about the traffic intensity between directed pairs of origins and destinations in networks. This is a class of problems very recently discussed by Vardi in a 1996 JASA article and is of interest in both communication and transportation network studies. The current article develops the theoretical framework of variants of the origin-destination flow problem and introduces Bayesian approaches to analysis and inference. In the first, the so-called fixed routing problem, traffic or messages pass between nodes in a network, with each message originating at a specific source node, and ultimately moving through the network to a predetermined destination node. All nodes are candidate origin and destination points. The framework assumes no travel time complications, considering only the number of messages passing between pairs of nodes in a specified time interval. The route count, or route flow, problem is to infer the set of actual number of messages passed between each directed origin-destination pair in the time interval, based on the observed counts flowing between all directed pairs of adjacent nodes. Based on some development of the theoretical structure of the problem and assumptions about prior distributional forms, we develop posterior distributions for inference on actual origin-destination counts and associated flow rates. This involves iterative simulation methods, or Markov chain Monte Carlo (MCMC), that combine Metropolis-Hastings steps within an overall Gibbs sampling framework. We discuss issues of convergence and related practical matters, and illustrate the approach in a network previously studied in Vardi's article. We explore both methodological and applied aspects much further in a concrete problem of a road network in North Carolina, studied in transportation flow assessment contexts by civil engineers. This investigation generates critical insight into limitations of statistical analysis, and particularly of non-Bayesian approaches, due to inherent structural features of the problem. A truly Bayesian approach, imposing partial stochastic constraints through informed prior distributions, offers a way of resolving these problems and is consistent with prevailing trends in updating traffic flow intensities in this field. Following this, we explore a second version of the problem that introduces elements of uncertainty about routes taken by individual messages in terms of Markov selection of outgoing links for messages at any given node. For specified route choice probabilities, we introduce the concept of a super-network-namely, a fixed routing problem in which the stochastic problem may be embedded. This leads to solution of the stochastic version of the problem using the methods developed for the original formulation of the fixed routing problem. This is also illustrated. Finally, we discuss various related issues and model extensions, including inference on stochastic route choice selection probabilities, questions of missing data and partially observed link counts, and relationships with current research on road traffic network problems in which travel times within links are nonnegligible and may be estimated from additional data.|Bayesian Inference on Network Traffic Using Link Count Data|http://www.jstor.org/stable/2670105|2670105|1998-06-01|1998|['eng']|['Information science - Coding theory']|['Science & Mathematics', 'Statistics']
The strength of a significance test is defined as a weighted average of the power especially if the weights are a prior density. The concept is a compromise between Neyman-Pearsonian and Bayesian methods and is applied to tests for multinomial equiprobability and for no association in contingency tables. The differences of the power functions of two criteria, X2 and G, are illustrated for the trinomial, where G arose earlier from a partially Bayesian approach. Other methods for apprehending power functions of several variables are discussed. For 2 × 2 contingency tables with equal row or column totals the power and strength functions of several statistics are equal.|The Powers and Strengths of Tests for Multinomials and Contingency Tables|http://www.jstor.org/stable/2287310|2287310|1982-12-01|1982|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
This article introduces new methods for performing classification of complex, high-dimensional functional data using the functional mixed model (FMM) framework. The FMM relates a functional response to a set of predictors through functional fixed and random effects, which allows it to account for various factors and between-function correlations. The methods include training and prediction steps. In the training steps we train the FMM model by treating class designation as one of the fixed effects, and in the prediction steps we classify the new objects using posterior predictive probabilities of class. Through a Bayesian scheme, we are able to adjust for factors affecting both the functions and the class designations. While the methods can be used in any FMM framework, we provide details for two specific Bayesian approaches: the Gaussian, wavelet-based FMM (G-WFMM) and the robust, wavelet-based FMM (R-WFMM). Both methods perform modeling in the wavelet space, which yields parsimonious representations for the functions, and can naturally adapt to local features and complex nonstationarities in the functions. The R-WFMM allows potentially heavier tails for features of the functions indexed by particular wavelet coefficients, leading to a down-weighting of outliers that makes the method robust to outlying functions or regions of functions. The models are applied to a pancreatic cancer mass spectroscopy data set and compared with other recently developed functional classification methods.|Robust Classification of Functional and Quantitative Image Data Using Functional Mixed Models|http://www.jstor.org/stable/41806044|41806044|2012-12-01|2012|['eng']|['Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
The proposal and study of dependent prior processes has been a major research focus in the recent Bayesian nonparametric literature. In this paper, we introduce a flexible class of dependent nonparametric priors, investigate their properties and derive a suitable sampling scheme which allows their concrete implementation. The proposed class is obtained by normalizing dependent completely random measures, where the dependence arises by virtue of a suitable construction of the Poisson random measures underlying the completely random measures. We first provide general distributional results for the whole class of dependent completely random measures and then we specialize them to two specific priors, which represent the natural candidates for concrete implementation due to their analytic tractability: the bivariate Dirichlet and normalized σ-stable processes. Our analytical results, and in particular the partially exchangeable partition probability function, form also the basis for the determination of a Markov Chain Monte Carlo algorithm for drawing posterior inferences, which reduces to the well-known Blackwell-MacQueen Pólya urn scheme in the univariate case. Such an algorithm can be used for density estimation and for analyzing the clustering structure of the data and is illustrated through a real two-sample dataset example.|Bayesian inference with dependent normalized completely random measures|http://www.jstor.org/stable/42919431|42919431|2014-08-01|2014|['eng']|['Mathematics - Mathematical logic']|['Mathematics', 'Science and Mathematics', 'Statistics']
This research models the dynamics of customer relationships using typical transaction data. Our proposed model permits not only capturing the dynamics of customer relationships, but also incorporating the effect of the sequence of customer-firm encounters on the dynamics of customer relationships and the subsequent buying behavior. Our approach to modeling relationship dynamics is structurally different from existing approaches. Specifically, we construct and estimate a nonhomogeneous hidden Markov model to model the transitions among latent relationship states and effects on buying behavior. In the proposed model, the transitions between the states are a function of time-varying covariates such as customer-firm encounters that could have an enduring impact by shifting the customer to a different (unobservable) relationship state. The proposed model enables marketers to dynamically segment their customer base and to examine methods by which the firm can alter long-term buying behavior. We use a hierarchical Bayes approach to capture the unobserved heterogeneity across customers. We calibrate the model in the context of alumni relations using a longitudinal gift-giving data set. Using the proposed model, we probabilistically classify the alumni base into three relationship states and estimate the effect of alumni-university interactions, such as reunions, on the movement of alumni between these states. Additionally, we demonstrate improved prediction ability on a hold-out sample.|A Hidden Markov Model of Customer Relationship Dynamics|http://www.jstor.org/stable/40057096|40057096|2008-03-01|2008|['eng']|['Applied sciences - Engineering']|['Marketing & Advertising', 'Business & Economics', 'Business']
ABSTRACT Climate change impact studies are subject to numerous uncertainties and assumptions. One of the main sources of uncertainty arises from the interpretation of climate model projections. Probabilistic procedures based on multimodel ensembles have been suggested in the literature to quantify this source of uncertainty. However, the interpretation of multimodel ensembles remains challenging. Several assumptions are often required in the uncertainty quantification of climate model projections. For example, most methods often assume that the climate models are independent and/or that changes in climate model biases are negligible. This study develops a Bayesian framework that accounts for model dependencies and changes in model biases and compares it to estimates calculated based on a frequentist approach. The Bayesian framework is used to investigate the effects of the two assumptions on the uncertainty quantification of extreme precipitation projections over Denmark. An ensemble of regional climate models from the Ensemble-Based Predictions of Climate Changes and their Impacts (ENSEMBLES) project is used for this purpose. The results confirm that the climate models cannot be considered independent and show that the bias depends on the value of precipitation. This has an influence on the results of the uncertainty quantification. Both the mean and spread of the change in extreme precipitation depends on both assumptions. If the models are assumed independent and the bias constant, the results will be overconfident and may be treated as more precise than they really are. This study highlights the importance of investigating the underlying assumptions in climate change impact studies, as these may have serious consequences for the design of climate change adaptation strategies.|A Bayesian Approach for Uncertainty Quantification of Extreme Precipitation Projections Including Climate Model Interdependency and Nonstationary Bias|http://www.jstor.org/stable/26194094|26194094|2014-09-15|2014|['eng']|['Physical sciences - Astronomy']|
Markov chain Monte Carlo (MCMC) methods have been used extensively in statistical physics over the last 40 years, in spatial statistics for the past 20 and in Bayesian image analysis over the last decade. In the last five years, MCMC has been introduced into significance testing, general Bayesian inference and maximum likelihood estimation. This paper presents basic methodology of MCMC, emphasizing the Bayesian paradigm, conditional probability and the intimate relationship with Markov random fields in spatial statistics. Hastings algorithms are discussed, including Gibbs, Metropolis and some other variations. Pairwise difference priors are described and are used subsequently in three Bayesian applications, in each of which there is a pronounced spatial or temporal aspect to the modeling. The examples involve logistic regression in the presence of unobserved covariates and ordinal factors; the analysis of agricultural field experiments, with adjustment for fertility gradients; and processing of low-resolution medical images obtained by a gamma camera. Additional methodological issues arise in each of these applications and in the Appendices. The paper lays particular emphasis on the calculation of posterior probabilities and concurs with others in its view that MCMC facilitates a fundamental breakthrough in applied Bayesian modeling.|Bayesian Computation and Stochastic Systems|http://www.jstor.org/stable/2246224|2246224|1995-02-01|1995|['eng']|['Mathematics - Mathematical logic']|['Science and Mathematics', 'Statistics']
In a clinical trial, when there is a continuous flow of patients, response by a patient may be delayed. Our object in the present investigation is to use the Bayesian technique for studying the nature of such delay is the case of bivariate response. Taking suitable priors, several posterior distributions of the probabilities of response are obtained by using Gibbs sampler technique. The applicability of the present approach for grouped data at different stages is also discussed.|DELAYED RESPONSE IN BIVARIATE SET UP USING GIBBS SAMPLER|http://www.jstor.org/stable/43600964|43600964|2000-06-01|2000|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Mathematics', 'Statistics']
"Elicitation is a key task for subjectivist Bayesians. Although skeptics hold that elicitation cannot (or perhaps should not) be done, in practice it brings statisticians closer to their clients and subject-matter expert colleagues. This article reviews the state of the art, reflecting the experience of statisticians informed by the fruits of a long line of psychological research into how people represent uncertain information cognitively and how they respond to questions about that information. In a discussion of the elicitation process, the first issue to address is what it means for an elicitation to be successful; that is, what criteria should be used. Our answer is that a successful elicitation faithfully represents the opinion of the person being elicited. It is not necessarily ""true"" in some objectivistic sense, and cannot be judged in that way. We see that elicitation as simply part of the process of statistical modeling. Indeed, in a hierarchical model at which point the likelihood ends and the prior begins is ambiguous. Thus the same kinds of judgment that inform statistical modeling in general also inform elicitation of prior distributions. The psychological literature suggests that people are prone to certain heuristics and biases in how they respond to situations involving uncertainty. As a result, some of the ways of asking questions about uncertain quantities are preferable to others, and appear to be more reliable. However, data are lacking on exactly how well the various methods work, because it is unclear, other than by asking using an elicitation method, just what the person believes. Consequently, one is reduced to indirect means of assessing elicitation methods. The tool chest of methods is growing. Historically, the first methods involved choosing hyperparameters using conjugate prior families, at a time when these were the only families for which posterior distributions could be computed. Modern computational methods, such as Markov chain Monte Carlo, have freed elicitation from this constraint. As a result, now both parametric and nonparametric methods are available for low-dimensional problems. High-dimensional problems are probably best thought of as lacking another hierarchical level, which has the effect of reducing the as-yet-unelicited parameter space. Special considerations apply to the elicitation of group opinions. Informal methods, such as Delphi, encourage the participants to discuss the issue in the hope of reaching consensus. Formal methods, such as weighted averages or logarithmic opinion pools, each have mathematical characteristics that are uncomfortable. Finally, there is the question of what a group opinion even means, because it is not necessarily the opinion of any participant."|Statistical Methods for Eliciting Probability Distributions|http://www.jstor.org/stable/27590587|27590587|2005-06-01|2005|['eng']|['Philosophy - Logic', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
The tempo and mode of species diversification and phenotypic evolution vary widely across the tree of life, yet the relationship between these processes is poorly known. Previous tests of the relationship between rates of phenotypic evolution and rates of species diversification have assumed that species richness increases continuously through time. If this assumption is violated, simple phylogenetic estimates of net diversification rate may bear no relationship to processes that influence the distribution of species richness among clades. Here, we demonstrate that the variation in species richness among plethodontid salamander clades is unlikely to have resulted from simple time-dependent processes, leading to fundamentally different conclusions about the relationship between rates of phenotypic evolution and species diversification. Morphological evolutionary rates of both size and shape evolution are correlated with clade species richness, but are uncorrelated with simple estimators of net diversification that assume constancy of rates through time. This coupling between species diversification and phenotypic evolution is consistent with the hypothesis that clades with high rates of morphological trait evolution may diversify more than clades with low rates. Our results indicate that assumptions about underlying processes of diversity regulation have important consequences for interpreting macroevolutionary patterns.|RATES OF MORPHOLOGICAL EVOLUTION ARE CORRELATED WITH SPECIES RICHNESS IN SALAMANDERS|http://www.jstor.org/stable/41503483|41503483|2012-06-01|2012|['eng']|['Biological sciences - Ecology', 'Biological sciences - Paleontology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
We introduce a class of Gibbs-Markov random fields built on regular tessellations that can be understood as discrete counterparts of Arak-Surgailis polygonal fields. We focus first on consistent polygonal fields, for which we show consistency, Markovianity and solvability by means of dynamic representations. Next, we develop disagreement loop as well as path creation and annihilation dynamics for their general Gibbsian modifications, which cover most lattice-based Gibbs-Markov random fields subject to certain mild conditions. Applications to foreground-background image segmentation problems are discussed.|Disagreement Loop and Path Creation/Annihilation Algorithms for Binary Planar Markov Fields with Applications to Image Segmentation|http://www.jstor.org/stable/41000879|41000879|2010-06-01|2010|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
We present a methodology for rating in real-time the creditworthiness of public companies in the U.S. from the prices of traded assets. Our approach uses asset pricing data to impute a term structure of risk neutral survival functions or default probabilities. Firms are then clustered into ratings categories based on their survival functions using a functional clustering algorithm. This allows all public firms whose assets are traded to be directly rated by market participants. For firms whose assets are not traded, we show how they can be indirectly rated by matching them to firms that are traded based on observable characteristics. We also show how the resulting ratings can be used to construct loss distributions for portfolios of bonds. Finally, we compare our ratings to Standard &amp; Poors and find that, over the period 2005 to 2011, our ratings lead theirs for firms that ultimately default.|Market-Based Credit Ratings|http://www.jstor.org/stable/43702771|43702771|2014-07-01|2014|['eng']|['Information science - Coding theory']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
Time- and state-domain methods are two common approaches to nonparametric prediction. Whereas the former uses data predominantly from recent history, the latter relies mainly on historical information. Combining these two pieces of valuable information is an interesting challenge in statistics. We surmount this problem by dynamically integrating information from both the time and state domains. The estimators from these two domains are optimally combined based on a data-driven weighting strategy, which provides a more efficient estimator of volatility. Asymptotic normality is separately established for the time domain, the state domain, and the integrated estimators. By comparing the efficiency of the estimators, we demonstrate that the proposed integrated estimator uniformly dominates the other two estimators. The proposed dynamic integration approach is also applicable to other estimation problems in time series. Extensive simulations are conducted to demonstrate that the newly proposed procedure outperforms some popular ones, such as the RiskMetrics and historical simulation approaches, among others. In addition, empirical studies convincingly endorse our integration method.|Dynamic Integration of Time- and State-Domain Methods for Volatility Estimation|http://www.jstor.org/stable/27639891|27639891|2007-06-01|2007|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
We use the theory of normal variance-mean mixtures to derive a data-augmentation scheme for a class of common regularization problems. This generalizes existing theory on normal variance mixtures for priors in regression and classification. It also allows variants of the expectationmaximization algorithm to be brought to bear on a wider range of models than previously appreciated. We demonstrate the method on several examples, focusing on the case of binary logistic regression. We also show that quasi-Newton acceleration can substantially improve the speed of the algorithm without compromising its robustness.|Data augmentation for non-Gaussian regression models using variance-mean mixtures|http://www.jstor.org/stable/43304570|43304570|2013-06-01|2013|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
The analysis of a sequence of decision problems of identical structure is considered, and the similarities and differences between the compound decision and empirical Bayes approaches explored. The paper includes a brief review of the literature and examines the criteria which have been used in comparing compound decision and empirical Bayes procedures with conventional methods. The performance of one of these procedures is examined in detail, and the previously accepted view that the compound decision approach is appropriate for situations outside those covered by empirical Bayes is questioned.|Compound Decisions and Empirical Bayes|http://www.jstor.org/stable/2984345|2984345|1969-01-01|1969|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic', 'Mathematics - Mathematical analysis']|['Science & Mathematics', 'Statistics']
This paper provides a new approach to forecasting time series that are subject to discrete structural breaks. We propose a Bayesian estimation and prediction procedure that allows for the possibility of new breaks occurring over the forecast horizon, taking account of the size and duration of past breaks (if any) by means of a hierarchical hidden Markov chain model. Predictions are formed by integrating over the parameters from the meta-distribution that characterizes the stochastic break-point process. In an application to U.S. Treasury bill rates, we find that the method leads to better out-of-sample forecasts than a range of alternative methods.|Forecasting Time Series Subject to Multiple Structural Breaks|http://www.jstor.org/stable/4123259|4123259|2006-10-01|2006|['eng']|['Physical sciences - Astronomy']|['Business & Economics', 'Business', 'Economics']
In this article, we estimate the parameters of a simple random network and a stochastic epidemic on that network using data consisting of recovery times of infected hosts. The SEIR epidemic model we fit has exponentially distributed transmission times with Gamma distributed exposed and infectious periods on a network where every edge exists with the same probability, independent of other edges. We employ a Bayesian framework and Markov chain Monte Carlo (MCMC) integration to make estimates of the joint posterior distribution of the model parameters. We discuss the accuracy of the parameter estimates under various prior assumptions and show that it is possible in many scientifically interesting cases to accurately recover the parameters. We demonstrate our approach by studying a measles outbreak in Hagelloch, Germany, in 1861 consisting of 188 affected individuals. We provide an R package to carry out these analyses, which is available publicly on the Comprehensive R Archive Network.|Bayesian Inference for Contact Networks Given Epidemic Data|http://www.jstor.org/stable/23015583|23015583|2011-09-01|2011|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Parametric empirical Bayes (EB) methods of point estimation for a vector of unknown parameters date to the landmark paper of James and Stein (1961). The usual approach is to use the mean of the estimated posterior distribution of each parameter, where the estimation of the prior parameters (`hyperparameters') is accomplished through the marginal distribution of the data. While point estimates computed this way usually perform well, interval estimates based on the estimated posterior (called `naive' EB intervals) do not. They fail to account for the variability in the estimation of the hyperparameters, generally resulting in subnominal coverage probability in the EB sense defined in Morris (1983a). In this paper we extend our earlier work in which we proposed a conditional bias correction method for developing EB intervals which corrects the deficiencies in the naive intervals. We show how bias correction can be implemented in general via a type III parametric bootstrap procedure, a sample reuse method first employed by Laird and Louis (1987). Theoretical and simulation work indicates that the resulting intervals are accurate with respect to nominal coverage. We give a specific application (to Poisson failure rate data) where we compute simultaneous point and bias-corrected interval estimates.|A Sample Reuse Method for Accurate Parametric Empirical Bayes Confidence Intervals|http://www.jstor.org/stable/2345734|2345734|1991-01-01|1991|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Characterizing the process by which molecular and cellular level changes occur over time will have broad implications for clinical decision making and help further our knowledge of disease etiology across many complex diseases. However, this presents an analytic challenge due to the large number of potentially relevant biomarkers and the complex, uncharacterized relationships among them. We propose an exploratory Bayesian model selection procedure that searches for model simplicity through independence testing of multiple discrete biomarkers measured over time. Bayes factor calculations are used to identify and compare models that are best supported by the data. For large model spaces, i.e., a large number of multi-leveled biomarkers, we propose a Markov chain Monte Carlo (MCMC) stochastic search algorithm for finding promising models. We apply our procedure to explore the extent to which HIV-1 genetic changes occur independently over time.|Exploratory Bayesian Model Selection for Serial Genetics Data|http://www.jstor.org/stable/3695982|3695982|2005-06-01|2005|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
The paper focuses on a Bayesian treatment of measurement error problems and on the question of the specification of the prior distribution of the unknown covariates. It presents a flexible semiparametric model for this distribution based on a mixture of normal distributions with an unknown number of components. Implementation of this prior model as part of a full Bayesian analysis of measurement error problems is described in classical set-ups that are encountered in epidemiological studies: logistic regression between unknown covariates and outcome, with a normal or log-normal error model and a validation group. The feasibility of this combined model is tested and its performance is demonstrated in a simulation study that includes an assessment of the influence of misspecification of the prior distribution of the unknown covariates and a comparison with the semiparametric maximum likelihood method of Roeder, Carroll and Lindsay. Finally, the methodology is illustrated on a data set on coronary heart disease and cholesterol levels in blood.|Mixture Models in Measurement Error Problems, with Reference to Epidemiological Studies|http://www.jstor.org/stable/3559702|3559702|2002-01-01|2002|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
The spectral density function plays a key role in fitting the tail of multivariate extremal data and so in estimating probabilities of rare events. This function satisfies moment constraints but unlike the univariate extreme value distributions has no simple parametric form. Parameterized subfamilies of spectral densities have been suggested for use in applications, and non-parametric estimation procedures have been proposed, but semiparametric models for multivariate extremes have hitherto received little attention. We show that mixtures of Dirichlet distributions satisfying the moment constraints are weakly dense in the class of all non-parametric spectral densities, and discuss frequentist and Bayesian inference in this class based on the EM algorithm and reversible jump Markov chain Monte Carlo simulation. We illustrate the ideas using simulated and real data.|A Mixture Model for Multivariate Extremes|http://www.jstor.org/stable/4623264|4623264|2007-01-01|2007|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
We consider the problem of variable selection in regression modeling in high-dimensional spaces where there is known structure among the covariates. This is an unconventional variable selection problem for two reasons: (1) The dimension of the covariate space is comparable, and often much larger, than the number of subjects in the study, and (2) the covariate space is highly structured, and in some cases it is desirable to incorporate this structural information in to the model building process. We approach this problem through the Bayesian variable selection framework, where we assume that the covariates lie on an undirected graph and formulate an Ising prior on the model space for incorporating structural information. Certain computational and statistical problems arise that are unique to such high-dimensional, structured settings, the most interesting being the phenomenon of phase transitions. We propose theoretical and computational schemes to mitigate these problems. We illustrate our methods on two different graph structures: the linear chain and the regular graph of degree k. Finally, we use our methods to study a specific application in genomics: the modeling of transcription factor binding sites in DNA sequences.|Bayesian Variable Selection in Structured High-Dimensional Covariate Spaces With Applications in Genomics|http://www.jstor.org/stable/27920144|27920144|2010-09-01|2010|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Population pharmacokinetic and pharmacodynamic studies require analyzing nonlinear growth curves fit to multiple measurements from study subjects. We propose a class of nonlinear population models with nonparametric second-stage priors for analyzing such data. The proposed models apply a flexible class of mixtures to implement the nonparametric second stage. The discussion is based on a pharmacodynamic study involving longitudinal data consisting of hematologic profiles (i.e., blood counts measured over time) of cancer patients undergoing chemotherapy. We describe a full posterior analysis in a Bayesian framework. This includes prediction of future observations (profiles and end points for new patients), estimation of the mean response function for observed individuals, and inference on population characteristics. The mixture model is specified and given a hyperprior distribution by means of a Dirichlet processes prior on the mixing measure. Estimation is implemented by a combination of various Markov chain Monte Carlo schemes, including a novel independence chain scheme for a logistic regression. The discussion is motivated by a pharmacodynamic case study; however, the concepts are more generally applicable to the wider class of population models.|A Bayesian Population Model With Hierarchical Mixture Priors Applied to Blood Count Data|http://www.jstor.org/stable/2965398|2965398|1997-12-01|1997|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Model choice plays an increasingly important role in statistics. From a Bayesian perspective a crucial goal is to compute the marginal likelihood of the data for a given model. However, this is typically a difficult task since it amounts to integrating over all model parameters. The aim of the paper is to illustrate how this may be achieved by using ideas from thermodynamic integration or path sampling. We show how the marginal likelihood can be computed via Markov chain Monte Carlo methods on modified posterior distributions for each model. This then allows Bayes factors or posterior model probabilities to be calculated. We show that this approach requires very little tuning and is straightforward to implement. The new method is illustrated in a variety of challenging statistical settings.|Marginal Likelihood Estimation via Power Posteriors|http://www.jstor.org/stable/20203843|20203843|2008-01-01|2008|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
A vector of continuous proportions consists of the proportions of some total accounted for by its constituent components. An example is the proportions of world motor vehicle production by Japan, the USA and all other countries. We consider the situation where time series data are available and where interest focuses on the proportions rather than the actual amounts. Reasons for analysing such times series include estimation of the underlying trend, estimation of the effect of covariates and interventions, and forecasting. We develop a state space model for time series of continuous proportions. Conditionally on the unobserved state, the observations are assumed to follow the Dirichlet distribution, often considered to be the most natural distribution on the simplex. The state follows the Dirichlet conjugate distribution which is introduced here. Thus the model, although based on the Dirichlet distribution, does not have its restrictive independence properties. Covariates, trends, seasonality and interventions may be incorporated in a natural way. The model has worked well when applied to several examples, and we illustrate with components of world motor vehicle production.|Time Series of Continuous Proportions|http://www.jstor.org/stable/2346067|2346067|1993-01-01|1993|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
We propose and illustrate a hierarchical Bayesian approach for matching statistical records observed on different occasions. We show how this model can be profitably adopted both in record linkage problems and in capturerecapture setups, where the size of a finite population is the real object of interest. There are at least two important differences between the proposed model-based approach and the current practice in record linkage. First, the statistical model is built up on the actually observed categorical variables and no reduction (to 0—1 comparisons) of the available information takes place. Second, the hierarchical structur of the model allows a two-way propagation of the uncertainty between the parameter estimation step and the matching procedure so that no plug-in estimates are used and the correct uncertainty is accounted for both in estimating the population size and in performing the record linkage. We illustrate and motivate our proposal through a real data example and simulations.|A HIERARCHICAL BAYESIAN APPROACH TO RECORD LINKAGE AND POPULATION SIZE PROBLEMS1|http://www.jstor.org/stable/23024863|23024863|2011-06-01|2011|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Mathematics', 'Science & Mathematics', 'Statistics']
The primary goal of multivariate statistical process performance monitoring is to identify deviations from normal operation within a manufacturing process. The basis of the monitoring schemes is historical data that have been collected when the process is running under normal operating conditions. These data are then used to establish confidence bounds to detect the onset of process deviations. In contrast with the traditional approaches that are based on the Gaussian assumption, this paper proposes the application of the infinite Gaussian mixture model (GMM) for the calculation of the confidence bounds, thereby relaxing the previous restrictive assumption. The infinite GMM is a special case of Dirichlet process mixtures and is introduced as the limit of the finite GMM, i.e. when the number of mixtures tends to ∞. On the basis of the estimation of the probability density function, via the infinite GMM, the confidence bounds are calculated by using the bootstrap algorithm. The methodology proposed is demonstrated through its application to a simulated continuous chemical process, and a batch semiconductor manufacturing process.|Probability Density Estimation via an Infinite Gaussian Mixture Model: Application to Statistical Process Monitoring|http://www.jstor.org/stable/3879119|3879119|2006-01-01|2006|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Using computer simulations, the finite sample performance of a number of classical and Bayesian wavelet shrinkage estimators for Poisson counts is examined. For the purpose of comparison, a variety of intensity functions, background intensity levels, sample sizes, primary resolution levels, wavelet filters and performance criteria are employed. A demonstration is given of the use of some of the estimators to analyse a data set arising in high-energy astrophysics. Following the philosophy of reproducible research, the Matlab programs and real-life data example used in this study are made freely available. /// Nous étudions l'aide de simulations numériques, le comportement à taille d'échantillon fini d'un certain nombre d'estimateurs de l'intensité de comptages Poissonniens, fondés sur des procédures de seuillage, classiques et bayesiennes, des coefficients d'ondelettes. A cet effet nous employons une large gamme de fonctions d'intensité, de bruit de fond, de tailles d'échantillons, de résolution primaire de décomposition et de famille d'ondelettes et des critères de performance variés. Les méthodes sont illustrées sur un exemple réel issus d'astrophysique. Selon les principes d'une recherche reproductible les procédures Matlab et les exemples traités sont gracieusement disponibles.|A Comparative Simulation Study of Wavelet Shrinkage Estimators for Poisson Counts|http://www.jstor.org/stable/1403855|1403855|2004-08-01|2004|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics', 'Mathematics - Mathematical analysis']|['Science & Mathematics', 'Statistics']
The CRASH computer model simulates the effect of a vehicle colliding against different barrier types. If it accurately represents real vehicle crashworthiness, the computer model can be of great value in various aspects of vehicle design, such as the setting of timing of air bag releases. The goal of this study is to address the problem of validating the computer model for such design goals, based on utilizing computer model runs and experimental data from real crashes. This task is complicated by the fact that (i) the output of this model consists of smooth functional data, and (ii) certain types of collision have very limited data. We address problem (i) by extending existing Gaussian process-based methodology developed for models that produce real-valued output, and resort to Bayesian hierarchical modeling to attack problem (ii). Additionally, we show how to formally test if the computer model reproduces reality. Supplemental materials for the article are available online.|Predicting Vehicle Crashworthiness: Validation of Computer Models for Functional and Hierarchical Data|http://www.jstor.org/stable/40592265|40592265|2009-09-01|2009|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
This article develops a comprehensive approach to formally introduce the prior for impulse responses into the Bayesian structural VAR (SVAR) models, with considerable attention paid to careful prior specification. Furthermore, we present an efficient algorithm to estimate the SVAR with a Gaussian prior for impulse responses using simulation methods. We also illustrate our methodology using an example in which we verifiy some facts concerning the impact of monetary policy shock on the economy.|A Prior for Impulse Responses in Bayesian Structural VAR Models|http://www.jstor.org/stable/27799118|27799118|2010-01-01|2010|['eng']|['Mathematics - Mathematical logic']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
We propose a Bayesian approach for inference in the multivariate probit model, taking into account the association structure between binary observations. We model the association through the correlation matrix of the latent Gaussian variables. Conditional independence is imposed by setting some off-diagonal elements of the inverse correlation matrix to zero and this sparsity structure is modeled using a decomposable graphical model. We propose an efficient Markov chain Monte Carlo algorithm relying on a parameter expansion scheme to sample from the resulting posterior distribution. This algorithm updates the correlation matrix within a simple Gibbs sampling framework and allows us to infer the correlation structure from the data, generalizing methods used for inference in decomposable Gaussian graphical models to multivariate binary observations. We demonstrate the performance of this model and of the Markov chain Monte Carlo algorithm on simulated and real datasets. This article has online supplementary materials.|Efficient Bayesian Inference for Multivariate Probit Models With Sparse Inverse Correlation Matrices|http://www.jstor.org/stable/41739816|41739816|2012-09-01|2012|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Computer Science', 'Statistics']
Hierarchical modeling is wonderful and here to stay, but hyperparameter priors are often chosen in a casual fashion. Unfortunately, as the number of hyperparameters grows, the effects of casual choices can multiply, leading to considerably inferior performance. As an extreme, but not uncommon, example use of the wrong hyperparameter priors can even lead to impropriety of the posterior. For exchangeable hierarchical multivariate normal models, we first determine when a standard class of hierarchical priors results in proper or improper posteriors. We next determine which elements of this class lead to admissible estimators of the mean under quadratic loss; such considerations provide one useful guideline for choice among hierarchical priors. Finally, computational issues with the resulting posterior distributions are addressed.|Posterior Propriety and Admissibility of Hyperpriors in Normal Hierarchical Models|http://www.jstor.org/stable/3448601|3448601|2005-04-01|2005|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
This article proposes a new semiparametric Bayesian hierarchical model for the joint modeling of longitudinal and survival data. We relax the distributional assumptions for the longitudinal model using Dirichlet process priors on the parameters defining the longitudinal model. The resulting posterior distribution of the longitudinal parameters is free of parametric constraints, resulting in more robust estimates. This type of approach is becoming increasingly essential in many applications, such as HIV and cancer vaccine trials, where patients' responses are highly diverse and may not be easily modeled with known distributions. An example will be presented from a clinical trial of a cancer vaccine where the survival outcome is time to recurrence of a tumor. Immunologic measures believed to be predictive of tumor recurrence were taken repeatedly during follow-up. We will present an analysis of this data using our new semiparametric Bayesian hierarchical joint modeling methodology to determine the association of these longitudinal immunologic measures with time to tumor recurrence.|A Bayesian Semiparametric Joint Hierarchical Model for Longitudinal and Survival Data|http://www.jstor.org/stable/3695499|3695499|2003-06-01|2003|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
It is shown how to discriminate between different linear Gaussian state space models for a given time series by means of a Bayesian approach which chooses the model that minimizes the expected loss. A practical implementation of this procedure requires a fully Bayesian analysis for both the state vector and the unknown hyperparameters and is carried out by Markov chain Monte Carlo methods. An application to some non-standard situations such as testing hypothesis on the boundary of the parameter space, discriminating non-nested models and discrimination of more than two models is discussed in detail.|Bayesian Model Discrimination and Bayes Factors for Linear Gaussian State Space Models|http://www.jstor.org/stable/2346097|2346097|1995-01-01|1995|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Equivalence testing for scalar data has been well addressed in the literature, however, the same cannot be said for functional data. The resultant complexity from maintaining the functional structure of the data, rather than using a scalar transformation to reduce dimensionality, renders the existing literature on equivalence testing inadequate for the desired inference. We propose a framework for equivalence testing for functional data within both the frequentist and Bayesian paradigms. This framework combines extensions of scalar methodologies with new methodology for functional data. Our frequentist hypothesis test extends the Two One-Sided Testing (TOST) procedure for equivalence testing to the functional regime. We conduct this TOST procedure through the use of the nonparametric bootstrap. Our Bayesian methodology employs a functional analysis of variance model, and uses a flexible class of Gaussian Processes for both modeling our data and as prior distributions. Through our analysis, we introduce a model for heteroscedastic variances within a Gaussian Process by modeling variance curves via Log-Gaussian Process priors. We stress the importance of choosing prior distributions that are commensurate with the prior state of knowledge and evidence regarding practical equivalence. We illustrate these testing methods through data from an ongoing method comparison study between two devices for pulmonary function testing. In so doing, we provide not only concrete motivation for equivalence testing for functional data, but also a blueprint for researchers who hope to conduct similar inference.|EQUIVALENCE TESTING FOR FUNCTIONAL DATA WITH AN APPLICATION TO COMPARING PULMONARY FUNCTION DEVICES|http://www.jstor.org/stable/24522373|24522373|2014-12-01|2014|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical analysis', 'Mathematics - Mathematical logic']|['Mathematics', 'Science & Mathematics', 'Statistics']
Inferences in measurement error models can be sensitive to modeling assumptions. Specifically, if the model is incorrect, the estimates can be inconsistent. To reduce sensitivity to modeling assumptions and yet still retain the efficiency of parametric inference, we propose using flexible parametric models that can accommodate departures from standard parametric models. We use mixtures of normals for this purpose. We study two cases in detail: a linear errors-in-variables model and a change-point Berkson model.|Flexible Parametric Measurement Error Models|http://www.jstor.org/stable/2533894|2533894|1999-03-01|1999|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
The authors consider the problem of searching for activation in brain images obtained from functional magnetic resonance imaging and the corresponding functional signal detection problem. They develop a Bayesian procedure to detect signals existing within noisy images when the image is modeled as a scale space random field. Their procedure is based on the Radon-Nikodym derivative, which is used as the Bayes factor for assessing the point null hypothesis of no signal. They apply their method to data from the Montreal Neurological Institute. /// Les auteurs s'intéressent au repérage de l'activation cérébrale à partir d'imagerie fonctionnelle par résonnance magnétique et au problème de détection du signal fonctionnel afférent. Ils développent une procédure bayésienne permettant de détecter un signal présent dans des images bruitées modélisées en champs aléatoires à espace d'échelle. Leur procédure s'appuie sur une dérivée de Radon-Nikodym qui sert de facteur de Bayes dans l'évaluation de l'hypothèse nulle d'absence de signal. Ils illustrent leur méthode au moyen de données provenant de l'Institut neurologique de Montréal.|A Bayesian Signal Detection Procedure for Scale-Space Random Fields|http://www.jstor.org/stable/20445203|20445203|2006-06-01|2006|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Networks of ambient monitoring stations are used to monitor environmental pollution fields such as those for acid rain and air pollution. Such stations provide regular measurements of pollutant concentrations. The networks are established for a variety of purposes at various times so often several stations measuring different subsets of pollutant concentrations can be found in compact geographical regions. The problem of statistically combining these disparate information sources into a single `network' then arises. Capitalizing on the efficiencies so achieved can then lead to the secondary problem of extending this network. The subject of this paper is a set of 31 air pollution monitoring stations in southern Ontario. Each of these regularly measures a particular subset of ionic sulphate, sulphite, nitrite and ozone. However, this subset varies from station to station. For example only two stations measure all four. Some measure just one. We describe a Bayesian framework for integrating the measurements of these stations to yield a spatial predictive distribution for unmonitored sites and unmeasured concentrations at existing stations. Furthermore we show how this network can be extended by using an entropy maximization criterion. The methods assume that the multivariate response field being measured has a joint Gaussian distribution conditional on its mean and covariance function. A conjugate prior is used for these parameters, some of its hyperparameters being fitted empirically.|Designing and Integrating Composite Networks for Monitoring Multivariate Gaussian Pollution Fields|http://www.jstor.org/stable/2680861|2680861|2000-01-01|2000|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
We consider the problem of variable selection in linear regression models. Bayesian model averaging has become an important tool in empirical settings with large numbers of potential regressors and relatively limited numbers of observations. We examine the effect of a variety of prior assumptions on the inference concerning model size, posterior inclusion probabilities of regressors and on predictive performance. We illustrate these issues in the context of cross-country growth regressions using three datasets with 41-67 potential drivers of growth and 72-93 observations. Finally, we recommend priors for use in this and related contexts.|On the Effect of Prior Assumptions in Bayesian Model Averaging with Applications to Growth Regression|http://www.jstor.org/stable/40206296|40206296|2009-06-01|2009|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Business', 'Economics']
Animal carcinogenicity studies, such as those conducted by the U.S. National Toxicology Program (NTP), focus on detecting trends in tumor rates across dose groups. Over time, the NTP has compiled vast amounts of data on tumors in control animals. Currently, this information is used informally, without the benefit of statistical tests for carcinogenicity that directly incorporate historical data on control animals. This article proposes a survival-adjusted test for detecting dose-related trends in tumor incidence rates, which incorporates data on historical control rates and formally accounts for variation in these rates among studies. An extensive simulation, based on a wide range of realistic situations, demonstrates that the proposed test performs well compared with the current NTP test, which does not incorporate historical control data. In particular, our test can aid in interpreting the occurence of a few tumors in treated animals that are rarely seen in controls. One such example, which motivates our work, concerns the analysis of histiocytic sarcoma in the NTP's 2-year cancer bioassay of benzophenone. Whereas the occurrence of three histiocytic sarcomas in female rats was not significant according to the current NTP testing procedure (p = .074), it was highly significant (p = .004) when control data from six recent historical studies were included and our test was applied to the combined data.|Incorporating Historical Control Data When Comparing Tumor Incidence Rates|http://www.jstor.org/stable/27639971|27639971|2007-12-01|2007|['eng']|['Health sciences - Health and wellness', 'Biological sciences - Biology']|['Science & Mathematics', 'Statistics']
Most regression problems in practice require flexible semiparametric forms of the predictor for modelling the dependence of responses on covariates. Moreover, it is often necessary to add random effects accounting for overdispersion caused by unobserved heterogeneity or for correlation in longitudinal or spatial data. We present a unified approach for Bayesian inference via Markov chain Monte Carlo simulation in generalized additive and semiparametric mixed models. Different types of covariates, such as the usual covariates with fixed effects, metrical covariates with non-linear effects, unstructured random effects, trend and seasonal components in longitudinal data and spatial covariates, are all treated within the same general framework by assigning appropriate Markov random field priors with different forms and degrees of smoothness. We applied the approach in several case-studies and consulting cases, showing that the methods are also computationally feasible in problems with many covariates and large data sets. In this paper, we choose two typical applications.|Bayesian Inference for Generalized Additive Mixed Models Based on Markov Random Field Priors|http://www.jstor.org/stable/2680887|2680887|2001-01-01|2001|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
"Understanding how genetic variants influence cellular-level processes is an important step toward understanding how they influence important organismal-level traits, or ""phenotypes,"" including human disease susceptibility. To this end, scientists are undertaking large-scale genetic association studies that aim to identify genetic variants associated with molecular and cellular phenotypes, such as gene expression, transcription factor binding, or chromatin accessibility. These studies use high-throughput sequencing assays (e.g., RNA-seq, ChIP-seq, DNase-seq) to obtain high-resolution data on how the traits vary along the genome in each sample. However, typical association analyses fail to exploit these high-resolution measurements, instead aggregating the data at coarser resolutions, such as genes, or windows of fixed length. Here we develop and apply statistical methods that better exploit the high-resolution data. The key idea is to treat the sequence data as measuring an underlying ""function"" that varies along the genome, and then, building on wavelet-based methods for functional data analysis, test for association between genetic variants and the underlying function. Applying these methods to identify genetic variants associated with chromatin accessibility (dsQTLs), we find that they identify substantially more associations than a simpler window-based analysis, and in total we identify 772 novel dsQTLs not identified by the original analysis."|WAVELET-BASED GENETIC ASSOCIATION ANALYSIS OF FUNCTIONAL PHENOTYPES ARISING FROM HIGH-THROUGHPUT SEQUENCING ASSAYS|http://www.jstor.org/stable/24522597|24522597|2015-06-01|2015|['eng']|['Biological sciences - Biology']|['Mathematics', 'Science & Mathematics', 'Statistics']
This article examines the macroeconomic impact of the first round of quantitative easing (QE) by the Bank of England. We attempt to quantify the effects of these purchases by focusing on the impact of lower long-term interest rates on the wider economy. We use three different models to estimate the impact of QE on output and inflation: a large Bayesian vector autoregression (VAR), a change-point structural VAR and a time-varying parameter VAR. Our estimates suggest that QE may have had a peak effect on the level of real GDP of around 1½% and a peak effect on annual CPI inflation of about 1¼% points.|ASSESSING THE ECONOMY-WIDE EFFECTS OF QUANTITATIVE EASING|http://www.jstor.org/stable/23324226|23324226|2012-11-01|2012|['eng']|['Economics - Economic disciplines']|['Business', 'Business & Economics Collection', 'Economics']
Statistical methods have been developed and applied to estimating populations that are difficult or too costly to enumerate. Known as multilist methods in epidemiological settings, individuals are matched across lists and estimation of population size proceeds by modeling counts in incomplete multidimensional contingency tables (based on patterns of presence/absence on lists). As multilist methods typically assume that lists are compiled instantaneously, there are few options available for estimating the unknown size of a closed population based on continuously (longitudinally) compiled lists. However, in epidemiological settings, continuous time lists are a routine byproduct of administrative functions. Existing methods are based on time-to-event analyses with a second step of estimating population size. We propose an alternative approach to address the twofold epidemiological problem of estimating population size and of identifying patient factors related to duration (in days) between visits to a health care facility. A Bayesian framework is proposed to model interval lengths because, for many patients, the data are sparse; many patients were observed only once or twice. The proposed method is applied to the motivating data to illustrate the methods' applicability. Then, a small simulation study explores the performance of the estimator under a variety of conditions. Finally, a small discussion section suggests opportunities for continued methodological development for continuous time population estimation.|A Multilevel Model for Continuous Time Population Estimation|http://www.jstor.org/stable/20640582|20640582|2009-09-01|2009|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
On March 13th, 1999, a highly anticipated prize-fight between heavyweight champions Evander Holyfield and Lennox Lewis was ruled a draw by the three official judges. Many observers of the fight felt that Lewis had clearly outperformed Holyfield; dissatisfaction with the result-particularly the pro-Holyfield score-card of judge Eugenia Williams-fuelled speculation that the fight had been fixed and prompted official investigations. In this paper, we examine whether the official judges scored the fight in a significantly different way from other professional observers of the fight. We do so by analysing the round-by-round scoring within the context of interrater agreement. The literature on interrater agreement typically considers a large number of samples rated by a small number of judges and relies on asymptotic results for tests. In our case, the sample size is too small to rely on asymptotics. Instead, we investigate several techniques that can be applied to small sample interrater agreement problems, including logistic regression, an exact test and some Bayesian approaches. We demonstrate these methods on both the March 1999 Holyfield-Lewis fight, as well as the September 1999 bout between welter-weights Oscar de la Hoya and Felix Trinidad.|Did Lennox Lewis Beat Evander Holyfield?: Methods for Analysing Small Sample Interrater Agreement Problems|http://www.jstor.org/stable/3650314|3650314|2002-01-01|2002|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
We have a monthly series of observations which are obtained from sample surveys and are therefore subject to survey errors. We also have a series of annual values, called benchmarks, which are either exact or are substantially more accurate than the survey observations; these can be either annual totals or accurate values of the underlying variable at a particular month. The benchmarking problem is the problem of adjusting the monthly series to be consistent with the annual values. We provide two solutions to this problem. The first of these is a two-stage method in which we first fit a state space model to the monthly data alone and then combine the results obtained at this stage with the benchmark data. In the second solution we construct a single series from the monthly and annual values together and fit a state space model to this series in a single stage. The treatment is extended to series which behave multiplicatively. The methods are illustrated by applying them to Canadian retail sales series. /// Nous avons une série d'observations mensuelles provenant d'une enquête par échantillonnage et nous avons aussi l'information sur les propriétés de l'erreur d'échantillonnage. De plus nous avons quelques valeurs annuelles qui sont très précises, par exemple, le vrai total annuel des valeurs mensuelles. Cet article discute du problème de l'adjustement des valeurs mensuelles afin de les rendre compatible avec les données annuelles.|Benchmarking by State Space Models|http://www.jstor.org/stable/1403431|1403431|1997-04-01|1997|['eng']|['Mathematics - Applied mathematics', 'Information science - Coding theory']|['Science & Mathematics', 'Statistics']
Masqueraders in computer intrusion detection are people who use somebody else's computer account. We investigate a number of statistical approaches for detecting masqueraders. To evaluate them, we collected UNIX command data from 50 users and then contaminated the data with masqueraders. The experiment was blinded. We show results from six methods, including two approaches from the computer science community.|Computer Intrusion: Detecting Masquerades|http://www.jstor.org/stable/2676780|2676780|2001-02-01|2001|['eng']|['Applied sciences - Engineering', 'Information science - Coding theory']|['Science and Mathematics', 'Statistics']
We propose a new model for cluster analysis in a Bayesian nonparametric framework. Our model combines two ingredients, species sampling mixture models of Gaussian distributions on one hand, and a deterministic clustering procedure (DBSCAN) on the other. Here, two observations from the underlying species sampling mixture model share the same cluster if the distance between the densities corresponding to their latent parameters is smaller than a threshold; this yields a random partition which is coarser than the one induced by the species sampling mixture. Since this procedure depends on the value of the threshold, we suggest a strategy to fix it. In addition, we discuss implementation and applications of the model; comparison with more standard clustering algorithms will be given as well. Supplementary materials for the article are available online.|"A ""Density-Based"" Algorithm for Cluster Analysis Using Species Sampling Gaussian Mixture Models"|http://www.jstor.org/stable/43304801|43304801|2014-12-01|2014|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Mathematical objects']|['Science & Mathematics', 'Computer Science', 'Statistics']
We consider situations in which the prior distribution of a parameter vector θ1 in the distribution of an observable random vector X contains a hyperparameter vector θ2. The experimenter specifies another distribution for θ2 that contains hyperparameters θ3, and so forth. One wants to learn about the hyperparameters at each level of this hierarchical model. We show that for many measures of information, the gain in information decreases as one moves to higher levels of hyperparameters. These results are illustrated for univariate normal models and a general linear hierarchical model. Examples of measures of information are given for which this property does not hold.|Information About Hyperparamters in Hierarchical Models|http://www.jstor.org/stable/2287059|2287059|1981-03-01|1981|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
This paper describes a Bayesian procedure for the simultaneous estimation of the probabilities in a histogram. A two-stage prior distribution is constructed which assumes that probabilities corresponding to adjacent intervals are likely to be closely related. The method employs multivariate logit transformations, and a covariance structure similar to that assumed in the first-order autoregressive process. Posterior estimates are obtained which combine information between the intervals and have the practical effect of smoothing the histogram.|A Bayesian Method for Histograms|http://www.jstor.org/stable/2334541|2334541|1973-08-01|1973|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Finding duplicates in homicide registries is an important step in keeping an accurate account of lethal violence. This task is not trivial when unique identifiers of the individuals are not available, and it is especially challenging when records are subject to errors and missing values. Traditional approaches to duplicate detection output independent decisions on the coreference status of each pair of records, which often leads to nontransitive decisions that have to be reconciled in some ad-hoc fashion. The task of finding duplicate-records in a data file can be alternatively posed as partitioning the data file into groups of coreferent records. We present an approach that targets this partition of the file as the parameter of interest, thereby ensuring transitive decisions. Our Bayesian implementation allows us to incorporate prior information on the reliability of the fields in the data file, which is especially useful when no training data are available, and it also provides a proper account of the uncertainty in the duplicate detection decisions. We present a study to detect killings that were reported multiple times to the United Nations Truth Commission for El Salvador.|DETECTING DUPLICATES IN A HOMICIDE REGISTRY USING A BAYESIAN PARTITIONING APPROACH|http://www.jstor.org/stable/24522389|24522389|2014-12-01|2014|['eng']|['Applied sciences - Engineering', 'Mathematics - Mathematical logic']|['Mathematics', 'Science & Mathematics', 'Statistics']
The recovery of gradients of sparsely observed functional data is a challenging ill-posed inverse problem. Given observations of smooth curves (e.g., growth curves) at isolated time points, the aim is to provide estimates of the underlying gradients (or growth velocities). To address this problem, we develop a Bayesian inversion approach that models the gradient in the gaps between the observation times by a tied-down Brownian motion, conditionally on its values at the observation times. The posterior mean and covariance kernel of the growth velocities are then found to have explicit and computationally tractable representations in terms of quadratic splines. The hyperparameters in the prior are specified via nonparametric empirical Bayes, with the prior precision matrix at the observation times estimated by constrained ℓ₁ minimization. The infinitessimal variance of the Brownian motion prior is selected by cross-validation. The approach is illustrated using both simulated and real data examples.|Recovering Gradients from Sparsely Observed Functional Data|http://www.jstor.org/stable/44695256|44695256|2013-06-01|2013|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
"The sampling frame in most social science surveys excludes members of certain groups, known as hard-to-reach groups. These groups, or subpopulations, may be difficult to access (the homeless, e.g.), camouflaged by stigma (individuals with HIV/AIDS), or both (commercial sex workers). Even basic demographic information about these groups is typically unknown, especially in many developing nations. We present statistical models which leverage social network structure to estimate demographic characteristics of these subpopulations using Aggregated relational data (ARD), or questions of the form ""How many X's do you know?"" Unlike other network-based techniques for reaching these groups, ARD require no special sampling strategy and are easily incorporated into standard surveys. ARD also do not require respondents to reveal their own group membership. We propose a Bayesian hierarchical model for estimating the demographic characteristics of hard-to-reach groups, or latent demographic profiles, using ARD. We propose two estimation techniques. First, we propose a Markov-chain Monte Carlo algorithm for existing data or cases where the full posterior distribution is of interest. For cases when new data can be collected, we propose guidelines and, based on these guidelines, propose a simple estimate motivated by a missing data approach. Using data from McCarty et al. [Human Organization 60 (2001) 28-39], we estimate the age and gender profiles of six hard-to-reach groups, such as individuals who have HIV, women who were raped, and homeless persons. We also evaluate our simple estimates using simulation studies."|LATENT DEMOGRAPHIC PROFILE ESTIMATION IN HARD-TO-REACH GROUPS|http://www.jstor.org/stable/41713495|41713495|2012-12-01|2012|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
Demographic changes in the United States over the past 20 years (i.e., the aging of the baby boomers) should have led to an increase in church attendance. However, church attendance patterns during this period have remained relatively stable. It is suggested that this is only possible if (1) baby boomers are not attending church with greater frequency as they age; (2) age effects are being offset by period effects moving in the opposite direction (a secularization hypothesis); or (3) age effects are being offset through a process of intergenerational exchange whereby younger cohorts throughout their life course attend church less frequently than older cohorts. These three possibilities are tested using Bayesian cohort analysis and the General Social Survey data set. Results support the third explanation. The gains associated with the aging of the baby boomers appear to be primarily offset by cohort effects. Specifically, pre-WWII cohorts attend church far more frequently than post-WWII cohorts. Therefore, as members of these older cohorts die and are replaced by younger cohorts, church attendance decreases, thereby offsetting any gains in attendance associated with the shifting age structure of the U.S. population.|On the Stability of Church Attendance Patterns during a Time of Demographic Change: 1965-1988|http://www.jstor.org/stable/1386559|1386559|1996-09-01|1996|['eng']|['Philosophy - Applied philosophy']|['Religion', 'Sociology', 'Social Sciences', 'Humanities']
A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.|Maximum Likelihood from Incomplete Data via the EM Algorithm|http://www.jstor.org/stable/2984875|2984875|1977-01-01|1977|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
"Complex stochastic models represented by directed acyclic graphs (DAGs) are increasingly employed to synthesise multiple, imperfect and disparate sources of evidence, to estimate quantities that are difficult to measure directly. The various data sources are dependent on shared parameters and hence have the potential to conflict with each other, as well as with the model. In a Bayesian framework, the model consists of three components: the prior distribution, the assumed form of the likelihood and structural assumptions. Any of these components may be incompatible with the observed data. The detection and quantification of such conflict and of data sources that are inconsistent with each other is therefore a crucial component of the model criticism process. We first review Bayesian model criticism, with a focus on conflict detection, before describing a general diagnostic for detecting and quantifying conflict between the evidence in different partitions of a DAG. The diagnostic is a p-value based on splitting the information contributing to inference about a ""separator"" node or group of nodes into two independent groups and testing whether the two groups result in the same inference about the separator node(s). We illustrate the method with three comprehensive examples: an evidence synthesis to estimate HIV prevalence; an evidence synthesis to estimate influenza case-severity; and a hierarchical growth model for rat weights."|Conflict Diagnostics in Directed Acyclic Graphs, with Applications in Bayesian Evidence Synthesis|http://www.jstor.org/stable/43288423|43288423|2013-08-01|2013|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
This article describes Bayesian methods for accelerated life test planning with one accelerating variable, when the acceleration model is linear in the parameters, based on censored data from a log-location-scale distribution. We use a Bayesian criterion based on estimation precision of a distribution quantile at a specified use condition to find optimum test plans. We also show how to compute optimized compromise plans that satisfy practical constraints. A large-sample normal approximation provides an easy-to-interpret yet useful simplification to this planning problem. We present a numerical example using the Weibull distribution with type I censoring to illustrate the methods and to examine the effects of the prior distribution, censoring, and sample size. The general equivalence theorem is used to verify that the numerical optimized test plans are globally optimum. The resulting optimum plans are also evaluated using simulation.|Bayesian Methods for Planning Accelerated Life Tests|http://www.jstor.org/stable/25471114|25471114|2006-02-01|2006|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
In this paper we derive the Bayes estimate of $P(X_2 &lt; X_1)$ in the bivariate Pareto distribution specified by $P(X_1 &gt; x_1,X_2 &gt; x_2) = \big(\frac{x_1}{\beta}\big)^{-\lambda_1} \big(\frac{x_2}{\beta}\big)^{-\lambda_2} \max \big(\frac{x_1}{\beta}, \frac{x_2}{\beta}\big)^{-\lambda_0}$ It is demonstrated that the estimate is robust with respect to the prior.|Bayes Estimation of <latex>$P(X_2 < X_1)$</latex> for a Bivariate Pareto Distribution.|http://www.jstor.org/stable/2988496|2988496|1997-01-01|1997|['eng']|['Mathematics - Mathematical logic', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Naturally and artificially selected populations usually exhibit some degree of stratification. In Genome-Wide Association Studies and in Whole-Genome Regressions (WGR) analyses, population stratification has been either ignored or dealt with as a potential confounder. However, systematic differences in allele frequency and in patterns of linkage disequilibrium can induce sub-population-specific effects. From this perspective, structure acts as an effect modifier rather than as a confounder. In this article, we extend WGR models commonly used in plant and animal breeding to allow for sub-population-specific effects. This is achieved by decomposing marker effects into main effects and interaction components that describe group-specific deviations. The model can be used both with variable selection and shrinkage methods and can be implemented using existing software for genomic selection. Using a wheat and a pig breeding data set, we compare parameter estimates and the prediction accuracy of the interaction WGR model with WGR analysis ignoring population stratification (across-group analysis) and with a stratified (i.e., within-sub-population) WGR analysis. The interaction model renders trait-specific estimates of the average correlation of effects between sub-populations; we find that such correlation not only depends on the extent of genetic differentiation in allele frequencies between groups but also varies among traits. The evaluation of prediction accuracy shows a modest superiority of the interaction model relative to the other two approaches. This superiority is the result of better stability in performance of the interaction models across data sets and traits; indeed, in almost all cases, the interaction model was either the best performing model or it performed close to the best performing model. Supplementary materials accompanying this paper appear on-line.|Incorporating Genetic Heterogeneity in Whole-Genome Regressions Using Interactions|http://www.jstor.org/stable/26451850|26451850|2015-12-01|2015|['eng']|['Biological sciences - Biology']|['Science & Mathematics', 'Agriculture', 'Statistics']
Mixture models for hazard rate functions are widely used tools for addressing the statistical analysis of survival data subject to a censoring mechanism. The present article introduced a new class of vectors of random hazard rate functions that are expressed as kernel mixtures of dependent completely random measures. This leads to define dependent nonparametric prior processes that are suitably tailored to draw inferences in the presence of heterogenous observations. Besides its flexibility, an important appealing feature of our proposal is analytical tractability: we are, indeed, able to determine some relevant distributional properties and a posterior characterization that is also the key for devising an efficient Markov chain Monte Carlo sampler. For illustrative purposes, we specialize our general results to a class of dependent extended gamma processes. We finally display a few numerical examples, including both simulated and real two-sample datasets: these allow us to identify the effect of a borrowing strength phenomenon and provide evidence of the effectiveness of the prior to deal with datasets for which the proportional hazards assumption does not hold true. Supplementary materials for this article are available online.|A Class of Hazard Rate Mixtures for Combining Survival Data From Different Experiments|http://www.jstor.org/stable/24247204|24247204|2014-06-01|2014|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
The problem of interest is to estimate a two-way table of means of grade point averages (GPAs) of University of Iowa freshmen grouped by levels of ACT scores and high school percentiles. Suppose one believes a priori that the table means satisfy a specific partial order. The use of two different classes of prior distributions is considered in modeling this prior information. The use of simulation and, in particular, the Gibbs sampler are outlined to summarize various posterior distributions of interest. The posterior distribution is used to predict the grade point averages of future Iowa freshmen.|A Bayesian Approach to Estimation of GPAs of University of Iowa Freshmen under Order Restrictions|http://www.jstor.org/stable/1165174|1165174|1994-04-01|1994|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Education', 'Statistics', 'Social Sciences']
This article presents a new way of modeling time-varying volatility. We generalize the usual stochastic volatility models to encompass regime-switching properties. The unobserved state variables are governed by a first-order Markov process. Bayesian estimators are constructed by Gibbs sampling. High-, medium- and low-volatility states are identified for the Standard and Poor's 500 weekly return data. Persistence in volatility is explained by the persistence in the low- and the medium-volatility states. The high-volatility regime is able to capture the 1987 crash and overlap considerably with four U.S. economic recession periods.|A Stochastic Volatility Model with Markov Switching|http://www.jstor.org/stable/1392580|1392580|1998-04-01|1998|['eng']|['Applied sciences - Engineering']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
Backcountry hikers' willingness-to-pay for removing grazing from trails in the Hoover Wilderness is analyzed using a multinomial Dirichlet negative binomial distribution. This multivariate discrete distribution allows the direct calculation of seasonal welfare measures that are derived from an incomplete demand specification. The welfare maximizing choice of activities is examined on a trail-by-trail basis using the results of the analysis. Our findings suggest that a mix of hiking and grazing activities provide the greatest social welfare.|Welfare Losses Due to Livestock Grazing on Public Lands: A Count Data Systemwide Treatment|http://www.jstor.org/stable/3697846|3697846|2005-05-01|2005|['eng']|['Biological sciences - Ecology']|['Business & Economics', 'Science & Mathematics', 'Agriculture', 'Business', 'Economics']
A Bayesian two-stage phase I–II design is proposed for optimizing administration schedule and dose of an experimental agent based on the times to response and toxicity in the case where schedules are non-nested and qualitatively different. Sequentially adaptive decisions are based on the joint utility of the two event times. A utility function is constructed by partitioning the two-dimensional positive real quadrant of possible event time pairs into rectangles, eliciting a numerical utility for each rectangle, and fitting a smooth parametric function to the elicited values. We assume that each event time follows a gamma distribution with shape and scale parameters both modeled as functions of schedule and dose. A copula is assumed to obtain a bivariate distribution. To ensure an ethical trial, adaptive safety and efficacy acceptability conditions are imposed on the (schedule, dose) regimes. In stage 1 of the design, patients are randomized fairly among schedules and, within each schedule, a dose is chosen using a hybrid algorithm that either maximizes posterior mean utility or randomizes among acceptable doses. In stage 2, fair randomization among schedules is replaced by the hybrid algorithm. A modified version of this algorithm is used for nested schedules. Extensions of the model and utility function to accommodate death or discontinuation of follow up are described. The method is illustrated by an autologous stem cell transplantation trial in multiple myeloma, including a simulation study.|Using Joint Utilities of the Times to Response and Toxicity to Adaptively Optimize Schedule–Dose Regimes|http://www.jstor.org/stable/24538133|24538133|2013-09-01|2013|['eng']|['Health sciences - Medical sciences', 'Health sciences - Health and wellness', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Prostate cancer is one of the most common cancers in American men. Management depends on the staging of prostate cancer. Only cancers that are confined to organs of origin are potentially curable. The article considers a hierarchical Bayesian neural network approach for posterior prediction probabilities of certain features indicative of non-organ-confined prostate cancer. The Bayesian procedure is implemented by an application of the Markov chain Monte Carlo numerical integration technique. For the problem at hand, the hierarchical Bayesian neural network approach is shown to be superior to the approach based on hierarchical Bayesian logistic regression model as well as the classical feedforward neural networks.|Hierarchical Bayesian Neural Networks: An Application to a Prostate Cancer Study|http://www.jstor.org/stable/27590434|27590434|2004-09-01|2004|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Even after careful calibration, the output of deterministic models of environmental systems usually still show systematic deviations from measured data. To analyse possible causes of these discrepancies, we make selected model parameters time variable by treating them as continuous time stochastic processes. This extends an approach that was proposed earlier using discrete time stochastic processes. We present a Markov chain Monte Carlo algorithm for Bayesian estimation of such parameters jointly with the other, constant, parameters of the model. The algorithm consists of Gibbs sampling between constant and time varying parameters by using a Metropolis-Hastings algorithm for each parameter type. For the time varying parameter, we split the overall time period into consecutive intervals of random length, over each of which we use a conditional Ornstein-Uhlenbeck process with fixed end points as the proposal distribution in a Metropolis-Hastings algorithm. The hyperparameters of the stochastic process are selected by using a cross-validation criterion which maximizes a pseudolikelihood value, for which we have derived a computationally efficient estimator. We tested our algorithm by using a simple climate model. The results show that the algorithm behaves well, is computationally tractable and improves the fit of the model to the data when applied to an additional time-dependent forcing component. However, this additional forcing term is too large to be a reasonable correction of estimated forcing and it alters the posterior distribution of the other, time constant parameters to unrealistic values. This difficulty, and the impossibility of achieving a good simulation when making other parameters time dependent, indicates a more fundamental, structural deficit of the climate model. This is probably related to the poor resolution of the ocean in the model. Our study demonstrates the technical feasibility of the smoothing technique but also the need for a careful interpretation of the results.|A Smoothing Algorithm for Estimating Stochastic, Continuous Time Model Parameters and Its Application to a Simple Climate Model|http://www.jstor.org/stable/40541621|40541621|2009-12-01|2009|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
ABSTRACT: Anthropogenic sound in the marine environment can have negative consequences for marine fauna. Since most sound sources are intermittent or continuous, estimating how many individuals are exposed over time remains challenging, as this depends on the animals’ mobility. Here we explored how animal movement influences how many, and how often, animals are impacted by sound. In a dedicated study, we estimated how different movement strategies affect the number of individual harbour porpoises Phocoena phocoena receiving temporary or permanent hearing loss due to underwater detonations of recovered explosives (mostly WWII aerial bombs). Geo-statistical distribution models were fitted to data from 4 marine mammal aerial surveys and used to simulate the distribution and movement of porpoises. Based on derived dose–response thresholds for temporary (TTS) or permanent threshold shifts (PTS), we estimated the number of animals affected in a single year. When individuals were free-roaming, an estimated 1200 and 24 000 unique individuals would suffer PTS and TTS, respectively. This equates to respectively 0.50 and 10% of the estimated North Sea population. In contrast, when porpoises remained in a local area, fewer animals would receive PTS and TTS (1100 [0.47%] and 15 000 [6.5%], respectively), but more individuals would be subjected to repeated exposures. Because most anthropogenic sound-producing activities operate continuously or intermittently, snapshot distribution estimates alone tend to underestimate the number of individuals exposed, particularly for mobile species. Hence, an understanding of animal movement is needed to estimate the impact of underwater sound or other human disturbance.|Harbour porpoise movement strategy affects cumulative number of animals acoustically exposed to underwater explosions|http://www.jstor.org/stable/24897410|24897410|2016-09-28|2016|['eng']|['Physical sciences - Astronomy']|['Ecology & Evolutionary Biology', 'Aquatic Sciences', 'Science & Mathematics', 'Biological Sciences']
This article reviews and unifies the hierarchical and empirical Bayes approach for estimating the multivariate normal mean. Both the ANOVA and the regression models are considered.|Hierarchical and Empirical Bayes Multivariate Estimation|http://www.jstor.org/stable/4355631|4355631|1992-01-01|1992|['eng']|['Mathematics - Mathematical logic']|['Mathematics', 'Science & Mathematics', 'Statistics']
Motivated by the common problem of constructing predictive distributions for daily asset returns over horizons of one to several trading days, this article introduces a new model for time series. This model is a generalization of the Markov normal mixture model in which the mixture components are themselves normal mixtures, and it is a specific case of an artificial neural network model with two hidden layers. The article uses the model to construct predictive distributions of daily S&amp; P 500 returns 1971-2005 and one-year maturity bond returns 1987-2007. For these time series the model compares favorably with ARCH and stochastic volatility models. The article concludes by using the model to form predictive distributions of one-to ten-day returns during volatile episodes for the S&amp;P 500 and bond return series.|HIERARCHICAL MARKOV NORMAL MIXTURE MODELS WITH APPLICATIONS TO FINANCIAL ASSET RETURNS|http://www.jstor.org/stable/41057403|41057403|2011-01-01|2011|['eng']|['Applied sciences - Engineering']|['Business', 'Business & Economics Collection', 'Economics']
In the statistical analysis of survival data arising from two populations, it often happens that the analyst knows, a priori, that the life lengths in one population are stochastically shorter than those in the other. Nevertheless, survival probability estimates, if determined separately from the corresponding samples, may not be consistent with this prior assumption, because of inherent statistical variability in the observations. This problem has been considered in a number of papers during the past decade, by adopting a (generalized) maximum likelihood approach. Our approach is Bayesian and, in essence, nonparametric. The a priori assumption regarding stochastic ordering is formulated naturally in terms of a joint prior distribution defined for pairs of survival functions. Nonparametric specification of the model, based on hazard rates and using a few hyperparameters, allows for sufficient flexibility in practical applications. The numerical computations are based on a coupled version of the Metropolis-Hastings algorithm. The results from a statistical analysis are summarized nicely by a pair of predictive survival functions that are consistent with the assumed stochastic ordering|Bayesian Inference of Survival Probabilities, Under Stochastic Ordering Constraints|http://www.jstor.org/stable/2291729|2291729|1996-09-01|1996|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics', 'Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
This paper outlines an approach to Bayesian semiparametric regression in multiple equation models which can be used to carry out inference in seemingly unrelated regressions or simultaneous equations models with nonparametric components. The approach treats the points on each nonparametric regression line as unknown parameters and uses a prior on the degree of smoothness of each line to ensure valid posterior inference despite the fact that the number of parameters is greater than the number of observations. We develop an empirical Bayesian approach that allows us to estimate the prior smoothing hyperparameters from the data. An advantage of our semiparametric model is that it is written as a seemingly unrelated regressions model with independent normal-Wishart prior. Since this model is a common one, textbook results for posterior inference, model comparison, prediction and posterior computation are immediately available. We use this model in an application involving a two-equation structural model drawn from the labour and returns to schooling literatures.|Semiparametric Bayesian Inference in Multiple Equation Models|http://www.jstor.org/stable/25146392|25146392|2005-09-01|2005|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Business', 'Economics']
This paper attempts to shed light on the following research questions: When a firm introduces a new product (or service) how can it effectively use the different information sources available to generate reliable new product performance forecasts? How can the firm account for varying information availability at different stages of the new product launch and generate forecasts at each stage? We address these questions in the context of the sequential launches of motion pictures in international markets. Players in the motion picture industry require forecasts at different stages of the movie launch process to aid decision-making, and the information sets available to generate such forecasts vary at different stages. Despite the importance of such forecasts, the industry struggles to understand and predict sales of new movies in domestic and overseas markets. We develop a Bayesian modeling framework that predicts first-week viewership for new movies in both domestic and several international markets. We focus on the first week because industry players involved in international markets (studios, distributors, and exhibitors) are most interested in these predictions. We draw on existing literature on forecasting performance of new movies to formulate our model. Specifically, we model the number of viewers of a movie in a given week using a Poisson count data model. The number of screens, distribution strategy, movie attributes such as genre, and presence/absence of stars are among the factors modeled to influence viewership. We employ a hierarchical Bayes formulation of the Poisson model that allows the determinants of viewership to vary across countries. We adopt the Bayesian approach for two reasons: First, it provides a convenient framework to model varying assumptions of information availability; specifically, it allows us to make forecasts by combining different sources of information such as domestic and international market-specific data. Second, this methodology provides us with the entire distribution of the new movie's performance forecast. Such a predictive distribution is more informative than a point estimate and provides a measure of the uncertainty in the forecasts. We propose a Bayesian prediction procedure that provides viewership forecasts at different stages of the new movie release process. The methodology provides forecasts under a number of information availability scenarios. Thus, forecasts can be obtained with just information from a historical database containing data on previous new product launches in several international markets. As more information becomes available, the forecasting methodology allows us to combine historical information with data on the performance of the new product in the domestic market and thereby to make forecasts with less uncertainty and greater accuracy. Our results indicate that for all the countries in the data set the number of screens on which a movie is released is the most important influence on viewership. Furthermore, we find that local distribution improves movie sales internationally in contrast to the domestic market. We also find evidence of similar genre preferences in geographically disparate countries. We find that the proposed model provides accurate forecasts at the movie-country level. Further, the model outperforms all the extant models in the marketing literature that could potentially be used for making these forcasts. A comparison of root mean square and mean absolute errors for movies in a hold out sample shows that the model that combines information available from the different sources generates the lowest errors. A Bayesian predictive model selection criterion corroborates the superior performance of this model. We demonstrate that the Bayesian model can be combined with industry rules of thumb to generate cumulative box office forecasts. In summary, this research demonstrates a Bayesian modeling framework that allows the use of different information sources to make new product forecasts in domestic and international markets. Our results underscore the theme that each movie is unique as is each country-and viewership results from an interaction of the product and the market. Hence, the motion picture industry should use both product-specific and market-specific information to make new movie performance forecasts.|A Bayesian Model to Forecast New Product Performance in Domestic and International Markets|http://www.jstor.org/stable/193212|193212|1999-01-01|1999|['eng']|['Physical sciences - Astronomy']|['Marketing & Advertising', 'Business & Economics', 'Business']
We discuss the analysis of data from single-nucleotide polymorphism arrays comparing tumour and normal tissues. The data consist of sequences of indicators for loss of heterozygosity (LOH) and involve three nested levels of repetition: chromosomes for a given patient, regions within chromosomes and single-nucleotide polymorphisms nested within regions. We propose to analyse these data by using a semiparametric model for multilevel repeated binary data. At the top level of the hierarchy we assume a sampling model for the observed binary LOH sequences that arises from a partial exchangeability argument. This implies a mixture of Markov chains model. The mixture is defined with respect to the Markov transition probabilities. We assume a non-parametric prior for the random-mixing measure. The resulting model takes the form of a semiparametric random-effects model with the matrix of transition probabilities being the random effects. The model includes appropriate dependence assumptions for the two remaining levels of the hierarchy, i.e. for regions within chromosomes and for chromosomes within patient. We use the model to identify regions of increased LOH in a data set coming from a study of treatment-related leukaemia in children with an initial cancer diagnostic. The model successfully identifies the desired regions and performs well compared with other available alternatives.|A Semiparametric Bayesian Model for Repeatedly Repeated Binary Outcomes|http://www.jstor.org/stable/20492615|20492615|2008-09-01|2008|['eng']|['Biological sciences - Biology']|['Science & Mathematics', 'Statistics']
We introduce a family of bivariate discrete distributions whose members are generated by a decreasing mass function p, and with margins given by p. Several properties and examples are obtained, including a family of seemingly novel bivariate Poisson distributions.|On a Simple Construction of a Bivariate Probability Function With a Common Marginal|http://www.jstor.org/stable/24591693|24591693|2014-08-01|2014|['eng']|['Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
We investigate the degree to which posterior expectations are sensitive to prior distributions, using a local method based on functional differentiation. Invariance considerations suggest a family of norms which can be used to measure perturbations to the prior. The sensitivity measure is seen to depend heavily on the choice of norm; asymptotic results suggest which norm will yield the most useful results in practice. We find that to maintain asymptotically sensible behaviour, it is necessary to reduce the richness of the class of prior perturbations as the dimension of the parameter space increases. Jeffrey's prior is characterized as the prior to which inference is least sensitive.|Local Sensitivity of Posterior Expectations|http://www.jstor.org/stable/2242614|2242614|1996-02-01|1996|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
We propose classification models for binary and multicategory data where the predictor is a random function. We use Bayesian modeling with wavelet basis functions that have nice approximation properties over a large class of functional spaces and can accommodate a wide variety of functional forms observed in real life applications. We develop an unified hierarchical model to encompass both the adaptive wavelet-based function estimation model and the logistic classification model. We couple together these two models are to borrow strengths from each other in a unified hierarchical framework. The use of Gibbs sampling with conjugate priors for posterior inference makes the method computationally feasible. We compare the performance of the proposed model with other classification methods, such as the existing naive plug-in methods, by analyzing simulated and real data sets.|Bayesian Curve Classification Using Wavelets|http://www.jstor.org/stable/27639938|27639938|2007-09-01|2007|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
A calibration-based approach is developed for predicting the behavior of a physical system that is modeled by a computer simulator. The approach is based on Bayes linear adjustment using both system observations and evaluations of the simulator at parameterizations that appear to give good matches to those observations. This approach can be applied to complex high-dimensional systems with expensive simulators, where a fully Bayesian approach would be impractical. It is illustrated with an example concerning the collapse of the thermohaline circulation (THC) in the Atlantic Ocean.|Bayes Linear Calibrated Prediction for Complex Systems|http://www.jstor.org/stable/27590790|27590790|2006-09-01|2006|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical analysis']|['Science & Mathematics', 'Statistics']
This article addresses modeling and inference for ordinal outcomes nested within categorical responses. We propose a mixture of normal distributions for latent variables associated with the ordinal data. This mixture model allows us to fix without loss of generality the cutpoint parameters that link the latent variable with the observed ordinal outcome. Moreover, the mixture model is shown to be more flexible in estimating cell probabilities when compared to the traditional Bayesian ordinal probit regression model with random cutpoint parameters. We extend our model to take into account possible dependence among the outcomes in different categories. We apply the model to a randomized phase III study to compare treatments on the basis of toxicities recorded by type of toxicity and grade within type. The data include the different (categorical) toxicity types exhibited in each patient. Each type of toxicity has an (ordinal) grade associated to it. The dependence among the different types of toxicity exhibited by the same patient is modeled by introducing patient-specific random effects.|Assessing Toxicities in a Clinical Trial: Bayesian Inference for Ordinal Data Nested within Categories|http://www.jstor.org/stable/40962468|40962468|2010-09-01|2010|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
"The members of a set of conditional probability density functions are called ""compatible"" if there exists a joint probability density function that generates them. We generalize this concept by calling the conditionals ""functionally compatible"" if there exists a non-negative function that behaves like a joint density as far as generating the conditionals according to the probability calculus, but whose integral over the whole space is not necessarily finite. A necessary and sufficient condition for functional compatibility is given that provides a method of calculating this function, if it exists. A Markov transition function is then constructed using a set of functionally compatible conditional densities and it is shown, using the compatibility results, that the associated Markov chain is positive recurrent if and only if the conditionals are compatible. A Gibbs Markov chain, constructed via ""Gibbs conditionals"" from a hierarchical model with an improper posterior, is a special case. Therefore, the results of this article can be used to evaluate the consequences of applying the Gibbs sampler when the posterior's impropriety is unknown to the user. Our results cannot, however, be used to detect improper posteriors. Monte Carlo approximations based on Gibbs chains are shown to have undesirable limiting behavior when the posterior is improper. The results are applied to a Bayesian hierarchical one-way random effects model with an improper posterior distribution. The model is simple, but also quite similar to some models with improper posteriors that have been used in conjunction with the Gibbs sampler in the literature."|Functional Compatibility, Markov Chains, and Gibbs Sampling with Improper Posteriors|http://www.jstor.org/stable/1390768|1390768|1998-03-01|1998|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Computer Science', 'Statistics']
This paper explores a class of empirical Bayes methods for level-dependent threshold selection in wavelet shrinkage. The prior considered for each wavelet coefficient is a mixture of an atom of probability at zero and a heavy-tailed density. The mixing weight, or sparsity parameter, for each level of the transform is chosen by marginal maximum likelihood. If estimation is carried out using the posterior median, this is a random thresholding procedure; the estimation can also be carried out using other thresholding rules with the same threshold. Details of the calculations needed for implementing the procedure are included. In practice, the estimates are quick to compute and there is software available. Simulations on the standard model functions show excellent performance, and applications to data drawn from various fields of application are used to explore the practical performance of the approach. By using a general result on the risk of the corresponding marginal maximum likelihood approach for a single sequence, overall bounds on the risk of the method are found subject to membership of the unknown function in one of a wide range of Besov classes, covering also the case of f of bounded variation. The rates obtained are optimal for any value of the parameter p in (0, ∞ [, simultaneously for a wide range of loss functions, each dominating the Lq norm of the σth derivative, with σ ≥ 0 and $0 &lt; q \leq 2$. Attention is paid to the distinction between sampling the unknown function within white noise and sampling at discrete points, and between placing constraints on the function itself and on the discrete wavelet transform of its sequence of values at the observation points. Results for all relevant combinations of these scenarios are obtained. In some cases a key feature of the theory is a particular boundary-corrected wavelet basis, details of which are discussed. Overall, the approach described seems so far unique in combining the properties of fast computation, good theoretical properties and good performance in simulations and in practice. A key feature appears to be that the estimate of sparsity adapts to three different zones of estimation, first where the signal is not sparse enough for thresholding to be of benefit, second where an appropriately chosen threshold results in substantially improved estimation, and third where the signal is so sparse that the zero estimate gives the optimum accuracy rate.|Empirical Bayes Selection of Wavelet Thresholds|http://www.jstor.org/stable/3448622|3448622|2005-08-01|2005|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
"Knowledge of temporal change in ecological condition is important for the understanding and management of ecosystems. However, analyses of trends in biological condition have been rare, as there are usually too few data points at any single site to use many trend analysis techniques. We used a Bayesian hierarchical model to analyse temporal trends in stream ecological condition (as measured by the invertebrate-based index SIGNAL) across Melbourne, Australia. The Bayesian hierarchical approach assumes dependency amongst the sampling sites. Results for each site ""borrow strength"" from the other data because model parameter values are assumed to be drawn from a larger common distribution. This leads to robust inference despite the few data that exist at each site. Utilising the flexibility of the Bayesian approach, we also modelled change over time as a function of catchment urbanisation, allowed for potential temporal and spatial autocorrelation of the data and trend estimates, and used prior information to improve the estimate of data uncertainty. We found strong evidence of a widespread decline in SIGNAL scores for edge habitats (areas of little or no flow). The rate of decline was positively associated with catchment urbanisation. There was no evidence of such declines for riffle habitats (areas with rapid and turbulent flow). Melbourne has experienced a decline in rainfall, indicative of either drought and/or longer-term climate change. The results are consistent with the expected coupled effects of these rainfall changes and increasing urbanisation, but more research is needed to isolate a causal mechanism. More immediately, however, the Bayesian hierarchical approach has allowed us to identify a pattern in a biological monitoring data set that might otherwise have gone un-noticed, and to demonstrate a large-scale temporal decline in biological condition."|A Bayesian Hierarchical Trend Analysis Finds Strong Evidence for Large-Scale Temporal Declines in Stream Ecological Condition around Melbourne, Australia|http://www.jstor.org/stable/30244677|30244677|2009-04-01|2009|['eng']|['Health sciences - Health and wellness', 'Biological sciences - Paleontology', 'Physical sciences - Earth sciences', 'Biological sciences - Ecology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
We consider hierarchical generalized linear models which allow extra error components in the linear predictors of generalized linear models. The distribution of these components is not restricted to be normal; this allows a broader class of models, which includes generalized linear mixed models. We use a generalization of Henderson's joint likelihood, called a hierarchical or h-likelihood, for inferences from hierarchical generalized linear models. This avoids the integration that is necessary when marginal likelihood is used. Under appropriate conditions maximizing the h-likelihood gives fixed effect estimators that are asymptotically equivalent to those obtained from the use of marginal likelihood; at the same time we obtain the random effect estimates that are asymptotically best unbiased predictors. An adjusted profile h-likelihood is shown to give the required generalization of restricted maximum likelihood for the estimation of dispersion components. A scaled deviance test for the goodness of fit, a model selection criterion for choosing between various dispersion models and a graphical method for checking the distributional assumption of random effects are proposed. The ideas of quasi-likelihood and extended quasi-likelihood are generalized to the new class. We give examples of the Poisson-gamma, binomial-beta and gamma-inverse gamma hierarchical generalized linear models. A resolution is proposed for the apparent difference between population-averaged and subject-specific models. A unified framework is provided for viewing and extending many existing methods.|Hierarchical Generalized Linear Models|http://www.jstor.org/stable/2346105|2346105|1996-01-01|1996|['eng']|['Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
Prior specification for non-parametric Bayesian inference involves the difficult task of quantifying prior knowledge about a parameter of high, often infinite, dimension. A statistician is unlikely to have informed opinions about all aspects of such a parameter but will have real information about functionals of the parameter, such as the population mean or variance. The paper proposes a new framework for non-parametric Bayes inference in which the prior distribution for a possibly infinite dimensional parameter is decomposed into two parts: an informative prior on a finite set of functionals, and a non-parametric conditional prior for the parameter given the functionals. Such priors can be easily constructed from standard non-parametric prior distributions in common use and inherit the large support of the standard priors on which they are based. Additionally, posterior approximations under these informative priors can generally be made via minor adjustments to existing Markov chain approximation algorithms for standard non-parametric prior distributions. We illustrate the use of such priors in the context of multivariate density estimation using Dirichlet process mixture models, and in the modelling of high dimensional sparse contingency tables.|Marginally specified priors for non-parametric Bayesian estimation|http://www.jstor.org/stable/24774724|24774724|2015-01-01|2015|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Consumer choice in surveys and in the marketplace reflects a complex process of screening and evaluating choice alternatives. Behavioral and economic models of choice processes are difficult to estimate when using stated and revealed preferences because the underlying process is latent. This paper introduces Bayesian methods for estimating two behavioral models that eliminate alternatives using specific attribute levels. The elimination by aspects theory postulates a sequential elimination of alternatives by attribute levels until a single one, the chosen alternative, remains. In the economic screening rule model, respondents screen out alternatives with certain attribute levels and then choose from the remaining alternatives, using a compensatory function of all the attributes. The economic screening rule model gives an economic justification as to why certain attributes are used to screen alternatives. A commercial conjoint study is used to illustrate the methods and assess their performance. In this data set, the economic screening rule model outperforms the EBA and other standard choice models and provides comparable results to an equivalent conjunctive screening rule model.|Estimating Heterogeneous EBA and Economic Screening Rule Choice Models|http://www.jstor.org/stable/40057038|40057038|2006-09-01|2006|['eng']|['Information science - Coding theory', 'Mathematics - Mathematical logic']|['Marketing & Advertising', 'Business & Economics', 'Business']
By setting up a suitable time series model in state space form, the latest estimate of the underlying current change in a series may be computed by the Kalman filter. This may be done even if the observations are only available in a time-aggregated form subject to survey sampling error. A related series, possibly observed more frequently, may be used to improve the estimate of change further. The paper applies these techniques to the important problem of estimating the underlying monthly change in unemployment in the UK measured according to the definition of the International Labour Organisation by the Labour Force Survey. The fitted models suggest a reduction in root-mean-squared error of around 10% over a simple estimate based on differences if a univariate model is used and a further reduction of 50% if information on claimant counts is taken into account. With seasonally unadjusted data, the bivariate model offers a gain of roughly 40% over the use of annual differences. For both adjusted and unadjusted data, there is a further gain of around 10% if the next month's figure on claimant counts is used. The method preferred is based on a bivariate model with unadjusted data. If the next month's claimant count is known, the root-mean-squared error for the estimate of change is just over 10000.|Estimating the Underlying Change in Unemployment in the UK|http://www.jstor.org/stable/2680518|2680518|2000-01-01|2000|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
"The ""leapfrog"" hybrid Monte Carlo algorithm is a simple and effective MCMC method for fitting Bayesian generalized linear models with canonical link. The algorithm leads to large trajectories over the posterior and a rapidly mixing Markov chain, having superior performance over conventional methods in difficult problems like logistic regression with quasicomplete separation. This method offers a very attractive solution to this common problem, providing a method for identifying datasets that are quasicomplete separated, and for identifying the covariates that are at the root of the problem. The method is also quite successful in fitting generalized linear models in which the link function is extended to include a feedforward neural network. With a large number of hidden units, however, or when the dataset becomes large, the computations required in calculating the gradient in each trajectory can become very demanding. In this case, it is best to mix the algorithm with multivariate random walk Metropolis-Hastings. However, this entails very little additional programming work."|Applications of Hybrid Monte Carlo to Bayesian Generalized Linear Models: Quasicomplete Separation and Neural Networks|http://www.jstor.org/stable/1390827|1390827|1999-12-01|1999|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Computer Science', 'Statistics']
Given a large number of t-statistics, we consider the problem of approximating the distribution of noncentrality parameters (NCPs) by a continuous density. This problem is closely related to the control of false discovery rates (FDR) in massive hypothesis testing applications, e.g., microarray gene expression analysis. Our methodology is similar to, but improves upon, the existing approach by Ruppert, Nettleton, and Hwang (2007, Biometrics, 63, 483-495). We provide parametric, nonparametric, and semiparametric estimators for the distribution of NCPs, as well as estimates of the FDR and local FDR. In the parametric situation, we assume that the NCPs follow a distribution that leads to an analytically available marginal distribution for the test statistics. In the nonparametric situation, we use convex combinations of basis density functions to estimate the density of the NCPs. A sequential quadratic programming procedure is developed to maximize the penalized likelihood. The smoothing parameter is selected with the approximate network information criterion. A semiparametric estimator is also developed to combine both parametric and nonparametric fits. Simulations show that, under a variety of situations, our density estimates are closer to the underlying truth and our FDR estimates are improved compared with alternative methods. Data-based simulations and the analyses of two microarray datasets are used to evaluate the performance in realistic situations.|Improved Estimation of the Noncentrality Parameter Distribution from a Large Number of t-Statistics, with Applications to False Discovery Rate Estimation in Microarray Data Analysis|http://www.jstor.org/stable/41806036|41806036|2012-12-01|2012|['eng']|['Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
We investigate the problem of fiducial prediction for unobserved quantities within the framework of the functional model described previously by Dawid and Stone. It is supposed that these are related to a completely unknown parameter by means of a regular functional model, and that the observations are either given as known functions of the predictands, or are themselves related to them by means of a functional model. We develop algebraic conditions which allow the application of fiducial logic to the prediction problem, and explore the consequences of such an application--some of which appear unacceptable unless still stronger conditions are imposed. A reinterpretation of the fiducial prediction problem is given which can be applied to yield an inferential distribution for the unknown parameter in the presence of partial prior information, expressible as a functional hypermodel for the parameter, governed by a completely unknown hyperparameter. This solution agrees with the fiducial distribution when the hypermodel is vacuous and with the Bayes posterior distribution when the hyperparameter is fully known, but allows in addition for intermediate levels of partial prior knowledge.|Fiducial Prediction and Semi-Bayesian Inference|http://www.jstor.org/stable/2242190|2242190|1993-09-01|1993|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Mixed model based approaches for semiparametric regression have gained much interest in recent years, both in theory and application. They provide a unified and modular framework for penalized likelihood and closely related empirical Bayes inference. In this article, we develop mixed model methodology for a broad class of Cox-type hazard regression models where the usual linear predictor is generalized to a geoadditive predictor incorporating non-parametric terms for the (log-) baseline hazard rate, time-varying coefficients and non-linear effects of continuous covariates, a spatial component, and additional cluster-specific frailties. Non-linear and time-varying effects are modelled through penalized splines, while spatial components are treated as correlated random effects following either a Markov random field or a stationary Gaussian random field prior. Generalizing existing mixed model methodology, inference is derived using penalized likelihood for regression coefficients and (approximate) marginal likelihood for smoothing parameters. In a simulation we study the performance of the proposed method, in particular comparing it with its fully Bayesian counterpart using Markov chain Monte Carlo methodology, and complement the results by some asymptotic considerations. As an application, we analyse leukaemia survival data from northwest England.|A Mixed Model Approach for Geoadditive Hazard Regression|http://www.jstor.org/stable/41548547|41548547|2007-03-01|2007|['eng']|['Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
In this paper, we present a hierarchical spatial model for the analysis of geographical variation in mortality between the Italian provinces in the year 2001, according to gender, age class, and cause of death. When analysing counts data specific to geographical locations, classical empirical rates or standardised mortality ratios may produce estimates that show a very high level of overdispersion due to the effect of spatial autocorrelation among the observations, and due to the presence of heterogeneity among the population sizes. We adopt a Bayesian approach and a Markov chain Monte Carlo computation with the goal of making more consistent inferences about the quantities of interest. While considering information for the year 1991, we also take into account a temporal effect from the previous geographical pattern. Results have demonstrated the flexibility of our proposal in evaluating specific aspects of a counts spatial process, such as the clustering effect and the heterogeneity effect.|Geographical mortality patterns in Italy|http://www.jstor.org/stable/26349322|26349322|2009-01-01|2009|['eng']|['Mathematics - Mathematical objects']|['Population Studies', 'Social Sciences']
For regression problems that involve many potential predictors, the Bayesian variable selection (BVS) method is a powerful tool. This method associates each model with its posterior probability and achieves excellent prediction performance through Bayesian model averaging. The main challenges of using such models include specifying a suitable prior and computing posterior quantities for inference. We contribute to the literature of BVS modelling in the following aspects. We first propose a new family of priors, called the mnet prior, which is indexed by a few hyperparameters that allow great flexibility in the prior density. The hyperparameters can also be treated as random, so that their values need not be tuned manually, but will instead adapt to the data. Simulation studies are used to demonstrate good prediction and variable selection performances of these models. Secondly, the analytical expression of the posterior distribution is unavailable for the BVS model under the mnet prior in general, as is the case for most BVS models. We develop an adaptive Markov chain Monte Carlo algorithm that facilitates the computation in high-dimensional regression problems. We finally showcase various ways to do inference with BVS models, highlighting a new way to visualize the importance of each predictor along with estimation of the coefficients and their uncertainties. These are demonstrated through the analysis of a breast cancer gene expression dataset. La sélection bayésienne de variables (SBV) est un outil puissant pour les problèmes de régression comportant de nombreux prédicteurs potentiels. Elle consiste à associer à chaque modèle sa probabilité a posteriori afin d'en effectuer une combinaison pondérée, ce qui mène à d'excellentes performances prédictives. Le choix d'une loi a priori appropriée et les calculs a posteriori pour l'inférence représentent toutefois des défis de taille. Les auteurs contribuent au dévelopement de la SBV sous plusieurs aspects. D'abord, ils proposent une nouvelle famille de lois a priori, les lois « mnet », qui dépendent de quelques hyperparamètres leur accordant une grande flexibilité. Ces hyperparamètres peuvent être traités comme des variables aléatoires afin de les laisser s'ajuster aux données plutôt que de les calibrer manuellement. Les auteurs présentent des études de simulation qui démontrent la bonne performance de leur méthode tant au niveau de la prévision que de la sélection de variables. Ensuite, puisque la loi a posteriori du modèle de SBV issue d'une loi a priori « mnet » n'a pas de forme analytique, les auteurs développent un algorithme de Monte Carlo par chaînes de Markov adaptatif qui simplifie les calculs pour les problèmes de régression en haute dimension. Finalement, les auteurs illustrent différentes façons de faire de l'inférence avec des modèles de SBV et présentent une nouvelle façon de visualiser l'importance de chaque prédicteur en parallèle avec l'estimé de leur coefficient et l'incertitude y étant associée. Ils analysent à cette fin un jeu de données génétiques sur le cancer du sein.|Bayesian inference for high-dimensional linear regression under mnet priors|http://www.jstor.org/stable/44709161|44709161|2016-06-01|2016|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Modern statistical problems often involve a large number of populations and hence a large number of parameters that characterize these populations. It is common for scientists to use data to select the most significant populations, such as those with the largest t statistics. The scientific interest often lies in studying and making inferences regarding these parameters, called the selected parameters, corresponding to the selected populations. The current statistical practices either apply a traditional procedure assuming there were no selection—a practice that is not valid—or they use the Bonferroni-type procedure that is valid but very conservative and often noninformative. In this article, we propose valid and sharp confidence intervals that allow scientists to select parameters and to make inferences for the selected parameters based on the same data. This type of confidence interval allows the users to zero in on the most interesting selected parameters without collecting more data. The validity of confidence intervals is defined as the controlling of Bayes coverage probability so that it is no less than a nominal level uniformly over a class of prior distributions for the parameter. When a mixed model is assumed and the random effects are the key parameters, this validity criterion is exactly the frequentist criterion, since the Bayes coverage probability is identical to the frequentist coverage probability. Assuming that the observations are normally distributed with unequal and unknown variances, we select parameters with the largest t statistics. We then construct sharp empirical Bayes confidence intervals for these selected parameters, which have either a large Bayes coverage probability or a small Bayes false coverage rate uniformly for a class of priors. Our intervals, applicable to any high-dimensional data, are applied to microarray data and are shown to be better than all the alternatives. It is also anticipated that the same intervals would be valid for any selection rule. Supplementary materials for this article are available online.|Empirical Bayes Confidence Intervals for Selected Parameters in High-Dimensional Data|http://www.jstor.org/stable/24246467|24246467|2013-06-01|2013|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Research in political science is increasingly, but independently, modeling heterogeneity and spatial dependence. This article draws together these two research agendas via spatial random effects survival models. In contrast to standard survival models, which assume spatial independence, spatial survival models allow for spatial autocorrelation at neighboring locations. I examine spatial dependence in both semiparametric Cox and parametric Weibull models and in both individual and shared frailty models. I employ a Bayesian approach in which spatial autocorrelation in unmeasured risk factors across neighboring units is incorporated via a conditionally autoregressive (CAR) prior. I apply the Bayesian spatial survival modeling approach to the timing of U.S. House members' position announcements on NAFTA. I find that spatial shared frailty models outperform standard nonfrailty models and nonspatial frailty models in both the semiparametric and parametric analyses. The modeling of spatial dependence also produces changes in the effects of substantive covariates in the analysis.|Bayesian Spatial Survival Models for Political Event Processes|http://www.jstor.org/stable/25193878|25193878|2009-01-01|2009|['eng']|['Mathematics - Applied mathematics', 'Biological sciences - Biology']|['Political Science', 'American Studies', 'Social Sciences', 'Area Studies']
We examined the morphology and somatic growth rate of Green Sea Turtles living in San Diego Bay, California; one of the northern-most foraging areas for the species in the eastern Pacific. A power plant had discharged heated effluent into the urbanized bay from 1960 to 2010. Straight carapace lengths of 101 Green Sea Turtles were recorded from 31 March 1990 to 15 April 2011 (45.4 to 110.4 cm). Green Sea Turtles in San Diego Bay were morphologically indistinguishable from those foraging in Baja California Sur, Mexico. The median growth rate was 1.03 cm/yr (— 1.6 to 11.4 cm/yr) for all turtles and was 4.9 cm/yr for turtles ≤90 cm. These growth rates were one of the fastest for the species in temperate areas and comparable to those reported for tropical regions. The estimated growth parameter of the von Bertalanffy growth function (mean growth coefficient = 0.21, 95% posterior interval = 0.19-0.23) also was greater than for other populations of Green Sea Turtles. Based on behavioral observations and information from other diet studies, we think that the altered environment from the power plant effluent affected the growth of the Green Sea Turtles directly (longer active periods) and via shifts in the environment (changes in prey composition, abundance, and distribution). With the termination of the power plant operation at the end of 2010, the ecosystem is reverting to its natural state, which we expect will result in decreased growth rates of these turtles in the coming years.|MORPHOLOGY AND GROWTH RATES OF THE GREEN SEA TURTLE (CHELONIA MYDAS) IN A NORTHERN-MOST TEMPERATE FORAGING GROUND|http://www.jstor.org/stable/41406820|41406820|2012-03-01|2012|['eng']|['Biological sciences - Ecology', 'Biological sciences - Biology']|['Science & Mathematics', 'Biological Sciences', 'Zoology']
Understanding how landscape, host, and pathogen traits contribute to disease exposure requires systematic evaluations of pathogens within and among host species and geographic regions. The relative importance of these attributes is critical for management of wildlife and mitigating domestic animal and human disease, particularly given rapid ecological changes, such as urbanization. We screened &gt;1000 samples from sympatric populations of puma (Puma concolor), bobcat (Lynx rufus), and domestic cat (Felis catus) across urban gradients in six sites, representing three regions, in North America for exposure to a representative suite of bacterial, protozoal, and viral pathogens (Bartonella sp., Toxoplasma gondii, feline herpesvirus-1, feline panleukopenea virus, feline calicivirus, and feline immunodeficiency virus). We evaluated prevalence within each species, and examined host trait and land cover determinants of exposure; providing an unprecedented analysis of factors relating to potential for infections in domesticated and wild felids. Prevalence differed among host species (highest for puma and lowest for domestic cat) and was greater for indirectly transmitted pathogens. Sex was inconsistently predictive of exposure to directly transmitted pathogens only, and age infrequently predictive of both direct and indirectly transmitted pathogens. Determinants of pathogen exposure were widely divergent between the wild felid species. For puma, suburban land use predicted increased exposure to Bartonella sp. in southern California, and FHV-1 exposure increased near urban edges in Florida. This may suggest interspecific transmission with domestic cats via flea vectors (California) and direct contact (Florida) around urban boundaries. Bobcats captured near urban areas had increased exposure to T. gondii in Florida, suggesting an urban source of prey. Bobcats captured near urban areas in Colorado and Florida had higher FIV exposure, possibly suggesting increased intraspecific interactions through pile-up of home ranges. Beyond these regional and pathogen specific relationships, proximity to the wildland–urban interface did not generally increase the probability of disease exposure in wild or domestic felids, emphasizing the importance of local ecological determinants. Indeed, pathogen exposure was often negatively associated with the wildland–urban interface for all felids. Our analyses suggest cross-species pathogen transmission events around this interface may be infrequent, but followed by self-sustaining propagation within the new host species.|Pathogen exposure varies widely among sympatric populations of wild and domestic felids across the United States|http://www.jstor.org/stable/24701667|24701667|2016-03-01|2016|['eng']|['Biological sciences - Ecology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
Delimitation of species based exclusively on genetic data has been advocated despite a critical knowledge gap: how might such approaches fail because they rely on genetic data alone, and would their accuracy be improved by using multiple data types. We provide here the requisite framework for addressing these key questions. Because both phenotypic and molecular data can be analyzed in a common Bayesian framework with our program iBPP, we can compare the accuracy of delimited taxa based on genetic data alone versus when integrated with phenotypic data. We can also evaluate how the integration of phenotypic data might improve species delimitation when divergence occurs with gene flow and/or is selectively driven. These two realities of the speciation process are ignored by currently available genetic approaches. Our model accommodates phenotypic characters that exhibit different degrees of divergence, allowing for both neutral traits and traits under selection. We found a greater accuracy of estimated species boundaries with the integration of phenotypic and genetic data, with a strong beneficial influence of phenotypic data from traits under selection when the speciation process involves gene flow. Our results highlight the benefits of multiple data types, but also draws into question the rationale of species delimitation based exclusively on genetic data.|Bayesian species delimitation combining multiple genes and traits in a unified framework|http://www.jstor.org/stable/24704277|24704277|2015-02-01|2015|['eng']|['Biological sciences - Biology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
Since population censuses are not annually implemented, population estimates are needed for the intercensal period. This paper describes simultaneous implementations of the temporal interpolation and forecasting of the population census data, aggregated by age and period. Since age equals period minus cohort, age-period-cohort decomposition suffers from the identification problem. In order to overcome this problem, the Bayesian cohort model is applied. The efficacy of the model for temporal interpolation is examined in comparison with official Japanese population estimates. Empirical results suggest that the model is expected to work well in temporal interpolation. Regarding the age-period-cohort decomposition of the Japanese census data, it is shown that the cohort effect is the largest while the other two effects are very small but not negligible. With regard to the forecasting of the Japanese population, the official population forecast considerably outperforms the forecast in most forecast horizons. However, the pace of increase in root mean square error for longer-term forecasting is larger in the official population forecast than in the forecasts. As a result, a variant of the forecast is best for 10-year forecast.|Interpolation and forecasting of population census data|http://www.jstor.org/stable/41110951|41110951|2010-01-01|2010|['eng']|['Physical sciences - Astronomy']|['Population Studies', 'Social Sciences']
This paper aims to connect Bayesian analysis and frequentist theory in the context of multiple comparisons. The authors show that when testing the equality of two sample means, the posterior probability of the one-sided alternative hypothesis, defined as a half-space, shares with the frequentist P-value the property of uniformity under the null hypothesis. Ultimately, the posterior probability may thus be used in the same spirit as a P-value in the Benjamini-Hochberg procedure, or in any of its extensions. /// Cet article vise à faire le lien entre l'analyse bayésienne et la théorie fréquentiste dans le contexte des comparaisons multiples. Les auteurs montrent que lorsque l'on teste l'égalité de deux moyennes, la probabilité a posteriori de la contre-hypothèse unilatérale, définie comme un demi-espace, partage avec le seuil observé la propriété d'uniformité sous l'hypothèse nulle. Ultimement, la probabilité a posteriori peut donc être utilisée dans le même esprit qu'un seuil observé dans le cadre de la procédure de Benjamini-Hochberg et dans chacune de ses généralisations.|Multiple Testing Using the Posterior Probabilities of Directional Alternatives, with Application to Genomic Studies|http://www.jstor.org/stable/20445238|20445238|2007-03-01|2007|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
This article deals with both exploration and interpretation problems related to posterior distributions for mixture models. The specification of mixture posterior distributions means that the presence of k! modes is known immediately. Standard Markov chain Monte Carlo (MCMC) techniques usually have difficulties with well-separated modes such as occur here; the MCMC sampler stays within a neighborhood of a local mode and fails to visit other equally important modes. We show that exploration of these modes can be imposed using tempered transitions. However, if the prior distribution does not distinguish between the different components, then the posterior mixture distribution is symmetric and standard estimators such as posterior means cannot be used. We propose alternatives for Bayesian inference for permutation invariant posteriors, including a clustering device and alternative appropriate loss functions.|Computational and Inferential Difficulties with Mixture Posterior Distributions|http://www.jstor.org/stable/2669477|2669477|2000-09-01|2000|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
After a brief review of previous frequentist and Bayesian approaches to multiple change-points, we describe a Bayesian model for multiple parameter changes in a multiparameter exponential family. This model has attractive statistical and computational properties and yields explicit recursive formulas for the Bayes estimates of the piecewise constant parameters. Efficient estimators of the hyperparameters of the Bayesian model for the parameter jumps can be used in conjunction, yielding empirical Bayes estimates. The empirical Bayes approach is also applied to solve long-standing frequentist problems such as significance testing of the null hypothesis of no change-points versus multiple change-point alternatives, and inference on the number and locations of change-points that partition the unknown parameter sequence into segments of equal values. Simulation studies of performance and an illustrative application to the British coal mine data are also given. Extensions from the exponential family to general parametric families and from independent observations to genearlized linear time series models are then provided.|A SIMPLE BAYESIAN APPROACH TO MULTIPLE CHANGE-POINTS|http://www.jstor.org/stable/24309531|24309531|2011-04-01|2011|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Mathematics', 'Science and Mathematics', 'Statistics']
"Effective connectivity analysis provides an understanding of the functional organization of the brain by studying how activated regions influence one other. We propose a nonparametric Bayesian approach to model effective connectivity assuming a dynamic nonstationary neuronal system. Our approach uses the Dirichlet process to specify an appropriate (most plausible according to our prior beliefs) dynamic model as the ""expectation"" of a set of plausible models upon which we assign a probability distribution. This addresses model uncertainty associated with dynamic effective connectivity. We derive a Gibbs sampling approach to sample from the joint (and marginal) posterior distributions of the unknowns. Results on simulation experiments demonstrate our model to be flexible and a better candidate in many situations. We also used our approach to analyzing functional Magnetic Resonance Imaging (fMRI) data on a Stroop task: our analysis provided new insight into the mechanism by which an individual brain distinguishes and learns about shapes of objects."|A NONSTATIONARY NONPARAMETRIC BAYESIAN APPROACH TO DYNAMICALLY MODELING EFFECTIVE CONNECTIVITY IN FUNCTIONAL MAGNETIC RESONANCE IMAGING EXPERIMENTS|http://www.jstor.org/stable/23024848|23024848|2011-06-01|2011|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
Control groups may be used in situations where there are time series observations. However, there appears to be no systematic treatment of the statistical issues in the literature. Structural time series models are formulated in terms of unobserved components, such as trends and seasonals, which have a direct interpretation, and multivariate structural time series models are shown to provide an ideal framework for carrying out intervention analysis with control groups. They also facilitate analysis of the potential gains from using control groups and the conditions under which single equation estimation is valid. /// Des groupes de contrôle peuvent être utilisés lorsqu'il y a des observations de séries temporelles. Cependant, il semblerait que dans la litérature, il n'y ait pas de traîtement systématique des problèmes statistiques associés. Des modèles structurels de séries temporelles sont formulés en termes de composantes inobservables telles que des trends et des effets saisonniers qui ont une interprétation directe. On montre que les modèles structurels multivariés de séries temporelles constituent un cadre ideal pour l'analysis d'interventions avec des groupes de contrôle. Ces derniers facilitent aussi l'analyse des gains potentiels dus à l'utilisation des groupes de contrôle et des conditions sous lesquelles l'estimation d'une unique équation est valide.|Intervention Analysis with Control Groups|http://www.jstor.org/stable/1403788|1403788|1996-12-01|1996|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
The estimation of a treatment contrast from experimental data and the estimation of a small-area mean are special cases of the prediction of the realization of a linear combination of fixed and random effects in a possibly unbalanced two-part mixed linear model. In this article a Bayesian approach to point and interval prediction is presented and its computational requirements are examined. Differences between the Bayesian approach and the traditional (classical) approach are discussed in general terms and, in addition, in terms of two examples taken from the literature: (1) the comparison of drug formulations in a bioavailability trial (Westlake) and (2) the estimation of corn-crop areas using satellite data (Battese, Harter, and Fuller). Some deficiencies in the classical approach are pointed out, and the Bayesian approach is considered from a frequentist perspective. It is shown, via a Monte Carlo study, that, for certain (noninformative) choices of the prior distribution, the frequentist properties of the Bayesian prediction procedures compare favorably with those of their classical counterparts and that, in certain situations, they produce different and more sensible answers.|Some Bayesian and Non-Bayesian Procedures for the Analysis of Comparative Experiments and for Small-Area Estimation: Computational Aspects, Frequentist Properties, and Relationships|http://www.jstor.org/stable/2290383|2290383|1991-09-01|1991|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
"Correlations among life history parameters have been discussed in the ecological literature for over 50 years, but are often estimated while treating model estimates of demographic rates such as natural mortality (M) or individual growth (k) as ""data."" This approach fails to propagate uncertainty appropriately because it ignores correlations in estimation errors between parameters within a species and differences in estimation error among species. An improved alternative is multi-species mixed-effects modeling, which we approximate using multivariate likelihood profiles in an approach that synthesizes information from several population dynamics models. Simulation modeling demonstrates that this approach has minimal bias, and that precision improves with increased number of species. As a case study, we demonstrate this approach by estimating M/k for 11 groundfish species off the U.S. West Coast using the data and functional forms on which pre-existing, peer-reviewed, population dynamics models are based. M/k is estimated to be 1.26 for Pacific rockfishes (Sebastes spp.), with a coefficient of variation of 76% for M given k. This represents the first-ever estimate of correlations among life history parameters for marine fishes using several age-structured population dynamics models, and it serves as a standard for future life history correlation studies. This approach can be modified to provide robust estimates of other life history parameters and correlations, and requires few changes to existing population dynamics models and software input files for both marine and terrestrial species. Specific results for Pacific rockfishes can be used as a Bayesian prior for estimating natural mortality in future fisheries management efforts. We therefore recommend that fish population dynamics models be compiled in a global database that can be used to simultaneously analyze observation-level data for many species in life history meta-analyses."|Rigorous meta-analysis of life history correlations by simultaneously analyzing multiple population dynamics models|http://www.jstor.org/stable/24432148|24432148|2014-03-01|2014|['eng']|['Biological sciences - Biology', 'Physical sciences - Astronomy', 'Physical sciences - Earth sciences']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
Recent research has focused on studying the patterns in the digits of closely followed stock market indexes. In this paper we find that the series of 1-day returns on the Dow-Jones Industrial Average Index (DJIA) and the Standard and Poor's Index (S&amp;P) reasonably agrees with Benford's law and therefore belongs to the family of anomalous or outlaw numbers.|On the Peculiar Distribution of the U.S. Stock Indexes' Digits|http://www.jstor.org/stable/2684926|2684926|1996-11-01|1996|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Our aim is to analyze the link between optimism and risk aversion in a subjective expected utility setting and to estimate the average level of optimism when weighted by risk tolerance. Its estimation leads to a non-trivial statistical problem. We start from a large lottery survey (1536 individuals). We assume that individuals have true unobservable characteristics. We adopt a Bayesian approach and use a hybrid MCMC approximation method to numerically estimate the distributions of the unobservable characteristics. We find that individuals are on average pessimistic and that pessimism and risk tolerance are positively correlated.|Are Risk-Averse Agents More Optimistic? A Bayesian Estimation Approach|http://www.jstor.org/stable/25144582|25144582|2008-09-01|2008|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Business', 'Economics']
The analysis of growth curves has long been important in biostatistics. Work has focused on two problems: the estimation of individual curves based on many data points, and the estimation of the mean growth curve for a group of individuals. This paper extends a recent approach that seeks to combine data from a group of individuals in order to improve the estimates of individual growth parameters. Growth is modeled as polynomial in time, and the group model is also linear, incorporating growth-related covariates into the model. The estimation used is empirical Bayes. The estimation formulas are illustrated with a set of data on rat growth, originally presented by Box (1950, Biometrics 6, 362-389).|Empirical Bayes Estimation of Individual Growth-Curve Parameters and Their Relationship to Covariates|http://www.jstor.org/stable/2530808|2530808|1983-03-01|1983|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
We present a statistical approach for indentifying residues in DNA sequences for which diversity may be maintained by natural selection. Bayesian generalized linear models (GLMs) are used to describe patterns of mutation in a DNA sequence alignment. Posterior distributions of key quantities, such as probabilities of nonsynonymous and synonymous mutation per site, are studied. Inference in this class of models is achived through customary Markov chain Monte Carlo methods. Model selection is dealt with by means of a minimum posterior predictive loss approach. We describe how information on the evolutionary process underlying the sequences can be formally incorporated into the models through structured priors. The proposed methodology was designed to analyze several DNA sequences encoding the vaccine candidate apical membrane antigen-1 (AMA-1) of the human malaria parasite plasmodium falciparum. The study of genetic variability in antigen sequences is relevant to determining whether a particular antigen is a viable target for a vaccine construct. Using a simulation study, we first compare the GLM-based approach to existing methods for detecting sites under selection that are based on stochastic models of sequence evolution. We then apply the proposed models to the AMA-1 sequence data, which allows us to identify residues with the greatest disparities between nonsynonymous and synonymous changes. Recent experimental evidence suggests that several of these residues are immunologically relevant, indicating that the proposed models may be used predictively to identify functionally significant residues in antigens for which experimental results are not yet available.|Assessing the Effect of Selection at the Amino Acid Level in Malaria Antigen Sequences Through Bayesian Generalized Linear Models|http://www.jstor.org/stable/27640198|27640198|2008-12-01|2008|['eng']|['Biological sciences - Biology', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
"With survival data there is often interest not only in the survival time distribution but also in the residual survival time distribution. In fact, regression models to explain residual survival time might be desired. Building upon recent work of Kottas &amp; Gelfand [""J. Amer. Statist. Assoc."" 96 (2001) 1458], we formulate a semiparametric median residual life regression model induced by a semiparametric accelerated failure time regression model. We utilize a Bayesian approach which allows full and exact inference. Classical work essentially ignores covariates and is either based upon parametric assumptions or is limited to asymptotic inference in non-parametric settings. No regression modelling of median residual life appears to exist. The Bayesian modelling is developed through Dirichlet process mixing. The models are fitted using Gibbs sampling. Residual life inference is implemented extending the approach of Gelfand &amp; Kottas [""J. Comput. Graph. Statist."" 11 (2002) 289]. Finally, we present a fairly detailed analysis of a set of survival times with moderate censoring for patients with small cell lung cancer."|Bayesian Semiparametric Regression for Median Residual Life|http://www.jstor.org/stable/4616794|4616794|2003-12-01|2003|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
A fit of the Poisson-gamma model to the pump-failure data quoted by George et al. (Scand. J. Statist. 20, 147-156) using maximum conjugate likelihood gives pump effects that agree closely with the values obtained from a full Bayesian model. However, the model is a poor fit, because the estimates of the pump effects do not look like a sample from a gamma distribution. A split of the pumps into two subgroups does give an adequate fit.|Reader Reaction: A Re-Analysis of the Pump-Failure Data [with Response]|http://www.jstor.org/stable/4616310|4616310|1994-06-01|1994|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
An important type of heterosis, known as hybrid vigor, refers to the enhancements in the phenotype of hybrid progeny relative to their inbred parents. Although hybrid vigor is extensively utilized in agriculture, its molecular basis is still largely unknown. In an effort to understand phenotypic heterosis at the molecular level, researchers are measuring transcript abundance levels of thousands of genes in parental inbred lines and their hybrid offspring using RNA sequencing (RNA-seq) technology. The resulting data allow researchers to search for evidence of gene expression heterosis as one potential molecular mechanism underlying heterosis of agriculturally important traits. The null hypotheses of greatest interest in testing for gene expression heterosis are composite null hypotheses that are difficult to test with standard statistical approaches for RNA-seq analysis. To address these shortcomings, we develop a hierarchical negative binomial model and draw inferences using a computationally tractable empirical Bayes approach to inference. We demonstrate improvements over alternative methods via a simulation study based on a maize experiment and then analyze that maize experiment with our newly proposed methodology. Supplementary materials accompanying this paper appear on-line.|Empirical Bayes Analysis of RNA-seq Data for Detection of Gene Expression Heterosis|http://www.jstor.org/stable/26451857|26451857|2015-12-01|2015|['eng']|['Biological sciences - Biology', 'Applied sciences - Engineering']|['Science & Mathematics', 'Agriculture', 'Statistics']
If dynamic multivariate models are to be used to guide decision-making, it is important that probability assessments of forecasts or policy projections be provided. When identified Bayesian vector autoregression (VAR) models are presented with error bands in the existing literature, both conceptual and numerical problems have not been dealt with in an internally consistent way. In this paper we develop methods to introduce prior information in both reducedform and structural VAR models without introducing substantial new computational burdens. Our approach makes it feasible to use a single, large dynamic framework (for example, 20-variable models) for tasks of policy projections.|Bayesian Methods for Dynamic Multivariate Models|http://www.jstor.org/stable/2527347|2527347|1998-11-01|1998|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics']|['Business & Economics', 'Business', 'Economics']
1. Coomes &amp; Allen (2009) showed that the Metabolic Scaling Theory of Plant Growth (MST-PG) has no empirical support, because the only piece of confirmatory evidence was an analysis of a small and noisy data set from which it was impossible to draw strong inferences. Our re-analyses showed that MST-PG predictions were contained within very broad 95% confidence intervals, creating an illusion of close adherence to theoretical predictions. In a response to our paper, Stark, Bentley &amp; Enquist (2010) acknowledge these shortcomings. 2. We reasoned that MST-PG makes inaccurate predictions because asymmetric competition for light is not included in the theoretical model. We argued that asymmetric competition has its greatest impact on small trees within populations, such that the mean size-scaling relationship of the population has a greater exponent than that of trees growing without competitors (i.e. the slope predicted by MST-PG). Stark, Bentley &amp; Enquist (2010) appear to dispute this logic and criticize the statistical approach we developed to test our hypothesis. 3. Here we use Bayesian hierarchical models (BHMs) to re-examine three forest data sets, and find no support for MST-PG. Based on a large data set from New Zealand, the majority of tall tree species have scaling exponents greater than predicted by MST-PG, suggesting that asymmetric competition may have influenced the scaling relationships. Parameters for many small tree and shrub species were very poorly estimated, in part because their size ranges were narrow, so these species cannot be used to argue for, or against, MST-PG. 4. We hypothesize that scaling relationships are most affected by asymmetric competition in high-productivity forests because, as many ecologists argue, competition for light is intense in such forests (the productivity-dependent scaling hypothesis). We estimate growth scaling exponents for Nothofagus forest sites which differed in above-ground biomass (AGB) production. As predicted, scaling exponents were greater in sites with high AGB production and close to MST-PG predictions in sites with low AGB production. 5. Synthesis. Competition from taller neighbours has strong effects on tree growth, particularly in forests growing in productive locations. We continue to advocate for the inclusion of asymmetric competition into general models of tree growth, mortality, recruitment and size structure.|Moving on from Metabolic Scaling Theory: hierarchical models of tree growth and asymmetric competition for light|http://www.jstor.org/stable/23028860|23028860|2011-05-01|2011|['eng']|['Biological sciences - Biology', 'Physical sciences - Astronomy']|['Biological Sciences', 'Ecology & Evolutionary Biology', 'Science and Mathematics']
"We develop a methodology to efficiently implement the reversible jump Markov chain Monte Carlo (RJ-MCMC) algorithms of Green, applicable for example to model selection inference in a Bayesian framework, which builds on the ""dragging fast variables"" ideas of Neal. We call such algorithms annealed importance sampling reversible jump (aisRJ). The proposed procedures can be thought of as being exact approximations of idealized RJ algorithms which in a model selection problem would sample the model labels only, but cannot be implemented. Central to the methodology is the idea of bridging different models with fictitious intermediate models, whose role is to introduce smooth intermodel transitions and, as we shall see, improve performance. Efficiency of the resulting algorithms is demonstrated on two standard model selection problems and we show that despite the additional computational effort incurred, the approach can be highly competitive computationally. Supplementary materials for the article are available online."|Annealed Importance Sampling Reversible Jump MCMC Algorithms|http://www.jstor.org/stable/43304852|43304852|2013-09-01|2013|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Science & Mathematics', 'Computer Science', 'Statistics']
Postseason harvest surveys provide data used in the management of Missouri wildlife. These surveys provide information on the number of animals harvested, hunting pressure, and hunter success rate. These estimates provide unbiased results at the statewide level due to the large sample size. However, if this survey information is used to make county estimates, poor results often occur due to small sample sizes. To estimate hunter success at the county level for the 1996 Missouri Turkey Hunting Survey, we developed a hierarchical Bayesian model. Specifically, we evaluated a generalized linear model that incorporates linear covariate terms in addition to a conditional autoregressive structure for spatial correlation. Calculation of the posterior distribution was achieved through Gibbs sampling and adaptive rejection sampling. The inclusion of covariate terms was then evaluated using Bayes factors.|Estimating Hunting Success Rates via Bayesian Generalized Linear Models|http://www.jstor.org/stable/1400502|1400502|1999-12-01|1999|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Agriculture', 'Statistics']
This article offers a synthesis of Bayesian and sample-reuse approaches to the problem of high structure model selection geared to prediction. Similar methods are used for low structure models. Nested and nonnested paradigms are discussed and examples given.|A Predictive Approach to Model Selection|http://www.jstor.org/stable/2286745|2286745|1979-03-01|1979|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Empirical studies of bilateral foreign direct investment (FDI) activity show substantial differences in specifications with little agreement on the set of included covariates. We use Bayesian statistical techniques that allow one to select from a large set of candidates those variables most likely to be determinants of FDI activity. The variables with consistently high inclusion probabilities include traditional gravity variables, cultural distance factors, relative labour endowments and trade agreements. There is little support for multilateral trade openness, most host-country business costs, host-country infrastructure and host-country institutions. Our results suggest that many covariates found significant by previous studies are not robust. Les études empiriques des déterminants des activités d'investissement direct bilatéral à l'étranger ont des spécifications substantiellement différentes et peu d'accord sur les variables co-reliées incluses. On utilise des techniques statistiques bayesiennes qui permettent de balayer un vaste ensemble de variables à la recherche de celles qui sont davantage susceptibles d'être des déterminants des activités d'investissement direct à l'étranger. Les variables qui se retrouvent de manière régulière dans la liste de haute probabilité d'impact sont les variables reliées à la gravité, les facteurs liés à la distance culturelle, les dotations relatives en facteur travail, et les accords commerciaux. Il y a peu de support pour des variables comme l'ouverture au commerce multilatéral, la plupart des coûts d'affaires, les infrastructures et les institutions dans les pays hôtes. Ces résultats suggèrent que plusieurs co-variations qu'on a jugées significatives dans les études antérieures ne sont pas robustes.|Determinants of foreign direct investment|http://www.jstor.org/stable/43818887|43818887|2014-08-01|2014|['eng']|['Information science - Informetrics', 'Applied sciences - Engineering', 'Physical sciences - Astronomy']|['Business & Economics', 'Business', 'Political Science', 'Economics', 'Social Sciences']
A Bayesian approach allows the statistician to compute the posterior probability for each model in a set of possible models and therefore to retain consideration of several or many models throughout the analysis rather than to restrict attention to just one `best' model. This paper illustrates the use of Monte Carlo integration in hierarchical Bayesian analysis when there are many possible models of different dimensions. A hierarchical approach can facilitate the choice of a prior distribution as well as the Monte Carlo computation. The methodology is illustrated by an example in multiple logistic regression involving 256 possible models.|Hierarchical Bayesian Analysis Using Monte Carlo Integration: Computing Posterior Distributions When There are Many Possible Models|http://www.jstor.org/stable/2348514|2348514|1987-01-01|1987|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
A procedure is proposed to derive reference posterior distributions which approximately describe the inferential content of the data without incorporating any other information. More explicitly, operational priors, derived from information-theoretical considerations, are used to obtain reference posteriors which may be expected to approximate the posteriors which would have been obtained with the use of proper priors describing vague initial states of knowledge. The results obtained unify and generalize some previous work and seem to overcome criticisms to which this has been subject.|Reference Posterior Distributions for Bayesian Inference|http://www.jstor.org/stable/2985028|2985028|1979-01-01|1979|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
New techniques for the analysis of stochastic volatility models in which the logarithm of conditional variance follows an autoregressive model are developed. A cyclic Metropolis algorithm is used to construct a Markov-chain simulation tool. Simulations from this Markov chain converge in distribution to draws from the posterior distribution enabling exact finite-sample inference. The exact solution to the filtering/smoothing problem of inferring about the unobserved variance states is a by-product of our Markov-chain method. In addition, multistep-ahead predictive densities can be constructed that reflect both inherent model variability and parameter uncertainty. We illustrate our method by analyzing both daily and weekly data on stock returns and exchange rates. Sampling experiments are conducted to compare the performance of Bayes estimators to method of moments and quasi-maximum likelihood estimators proposed in the literature. In both parameter estimation and filtering, the Bayes estimators outperform these other approaches.|Bayesian Analysis of Stochastic Volatility Models|http://www.jstor.org/stable/1392199|1392199|1994-10-01|1994|['eng']|['Applied sciences - Engineering']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
In applications that involve clustered data, such as longitudinal studies and developmental toxicity experiments, the number of subunits within a cluster is often correlated with outcomes measured on the individual subunits. Analyses that ignore this dependency can produce biased inferences. This article proposes a Bayesian framework for jointly modeling cluster size and multiple categorical and continuous outcomes measured on each subunit. We use a continuation ratio probit model for the cluster size and underlying normal regression models for each of the subunit-specific outcomes. Dependency between cluster size and the different outcomes is accommodated through a latent variable structure. The form of the model facilitates posterior computation via a simple and computationally efficient Gibbs sampler. The approach is illustrated with an application to developmental toxicity data, and other applications, to joint modeling of longitudinal and event time data, are discussed.|A Bayesian Approach for Joint Modeling of Cluster Size and Subunit-Specific Outcomes|http://www.jstor.org/stable/3695428|3695428|2003-09-01|2003|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Ecologists often interpret variation in the spatial distribution of populations in terms of responses to environmental features, but disentangling the effects of individual variables can be difficult if latent effects and spatial and temporal correlations are not accounted for properly. Here, we use hierarchical models based on a Poisson-lognormal mixture to understand the spatial variation in relative abundance (counts per standardized unit of effort) of yellow perch, Perca flavescens, the most abundant fish species in Lake Saint Pierre, Quebec, Canada. The mixture incorporates spatially varying environmental covariates that represent local habitat characteristics, and random temporal and spatial effects that capture the effects of unobserved ecological processes. The sampling design covers the margins but not the central region of the lake. We fit spatial generalized linear mixed models based on three different prior covariance structures for the local latent effects: a single Gaussian process (GP) over the lake, a GP over a circle, and independent GP for each shore. The models allow for independence, isotropy, or nonstationary spatial effects. Nonstationarity is dealt with using two different approaches, geometric anisotropy and the inclusion of covariates in the correlation structure of the latent spatial process. The proposed approaches for specification of spatial domain and choice of Gaussian process priors may prove useful in other applications that involve spatial correlation along an irregular contour or in discontinuous spatial domains.|POPULATION COUNTS ALONG ELLIPTICAL HABITAT CONTOURS: HIERARCHICAL MODELING USING POISSON-LOGNORMAL MIXTURES WITH NONSTATIONARY SPATIAL STRUCTURE|http://www.jstor.org/stable/43826425|43826425|2015-09-01|2015|['eng']|['Mathematics - Mathematical objects']|['Mathematics', 'Science & Mathematics', 'Statistics']
There has been great interest in developing nonlinear structural equation models and associated statistical inference procedures, including estimation and model selection methods. In this paper a general semiparametric structural equation model (SSEM) is developed in which the structural equation is composed of nonparametric functions of exogenous latent variables and fixed covariates on a set of latent endogenous variables. A basis representation is used to approximate these nonparametric functions in the structural equation and the Bayesian Lasso method coupled with a Markov Chain Monte Carlo (MCMC) algorithm is used for simultaneous estimation and model selection. The proposed method is illustrated using a simulation study and data from the Affective Dynamics and Individual Differences (ADID) study. Results demonstrate that our method can accurately estimate the unknown parameters and correctly identify the true underlying model.|Bayesian Lasso for Semiparametric Structural Equation Models|http://www.jstor.org/stable/23270459|23270459|2012-06-01|2012|['eng']|['Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
Hybrid versions of independent and identically distributed weighted Chinese restaurant (WCR) algorithms are developed for inference in semiparametric linear mixed models under minimal assumptions for the random-effects distributions. The WCR method of working with the posterior partition structure leads to Rao-Blackwell estimates for higher-order moments of random effects, such as skewness and kurtosis, and can be used to estimate densities for random effects. A key feature of our approach is the manner in which we incorporate external estimates into our algorithms. The use of such information leads to simplified computational procedures, reduces the amount of user input required for specifying models, and results in numerical stability and accuracy. The resulting procedures are automated and can be readily used in standard statistical software. Our methods are tested by simulation and illustrated by application to a longitudinal study involving chronic renal disease.|Independent and Identically Distributed Monte Carlo Algorithms for Semiparametric Linear Mixed Models|http://www.jstor.org/stable/3085838|3085838|2002-12-01|2002|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Material indentation studies, in which a probe is brought into controlled physical contact with an experimental sample, have long been a primary means by which scientists characterize the mechanical properties of materials. More recently, the advent of atomic force microscopy, which operates on the same fundamental principle, has in turn revolutionized the nanoscale analysis of soft biomaterials such as cells and tissues. The paper addresses the inferential problems that are associated with material indentation and atomic force microscopy, through a framework for the change-point analysis of pre-contact and post-contact data that is applicable to experiments across a variety of physical scales. A hierarchical Bayesian model is proposed to account for experimentally observed change-point smoothness constraints and measurement error variability, with efficient Monte Carlo methods developed and employed to realize inference via posterior sampling for parameters such as Young's modulus, which is a key quantifier of material stiffness. These results are the first to provide the materials science community with rigorous inference procedures and quantification of uncertainty, via optimized and fully automated high throughput algorithms, implemented as the publicly available software package BayesCP. To demonstrate the consistent accuracy and wide applicability of this approach, results are shown for a variety of data sets from both macromaterials and micromaterials experiments—including silicone, neurons and red blood cells—conducted by the authors and others.|<strong>Bayesian change-point analysis for atomic force microscopy and soft material indentation</strong>|http://www.jstor.org/stable/40783092|40783092|2010-08-01|2010|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Weather predictions are uncertain by nature. This uncertainty is dynamically assessed by a finite set of trajectories, called ensemble members. Unfortunately, ensemble prediction systems underestimate the uncertainty and thus are unreliable. Statistical approaches are proposed to post-process ensemble forecasts, including Bayesian model averaging and the Bayesian processor of output. We develop a methodology, called the Bayesian processor of ensemble members, from a hierarchical model and combining the two aforementioned frameworks to calibrate ensemble forecasts. The Bayesian processor of ensemble members is compared with Bayesian model averaging and the Bayesian processor of output by calibrating surface temperature forecasting over eight stations in the province of Quebec (Canada). Results show that ensemble forecast skill is improved by the method developed.|Combining the Bayesian processor of output with Bayesian model averaging for reliable ensemble forecasting|http://www.jstor.org/stable/24771864|24771864|2015-01-01|2015|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
This article presents a new modeling strategy in functional data analysis. We consider the problem of estimating an unknown smooth function given functional data with noise. The unknown function is treated as the realization of a stochastic process, which is incorporated into a diffusion model. The method of smoothing spline estimation is connected to a special case of this approach. The resulting models offer great flexibility to capture the dynamic features of functional data, and allow straightforward and meaningful interpretation. The likelihood of the models is derived with Euler approximation and data augmentation. A unified Bayesian inference method is carried out via a Markov chain Monte Carlo algorithm including a simulation smoother. The proposed models and methods are illustrated on some prostate-specific antigen data, where we also show how the models can be used for forecasting.|Stochastic Functional Data Analysis: A Diffusion Model-Based Approach|http://www.jstor.org/stable/41434434|41434434|2011-12-01|2011|['eng']|['Physical sciences - Astronomy']|['Science and Mathematics', 'Statistics']
In contemporary military endeavours, Command and Control (C2) arrangements generally aim to ensure an appropriate regulation of command-decision autonomy such that decision makers are able to act in a way that is consistent with the overall set of commanders' intents and according to the nature of the unfolding situation. This can be a challenge, especially in situations with increasing degrees of uncertainty, ambiguity and complexity, also where individual commanders are faced with conflicting objectives. Increasingly, it seems that command decisions are being taken under conditions of internal command contention; for example, when the likely successful outcome of a tactical mission can often be at odds with the overall strategic and political aims of the campaign. The work in the paper builds on our previous research in decision making under uncertainty and conflicting objectives, where we analysed the responses of military commanders in decision experiments. We demonstrated how multi-attribute utility theory could be used to represent and understand the effects of uncertainty and conflicting objectives on a particular commander's choices. In this paper, we further develop and generalize the theory to show that the geometrical forms of expected utilities, which arise from the assumption of commander rationality, are qualitatively stable in a wide range of scenarios. This opens out into further analysis linking to Catastrophe Theory as it relates to C2 regulatory frameworks for devolving command decision freedoms. We demonstrate how an appreciation of this geometry can aid understanding of the relationship between socially complex operational environments and the prevailing C2, which can also inform selection and training of personnel, to address issues of devolving command decision-rights, as appropriate for the endeavour as a whole. The theory presented in the paper, therefore, provides a means to explore and gain insight into different approaches to regulation of C2 decision making aimed ultimately at achieving C2 agility, or at least at a conceptual language to allow its formal representation. C2 regulatory agents are discussed in terms of detailed functions for moderating command decision making, as appropriate for the degrees of uncertainty and goal contention being faced. The work also begins to address implications of any lack of experience and any differences in personality-type of the individual commanders with respect to risk-taking, open-mindedness and creativity.|Devolving command decisions in complex operations|http://www.jstor.org/stable/23355369|23355369|2013-01-01|2013|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Pure mathematics', 'Mathematics - Mathematical logic']|['Business', 'Business & Economics Collection']
This paper presents a general framework for model-based statistics. The framework is based on extended concepts of sufficiency and ancillarity. The fundamental theorem of the new theory exhibits in a single equation the reference distributions for decision theory, model checking, and conditional inference for a general statistical model. The theorem includes the equations upon which Bayesian and frequentist statistics are founded. The general theory also fills two gaps previously missing from empirical Bayesian statistics: concepts of sufficiency and conditional inference.|A General Framework for Model-Based Statistics|http://www.jstor.org/stable/2336054|2336054|1990-03-01|1990|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Posterior expectation is widely used as a Bayesian point estimator. In this note we extend it from parametric models to nonparametric models using empirical likelihood, and develop a nonparametric analogue of James-Stein estimation. We use the Laplace method to establish asymptotic approximations to our proposed posterior expectations, and show by simulation that they are often more efficient than the corresponding classical nonparametric procedures, especially when the underlying data are skewed.|Posterior expectation based on empirical likelihoods|http://www.jstor.org/stable/43304678|43304678|2014-09-01|2014|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Mathematical analysis']|['Science & Mathematics', 'Statistics']
We propose a Poisson-compound gamma approach for species richness estimation. Based on the denseness and nesting properties of the gamma mixture, we fix the shape parameter of each gamma component at a unified value, and estimate the mixture using nonparametric maximum likelihood. A least-squares crossvalidation procedure is proposed for the choice of the common shape parameter. The performance of the resulting estimator of N is assessed using numerical studies and genomic data.|Estimating species richness by a Poisson-compound gamma model|http://www.jstor.org/stable/25734119|25734119|2010-09-01|2010|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
First a comprehensive treatment of the hierarchical-conjugate Bayesian predictive approach to binary survey data is presented, encompassing simple random, stratified, cluster, and two-stage sampling, as well as two-stage sampling within strata. For the case of two-stage sampling within strata when there is more than one variable of stratification, analysis using an unsaturated logit linear model on the prior means is proposed. This allows there to be cells containing no sampled clusters. Formulas for posterior predictive means, variances, and covariances of numbers of successes in unsampled portions of clusters are presented in terms of posterior expectations of certain functions of hyperparameters; these may be evaluated by existing methods. The technique is illustrated using a small subset of Canada Youth &amp; AIDS Study data. A sample of students within each of various selected school boards was chosen and interviewed via questionnaire. The boards were stratified/poststratified in two dimensions, but some of the resulting cells contained no data. The additive logit linear model on the prior means produced estimates and posterior variances for boards in all cells. Data showed the additive model to be plausible. /// En première partie, nous présentons de façon rigoureuse l'approche prédictive de Bayes hiérarchique-conjuguée pour des données d'enquête binaires, recouvrant l'échantillonnage simple aléatoire, stratifié, en grappes, et à deux degrés, ainsi que l'échantillonnage à deux degrés à l'intérieur des strates. Pour l'échantillonnage à deux degrés à l'intérieur des strates, lorsqu'il y a plus d'une variable de stratification, nous proposons l'analyse selon un modèle linéaire logit non-saturé pour les moyennes a priori. Ceci permet d'avoir des cellules contenant aucune grappe de référence. Nous présentons des formules pour les moyennes, variances et covariances prédictives a posteriori des nombres de succès parmi les portions non-étudiées des grappes, en termes d'espérances a posteriori de certaines fonctions d'hyperparamètres qui peuvent être évaluées selon les méthodes existantes. Cette technique est illustrée à l'aide d'un petit sous-ensemble des données de l'étude jeunesse canadienne et SIDA. Un échantillon d'étudiants, à l'intérieur de chacun des conseils scolaires sélectionnés, fut choisi et interviewé par questionnaire. Les conseils furent stratifiés et/ou stratifiés a posteriori selon deux dimensions, mais certaines des cellules résultantes s'avérèrent sans données. Le modèle linéaire logit additif pour les moyennes a priori produisit des estimations et variances a posteriori pour les conseils dans chaque cellule. Les données montrèrent que le modèle additif est plausible.|Bayesian Analysis of Binary Survey Data|http://www.jstor.org/stable/3315821|3315821|1994-03-01|1994|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
The classification of Neolithic tools by using cluster analysis enables archaeologists to understand the function of the tools and the technological and cultural conditions of the societies that made them. In this paper, Bayesian classification is adopted to analyse data which raise the question whether the observed variability, e.g. the shape and dimensions of the tools, is related to their use. The data present technical difficulties for the practitioner, such as the presence of mixed mode data, missing data and errors in variables. These complications are overcome by employing a finite mixture model and Markov chain Monte Carlo methods. The analysis uses prior information which expresses the archaeologist's belief that there are two tool groups that are similar to contemporary adzes and axes. The resulting mixing densities provide evidence that the morphological dimensional variability among tools is related to the existence of these two tool groups.|Bayesian Classification of Neolithic Tools|http://www.jstor.org/stable/2988355|2988355|1998-01-01|1998|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
We describe methods used to create a new Census data base that can be used to study comparability of industry and occupation classification systems. This project represents the most extensive application of multiple imputation to date, and the modeling effort was considerable as well-hundreds of logistic regressions were estimated. One goal of this article is to summarize the strategies used in the project so that researchers can better understand how the new data bases were created. Another goal is to show how modifications of maximum likelihood methods were made for the modeling and imputation phases of the project. To multiply-impute 1980 census-comparable codes for industries and occupations in two 1970 census public-use samples, logistic regression models were estimated with flattening constants. For many of the regression models considered, the data were too sparse to support conventional maximum likelihood analysis, so some alternative had to be employed. These methods solve existence and related computational problems often encountered with maximum likelihood methods. Inferences pertaining to effects of predictor variables and inferences regarding predictions from logit models are also more satisfactory. The Bayesian strategy used in this project can be applied in other sparse-data settings where logistic regression is used because the approach can be implemented easily with any standard computer program for logit regression or log-linear analysis.|Multiple Imputation of Industry and Occupation Codes in Census Public-Use Samples Using Bayesian Logistic Regression|http://www.jstor.org/stable/2289716|2289716|1991-03-01|1991|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering', 'Philosophy - Logic']|['Science & Mathematics', 'Statistics']
Recent technological advances, such as proximity loggers, allow researchers to collect complete interaction histories, day and night, among sampled individuals over several months to years. Social network analyses are an obvious approach to analyzing interaction data because of their flexibility for fitting many different social structures as well as the ability to assess both direct contacts and indirect associations via intermediaries. For many network properties, however, it is not clear whether estimates based upon a sample of the network are reflective of the entire network. In wildlife applications, networks may be poorly sampled and boundary effects will be common. We present an alternative approach that utilizes a hierarchical modeling framework to assess the individual, dyadic, and environmental factors contributing to variation in the interaction rates and allows us to estimate the underlying process variation in each. In a disease control context, this approach will allow managers to focus efforts on those types of individuals and environments that contribute the most toward super-spreading events. We account for the sampling distribution of proximity loggers and the non-independence of contacts among groups by only using contact data within a group during days when the group membership of proximity loggers was known. This allows us to separate the two mechanisms responsible for a pair not contacting one another: they were not in the same group or they were in the same group but did not come within the specified contact distance. We illustrate our approach with an example dataset of female elk from northwestern Wyoming and conclude with a number of important future research directions.|Wildlife contact analysis: emerging methods, questions, and challenges|http://www.jstor.org/stable/23270636|23270636|2012-10-01|2012|['eng']|['Applied sciences - Engineering']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
Projection pursuit regression and kernel regression are methods for estimating a smooth function of several variables from noisy data obtained at scattered sites. Methods based on local averaging can perform poorly in high dimensions (curse of dimensionality). Intuition and examples have suggested that projection based approaches can provide better fits. For what sorts of regression functions is this true? When and by how much do projection methods reduce the curse of dimensionality? We make a start by focusing on the two-dimensional problem and study the L2 approximation error (bias) of the two procedures with respect to Gaussian measure. Let RA stand for a certain PPR-type approximation and KA for a particular kernel-type approximation. Building on a simple but striking duality for polynomials, we show that RA behaves significantly better than the minimax rate of approximation for radial functions, while KA performs significantly better than the minimax rate for harmonic functions. In fact, the rate improvements carry over to large classes, RA behaving very well for functions with enough angular smoothness (oscillating slowly with angle), while KA behaves very well for functions with enough Laplacian smoothness, (oscillations averaging out locally). The rate improvements matter: They are equivalent to lowering the dimensionality of the problem. For example, for functions with nice tail behavior, RA behaves as if the dimensionality of the problem were 1.5 rather than its nominal value 2. Also, RA and KA are complementary: For a given function, if one method offers a dimensionality reduction, the other does not.|Projection-Based Approximation and a Duality with Kernel Methods|http://www.jstor.org/stable/2241504|2241504|1989-03-01|1989|['eng']|['Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
The assessment of the performance of learners by means of benchmark experiments is an established exercise. In practice, benchmark studies are a tool to compare the performance of several competing algorithms for a certain learning problem. Cross-validation or resampling techniques are commonly used to derive point estimates of the performances which are compared to identify algorithms with good properties. For several benchmarking problems, test procedures taking the variability of those point estimates into account have been suggested. Most of the recently proposed inference procedures are based on special variance estimators for the cross-validated performance. We introduce a theoretical framework for inference problems in benchmark experiments and show that standard statistical test procedures can be used to test for differences in the performances. The theory is based on well-defined distributions of performance measures which can be compared with established tests. To demonstrate the usefulness in practice, the theoretical results are applied to regression and classification benchmark studies based on artificial and real world data.|The Design and Analysis of Benchmark Experiments|http://www.jstor.org/stable/27594139|27594139|2005-09-01|2005|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Computer Science', 'Statistics']
Bayesian analysis is given of a random effects binary probit model that allows for heteroscedasticity. Real and simulated examples illustrate the approach and show that ignoring heteroscedasticity when it exists may lead to biased estimates and poor prediction. The computation is carried out by an efficient Markov chain Monte Carlo sampling scheme that generates the parameters in blocks. We use the Bayes factor, cross-validation of the predictive density, the deviance information criterion and Receiver Operating Characteristic (ROC) curves for model comparison.|Bayesian estimation of a random effects heteroscedastic probit model|http://www.jstor.org/stable/23118096|23118096|2009-01-01|2009|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Business & Economics', 'Business', 'Economics']
Phage display is a biological process that is used to screen random peptide libraries for ligands that bind to a target of interest with high affinity. On the basis of a count data set from an innovative multistage phage display experiment, we propose a class of Bayesian mixture models to cluster peptide counts into three groups that exhibit different display patterns across stages. Among the three groups, the investigators are particularly interested in that with an ascending display pattern in the counts, which implies that the peptides are likely to bind to the target with strong affinity. We apply a Bayesian false discovery rate approach to identify the peptides with the strongest affinity within the group. A list of peptides is obtained, among which important ones with meaningful functions are further validated by biologists. To examine the performance of the Bayesian model, we conduct a simulation study and obtain desirable results.|Bayesian Mixture Models for Complex High Dimensional Count Data in Phage Display Experiments|http://www.jstor.org/stable/4626758|4626758|2007-01-01|2007|['eng']|['Biological sciences - Biology', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Bayesian networks are frequently used in educational assessments primarily for learning about students' knowledge and skills. There is a lack of works on assessing fit of Bayesian networks. This article employs the posterior predictive model checking method, a popular Bayesian model checking tool, to assess fit of simple Bayesian networks. A number of aspects of model fit, those of usual interest to practitioners, are assessed using various diagnostic tools. This article suggests a direct data display for assessing overall fit, suggests several diagnostics for assessing item fit, suggests a graphical approach to examine if the model can explain the association among the items, and suggests a version of the Mantel-Haenszel statistic for assessing differential item functioning. Limited simulation studies and a real data application demonstrate the effectiveness of the suggested model diagnostics.|Model Diagnostics for Bayesian Networks|http://www.jstor.org/stable/3701286|3701286|2006-04-01|2006|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Education', 'Statistics', 'Social Sciences']
We consider adapting a canonical computer model calibration apparatus, involving coupled Gaussian process (GP) emulators, to a computer experiment simulating radiative shock hydrodynamics that is orders of magnitude larger than what can typically be accommodated. The conventional approach calls for thousands of large matrix inverses to evaluate the likelihood in an MCMC scheme. Our approach replaces that costly ideal with a thrifty take on essential ingredients, synergizing three modern ideas in emulation, calibration and optimization: local approximate GP regression, modularization, and mesh adaptive direct search. The new methodology is motivated both by necessity—considering our particular application—and by recent trends in the supercomputer simulation literature. A synthetic data application allows us to explore the merits of several variations in a controlled environment and, together with results on our motivating real-data experiment, lead to noteworthy insights into the dynamics of radiative shocks as well as the limitations of the calibration enterprise generally.|CALIBRATING A LARGE COMPUTER EXPERIMENT SIMULATING RADIATIVE SHOCK HYDRODYNAMICS|http://www.jstor.org/stable/43826416|43826416|2015-09-01|2015|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Mathematics', 'Science & Mathematics', 'Statistics']
Investigators often change how variables are measured during the middle of data-collection, for example, in hopes of obtaining greater accuracy or reducing costs. The resulting data comprise sets of observations measured on two (or more) different scales, which complicates interpretation and can create bias in analyses that rely directly on the differentially measured variables. We develop approaches based on multiple imputation for handling mid-study changes in measurement for settings without calibration data, that is, no subjects are measured on both (all) scales. This setting creates a seemingly insurmountable problem for multiple imputation: since the measurements never appear jointly, there is no information in the data about their association. We resolve the problem by making an often scientifically reasonable assumption that each measurement regime accurately ranks the samples but on differing scales, so that, for example, an individual at the qth percentile on one scale should be at about the qth percentile on the other scale. We use rank-preservation assumptions to develop three imputation strategies that flexibly transform measurements made in one scale to measurements made in another: a Markov chain Monte Carlo (MCMC)-free approach based on permuting ranks of measurements, and two approaches based on dependent Dirichlet process (DDP) mixture models for imputing values conditional on covariates. We use simulations to illustrate conditions under which each strategy performs well, and present guidance on when to apply each. We apply these methods to a study of birth outcomes in which investigators collected mothers' blood samples to measure levels of environmental contaminants. Midway through data ascertainment, the study switched from one analytical lab to another. The distributions of blood lead levels differ greatly across the two labs, suggesting that the labs report measurements according to different scales. We use nonparametric Bayesian imputation models to obtain sets of plausible measurements on a common scale, and estimate quantile regressions of birth weight on various environmental contaminants.|Nonparametric Bayesian Multiple Imputation for Missing Data Due to Mid-Study Switching of Measurement Methods|http://www.jstor.org/stable/23239582|23239582|2012-06-01|2012|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Tumours develop in an evolutionary process, in which the accumulation of mutations produces subpopulations of cells with distinct mutational profiles, called clones. This process leads to the genetic heterogeneity widely observed in tumour sequencing data, but identifying the genotypes and frequencies of the different clones is still a major challenge. Here, we present Cloe, a phylogenetic latent feature model to deconvolute tumour sequencing data into a set of related genotypes. Our approach extends latent feature models by placing the features as nodes in a latent tree. The resulting model can capture both the acquisition and the loss of mutations, as well as episodes of convergent evolution. We establish the validity of Cloe on synthetic data and assess its performance on controlled biological data, comparing our reconstructions to those of several published state-of-the-art methods. We show that our method provides highly accurate reconstructions and identifies the number of clones, their genotypes and frequencies even at a modest sequencing depth. As a proof of concept, we apply our model to clinical data from three cases with chronic lymphocytic leukaemia and one case with acute myeloid leukaemia.|A PHYLOGENETIC LATENT FEATURE MODEL FOR CLONAL DECONVOLUTION|http://www.jstor.org/stable/44252239|44252239|2016-12-01|2016|['eng']|['Biological sciences - Biology']|['Mathematics', 'Science & Mathematics', 'Statistics']
We describe a Bayesian technique to (a) perform a sparse joint selection of significant predictor variables and significant inverse covariance matrix elements of the response variables in a high-dimensional linear Gaussian sparse seemingly unrelated regression (SSUR) setting and (b) perform an association analysis between the high-dimensional sets of predictors and responses in such a setting. To search the high-dimensional model space, where both the number of predictors and the number of possibly correlated responses can be larger than the sample size, we demonstrate that a marginalization-based collapsed Gibbs sampler, in combination with spike and slab type of priors, offers a computationally feasible and efficient solution. As an example, we apply our method to an expression quantitative trait loci (eQTL) analysis on publicly available single nucleotide polymorphism (SNP) and gene expression data for humans where the primary interest lies in finding the significant associations between the sets of SNPs and possibly correlated genetic transcripts. Our method also allows for inference on the sparse interaction network of the transcripts (response variables) after accounting for the effect of the SNPs (predictor variables). We exploit properties of Gaussian graphical models to make statements concerning conditional independence of the responses. Our method compares favorably to existing Bayesian approaches developed for this purpose.|Joint High-Dimensional Bayesian Variable and Covariance Selection with an Application to eQTL Analysis|http://www.jstor.org/stable/44695261|44695261|2013-06-01|2013|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
We explore and illustrate the use of time series decomposition methods for evaluating and comparing latent structure in nonstationary electroencephalographic (EEG) traces obtained from depressed patients during brain seizures induced as part of electro-convulsive therapy (ECT). Analysis of the patterns of change over time in the frequency structure of such EEG data provides insight into the neurophysiological mechanisms of action of this effective but poorly understood antidepressant treatment, and allows clinicians to modify ECT treatments to optimize therapeutic benefits while minimizing associated side effects. Our work has introduced new methods of time-frequency analysis of EEG series that identify the complete pattern of time evolution of frequency structure over the course of a seizure, and usefully assist in these scientific and clinical studies. New methods of decomposition of flexible dynamic models provide time domain decompositions of individual EEG series into collections of latent components in different frequency bands. This allows us to explore ECT seizure characteristics via inferences on the time-varying parameters that characterize these latent components, and to relate differences in such characteristics across seizures to differences in the therapeutic effectiveness and cognitive side effects of those seizures. This article discusses the scientific context and problems, development of nonstationary time series models and new methods of decomposition to explore time-frequency structure, and aspects of model fitting and analysis. We include applied studies on two datasets from recent clinical ECT studies. One is an initial illustrative analysis of a single EEG trace, the second compares the EEG data recorded during two types of ECT treatment that differ in therapeutic effectiveness and cognitive side effects. The uses of these models and time series decomposition methods in extracting and contrasting key features of the seizure underlying the EEG signals are highlighted. Through the use of these models we have quantified, for the first time, decreases in the dominant frequencies of low-frequency EEG components during ECT seizures. We have also identified preliminary evidence that such decreases are enhanced under the more effective ECTs at higher electrical dosages, a finding consistent with prior reports and the hypothesis that more effective forms of ECT are more effective in eliciting neurophysiological inhibitory processes.|Evaluation and Comparison of EEG Traces: Latent Structure in Nonstationary Time Series|http://www.jstor.org/stable/2670154|2670154|1999-06-01|1999|['eng']|['Biological sciences - Biology']|['Science & Mathematics', 'Statistics']
Although basketball is a dualistic sport, with all players competing on both offense and defense, almost all of the sport's conventional metrics are designed to summarize offensive play. As a result, player valuations are largely based on offensive performances and to a much lesser degree on defensive ones. Steals, blocks and defensive rebounds provide only a limited summary of defensive effectiveness, yet they persist because they summarize salient events that are easy to observe. Due to the inefficacy of traditional defensive statistics, the state of the art in defensive analytics remains qualitative, based on expert intuition and analysis that can be prone to human biases and imprecision. Fortunately, emerging optical player tracking systems have the potential to enable a richer quantitative characterization of basketball performance, particularly defensive performance. Unfortunately, due to computational and methodological complexities, that potential remains unmet. This paper attempts to fill this void, combining spatial and spatio-temporal processes, matrix factorization techniques and hierarchical regression models with player tracking data to advance the state of defensive analytics in the NBA. Our approach detects, characterizes and quantifies multiple aspects of defensive play in basketball, supporting some common understandings of defensive effectiveness, challenging others and opening up many new insights into the defensive elements of basketball.|CHARACTERIZING THE SPATIAL STRUCTURE OF DEFENSIVE SKILL IN PROFESSIONAL BASKETBALL|http://www.jstor.org/stable/24522412|24522412|2015-03-01|2015|['eng']|['Mathematics - Mathematical objects']|['Mathematics', 'Science & Mathematics', 'Statistics']
A rich nonparametric analysis of the finite normal mixture model is obtained by working with a precise truncation approximation of the Dirichlet process. Model fitting is carried out by a simple Gibbs sampling algorithm that directly samples the nonparametric posterior. The proposed sampler mixes well, requires no tuning parameters, and involves only draws from simple distributions, including the draw for the mass parameter that controls clustering, and the draw for the variances with the use of a nonconjugate uniform prior. Working directly with the nonparametric prior is conceptually appealing and among other things leads to graphical methods for studying the posterior mixing distribution as well as penalized MLE procedures for deriving point estimates. We discuss methods for automating selection of priors for the mean and variance components to avoid over or undersmoothing the data. We also look at the effectiveness of incorporating prior information in the form of frequentist point estimates.|Approximate Dirichlet Process Computing in Finite Normal Mixtures: Smoothing and Prior Information|http://www.jstor.org/stable/1391111|1391111|2002-09-01|2002|['eng']|['Applied sciences - Engineering', 'Physical sciences - Astronomy']|['Science & Mathematics', 'Computer Science', 'Statistics']
This article establishes a general formulation for Bayesian model-based clustering, in which subset labels are exchangeable, and items are also exchangeable, possibly up to covariate effects. The notational framework is rich enough to encompass a variety of existing procedures, including some recently discussed methods involving stochastic search or hierarchical clustering, but more importantly allows the formulation of clustering procedures that are optimal with respect to a specified loss function. Our focus is on loss functions based on pairwise coincidences, that is, whether pairs of items are clustered into the same subset or not. Optimization of the posterior expected loss function can be formulated as a binary integer programming problem, which can be readily solved by standard software when clustering a modest number of items, but quickly becomes impractical as problem scale increases. To combat this, a new heuristic item-swapping algorithm is introduced. This performs well in our numerical experiments, on both simulated and real data examples. The article includes a comparison of the statistical performance of the (approximate) optimal clustering with earlier methods that are model-based but ad hoc in their detailed definition.|Bayesian Model-Based Clustering Procedures|http://www.jstor.org/stable/27594259|27594259|2007-09-01|2007|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Computer Science', 'Statistics']
Methods are introduced and illustrated for synthesizing evidence from case-control and cohort studies, and controlled trials, accounting for differences among the studies in their design, length of follow-up and quality. The methods, based on hierarchical but nonexchangeable Bayesian models, are illustrated in a synthesis of disparate information about the health effects of passive exposure to environmental tobacco smoke.|Adjusted Likelihoods for Synthesizing Empirical Evidence from Studies That Differ in Quality and Design: Effects of Environmental Tobacco Smoke|http://www.jstor.org/stable/4144397|4144397|2004-08-01|2004|['eng']|['Information science - Coding theory', 'Health sciences - Health and wellness', 'Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
We investigated the risk factors for childhood malnutrition in India based on the 2005/2006 Demographic and Health Survey by applying a novel estimation technique for additive quantile regression. Ordinary linear and generalized linear regression models relate the mean of a response variable to a linear combination of covariate effects, and, as a consequence, focus on average properties of the response. The use of such a regression model for analyzing childhood malnutrition in developing or transition countries implies that the estimated effects describe the average nutritional status. However, it is of even greater interest to analyze quantiles of the response distribution, such as the 5% or 10% quantile, which relate to the risk of extreme malnutrition. Our investigation is based on a semiparametric extension of quantile regression models where different types of nonlinear effects are included in the model equation, leading to additive quantile regression. We addressed the variable selection and model choice problems associated with estimating such an additive quantile regression model using a novel boosting approach. Our proposal allows for data-driven determination of the amount of smoothness required for the nonlinear effects and combines model choice with an automatic variable selection property. In an empirical evaluation, we compared our boosting approach with state-of-the-art methods for additive quantile regression. The results suggest that boosting is an appropriate tool for estimation and variable selection in additive quantile regression models and helps to identify yet unknown risk factors for childhood malnutrition. This article has supplementary material online.|Identifying Risk Factors for Severe Childhood Malnutrition by Boosting Additive Quantile Regression|http://www.jstor.org/stable/41416387|41416387|2011-06-01|2011|['eng']|['Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
In this article a time-varying coefficient model is developed to examine the relationship between adverse health and short-term (acute) exposure to air pollution. This model allows the relative risk to evolve over time, which may be due to an interaction with temperature, or from a change in the composition of pollutants, such as particulate matter, over time. The model produces a smooth estimate of these time-varying effects, which are not constrained to follow a fixed parametric form set by the investigator. Instead, the shape is estimated from the data using penalized natural cubic splines. Poisson regression models, using both quasi-likelihood and Bayesian techniques, are developed, with estimation performed using an iteratively re-weighted least squares procedure and Markov chain Monte Carlo simulation, respectively. The efficacy of the methods to estimate different types of time-varying effects are assessed via a simulation study, and the models are then applied to data from four cities that were part of the National Morbidity, Mortality, and Air Pollution Study. /// Nous présentons un modèle à coefficient dépendant du temps pour étudier la relation entre exposition aiguë (à court terme) à la pollution atmosphérique et altérations de la santé. Ce modèle permet au risque relatif de varier en fonction du temps comme cela peut se produire en cas d'interaction avec la température ou de modification de la composition en polluants, par exemple en particules solides. Le modèle fournit une estimation lissée de ces effets dépendant du temps sans qu'il soit nécessaire de spécifier une forme paramétrique particulière. Cette forme est estimée à partir des données à l'aide de fonctions spline cubiques pénalisées. Nous développons des modèles de régression de Poisson dont les coefficients sont estimés soit en maximisant une quasi-vraisemblance par une procédure itérative de moindres carrés repondérés, soit par une approche bayésienne utilisant une méthode de Monte Carlo par chaîne de Markov (MCMC). L'efficacité de ces approches pour estimer différents effets dépendant du temps est évaluée par simulations et les modèles sont appliqués aux données recueillies dans quatre villes pour l'enquête NMMAPS (Enquête Nationale Morbidité, Mortalité et Pollution Atmosphérique).|Time-Varying Coefficient Models for the Analysis of Air Pollution and Health Outcome Data|http://www.jstor.org/stable/4541481|4541481|2007-12-01|2007|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
We propose a class of longitudinal data models with random effects that generalizes currently used models in two important ways. First, the random-effects model is a flexible mixture of multivariate normals, accommodating population heterogeneity, outliers, and nonlinearity in the regression on subject-specific covariates. Second, the model includes a hierarchical extension to allow for meta-analysis over related studies. The random-effects distributions are decomposed into one part that is common across all related studies (common measure), and one part that is specific to each study and that captures the variability intrinsic between patients within the same study. Both the common measure and the study-specific measures are parameterized as mixture-of-normais models. We carry out inference using reversible jump posterior simulation to allow a random number of terms in the mixtures. The sampler takes advantage of the small number of entertained models. The motivating application is the analysis of two studies carried out by the Cancer and Leukemia Group B (CALGB). In both studies, we record for each patient white blood cell counts (WBC) over time to characterize the toxic effects of treatment. The WBCs are modeled through a nonlinear hierarchical model that gathers the information from both studies.|Bayesian Meta-Analysis for Longitudinal Data Models Using Multivariate Mixture Priors|http://www.jstor.org/stable/3695813|3695813|2003-03-01|2003|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Finding an unconstrained and statistically interpretable reparameterization of a covariance matrix is still an open problem in statistics. Its solution is of central importance in covariance estimation, particularly in the recent high-dimensional data environment where enforcing the positive-definiteness constraint could be computationally expensive. We provide a survey of the progress made in modeling covariance matrices from two relatively complementary perspectives: (1) generalized linear models (GLM) or parsimony and use of covariates in low dimensions, and (2) regularization or sparsity for high-dimensional data. An emerging, unifying and powerful trend in both perspectives is that of reducing a covariance estimation problem to that of estimating a sequence of regression problems. We point out several instances of the regression-based formulation. A notable case is in sparse estimation of a precision matrix or a Gaussian graphical model leading to the fast graphical LASSO algorithm. Some advantages and limitations of the regression-based Cholesky decomposition relative to the classical spectral (eigenvalue) and variance-correlation decompositions are highlighted. The former provides an unconstrained and statistically interpretable reparameterization, and guarantees the positive-definiteness of the estimated covariance matrix. It reduces the unintuitive task of covariance estimation to that of modeling a sequence of regressions at the cost of imposing an a priori order among the variables. Elementwise regularization of the sample covariance matrix such as banding, tapering and thresholding has desirable asymptotic properties and the sparse estimated covariance matrix is positive definite with probability tending to one for large samples and dimensions.|Covariance Estimation: The GLM and Regularization Perspectives|http://www.jstor.org/stable/23059137|23059137|2011-08-01|2011|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Mathematical logic']|['Science and Mathematics', 'Statistics']
Using annual observations on industrial production over the last three centuries, and on GDP over a 100-year period, we seek an historical perspective on the forecastability of these UK output measures. The series are dominated by strong upward trends, so we consider various specifications of this, including the local linear trend structural time-series model, which allows the level and slope of the trend to vary. Our results are not unduly sensitive to how the trend in the series is modelled: the average sizes of the forecast errors of all models, and the wide span of prediction intervals, attests to a great deal of uncertainty in the economic environment. It appears that, from an historical perspective, the postwar period has been relatively more forecastable.|AN HISTORICAL PERSPECTIVE ON FORECAST ERRORS|http://www.jstor.org/stable/23872693|23872693|2002-07-01|2002|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Economics']
Many basic questions in the social network literature center on the distribution of aggregate structural properties within and across populations of networks. Such questions are of increasing relevance given the growing availability of network data suitable for meta-analytic studies, as well as the rise of study designs that involve the collection of data on multiple networks drawn from a larger population. Despite this, little work has been done on model-based inference for the properties of graph populations, or on methods for comparing such populations. Here, we attempt to rectify this gap by introducing a family of techniques that combines an existing approach to the identification of structural biases in network data (the use of conditional uniform graph quantiles) with strategies drawn from nonparametric Bayesian analysis. Conditional uniform graph quantiles are the quantiles of an observed structural property in the reference distribution produced by evaluating that property over all graphs with certain fixed characteristics (e.g., size or density). These quantiles have long been used to measure the extent to which a property of interest on a single network deviates from what would be expected given that network's other characteristics. The methods introduced here employ such quantile information to allow for principled inference regarding the distribution of structural biases within (and comparison across) populations of networks, given data sampled at the network level. The data requirements of these methods are minimal, thus making them well-suited to meta-analytic applications for which complete network data (as opposed to summary statistics) are often unavailable. The structural biases inferred using these methods can be expressed in terms of posterior predictives for familiar and easily communicated quantities, such as p-values. In addition to the methods themselves, we present algorithms for posterior simulation from this model class, illustrating their use with applications to the analysis of social structure within urban communes and radio communications among emergency personnel. We also discuss how this approach may applied to quantiles arising from other reference distributions, such as those obtained using general exponential-family random graph models.|BAYESIAN META-ANALYSIS OF SOCIAL NETWORK DATA VIA CONDITIONAL UNIFORM GRAPH QUANTILES|http://www.jstor.org/stable/41336924|41336924|2011-01-01|2011|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics']|['Sociology', 'Social Sciences']
The paper develops a unified theoretical and computational framework for false discovery control in multiple testing of spatial signals. We consider both pointwise and clusterwise spatial analyses, and derive oracle procedures which optimally control the false discovery rate, false discovery exceedance and false cluster rate. A data-driven finite approximation strategy is developed to mimic the oracle procedures on a continuous spatial domain. Our multiple-testing procedures are asymptotically valid and can be effectively implemented using Bayesian computational algorithms for analysis of large spatial data sets. Numerical results show that the procedures proposed lead to more accurate error control and better power performance than conventional methods. We demonstrate our methods for analysing the time trends in tropospheric ozone in eastern USA.|False discovery control in large-scale spatial multiple testing|http://www.jstor.org/stable/24774725|24774725|2015-01-01|2015|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
The Caloplaca saxícola group is the main group of saxicolous, lobed-effigurate species within genus Caloplaca (Teloschistaceae, lichen-forming Ascomycota). A recent monographic revision by the first author detected a wide range of morphological variation. To confront the phenotypically based circumscription of these taxa and to resolve their relationships morphological and ITS rDNA data were obtained for 56 individuals representing eight Caloplaca species belonging to the C. saxícola group. We tested the monophyly of these eight morphospecies by performing maximum parsimony, maximum likelihood and two different types of Bayesian analyses (with and without a priori alignments). Restricting phylogenetic analyses to unambiguously aligned portions of ITS was sufficient to resolve, with high bootstrap support, five of the eight previously recognized species within the C. saxícola group. However, phylogenetic resolution of all or most of the eight species currently included as two distinct subgroups within the C. saxícola group was possible only by combining morphological characters and signal from ambiguously aligned regions with the unambiguously aligned ITS sites or when the entire ITS1 and 2 regions were not aligned a priori and included as an integral component of a Bayesian analysis (BAli-Phy). The arnoldii subgroup includes C. arnoldii, comprising four subspecies, and the to the C. saxícola subgroup, monophyly of taxa included within the arnoldii subgroup and their relationships could not be resolved with combined ITS and morphological data. Unequivocal morphological synapomorphies for all species except C. arnoldii and C. pusilla are recognized and presented.|Align or not to align? Resolving species complexes within the Caloplaca saxicola group as a case study|http://www.jstor.org/stable/41151437|41151437|2011-03-01|2011|['eng']|['Biological sciences - Biology']|['Biological Sciences', 'Botany & Plant Sciences', 'Science and Mathematics']
The Baysian analysis of loglinear models requires the evaluation of high-dimensional integrals. Such an evaluation is frequently computationally prohibitive even with modern computers. We provide a parameterization of the loglinear model which renders these integrations amenable to the numerical methods of adaptive important sampling. This approach is applied in the analysis of two-way contingency tables using Goodman's RC model. We base the analysis on the full posterior distribution for the loglinear model and obtain the posterior distribution of a goodness-of-fit measure for Goodman's RC model.|COMPUTATIONAL ISSUES IN THE BAYESIAN ANALYSIS OF CATEGORICAL DATA: LOG-LINEAR AND GOODMAN'S RC MODEL|http://www.jstor.org/stable/24304965|24304965|1993-07-01|1993|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Mathematics', 'Science and Mathematics', 'Statistics']
We develop a feature allocation model for inference on genetic tumor variation using next-generation sequencing data. Specifically, we record single nucleotide variants (SNVs) based on short reads mapped to human reference genome and characterize tumor heterogeneity by latent haplotypes defined as a scaffold of SNVs on the same homologous genome. For multiple samples from a single tumor, assuming that each sample is composed of some sample-specific proportions of these haplotypes, we then fit the observed variant allele fractions of SNVs for each sample and estimate the proportions of haplotypes. Varying proportions of haplotypes across samples is evidence of tumor heterogeneity since it implies varying composition of cell subpopulations. Taking a Bayesian perspective, we proceed with a prior probability model for all relevant unknown quantities, including, in particular, a prior probability model on the binary indicators that characterize the latent haplotypes. Such prior models are known as feature allocation models. Specifically, we define a simplified version of the Indian buffet process, one of the most traditional feature allocation models. The proposed model allows overlapping clustering of SNVs in defining latent haplotypes, which reflects the evolutionary process of subclonal expansion in tumor samples.|A BAYESIAN FEATURE ALLOCATION MODEL FOR TUMOR HETEROGENEITY|http://www.jstor.org/stable/24522595|24522595|2015-06-01|2015|['eng']|['Biological sciences - Biology']|['Mathematics', 'Science & Mathematics', 'Statistics']
This paper studies U. S. inflation adjustment speed to aggregate technology shocks and to monetary policy shocks in a medium size Bayesian vector autoregression model. According to the model estimated on the 1959-2007 sample, inflation adjusts much faster to aggregate technology shocks than to monetary policy shocks. These results are robust to different identification assumptions and measures of aggregate prices. However, by separately estimating the model over the pre-and post-1980 periods, this paper further shows that inflation adjusts much faster to technology shocks than to monetary policy shocks in the post-1980 period, but not in the pre-1980 period.|Does Inflation Adjust Faster to Aggregate Technology Shocks than to Monetary Policy Shocks?|http://www.jstor.org/stable/41304419|41304419|2011-12-01|2011|['eng']|['Information science - Informetrics', 'Economics - Economic disciplines', 'Business - Business operations']|['Business', 'Business & Economics Collection', 'Economics', 'Finance']
Theories of grammatical development differ in how much abstract knowledge they attribute to young children. Here, we report a series of experiments using a computational model to evaluate the explanatory power of child grammars based not on abstract rules but on concrete words and phrases and some local abstractions associated with these words and phrases. We use a Bayesian procedure to extract such item-based grammars from transcriptions of 28+ h of each of two children's speech at 2 and 3 years of age. We then use these grammars to parse all of the unique multiword utterances from transcriptions of separate recordings of these same children at each of the two ages. We found that at 2 years of age such a model had good coverage and predictive fit, with the children showing radically limited productivity. Furthermore, adding expert-annotated parts of speech to the induction procedure had little effect on coverage, with the exception of the category of noun. At age 3, the children's productivity sharply increased and the addition of a verb and a noun category markedly improved the model's performance.|Modeling Children's Early Grammatical Knowledge|http://www.jstor.org/stable/40485180|40485180|2009-10-13|2009|['eng']|['Linguistics - Grammar', 'Linguistics - Language', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Biological Sciences', 'General Science']
Motivated by a study exploring geographic disparities in test scores among fourth graders in North Carolina, we develop a multivariate mixture model for the spatial analysis of correlated continuous outcomes. The responses are modelled as a finite mixture of multivariate normal distributions, which accommodates a wide range of marginal response distributions and allows investigators to examine covariate effects within subpopulations of interest. The model has a hierarchical structure incorporating both individual and areal level predictors as well as spatial random effects for each mixture component. Conditional auto-regressive priors on the random effects provide spatial smoothing and allow the shape of the multivariate distribution to vary flexibly across geographic regions. By integrating over this distribution, we obtain region-specific joint, marginal and conditional inferences of interest. We adopt a Bayesian modelling approach and develop an efficient posterior sampling algorithm that relies primarily on closed form full conditionals. Our results show that students in the central and coastal counties of North Carolina demonstrate higher achievement on average than students in the other parts of the state. These findings can be used to guide county level initiatives, such as school-based literacy programmes, to improve elementary education.|A multivariate spatial mixture model for areal data: examining regional differences in standardized test scores|http://www.jstor.org/stable/24771886|24771886|2014-11-01|2014|['eng']|['Applied sciences - Engineering', 'Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
Assuming a set of observations is available from the general linear model, and assuming prior information about parameters, we propose a method of assessing the influence of specified subsets of the data when the goal is to predict future observations.|A Predictive View of the Detection and Characterization of Influential Observations in Regression Analysis|http://www.jstor.org/stable/2287120|2287120|1983-03-01|1983|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Bayesian Analysis, Computation and Communication (BACC) is a new Bayesian software package which is linked to Gauss and takes the form of a set of Gauss commands. In this review, I outline a list of qualities that a Bayesian software package should have. I then discuss whether BACC has these qualities in the context of a brief description and an extended example.|Bayesian Analysis, Computation and Communication Software|http://www.jstor.org/stable/223215|223215|1999-11-01|1999|['eng']|['Applied sciences - Engineering', 'Information science - Data products']|['Business & Economics', 'Business', 'Economics']
The $F$-test for one-way analysis of variance and the $\chi^2$-test are reduced to one-dimensional tests of the non-centrality parameter by using marginal likelihoods. The Bayesian approach, therefore, can be based in the use of suitable prior distributions of the non-centrality parameter. To carry out a robustness analysis, we calculate the upper and lower bounds of the Bayes factors in favour of $H_o$ by using suitable classes of priors. Examples of applications and comparisons with results obtained from other procedures are also given.|Robust Bayesian Analysis in Analysis of Variance and the $\chi^2$-Test by Using Marginal Likelihoods|http://www.jstor.org/stable/2348943|2348943|1994-01-01|1994|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic', 'Mathematics - Mathematical analysis']|['Science & Mathematics', 'Statistics']
Empirical Bayes regression procedures are often used in educational and psychological testing as extensions to latent variables models. The National Assessment of Educational Progress (NAEP) is an important national survey using such procedures. The NAEP applies empirical Bayes methods to models from item response theory to calibrate student responses to questions of varying difficulty. Due partially to the limited computing technology that existed when NAEP was first conceived, NAEP analyses are carried out using a two-stage estimation procedure that ignores uncertainty about some model parameters. Furthermore, the item response theory model that NAEP uses ignores the effect of item clustering created by the design of a test form. Using Markov chain Monte Carlo, we simultaneously estimate all parameters of an expanded model that considers item clustering to investigate the impact of item clustering and ignoring uncertainty about model parameters on an important outcome measure that NAEP report. Ignoring these two effects causes substantial underestimation of standard errors and induces a modest bias in location estimates.|Empirical Bayes and Item-Clustering Effects in a Latent Variable Hierarchical Model: A Case Study from the National Assessment of Educational Progress|http://www.jstor.org/stable/3085654|3085654|2002-06-01|2002|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Road safety has recently become a major concern in most modern societies. The identification of sites that are more dangerous than others (black spots) can help in better scheduling road safety policies. This paper proposes a methodology for ranking sites according to their level of hazard. The model is innovative in at least two respects. Firstly, it makes use of all relevant information per accident location, including the total number of accidents and the number of fatalities, as well as the number of slight and serious injuries. Secondly, the model includes the use of a cost function to rank the sites with respect to their total expected cost to society. Bayesian estimation for the model via a Markov chain Monte Carlo approach is proposed. Accident data from 519 intersections in Leuven (Belgium) are used to illustrate the methodology proposed. Furthermore, different cost functions are used to show the effect of the proposed method on the use of different costs per type of injury.|A Bayesian Model for Ranking Hazardous Road Sites|http://www.jstor.org/stable/4623224|4623224|2007-01-01|2007|['eng']|['Information science - Coding theory']|['Science & Mathematics', 'Statistics']
We show how a retailer can estimate the optimal price of a new product using observed transaction prices from online second-price auction experiments. For this purpose we propose a Bayesian Pólya tree approach which, given the limited nature of the data, requires a specially tailored implementation. Avoiding the need for a priori parametric assumptions, the Pólya tree approach allows for flexible inference of the valuation distribution, leading to more robust estimation of optimal price than competing parametric approaches. In collaboration with an online jewelry retailer, we illustrate how our methodology can be combined with managerial prior knowledge to estimate the profit maximizing price of a new jewelry product.|OPTIMAL PRICING USING ONLINE AUCTION EXPERIMENTS: A PÓLYA TREE APPROACH|http://www.jstor.org/stable/41713441|41713441|2012-03-01|2012|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
"This article provides a new methodology for estimating the term structure of corporate debt using a semiparametric penalized spline model. The method is applied to a case study of AT&amp;T bonds. Typically, very few data are available on individual corporate bond prices, too little to find a nonparametric estimate of term structure from these bonds alone. This problem is solved by ""borrowing strength"" from Treasury bond data. More specifically, we combine a nonparametric model for the term structure of Treasury bonds with a parametric component for the credit spread. Our methodology generalizes the work of Fisher, Nychka, and Zervos in several ways. First, their model was developed for Treasury bonds only and cannot be applied directly to corporate bonds. Second, we more fully investigate the problem of choosing the smoothing parameter, a problem that is complicated because the forward rate is the derivative — log{D(t)}, where the discount function D is the function fit to the data. In our case study, estimation of the derivative requires substantially more smoothing than selected by generalized cross-validation (GCV). Another problem for smoothing parameter selection is possible correlation of the errors. We compare three methods of choosing the penalty parameter: generalized cross-validation (GCV), the residual spatial autocorrelation (RSA) method of Ellner and Seifu, and an extension of Ruppert's empirical bias bandwidth selection (EBBS) to splines. Third, we provide approximate sampling distributions based on asymptotics for the Treasury forward rate and the bootstrap for corporate bonds. Confidence bands and tests of interesting hypotheses, for example, about the functional form of the credit spreads, are also discussed."|Estimating the Interest Rate Term Structure of Corporate Debt with a Semiparametric Penalized Spline Model|http://www.jstor.org/stable/27590353|27590353|2004-03-01|2004|['eng']|['Economics - Economic disciplines']|['Science & Mathematics', 'Statistics']
Greenland (2000, Biometrics 56, 915-921) describes the use of random coefficient regression to adjust for residual confounding in a particular setting. We examine this setting further, giving theoretical and empirical results concerning the frequentist and Bayesian performance of random coefficient regression. Particularly, we compare estimators based on this adjustment for residual confounding to estimators based on the assumption of no residual confounding. This devolves to comparing an estimator from a nonidentified but more realistic model to an estimator from a less realistic but identified model. The approach described by Gustafson (2005, Statistical Science 20, 111-140) is used to quantify the performance of a Bayesian estimator arising from a nonidentified model. From both theoretical calculations and simulations we find support for the idea that superior performance can be obtained by replacing unrealistic identifying constraints with priors that allow modest departures from those constraints. In terms of point-estimator bias this superiority arises when the extent of residual confounding is substantial, but the advantage is much broader in terms of interval estimation. The benefit from modeling residual confounding is maintained when the prior distributions employed only roughly correspond to reality, for the standard identifying constraints are equivalent to priors that typically correspond much worse.|The Performance of Random Coefficient Regression in Accounting for Residual Confounding|http://www.jstor.org/stable/4124585|4124585|2006-09-01|2006|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
The authors develop a new class of distributions by introducing skewness in multivariate elliptically symmetric distributions. The class, which is obtained by using transformation and conditioning, contains many standard families including the multivariate skew-normal and t distributions. The authors obtain analytical forms of the densities and study distributional properties. They give practical applications in Bayesian regression models and results on the existence of the posterior distributions and moments under improper priors for the regression coefficients. They illustrate their methods using practical examples. /// Les auteurs engendrent une nouvelle classe de lois en introduisant un facteur d'asymétrie dans la famille des distributions multivariées elliptiquement symétriques. La classe, qui est obtenue par transformation et conditionnement, inclut plusieurs familles de lois connues, dont la Student et la normale multivariées asymétriques. Les auteurs donnent la forme explicite de la densité de ces lois et en examinent les propriétés. Ils en présentent des applications pratiques dans le cadre des modèles de régression bayésiens, où ils démontrent l'existence de lois a posteriori et de leurs moments lorsque les lois a priori des paramètres de la régression sont impropres. Ils illustrent en outre leurs méthodes dans des cas concrets.|A New Class of Multivariate Skew Distributions with Applications to Bayesian Regression Models|http://www.jstor.org/stable/3316064|3316064|2003-06-01|2003|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
We introduce a flexible spatial point process model for spatial point patterns exhibiting linear structures, without incorporating a latent line process. The model is given by an underlying sequential point process model. Under this model, the points can be of one of three types: a 'background point' an 'independent cluster point' or a 'dependent cluster point'. The background and independent cluster points are thought to exhibit 'complete spatial randomness', whereas the dependent cluster points are likely to occur close to previous cluster points. We demonstrate the flexibility of the model for producing point patterns with linear structures and propose to use the model as the likelihood in a Bayesian setting when analysing a spatial point pattern exhibiting linear structures. We illustrate this methodology by analysing two spatial point pattern datasets (locations of bronze age graves in Denmark and locations of mountain tops in Spain).|A Sequential Point Process Model and Bayesian Inference for Spatial Point Patterns with Linear Structures|http://www.jstor.org/stable/23357214|23357214|2012-12-01|2012|['eng']|['Mathematics - Mathematical objects']|['Science and Mathematics', 'Statistics']
Employing a unique and rich data set of water quality attributes in conjunction with detailed household characteristics and trip information, we develop a mixed logit model of recreational lake usage and undertake thorough model specification and fitting procedures to identify the best set of explanatory variables, and their functional form for the estimated model. Our empirical analysis shows that individuals are responsive to the full set of water quality measures used by biologists to identify the impaired status of lakes. Thus, changes in these quality measures are not simply a scientific exercise, but they also translate into changes in the recreational usage patterns and well-being of individual households. Willingness-to-pay (WTP) estimates are reported based on improvements in these physical measures.|Valuing Water Quality as a Function of Water Quality Measures|http://www.jstor.org/stable/20492412|20492412|2009-02-01|2009|['eng']|['Applied sciences - Engineering']|['Business & Economics', 'Science & Mathematics', 'Agriculture', 'Business', 'Economics']
We develop a flexible business cycle indicator that accounts for potential time variation in macroeconomic variables. The coincident economic indicator is based on a multi variate trend cycle decomposition model and is constructed from a moderate set of US macroeconomic time series. In particular, we consider an unobserved components time series model with a common cycle that is shared across different time series but adjusted for phase shift and amplitude. The extracted cycle can be interpreted as a model-based bandpass filter and is designed to emphasize the business cycle frequencies that are of interest to applied researchers and policymakers. Stochastic volatility processes and mixture distributions for the irregular components and the common cycle disturbances enable us to account for the heteroskedasticity present in the data. Forecasting results are presented for a set of different specifications. Point forecasts from the preferred model indicate a future recession with the uncertainty over the business cycle growing quickly as the forecast horizon increases.|EXTRACTING A ROBUST US BUSINESS CYCLE USING A TIME-VARYING MULTIVARIATE MODEL-BASED BANDPASS FILTER|http://www.jstor.org/stable/40607052|40607052|2010-06-01|2010|['eng']|['Physical sciences - Astronomy']|['Business & Economics', 'Business', 'Economics']
We prove that multifractal functions, characterized by their wavelet representation, can be estimated in the white noise model by a Bayesian method. We give rates of convergence for two different models. Further, we study empirical methods for estimating the hyperparameters of the model, which lead to a fully tractable estimator.|Wavelet Estimation of a Multifractal Function|http://www.jstor.org/stable/3318926|3318926|2005-04-01|2005|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Mathematical analysis']|['Mathematics', 'Science and Mathematics', 'Statistics']
A mixed model is proposed for the analysis of geographic variability in mortality rates. In addition to demographic parameters and random geographic parameters, the model includes additional random-effects parameters to adjust for extra-Poisson variability. The model uses a gamma-Poisson distribution with a random scale parameter having an inverse gamma prior. An empirical Bayes approach is used to estimate relative risks for geographic regions and annual rates for demographic groups within each region. Lung cancer in Missouri is used to motivate and illustrate the procedure. Observed disease-specific death rates of specific age/sex groups, within regional units such as counties or cities, are generally quite unreliable for all but the largest units. The amount of information available from any one unit is generally limited. But modeling the variability between and within units can improve estimates, as demonstrated frequently in empirical Bayes examples. A numerical comparison with the fixed effects multiplicative Poisson model demonstrates the considerable flexibility the random effects model has in showing how geographic effects change over different age/sex groups. Computing maximum likelihood estimates of hyper-parameters requires a fair amount of work, since the solution is iterative and requires numerical integration. Expressions are provided to facilitate computation for similar problems.|Mixed Model for Analyzing Geographic Variability in Mortality Rates|http://www.jstor.org/stable/2288916|2288916|1988-03-01|1988|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
Transcriptional reprogramming forms a major part of a plant's response to pathogen infection. Many individual components and pathways operating during plant defense have been identified, but our knowledge of how these different components interact is still rudimentary. We generated a high-resolution time series of gene expression profiles from a single Arabidopsis thaliana leaf during infection by the necrotrophic fungal pathogen Botrytis cinerea. Approximately one-third of the Arabidopsis genome is differentially expressed during the first 48 h after infection, with the majority of changes in gene expression occurring before significant lesion development. We used computational tools to obtain a detailed chronology of the defense response against B. cinerea, highlighting the times at which signaling and metabolic processes change, and identify transcription factor families operating at different times after infection. Motif enrichment and network inference predicted regulatory interactions, and testing of one such prediction identified a role for TGA3 in defense against necrotrophic pathogens. These data provide an unprecedented level of detail about transcriptional changes during a defense response and are suited to systems biology analyses to generate predictive models of the gene regulatory networks mediating the Arabidopsis response to B. cinerea.|Arabidopsis Defense against Botrytis cinerea: Chronology and Regulation Deciphered by High-Resolution Temporal Transcriptomic Analysis|http://www.jstor.org/stable/41692823|41692823|2012-09-01|2012|['eng']|['Biological sciences - Biology']|['Science & Mathematics', 'Botany & Plant Sciences', 'Biological Sciences']
Hierarchical models are widely used to characterize the performance of individual healthcare providers. However, little attention has been devoted to systemwide performance evaluations, the goals of which include identifying extreme (e.g. the top 10%) provider performance and developing statistical benchmarks to define high quality care. Obtaining optimal estimates of these quantities requires estimating the empirical distribution function (EDF) of provider-specific parameters that generate the data set under consideration. However, the difficulty of obtaining uncertainty bounds for a squared error loss minimizing EDF estimate has hindered its use in systemwide performance evaluations. We therefore develop and study a percentile-based EDF estimate for univariate provider-specific parameters. We compute order statistics of samples drawn from the posterior distribution of provider-specific parameters to obtain relevant assessments of uncertainty of an EDF estimate and its features, such as thresholds and percentiles. We apply our method to data from the Medicare end stage renal disease programme, which is a health insurance programme for people with irreversible kidney failure. We highlight the risk of misclassifying providers as exceptionally good or poor performers when uncertainty in statistical benchmark estimates is ignored. Given the high stakes of performance evaluations, statistical benchmarks should be accompanied by precision estimates.|Percentile-based empirical distribution function estimates for performance evaluation of healthcare providers|http://www.jstor.org/stable/41262293|41262293|2011-08-01|2011|['eng']|['Applied sciences - Engineering', 'Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
Mapping of the human brain by means of functional magnetic resonance imaging (fMRI) is an emerging field in cognitive and clinical neuroscience. Current techniques to detect activated areas of the brain mostly proceed in two steps. First, conventional methods of correlation, regression, and time series analysis are used to assess activation by a separate, pixelwise comparison of the fMRI signal time courses to the reference function of a presented stimulus. Spatial aspects caused by correlations between neighboring pixels are considered in a separate second step, if at all. The aim of this article is to present hierarchical Bayesian approaches that allow one to simultaneously incorporate temporal and spatial dependencies between pixels directly in the model formulation. For reasons of computational feasibility, models have to be comparatively parsimonious, without oversimplifying. We introduce parametric and semiparametric spatial and spatiotemporal models that proved appropriate and illustrate their performance applied to visual fMRI data.|Bayesian Spatiotemporal Inference in Functional Magnetic Resonance Imaging|http://www.jstor.org/stable/3068366|3068366|2001-06-01|2001|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
The sea surface temperature (SST) is an important factor of the earth climate system. A deep understanding of SST is essential for climate monitoring and prediction. In general, SST follows a nonlinear pattern in both time and location and can be modeled by a dynamic system which changes with time and location. In this article, we propose a radial basis function network-based dynamic model which is able to catch the nonlinearity of the data and propose to use the dynamically weighted particle filter to estimate the parameters of the dynamic model. We analyze the SST observed in the Caribbean Islands area after a hurricane using the proposed dynamic model. Comparing to the traditional grid-based approach that requires a supercomputer due to its high computational demand, our approach requires much less CPU time and makes real-time forecasting of SST doable on a personal computer. Supplementary materials for this article are available online.|Sea Surface Temperature Modeling using Radial Basis Function Networks With a Dynamically Weighted Particle Filter|http://www.jstor.org/stable/23427515|23427515|2013-03-01|2013|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
Accurate classification of tissue samples is an essential tool in disease diagnosis and treatment. The DNA microarray technology enables disease classification based only on gene expression analysis, without prior biological insights. We present a classification method based on modeling the distribution of the gene expression profile of a test sample as a mixture of distributions, each of which characterizes the levels of gene expression within a class. Class assignment for a test sample is based on the predictive probabilities of class memberships. We believe that this general modeling framework is a flexible scheme for multi-type classification. Since most of the thousands of genes whose expression levels are measured do not contribute to the separation between types of tissue samples, we also explore several measures for gene selection, including T, NPT, BW, NPBW, and a mixture modeling approach based on Markov chain Monte Carlo (MCMC) estimation of parameters. For a classifier based on a gene selection measure, such as the T classifier, the number of genes selected is achieved by cross-validation. The methods are applied to a leukemia dataset; our results are comparable with the best results achieved in a comparative study done by Professor Terry Speed and colleagues.|Classification of Tissue Samples Using Mixture Modeling of Microarray Gene Expression Data|http://www.jstor.org/stable/4356199|4356199|2003-01-01|2003|['eng']|['Applied sciences - Engineering', 'Biological sciences - Biology']|['Mathematics', 'Science & Mathematics', 'Statistics']
This article estimates technical and allocative inefficiencies and increase in costs therefrom of individual firms using a translog cost system consisting of the cost function and the cost share equations. We call it a nonlinear random-effects system because technical and allocative inefficiencies are random (hence the term random effects) and are separated from the random noise terms appearing in each equation of the system, and because the inefficiency terms appear in the system in a highly nonlinear fashion, which helps in separating them from random errors. We use Bayesian inference procedures based on Markov chain Monte Carlo (MCMC) techniques to estimate the proposed system. Inferences on firm-specific technical inefficiency and both input-specific and firm-specific allocative inefficiencies are developed using MCMC techniques. We apply the new methods to a sample of 500 U.S. commercial banks. We focus on input allocation problem based on the assumption that banks minimize cost. Empirical results show that cost for the top (bottom) 10% of banks is increased by at least 3% (11%) due to technical inefficiency. In contrast, very few banks are found to be efficient in allocating all the inputs. Costs are increased by 13% on average due to input misallocation. Increase in costs due to both technical and allocative inefficiencies for the top (bottom) 10% of the banks is at least 11% (29%). When translated into dollar figures, this result indicates that elimination of technical and allocative inefficiencies would save the top (bottom) 10% of the banks more than $.20 ($3.56) million. We also find that none of the banks in our sample exceeded the efficient scale size, although most of them were operating near their optimum scale (unitary returns to scale).|The Joint Measurement of Technical and Allocative Inefficiencies: An Application of Bayesian Inference in Nonlinear Random-Effects Models|http://www.jstor.org/stable/27590611|27590611|2005-09-01|2005|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
We develop and analyze models of the spatio-temporal organization of lymphocytes in the lymph nodes and spleen. The spatial dynamics of these immune system white blood cells are influenced by biochemical fields and represent key components of the overall immune response to vaccines and infections. A primary goal is to learn about the structure of these fields that fundamentally shape the immune response. We define dynamic models of single-cell motion involving nonparametric representations of scalar potential fields underlying the directional biochemical fields that guide cellular motion. Bayesian hierarchical extensions define multicellular models for aggregating models and data on colonies of cells. Analysis via customized Markov chain Monte Carlo methods leads to Bayesian inference on cell-specific and population parameters together with the underlying spatial fields. Our case study explores data from multiphoton intravital microscopy in lymph nodes of mice, and we use a number of visualization tools to summarize and compare posterior inferences on the three-dimensional taxic fields.|Bayesian Spatio-Dynamic Modeling in Cell Motility Studies: Learning Nonlinear Taxic Fields Guiding the Immune Response|http://www.jstor.org/stable/23427391|23427391|2012-09-01|2012|['eng']|['Biological sciences - Biology']|['Science & Mathematics', 'Statistics']
We consider the problem of imprecise statistical inference when there is little or no prior information about the parameter of interest. Walley (1991) proposed the use of wide classes of priors that he called Near Ignorance Classes. We review some recent developments of applications of Near Ignorance Classes to some fundamental inferential problems.|ON NEAR IGNORANCE CLASSES|http://www.jstor.org/stable/43600857|43600857|1994-11-01|1994|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Mathematics', 'Statistics']
Increasingly, scientific studies yield functional data, in which the ideal units of observation are curves and the observed data consist of sets of curves that are sampled on a fine grid. We present new methodology that generalizes the linear mixed model to the functional mixed model framework, with model fitting done by using a Bayesian wavelet-based approach. This method is flexible, allowing functions of arbitrary form and the full range of fixed effects structures and between-curve covariance structures that are available in the mixed model framework. It yields nonparametric estimates of the fixed and random-effects functions as well as the various between-curve and within-curve covariance matrices. The functional fixed effects are adaptively regularized as a result of the non-linear shrinkage prior that is imposed on the fixed effects' wavelet coefficients, and the random-effect functions experience a form of adaptive regularization because of the separately estimated variance components for each wavelet coefficient. Because we have posterior samples for all model quantities, we can perform pointwise or joint Bayesian inference or prediction on the quantities of the model. The adaptiveness of the method makes it especially appropriate for modelling irregular functional data that are characterized by numerous local features like peaks.|Wavelet-Based Functional Mixed Models|http://www.jstor.org/stable/3647565|3647565|2006-01-01|2006|['eng']|['Applied sciences - Engineering', 'Biological sciences - Biology']|['Science & Mathematics', 'Statistics']
Multilevel analysis is a useful technique for analyzing longitudinal data. To describe a person's development across time, the quality of the estimates of the random coefficients, which relate time to individual changes in a relevant dependent variable, is of importance. The present study compares three estimators of the random coefficients: the Bayes estimator (BE), the empirical Bayes estimator (EBE), and the ordinary least squares estimator (OLSE). Using MLwiN, Monte Carlo simulations are carried out to study the performance of the estimators, while systematically varying the size of the sample as well as the number of measurement occasions. First, we examine for normally distributed random coefficients to what extent the EBE performs better than the OLSE and to what extent the EBE preserves the good properties of the BE. Second, we examine the robustness of the EBE which is based on a normal distribution of the random parameters, by comparing its performance to the OLSE for data being generated from two distributions other than the normal distribution: a modified t-distribution and a modified exponential distribution. As performance criteria we examine the Bayes risk as well as a criterion based on the frequentist notion of mean squared error.|Performance of Empirical Bayes Estimators of Level-2 Random Parameters in Multilevel Analysis: A Monte Carlo Study for Longitudinal Designs|http://www.jstor.org/stable/3701260|3701260|2003-07-01|2003|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Education', 'Statistics', 'Social Sciences']
1. In large-scale field surveys, a binary recording of each species' detection or nondetection has been increasingly adopted for its simplicity and low cost. Because of the importance of abundance in many studies, it is desirable to obtain inferences about abundance at species-, functional group-, and community-levels from such binary data. 2. We developed a novel hierarchical multi-species abundance model based on species-level detection/nondetection data. The model accounts for the existence of undetected species, and variability in abundance and detectability among species. Species-level detection/nondetection is linked to species-level abundance via a detection model that accommodates the expectation that probability of detection (at least one individuals is detected) increases with local abundance of the species. We applied this model to a 9-year dataset composed of the detection/nondetection of forest birds, at a single post-fire site (from 7 to 15 years after fire) in a montane area of central Japan. The model allocated undetected species into one of the predefined functional groups by assuming a prior distribution on individual group membership. 3. The results suggest that 15—20 species were missed in each year, and that species richness of communities and functional groups did not change with post-fire forest succession. Overall abundance of birds and abundance of functional groups tended to increase over time, although only in the winter, while decreases in detectabilities were observed in several species. 4. Synthesis and applications. Understanding and prediction of large-scale biodiversity dynamics partly hinge on how we can use data effectively. Our hierarchical model for detection/nondetection data estimates abundance in space/time at species-, functional group-, and community-levels while accounting for undetected individuals and species. It also permits comparison of multiple communities by many types of abundance-based diversity and similarity measures under imperfect detection.|Modelling community dynamics based on species-level abundance models from detection/nondetection data|http://www.jstor.org/stable/20869913|20869913|2011-02-01|2011|['eng']|['Biological sciences - Biogeography', 'Biological sciences - Biology', 'Biological sciences - Ecology']|['Biological Sciences', 'Ecology & Evolutionary Biology', 'Science and Mathematics']
A result can be regarded as routinely predictable when it has recurred consistently under a known range of different conditions. This depends on the previous analysis of many sets of data, drawn from different populations. There is no such basis of extensive experience when a prediction is derived from the analysis of only a single set of data. Yet that is what is mainly discussed in our statistical texts. The paper discusses the design and analysis of studies aimed at achieving routinely predictable results. It uses two running case history examples.|Predictability and Prediction|http://www.jstor.org/stable/2982727|2982727|1993-01-01|1993|['eng']|['Philosophy - Logic']|['Science & Mathematics', 'Statistics']
In this paper a Bayesian approach is applied to the correlated binomial model, CB(n, p, ρ), proposed by Luceño (Comput. Statist. Data Anal. 20 (1995) 511-520). The data augmentation scheme is used in order to overcome the complexity of the mixture likelihood. MCMC methods, including Gibbs sampling and Metropolis within Gibbs, are applied to estimate the posterior marginal for the probability of success p and for the correlation coefficient ρ. The sensitivity of the posterior is studied taking into account several reference priors and it is shown that the posterior characteristics appear not to be influenced by these prior distributions. The article is motivated by a study of plant selection.|Bayesian analysis of a correlated binomial model|http://www.jstor.org/stable/43601144|43601144|2010-03-01|2010|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Mathematics', 'Statistics']
"Lawrence D. Brown was born on December 16, 1940 in Los Angeles, California. He obtained his Ph.D. in mathematics from Cornell University in 1964. He has been on the faculty of the University of California, Berkeley, Cornell University, Rutgers University and, most recently, the Wharton School of the University of Pennsylvania, where he holds the Miers Busch Professorship of Statistics. Professor Brown was President of the Institute of Mathematical Statistics in 1992-1993, Coeditor of ""The Annals of Statistics"" for 1995-1997 and gave the prestigious Wald Memorial Lectures in 1985. In 1990, Professor Brown was elected to the U.S. National Academy of Sciences. In 1993, Purdue University awarded him an honorary D.Sc. degree in recognition of his distinguished achievements, and in 2002 he was named winner of the Wilks Memorial Award of the American Statistical Association. Professor Brown is probably best known for his extensive work on the admissibility of estimators of one or more parameters. He has also published research on a broad variety of other topics including general decision theory, sequential analysis, properties of exponential families, foundations of statistical inference, conditional confidence, interval estimation and Edgeworth expansions, and bioequivalence. His current interests include functional non-parametrics, analysis of census data and queuing theory as involved in the analysis of call-center data."|A Conversation with Larry Brown|http://www.jstor.org/stable/20061170|20061170|2005-05-01|2005|['eng']|['Philosophy - Logic']|['Science and Mathematics', 'Statistics']
Consider the problem where a retailer or manufacturer wants to estimate product price and promotional elasticities based on supermarket scanner data. Classical linear modeling suffers from the following aggregation dilemma. Price and promotional elasticities appear to vary considerably among chains and brands so that one overall model is too restrictive. Alternatively, the use of a different model for each chain and brand leads to noisy and often nonsensical estimates of separate elasticities because of excessive data variation. To resolve this dilemma, shrinkage estimation procedures are proposed. By borrowing strength across chains and brands, these procedures reduce variability while providing flexibility that allows for separate elasticity estimates. Application of these procedures to a large data set yields not only more reasonable model estimates but also improved predictive power.|Shrinkage Estimation of Price and Promotional Elasticities: Seemingly Unrelated Equations|http://www.jstor.org/stable/2290562|2290562|1991-06-01|1991|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
We propose a nonparametric Bayesian local clustering (NoB-LoC) approach for heterogeneous data. NoB-LoC implements inference for nested clusters as posterior inference under a Bayesian model. Using protein expression data as an example, the NoB-LoC model defines a protein (column) cluster as a set of proteins that give rise to the same partition of the samples (rows). In other words, the sample partitions are nested within protein clusters. The common clustering of the samples gives meaning to the protein clusters. Any pair of samples might belong to the same cluster for one protein set but to different clusters for another protein set. These local features are different from features obtained by global clustering approaches such as hierarchical clustering, which create only one partition of samples that applies for all the proteins in the dataset. In addition, the NoB-LoC model is different from most other local or nested clustering methods, which define clusters based on common parameters in the sampling model. As an added and important feature, the NoB-LoC method probabilistically excludes sets of irrelevant proteins and samples that do not meaningfully cocluster with other proteins and samples, thus improving the inference on the clustering of the remaining proteins and samples. Inference is guided by a joint probability model for all the random elements. We provide a simulation study and a motivating example to demonstrate the unique features of the NoB-LoC model. Supplementary materials for this article are available online.|A Nonparametric Bayesian Model for Local Clustering With Application to Proteomics|http://www.jstor.org/stable/24246863|24246863|2013-09-01|2013|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
1. We investigated the relationships of seed size, dispersal mode and other species characteristics to interspecific variation in mean primary seed dispersal distances, mean annual seed production per unit basal area, and clumping of seed deposition among 41 tropical tree species on Barro Colorado Island, Panama. 2. A hierarchical Bayesian model incorporating interannual variation in seed production was used to estimate seed dispersal, seed production, and clumping of seed rain for each species from 19 years of data for 188 seed traps on a 50-ha plot in which all adult trees were censused every 5 years. 3. Seed dispersal was modelled as a two-dimensional Student's T distribution with the degrees of freedom parameter fixed at 3, interannual variation in seed production per basal area was modelled as a lognormal, and the clumping of seed rain around its expected value was modelled as a negative binomial distribution. 4. There was wide variation in seed dispersal distances among species sharing the same mode of seed dispersal. Seed dispersal mode did not explain significant variation in seed dispersal distances, but did explain significant variation in clumping: animal-dispersed species showed higher clumping of seed deposition. 5. Among nine wind-dispersed species, the combination of diaspore terminal velocity, tree height and wind speed in the season of peak dispersal explained 40% of variation in dispersal distances. Among 31 animal-dispersed species, 20% of interspecific variation in dispersal distances was explained by seed mass (a negative effect) and tree height (a positive effect). 6. Among all species, seed mass, tree height and dispersal syndrome explained 28% of the variation in mean dispersal distance and seed mass alone explained 45% of the variation in estimated seed production per basal area. 7. Synthesis. There is wide variation in patterns of primary seed rain among tropical tree species. Substantial proportions of interspecific variation in seed production, seed dispersal distances, and clumping of seed deposition are explained by relatively easily measured plant traits, especially dispersal mode, seed mass, and tree height. This provides hope for trait-based generalization and modelling of seed dispersal in tropical forests.|Interspecific Variation in Primary Seed Dispersal in a Tropical Forest|http://www.jstor.org/stable/20143507|20143507|2008-07-01|2008|['eng']|['Physical sciences - Astronomy']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
Understanding the relationship between body size and mortality is an important problem in ecology. We introduce a novel Bayesian method that can be used to quantify this relationship when the only data available are size-frequency distributions of unmarked individuals measured at two successive time periods. The inverse Gaussian distribution provides a parametric form for the statistical model development, and we use Markov chain Monte Carlo methods to evaluate posterior distributions. We illustrate the method using data on threespine stickleback (Gasterosteus aculeatus) collected before and after the winter season in an Alaskan lake. Our method allows us to compare the intensity of size-biased mortality in different years. We discuss generalizations that include more complicated relationships between size and survival as well as time-series modeling.|Bayesian analysis of size-dependent overwinter mortality from size-frequency distributions|http://www.jstor.org/stable/25661144|25661144|2010-04-01|2010|['eng']|['Applied sciences - Engineering']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
Federal law prohibits discrimination in employment decisions against persons in certain protected categories. The common method for measuring discrimination involves a comparison of some aggregate statistic for protected and nonprotected individuals. This approach is open to question when employment decisions are made over an extended time period. We show how to use hierarchical proportional hazards models (Cox regression models) to analyze such data.|Hierarchical Models for Employment Decisions|http://www.jstor.org/stable/1392177|1392177|2004-04-01|2004|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
Proposed offshore wind energy development on the Atlantic Outer Continental Shelf has brought attention to the need for baseline studies of the distribution and abundance of marine birds. We compiled line transect data from 15 shipboard surveys (June 2012–April 2014), along with associated remotely sensed habitat data, in the lower Mid-Atlantic Bight off the coast of Delaware, Maryland, and Virginia, USA. We implemented a recently developed hierarchical community distance sampling model to estimate the seasonal abundance of 40 observed marine bird species. Treating each season separately, we included six oceanographic parameters to estimate seabird abundance: three static (distance to shore, slope, sediment grain size) and three dynamic covariates (sea surface temperature [SST], salinity, primary productivity). We expected that avian bottom-feeders would respond primarily to static covariates that characterize seafloor variability, and that surface-feeders would respond more to dynamic covariates that quantify surface productivity. We compared the variation in species-specific and community-level responses to these habitat features, including for rare species, and we predicted species abundance across the study area. While several protected species used the study area in summer during their breeding season, estimated abundance and observed diversity were highest for nonbreeding species in winter. Distance to shore was the most common significant predictor of abundance, and thus useful in estimating the potential exposure of marine birds to offshore development. In many cases, our expectations based on feeding ecology were confirmed, such as in the first winter season, when bottom-feeders associated significantly with the three static covariates (distance to shore, slope, and sediment grain size), and surface-feeders associated significantly with two dynamic covariates (SST, primary productivity). However, other cases revealed significant relationships between static covariates and surface-feeders (e.g., distance to shore) and between dynamic covariates and bottom-feeders (e.g., primary productivity during that same winter). More generally, we found wide interannual, seasonal, and interspecies variation in habitat relationships with abundance. These results show the importance of quantifying detection and determining the ecological drivers of a community's distribution and abundance, within and among species, for evaluating the potential exposure of marine birds to offshore development.|Predicting the offshore distribution and abundance of marine birds with a hierarchical community distance sampling model|http://www.jstor.org/stable/24818214|24818214|2016-09-01|2016|['eng']|['Biological sciences - Ecology', 'Biological sciences - Paleontology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
"A key task of advertising media planners is to determine the best media schedule of advertising exposures for a certain budget. Conceptually, the planner could choose to do continuous advertising (i.e., schedule ad exposures evenly over all weeks) or follow a strategy of plusing (i.e., advertise in some weeks of the year and not at other times). Previous theoretical analyses have shown that continuous advertising is optimal for nearly all situations. However, pulsing schedules are very common in practice. Either the practice of plusing is inappropriate or extant models have not adequately conceptualized the effects of advertising spending over time. This paper offers a model that shows pulsing strategies can generate greater total awareness than the continuous advertising when the effectiveness of advertisement (i.e., ad quality) varies over time. Specifically, ad quality declines because of advertising wearout during periods of continuous advertising and it restores, due to forgetting effects, during periods of no advertising. Such dynamics make it worth-while for advertisers to stop advertising when ad quality becomes very low and wait for ad quality to restore before starting the next ""burst"" again, as is common in practice. Based on the extensive behavioral research on advertising repetition and advertising wearout, we extend the classical Nerlove and Arrow (1962) model by incorporating the notions of repetition wearout, copy wearout, and ad quality restoration. Repetition wearout is a result of excessive frequency because ad viewers perceive that there is nothing new to be gained from processing the ad, they withdraw their attention, or they become unmotivated to react to advertising information. Copy wearout refers to the decline in ad quality due to passage of time independent of the level of frequency. Ad quality restoration is the enhancement of ad quality during media hiatus as a consequence of viewers forgetting the details of the advertised messages, thus making ads appear ""like new"" when reintroduced later. The proposed model has the property that, when wearout effects are present, a strategy of plusing is superior to continuous advertising even when the advertising response function is concave. This is illustrated by a numerical example that compares the total awareness generated by a single concentrated pulse of varying duration (blitz schedules) and continuous advertising (the even schedule). This property can be explained by the tension between the pressure to spend the fixed media budget quickly to avoid copy wearout and the opposing pressure to spread out the media spending over time to mitigate repetition wearout. The proposed model is empirically tested by using brand-level data from two advertising awareness tracking studies that also include the actual spending schedules. The first data set is for a major cereal brand, while the other is for a brand of milk chocolate. Such advertising tracking studies are now a common and popular means for evaluating advertising effectiveness in many markets (e.g., Millward Brown, MarketMind). In the empirical tests, the model parameters are estimated by using the Kalman filter procedure, which is eminently suited for dynamic models because it attends to the intertemporal dependencies in awareness build-up and decay via the use of conditional densities. The estimated parameters are statistically significant, have the expected signs, and are meaningful from both theoretical and managerial view-points. The proposed model fits both the data sets rather well and better than several well-known advertising models, namely, the Vidale-Wolfe, Brandaid, Litmus, and Tracker models, but not decisively better than the Nerlove-Arrow model. However, unlike the Nerlove-Arrow model, the proposed model yields different total awareness for different strategies of spending the same fixed budget, thus allowing media planners to discriminate among several media schedules. Given the empirical support for the model, the paper presents an implementable approach for utilizing it to evaluate large numbers of alternative media schedules and determine the best set of media schedules for consideration in media planning. This approach is based on an algorithm that combines a genetic algorithm with the Kalman filter procedure. The paper presents the results of applying this approach in the case studies of the cereal and milk chocolate brands. The form of the best advertising spending strategies in each case was a pulsing strategy, and there were many schedules that were an improvement over the media schedule actually used in each campaign."|Planning Media Schedules in the Presence of Dynamic Advertising Quality|http://www.jstor.org/stable/193228|193228|1998-01-01|1998|['eng']|['Mathematics - Applied mathematics', 'Information science - Information analysis', 'Information science - Data products']|['Marketing & Advertising', 'Business & Economics', 'Business']
"Model search in regression with very large numbers of candidate predictors raises challenges for both model specification and computation, for which standard approaches such as Markov chain Monte Carlo (MCMC) methods are often infeasible or ineffective. We describe a novel ""Shotgun stochastic search"" (SSS) approach that explores ""interesting"" regions of the resulting high-dimensional model spaces and quickly identifies regions of high posterior probability over models. We describe algorithmic and modeling aspects, priors over the model space that induce sparsity and parsimony over and above the traditional dimension penalization implicit in Bayesian and likelihood analyses, and parallel computation using cluster computers. We discuss an example from gene expression cancer genomics, comparisons with MCMC and other methods, and theoretical and simulation-based aspects of performance characteristics in large-scale regression model searches. We also provide software implementing the methods."|"Shotgun Stochastic Search for ""Large p"" Regression"|http://www.jstor.org/stable/27639881|27639881|2007-06-01|2007|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
"We consider the problem of testing a one-sided Normal hypothesis when there is little or no prior information concerning the hypothesis. In Bayesian sensitivity analysis (also known as ""robust Bayes""), the prior information is modelled by a class of prior distributions. When there is little prior information the null hypothesis should have prior upper probability (supremum probability over the class) that is close to one and prior lower probability close to zero. In testing a one-sided Normal hypothesis it is possible to draw inferences from a near-ignorance class of priors, for which the prior upper and lower probabilities of the null hypothesis are actually one and zero. This has a simple behavioral meaning: we are initially unwilling to bet on or against either hypothesis at any odds. We examine a particular near-ignorance class which can be easily updated and yields realistic posterior upper and lower probabilities. The posterior imprecision, measured by the difference between posterior upper and lower probabilities, reflects the amount of information provided by the experiment for discriminating between the hypotheses."|ONE-SIDED HYPOTHESIS TESTING WITH NEAR-IGNORANCE PRIORS|http://www.jstor.org/stable/43601414|43601414|1990-05-01|1990|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Mathematics', 'Statistics']
In a Bayesian analysis of finite mixture models, parameter estimation and clustering are sometimes less straightforward than might be expected. In particular, the common practice of estimating parameters by their posterior mean, and summarizing joint posterior distributions by marginal distributions, often leads to nonsensical answers. This is due to the so-called 'label switching' problem, which is caused by symmetry in the likelihood of the model parameters. A frequent response to this problem is to remove the symmetry by using artificial identifiability constraints. We demonstrate that this fails in general to solve the problem, and we describe an alternative class of approaches, relabelling algorithms, which arise from attempting to minimize the posterior expected loss under a class of loss functions. We describe in detail one particularly simple and general relabelling algorithm and illustrate its success in dealing with the label switching problem on two examples.|Dealing with Label Switching in Mixture Models|http://www.jstor.org/stable/2680622|2680622|2000-01-01|2000|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
We consider the problem of identifying differentially expressed genes under different conditions using gene expression microarrays. Because of the many steps involved in the experimental process, from hybridization to image analysis, cDNA microarray data often contain outliers. For example, an outlying data value could occur because of scratches or dust on the surface, imperfections in the glass, or imperfections in the array production. We develop a robust Bayesian hierarchical model for testing for differential expression. Errors are modeled explicitly using a t-distribution, which accounts for outliers. The model includes an exchangeable prior for the variances, which allows different variances for the genes but still shrinks extreme empirical variances. Our model can be used for testing for differentially expressed genes among multiple samples, and it can distinguish between the different possible patterns of differential expression when there are three or more samples. Parameter estimation is carried out using a novel version of Markov chain Monte Carlo that is appropriate when the model puts mass on subspaces of the full parameter space. The method is illustrated using two publicly available gene expression data sets. We compare our method to six other baseline and commonly used techniques, namely the t-test, the Bonferroni-adjusted t-test, significance analysis of microarrays (SAM), Efron's empirical Bayes, and EBarrays in both its lognormal-normal and gamma-gamma forms. In an experiment with HIV data, our method performed better than these alternatives, on the basis of between-replicate agreement and disagreement.|Bayesian Robust Inference for Differential Gene Expression in Microarrays with Multiple Samples|http://www.jstor.org/stable/3695699|3695699|2006-03-01|2006|['eng']|['Biological sciences - Biology', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
The controversy concerning the fundamental principles of statistics still remains unresolved. It is suggested that one key to resolving the conflict lies in recognizing that inferential probability derived from observational data is inherently noncoherent, in the sense that their inferential implications cannot be represented by a single probability distribution on the parameter space (except in the Objective Bayesian case). More precisely, for a parameter space R1, the class of all functions of the parameter comprise equivalence classes of invertibly related functions, and to each such class a logically distinct inferential probability distribution pertains. (There is an additional cross-coherence requirement for simultaneous inference.) The non-coherence of these distributions flows from the nonequivalence of the relevant components of the data for each. Noncoherence is mathematically inherent in confidence and fiducial theory, and provides a basis for reconciling the Fisherian and Neyman-Pearsonian viewpoints. A unified theory of confidence-based inferential probability is presented, and the fundamental incompatibility of this with Subjective Bayesian theory is discussed.|On Resolving the Controversy in Statistical Inference|http://www.jstor.org/stable/2984795|2984795|1977-01-01|1977|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
This article considers the general linear model when the parameter space is subject to linear inequality constraints. A Bayesian analysis of this model is presented using a natural conjugate prior of the mixed type. Expressions are given for the probability that constraints are binding and for the distribution of the parameters. When prior information about the parameters is vague, the Bayesian and sampling methods of model selection are compared. The techniques are applied to a time series of temperatures of a chemical reaction.|Bayesian Analysis of the Linear Model Subject to Linear Inequality Constraints|http://www.jstor.org/stable/2286603|2286603|1978-09-01|1978|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
This article presents a Bayesian method for estimating nonparametrically a high-dimensional multinomial regression model. The regression functions are expressed as sums of main effects and interactions and our approach is able to select the significant components entering the model. Each of the main effects and interactions is written as a linear combination of basis terms with a variance components type prior on the regression coefficients. The conditional class probabilities are estimated using both variable selection and model averaging. Our approach can also be used for classification and gives results that are comparable to modern classification methods, but at the same time the results are highly interpretable to the practitioner. All computation is carried out using Markov chain Monte Carlo simulation.|Bayesian Variable Selection and Model Averaging in High-Dimensional Multinomial Nonparametric Regression|http://www.jstor.org/stable/1391068|1391068|2003-03-01|2003|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Computer Science', 'Statistics']
Network inference approaches are now widely used in biological applications to probe regulatory relationships between molecular components such as genes or proteins. Many methods have been proposed for this setting, but the connections and differences between their statistical formulations have received less attention. In this paper, we show how a broad class of statistical network inference methods, including a number of existing approaches, can be described in terms of variable selection for the linear model. This reveals some subtle but important differences between the methods, including the treatment of time intervals in discretely observed data. In developing a general formulation, we also explore the relationship between single-cell stochastic dynamics and network inference on averages over cells. This clarifies the link between biochemical networks as they operate at the cellular level and network inference as carried out on data that are averages over populations of cells. We present empirical results, comparing thirty-two network inference methods that are instances of the general formulation we describe, using two published dynamical models. Our investigation sheds light on the applicability and limitations of network inference and provides guidance for practitioners and suggestions for experimental design.|NETWORK INFERENCE AND BIOLOGICAL DYNAMICS|http://www.jstor.org/stable/41713521|41713521|2012-09-01|2012|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
Climate change may lead to changes in several aspects of the distribution of climate variables, including changes in the mean, increased variability and severity of extreme events. We propose the use of spatiotemporal quantile regression as a flexible and interpretable method for simultaneously detecting changes in several features of the distribution of climate variables. The spatiotemporal quantile regression model assumes that each quantile level changes linearly in time, permitting straightforward inference on the time trend for each quantile level. Unlike classical quantile regression which uses model-free methods to analyse a single quantile or several quantiles separately, we take a model-based approach which jointly models all quantiles, and thus the entire response distribution. In the spatiotemporal quantile regression model, each spatial location has its own quantile function that evolves over time, and the quantile functions are smoothed spatially by using Gaussian process priors. We propose a basis expansion for the quantile function that permits a closed form for the likelihood and allows for residual correlation modelling via a Gaussian spatial copula. We illustrate the methods by using temperature data for the south-east USA from the years 1931—2009. For these data, borrowing information across space identifies more significant time trends than classical non-spatial quantile regression. We find a decreasing time trend for much of the spatial domain for monthly mean and maximum temperatures. For the lower quantiles of monthly minimum temperature, we find a decrease in Georgia and Florida, and an increase in Virginia and the Carolinas.|Spatiotemporal quantile regression for detecting distributional changes in environmental processes|http://www.jstor.org/stable/23251159|23251159|2012-08-01|2012|['eng']|['Mathematics - Applied mathematics']|['Science and Mathematics', 'Statistics']
Monetary policy and the private sector behaviour of the U.S. economy are modelled as a time varying structural vector autoregression, where the sources of time variation are both the coefficients and the variance covariance matrix of the innovations. The paper develops a new, simple modelling strategy for the law of motion of the variance covariance matrix and proposes an efficient Markov chain Monte Carlo algorithm for the model likelihood/posterior numerical evaluation. The main empirical conclusions are: (1) both systematic and non-systematic monetary policy have changed during the last 40 years-in particular, systematic responses of the interest rate to inflation and unemployment exhibit a trend toward a more aggressive behaviour, despite remarkable oscillations; (2) this has had a negligible effect on the rest of the economy. The role played by exogenous non-policy shocks seems more important than interest rate policy in explaining the high inflation and unemployment episodes in recent U.S. economic history.|Time Varying Structural Vector Autoregressions and Monetary Policy|http://www.jstor.org/stable/3700675|3700675|2005-07-01|2005|['eng']|['Mathematics - Mathematical logic']|['Business & Economics', 'Business', 'Economics']
"Stable isotopes are valuable tools for partitioning the components contributing to ecological processes of interest, such as animal diets and trophic interactions, plant resource use, ecosystem gas fluxes, streamflow, and many more. Stable isotope data are often analyzed with simple linear mixing (SLM) models to partition the contributions of different sources, but SLM models cannot incorporate a mechanistic understanding of the underlying processes and do not accommodate additional data associated with these processes (e.g., environmental covariates, flux data, gut contents). Thus, SLM models lack predictive ability. We describe a process-based mixing (PBM) model approach for integrating stable isotopes, other data sources, and process models to partition different sources or process components. This is accomplished via a hierarchical Bayesian framework that quantifies multiple sources of uncertainty and enables the incorporation of process models and prior information to help constrain the source-specific proportional contributions, thereby potentially avoiding identifiability issues that plague SLM models applied to ""too many"" sources. We discuss the application of the PBM model framework to three diverse examples: temporal and spatial partitioning of streamflow, estimation of plant rooting profiles and water uptake profiles (or water sources) with extension to partitioning soil and ecosystem CO 2 fluxes, and reconstructing animal diets. These examples illustrate the advantages of the PBM modeling approach, which facilitates incorporation of ecological theory and diverse sources of information into the mixing model framework, thus enabling one to partition key process components across time and space."|Beyond simple linear mixing models: process-based isotope partitioning of ecological processes|http://www.jstor.org/stable/23596812|23596812|2014-01-01|2014|['eng']|['Biological sciences - Ecology', 'Applied sciences - Engineering']|['Biological Sciences', 'Ecology & Evolutionary Biology', 'Science and Mathematics']
Graphical models are widely used to make inferences concerning interplay in multivariate systems. In many applications, data are collected from multiple related but nonidentical units whose underlying networks may differ but are likely to share features. Here we present a hierarchical Bayesian formulation for joint estimation of multiple networks in this nonidentically distributed setting. The approach is general: given a suitable class of graphical models, it uses an exchangeability assumption on networks to provide a corresponding joint formulation. Motivated by emerging experimental designs in molecular biology, we focus on time-course data with interventions, using dynamic Bayesian networks as the graphical models. We introduce a computationally efficient, deterministic algorithm for exact joint inference in this setting. We provide an upper bound on the gains that joint estimation offers relative to separate estimation for each network and empirical results that support and extend the theory, including an extensive simulation study and an application to proteomic data from human cancer cell lines. Finally, we describe approximations that are still more computationally efficient than the exact algorithm and that also demonstrate good empirical performance.|JOINT ESTIMATION OF MULTIPLE RELATED BIOLOGICAL NETWORKS|http://www.jstor.org/stable/24522288|24522288|2014-09-01|2014|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
Parametric mixture models are commonly used in the analysis of clustered data. Parametric families are specified for the conditional distribution of the response variable given a cluster-specific effect, and for the marginal distribution of the cluster-specific effects. This latter distribution is referred to as the mixing distribution. If the form of the mixing distribution is misspecified, then Bayesian and maximum-likelihood estimators of parameters associated with either distribution may be inconsistent. The magnitude of the asymptotic bias is investigated, using an approximation based on infinitesimal contamination of the mixing distribution. The approximation is useful when there is a closed-form expression for the marginal distribution of the response under the assumed mixing distribution, but not under the true mixing distribution. Typically this occurs when the assumed mixing distribution is conjugate, meaning that the conditional distribution of the cluster-specific parameter given the response variable belongs to the same parametric family as the mixing distribution. /// Les modèles de mélange paramétriques sont fréquemment utilisés au cours de l'analyse de données en grappe. Nous spécifions des familles paramétriques pour la distribution conditionnelle de la variable de réponse étant donné un effet spécifique à la grappe, et pour la distribution marginale des effets spécifiques à la grappe. Nous qualifions cette dernière distribution de distribution de mélange. Si la forme de la distribution de mélange est mal spécifiée, c'est alors que les estimateurs Bayesiens et maximum de vraisemblance des paramètres associés à l'une des distribution sont peut être inconsistants. Nous étudions la magnitude du biais asymptotique à l'aide d'une approximation fondée sur la contamination infinitésimale de la distribution de mélange. L'approximation s'avère utile lorsqu'il y a une expression en forme fermée de la distribution marginale de la réponse d'après la distribution de mélange présumée, mais non d'après la vraie distribution de mélange. Cela arrive habituellement lorsque la distribution de mélange présumée est conjuguée, c'est à dire lorsque la distribution conditionnelle de paramètre spécifique à la grappe, étant donnée la variable de réponse, appartient à la même famille paramétrique que la distribution de mélange.|The Effect of Mixing-Distribution Misspecification in Conjugate Mixture Models|http://www.jstor.org/stable/3315741|3315741|1996-09-01|1996|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
This paper considers from a Bayesian viewpoint inferences about the size of a closed animal population from data obtained by a multiple-recapture sampling scheme. The method developed enables prior information about the population size and the catch probabilities to be utilized to produce considerable improvements in certain cases on ordinary maximum likelihood methods. Several ways of expressing such prior information are explored and a practical example of the uses of these ways is given. The main result of the paper is an approximation to the posterior distribution of sample size that exhibits the contributions made by the likelihood and the prior ideas.|A Bayesian Analysis of Multiple-Recapture Sampling for a Closed Population|http://www.jstor.org/stable/2335820|2335820|1981-04-01|1981|['eng']|['Mathematics - Mathematical analysis', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Bayesian inference is considered for statistical models that depend on the evaluation of a computationally expensive computer code or simulator. For such situations, the number of evaluations of the likelihood function, and hence of the unnormalized posterior probability density function, is determined by the available computational resource and may be extremely limited. We present a new example of such a simulator that describes the properties of human embryonic stem cells using data from optical trapping experiments. This application is used to motivate a novel strategy for Bayesian inference which exploits a Gaussian process approximation of the simulator and allows computationally efficient Markov chain Monte Carlo inference. The advantages of this strategy over previous methodology are that it is less reliant on the determination of tuning parameters and allows the application of model diagnostic procedures that require no additional evaluations of the simulator. We show the advantages of our method on synthetic examples and demonstrate its application on stem cell experiments.|A Strategy for Bayesian Inference for Computationally Expensive Models with Application to the Estimation of Stem Cell Properties|http://www.jstor.org/stable/44695262|44695262|2013-06-01|2013|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
In longitudinal random effects models, the random effects are typically assumed to have a normal distribution in both Bayesian and classical models. We provide a Bayesian model that allows the random effects to have a nonparametric prior distribution. We propose a Dirichlet process prior for the distribution of the random effects; computation is made possible by the Gibbs sampler. An example using marker data from an AIDS study is given to illustrate the methodology.|A Semiparametric Bayesian Approach to the Random Effects Model|http://www.jstor.org/stable/2533846|2533846|1998-09-01|1998|['eng']|['Applied sciences - Engineering', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
"This note is based on an invited discussion of the article, ""A Moving Average Approach for Spatial Statistical Models on Stream Networks"" by Jay M. Ver Hoef and Erin E. Peterson. Ver Hoef and Peterson (hereafter VHP) have extended the idea of flow-related statistical dependence in streams to one where dependence may not respect flow, such as might happen when modeling data on fish in connected streams. We congratulate VHP for their innovative paper on using moving average models in stream networks."|Comment: Statistical Dependence in Stream Networks [with Comment and Rejoinder]|http://www.jstor.org/stable/29747005|29747005|2010-03-01|2010|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
We propose discrete determinantal point processes (DPPs) for priors on the model parameter in Bayesian variable selection. By our variable selection method, collinear predictors are less likely to be simultaneously selected due to the repulsion property of discrete DPPs. Three types of DPP priors are proposed. Our method is an empirical Bayes approach, so hyperparameters are estimated by maximizing the marginal likelihood. We show the efficiency of the proposed priors through numerical experiments and applications to collinear datasets.|DETERMINANTAL POINT PROCESS PRIORS FOR BAYESIAN VARIABLE SELECTION IN LINEAR REGRESSION|http://www.jstor.org/stable/24721192|24721192|2016-01-01|2016|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Applied mathematics']|['Mathematics', 'Science & Mathematics', 'Statistics']
A new approach to clustering multivariate data, based on a multilevel linear mixed model, is proposed. A key feature of the model is that observations from the same cluster are correlated, because they share cluster-specific random effects. The inclusion of cluster-specific random effects allows parsimonious departure from an assumed base model for cluster mean profiles. This departure is captured statistically via the posterior expectation, or best linear unbiased predictor. One of the parameters in the model is the true underlying partition of the data, and the posterior distribution of this parameter, which is known up to a normalizing constant, is used to cluster the data. The problem of finding partitions with high posterior probability is not amenable to deterministic methods such as the EM algorithm. Thus, we propose a stochastic search algorithm that is driven by a Markov chain that is a mixture of two Metropolis-Hastings algorithms-one that makes small scale changes to individual objects and another that performs large scale moves involving entire clusters. The methodology proposed is fundamentally different from the well-known finite mixture model approach to clustering, which does not explicitly include the partition as a parameter, and involves an independent and identically distributed structure.|Clustering Using Objective Functions and Stochastic Search|http://www.jstor.org/stable/20203814|20203814|2008-01-01|2008|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Bathtub shape is one of the most important behaviors of the hazard rate function that is quite common in lifetime data analysis. Such shapes are actually the combination of three different shapes and, as such, there have been several proposals to model such behavior. One such proposal is to combine at most three different distributions, often the Weibull or some similar model, separately for decreasing, constant, and increasing shapes of the hazard rate. Sometimes combination of two different models may also result in the required bathtub shape. The other proposal includes generalizing or modifying the two-parameter distribution by adding an extra parameter into it. It is often seen that the first proposal is quite cumbersome whereas the second fails to capture some important aspects of the data. The present work considers two recent generalizations/modifications of the two-parameter Weibull model, namely the Weibull extension and the modified Weibull models, and proposes mixing the two families separately with the three-parameter Weibull distribution in order to see if the mixing results in some real benefit though at the cost of too many parameters. The paper finally considers the complete Bayes analysis of the proposed models using Markov chain Monte Carlo simulation and compares them with both Weibull extension and the modified Weibull models in a Bayesian framework. It is observed that the mixture models offer drastic improvement over the individual models not only in terms of hazard rate but also in terms of overall performance. The results are illustrated with the help of a real data based example.|Bayesian modeling of bathtub shaped hazard rate using various Weibull extensions and related issues of model selection|http://www.jstor.org/stable/23349862|23349862|2012-05-01|2012|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
We develop a sequential Monte Carlo (SMC) algorithm for estimating Bayesian dynamic stochastic general equilibrium (DSGE) models; wherein a particle approximation to the posterior is built iteratively through tempering the likelihood. Using two empirical illustrations consisting of the Smets and Wouters model and a larger news shock model we show that the SMC algorithm is better suited for multimodal and irregular posterior distributions than the widely used random walk Metropolis–Hastings algorithm. We find that a more diffuse prior for the Smets and Wouters model improves its marginal data density and that a slight modification of the prior for the news shock model leads to drastic changes in the posterior inference about the importance of news shocks for fluctuations in hours worked. Unlike standard Markov chain Monte Carlo (MCMC) techniques; the SMC algorithm is well suited for parallel computing.|SEQUENTIAL MONTE CARLO SAMPLING FOR DSGE MODELS|http://www.jstor.org/stable/26609010|26609010|2014-11-01|2014|['eng']|['Mathematics - Mathematical logic', 'Applied sciences - Engineering', 'Mathematics - Mathematical analysis']|['Business & Economics', 'Business', 'Economics']
This article considers how to estimate Bayesian credible and highest probability density (HPD) intervals for parameters of interest and provides a simple Monte Carlo approach to approximate these Bayesian intervals when a sample of the relevant parameters can be generated from their respective marginal posterior distribution using a Markov chain Monte Carlo (MCMC) sampling algorithm. We also develop a Monte Carlo method to compute HPD intervals for the parameters of interest from the desired posterior distribution using a sample from an importance sampling distribution. We apply our methodology to a Bayesian hierarchical model that has a posterior density containing analytically intractable integrals that depend on the (hyper) parameters. We further show that our methods are useful not only for calculating the HPD intervals for the parameters of interest but also for computing the HPD intervals for functions of the parameters. Necessary theory is developed and illustrative examples--including a simulation study--are given.|Monte Carlo Estimation of Bayesian Credible and HPD Intervals|http://www.jstor.org/stable/1390921|1390921|1999-03-01|1999|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Computer Science', 'Statistics']
Mixture item response theory models have been suggested as a potentially useful methodology for identifying latent groups formed along secondary, possibly nuisance dimensions. In this article, we describe a multilevel mixture item response theory (IRT) model (MMixIRTM) that allows for the possibility that this nuisance dimensionality may function differently at different levels. A MMixIRT model is described that enables simultaneous detection of differences in latent class composition at both examinee and school levels. The MMixIRTM can be viewed as a combination of an IRT model, an unrestricted latent class model, and a multilevel model. A Bayesian estimation of the MMixIRTM is described including analysis of label switching, use of priors, and model selection strategies. Results of a simulation study indicated that the generated parameters were recovered very well for the conditions considered. Use of MMixIRTM also was illustrated with the standardized mathematics test.|A Multilevel Mixture IRT Model With an Application to DIF|http://www.jstor.org/stable/40785075|40785075|2010-06-01|2010|['eng']|['Mathematics - Mathematical logic', 'Applied sciences - Engineering']|['Science & Mathematics', 'Education', 'Statistics', 'Social Sciences']
The online quality monitoring procedure for attributes proposed by Taguchi has been critically studied and extended by a few researchers. Determination of the optimum diagnosis interval requires estimation of some parameters related to the process failure mechanism. Improper estimates of these parameters may lead to an incorrect choice of the diagnosis interval and thus huge economic penalties. We propose a Bayesian approach to estimate the process parameters under two different process models, commonly called as the case II and case III models in the literature. We discuss a systematic way to use available engineering knowledge in eliciting the prior for the parameters, and demonstrate the performance of the proposed method using extensive simulation and a case study from a hot rolling mill.|Estimation of Process Parameters to Determine the Optimum Diagnosis Interval for Control of Defective Items|http://www.jstor.org/stable/25471457|25471457|2008-05-01|2008|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
When paired comparisons are made sequentially over time as for example in chess competitions, it is natural to assume that the underlying abilities do change with time. Previous approaches are based on fixed updating schemes where the increments and decrements are fixed functions of the underlying abilities. The parameters that determine the functions have to be specified a priori and are based on rational reasoning. We suggest an alternative scheme for keeping track with the underlying abilities. Our approach is based on two components: a response model that specifies the connection between the observations and the underlying abilities and a transition model that specifies the variation of abilities over time. The response model is a very general paired comparison model allowing for ties and ordered responses. The transition model incorporates random walk models and local linear trend models. Taken together, these two components form a non-Gaussian state-space model. Based on recent results, recursive posterior mode estimation algorithms are given and the relation to previous approaches is worked out. The performance of the method is illustrated by simulation results and an application to soccer data of the German Bundesliga.|Dynamic Stochastic Models for Time-Dependent Ordered Paired Comparison Systems|http://www.jstor.org/stable/2291005|2291005|1994-12-01|1994|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
We provide an approach to forecasting the long-run (unconditional) distribution of equity returns making optimal use of historical data in the presence of structural breaks. Our focus is on learning about breaks in real time and assessing their impact on out-of-sample density forecasts. Forecasts use a probability-weighted average of submodels, each of which is estimated over a different history of data. The empirical results strongly reject ignoring structural change or using a fixed-length moving window. The shape of the long-run distribution is affected by breaks, which has implications for risk management and long-run investment decisions.|How Useful Are Historical Data for Forecasting the Long-Run Equity Return Distribution?|http://www.jstor.org/stable/27639022|27639022|2009-01-01|2009|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
We explore and illustrate the use of time series decomposition methods for evaluating and comparing latent structure in nonstationary electroencephalographic (EEG) traces obtained from depressed patients during brain seizures induced as part of electroconvulsive therapy (ECT). Analysis of the patterns of change over time in the frequency structure of such EEG data provides insight into the neurophysiological mechanisms of action of this effective but poorly understood antidepressant treatment, and allows clinicians to modify ECT treatments to optimize therapeutic benefits while minimizing associated side effects. Our work has introduced new methods of time-frequency analysis of EEG series that identify the complete pattern of time evolution of frequency structure over the course of a seizure, and usefully assist in these scientific and clinical studies. New methods of decomposition of flexible dynamic models provide time domain decompositions of individual EEG series into collections of latent components in different frequency bands. This allows us to explore ECT seizure characteristics via inferences on the time-varying parameters that characterize these latent components, and to relate differences in such characteristics across seizures to differences in the therapeutic effectiveness and cognitive side effects of those seizures. This article discusses the scientific context and problems, development of nonstationary time series models and new methods of decomposition to explore time-frequency structure, and aspects of model fitting and analysis. We include applied studies on two datasets from recent clinical ECT studies. One is an initial illustrative analysis of a single EEG trace, the second compares the EEG data recorded during two types of ECT treatment that differ in therapeutic effectiveness and cognitive side effects. The uses of these models and time series decomposition methods in extracting and contrasting key features of the seizure underlying the EEG signals are highlighted. Through the use of these models we have quantified, for the first time, decreases in the dominant frequencies of low-frequency EEG components during ECT seizures. We have also identified preliminary evidence that such decreases are enhanced under the more effective ECTs at higher electrical dosages, a finding consistent with prior reports and the hypothesis that more effective forms of ECT are more effective in eliciting neurophysiological inhibitory processes.|Evaluation and Comparison of EEG Traces: Latent Structure in Nonstationary Time Series|http://www.jstor.org/stable/2669922|2669922|1999-12-01|1999|['eng']|['Biological sciences - Biology', 'Mathematics - Mathematical analysis']|['Science & Mathematics', 'Statistics']
Network models are widely used to represent relations between interacting units or actors. Network data often exhibit transitivity, meaning that two actors that have ties to a third actor are more likely to be tied than actors that do not, homophily by attributes of the actors or dyads, and clustering. Interest often focuses on finding clusters of actors or ties, and the number of groups in the data is typically unknown. We propose a new model, the latent position cluster model, under which the probability of a tie between two actors depends on the distance between them in an unobserved Euclidean 'social space', and the actors' locations in the latent social space arise from a mixture of distributions, each corresponding to a cluster. We propose two estimation methods: a two-stage maximum likelihood method and a fully Bayesian method that uses Markov chain Monte Carlo sampling. The former is quicker and simpler, but the latter performs better. We also propose a Bayesian way of determining the number of clusters that are present by using approximate conditional Bayes factors. Our model represents transitivity, homophily by attributes and clustering simultaneously and does not require the number of clusters to be known. The model makes it easy to simulate realistic networks with clustering, which are potentially useful as inputs to models of more complex systems of which the network is part, such as epidemic models of infectious disease. We apply the model to two networks of social relations. A free software package in the R statistical language, latentnet, is available to analyse data by using the model.|Model-Based Clustering for Social Networks|http://www.jstor.org/stable/4623163|4623163|2007-01-01|2007|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
This study contributes to the q-based empirical investment literature by using a Bayesian approach to analyze the impact of internal financial variables on a q-based investment model, accounting specifically for variable selection and incorporating outliers explicitly within the advocated modeling framework. For the balanced panel analyzed, a farm's liquidity situation is found to affect its investment significantly. Incorporation of an outlier detection component changes the results drastically, in both the variables chosen and the parameter estimates. The results and the nature of most investment data suggest that not accounting for outliers may lead to inaccurate inference.|Financial Constraints and Farm Investment: A Bayesian Examination|http://www.jstor.org/stable/27638781|27638781|2004-01-01|2004|['eng']|['Applied sciences - Engineering']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
Improving health worldwide will require rigorous quantification of population-level trends in health status. However, global-level surveys are not available, forcing researchers to rely on fragmentary country-specific data of varying quality. We present a Bayesian model that systematically combines disparate data to make country-, region- and global-level estimates of time trends in important health indicators. The model allows for time and age nonlinearity, and it borrows strength in time, age, covariates, and within and across regional country clusters to make estimates where data are sparse. The Bayesian approach allows us to account for uncertainty from the various aspects of missingness as well as sampling and parameter uncertainty. MCMC sampling allows for inference in a highdimensional, constrained parameter space, while providing posterior draws that allow straightforward inference on the wide variety of functionals of interest. Here we use blood pressure as an example health metric. High blood pressure is the leading risk factor for cardiovascular disease, the leading cause of death worldwide. The results highlight a risk transition, with decreasing blood pressure in high-income regions and increasing levels in many lowerincome regions.|Bayesian Estimation of Population-Level Trends in Measures of Health Status|http://www.jstor.org/stable/43288446|43288446|2014-02-01|2014|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
This article examines the consistency, interpretation and application of Bayes factors constructed from standard test statistics. Primary conclusions are that Bayes factors based on multinomial and normal test statistics are consistent for suitable choices of the hyperparameters used to specify alternative hypotheses, and that such constructions can be extended to obtain consistent Bayes factors based on likelihood ratio statistics. A connection between Bayes factors based on likelihood ratio statistics and the Bayesian information criterion is exposed, as is a connection between Bayes factors based on F statistics and parametric Bayes factors based on normal-inverse gamma models. Similarly, Bayes factors based on chi-squared statistics for multinomial data are shown to provide accurate approximations to Bayes factors based on multinomial/Dirichlet models. An illustration of how the simple form of these Bayes factors can be exploited to generate easily interpretable summaries of the experimental 'weight of evidence' is provided.|Properties of Bayes Factors Based on Test Statistics|http://www.jstor.org/stable/41548598|41548598|2008-06-01|2008|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
In this paper, we propose a general model-determination strategy based on Bayesian methods for nonlinear mixed effects models. Adopting an exploratory data analysis viewpoint, we develop diagnostic tools based on conditional predictive ordinates that conveniently get tied in with Markov chain Monte Carlo fitting of models. Sampling-based methods are used to carry out these diagnostics. Two examples are presented to illustrate the effectiveness of these criteria. The first one is the famous Langmuir equation, commonly used in pharmacokinetic models, whereas the second model is used in the growth curve model for longitudinal data.|Bayesian Approach for Nonlinear Random Effects Models|http://www.jstor.org/stable/2533493|2533493|1997-12-01|1997|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
"Bayesian hierarchical models typically involve specifying prior distributions for one or more variance components. This is rather removed from the observed data, so specification based on expert knowledge can be difficult. While there are suggestions for ""default"" priors in the literature, often a conditionally conjugate inverse-gamma specification is used, despite documented drawbacks of this choice. The authors suggest ""conservative"" prior distributions for variance components, which deliberately give more weight to smaller values. These are appropriate for investigators who are skeptical about the presence of variability in the second-stage parameters (random effects) and want to particularly guard against inferring more structure than is really present. The suggested priors readily adapt to various hierarchical modelling settings, such as fitting smooth curves, modelling spatial variation and combining data from multiple sites. /// Les modèles bayésiens hiérarchiques comportent généralement une ou des composantes de variance que l'on doit doter de lois a priori. Le choix de ces lois est délicat car la variation est un aspect des données difficile à cerner. De toutes les lois a priori ""par défaut,"" une loi conjuguée inverse-gamma conditionnelle est la plus souvent employée, malgré ses inconvénients. Les auteurs proposent des lois a priori ""conservatrices"" pour les composantes de la variance qui privilégient les petites valeurs. Elles conviennent bien aux situations où le chercheur s'interroge sur la présence réelle de variabilité dans les paramètres de deuxième degré (effets aléatoires) et qu'il veut éviter d'imposer une structure artificielle. Les lois a priori suggérées s'adaptent à diverses situations propices à la modélisation hiérarchique, notamment l'ajustement de courbes lisses et la modélisation de variation spatiale ou de données issues de nombreux sites."|Conservative Prior Distributions for Variance Parameters in Hierarchical Models|http://www.jstor.org/stable/20445210|20445210|2006-09-01|2006|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
We consider a multiscale model for intensities in photon-limited ¡mages using a Bayesian framework. A typical Dirichlet prior on relative intensities is not efficient in picking up structures owing to the continuity of intensities. We propose a novel prior using the socalled 'Chinese restaurant process' to create structures in the form of equal intensities of some neighbouring pixels. Simulations are conducted using several photon-limited images, which are common in X-ray astronomy and other high energy photon-based ¡mages. Applications to astronomical ¡mages from the Chandra X-ray Observatory satellite are shown. The new methodology outperforms most existing methods ¡n terms of image processing quality, speed and the ability to select smoothing parameters automatically.|Bayesian smoothing of photon-limited images with applications in astronomy|http://www.jstor.org/stable/41262264|41262264|2011-09-01|2011|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
This paper provides a new method and algorithm for making inferences about the parameters of a two-level multivariate normal hierarchical model. One has observed J p-dimensional vector outcomes, distributed at level 1 as multivariate normal with unknown mean vectors and with known covariance matrices. At level 2, the unknown mean vectors also have normal distributions, with common unknown covariance matrix A and with means depending on known covariates and on unknown regression coefficients. The algorithm samples independently from the marginal posterior distribution of A by using rejection procedures. Functions such as posterior means and covariances of the level 1 mean vectors and of the level 2 regression coefficient are estimated by averaging over posterior values calculated conditionally on each value of A drawn. This estimation accounts for the uncertainty in A, unlike standard restricted maximum likelihood empirical Bayes procedures. It is based on independent draws from the exact posterior distributions, unlike Gibbs sampling. The procedure is demonstrated for profiling hospitals based on patients' responses concerning p = 2 types of problems (non-surgical and surgical). The frequency operating characteristics of the rule corresponding to a particular vague multivariate prior distribution are shown via simulation to achieve their nominal values in that setting.|Inference for Multivariate Normal Hierarchical Models|http://www.jstor.org/stable/3088867|3088867|2000-01-01|2000|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
The aim of this paper is to develop a Bayesian functional linear Cox regression model (BFLCRM) with both functional and scalar covariates. This new development is motivated by establishing the likelihood of conversion to Alzheimer's disease (AD) in 346 patients with mild cognitive impairment (MCI) enrolled in the Alzheimer's Disease Neuroimaging Initiative 1 (ADNI-1) and the early markers of conversion. These 346 MCI patients were followed over 48 months, with 161 MCI participants progressing to AD at 48 months. The functional linear Cox regression model was used to establish that functional covariates including hippocampus surface morphology and scalar covariates including brain MRI volumes, cognitive performance (ADAS-Cog) and APOE-ε4 status can accurately predict time to onset of AD. Posterior computation proceeds via an efficient Markov chain Monte Carlo algorithm. A simulation study is performed to evaluate the finite sample performance of BFLCRM.|BFLCRM: A BAYESIAN FUNCTIONAL LINEAR COX REGRESSION MODEL FOR PREDICTING TIME TO CONVERSION TO ALZHEIMER'S DISEASE|http://www.jstor.org/stable/43826461|43826461|2015-12-01|2015|['eng']|['Applied sciences - Engineering', 'Biological sciences - Biology']|['Mathematics', 'Science & Mathematics', 'Statistics']
We develop a model to understand and describe the inherent behaviours and interactions of members over time through the medium of user-generated content (UGC) in an on-line community. Because the behavioural event counts of generating and accessing UGC by on-line users serve as the two most important metrics to judge the success of on-line communities, we propose a bivariate zero-inflated Poisson model to model simultaneously the daily counts of the two main UGC-related activities. In particular, we consider interdependences of the repeatedly measured behavioural events within members, model the dependence of the current event counts on the past event counts and explore the probable non-linear effects of time at the individual level. Furthermore, we incorporate interactions between members by constructing a set of individual-specific time varying measures in an integrated modelling framework. In our empirical applications, we examine key behavioural determinants influencing member behaviours in the UGC site. As part of our substantive contribution, we highlight the model's ability to make accurate predictions about the evolution of the on-line community.|Modelling member behaviour in on-line user-generated content sites: a semiparametric Bayesian approach|http://www.jstor.org/stable/41409695|41409695|2011-10-01|2011|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
"This article presents new computational techniques for multivariate longitudinal or clustered data with missing values. Current methodology for linear mixed-effects models can accommodate imbalance or missing data in a single response variable, but it cannot handle missing values in multiple responses or additional covariates. Applying a multivariate extension of a popular linear mixed-effects model, we create multiple imputations of missing values for subsequent analyses by a straightforward and effective Markov chain Monte Carlo procedure. We also derive and implement a new EM algorithm for parameter estimation which converges more rapidly than traditional EM algorithms because it does not treat the random effects as ""missing data,"" but integrates them out of the likelihood function analytically. These techniques are illustrated on models for adolescent alcohol use in a large school-based prevention trial."|Computational Strategies for Multivariate Linear Mixed-Effects Models with Missing Values|http://www.jstor.org/stable/1391061|1391061|2002-06-01|2002|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Computer Science', 'Statistics']
We propose a hybrid approach for estimating beta that shrinks rolling window estimates toward firm-specific priors motivated by economic theory. Our method yields superior forecasts of beta that have important practical implications. First, unlike standard rolling window betas, hybrid betas carry a significant price of risk in the cross-section even after controlling for characteristics. Second, the hybrid approach offers statistically and economically significant out-of-sample benefits for investors who use factor models to construct optimal portfolios. We show that the hybrid estimator outperforms existing estimators because shrinkage toward a fundamentals-based prior is effective in reducing measurement noise in extreme beta estimates.|Estimating Security Betas Using Prior Information Based on Firm Fundamentals|http://www.jstor.org/stable/43866044|43866044|2016-04-01|2016|['eng']|['Physical sciences - Astronomy']|['Business & Economics', 'Business', 'Finance']
This paper describes a method for choosing a natural conjugate prior distribution for a normal linear sampling model. A person using the method to quantify his/her opinions performs specified elicitation tasks. The hyperparameters of the conjugate distribution are estimated from the elicited values. The method is designed to require elicitation tasks that people can perform competently and introduces a type of task not previously reported. A property of the method is that the assessed variance matrices are certain to be positive definite. The method is sufficiently simple to implement with an interactive computer program on a microcomputer.|Quantifying Expert Opinion in Linear Regression Problems|http://www.jstor.org/stable/2345708|2345708|1988-01-01|1988|['eng']|['Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
A standard tool for model selection in a Bayesian framework is the Bayes factor which compares the marginal likelihood of the data under two given different models. In this paper, we consider the class of hierarchical loglinear models for discrete data given under the form of a contingency table with multinomial sampling. We assume that the prior distribution on the loglinear parameters is the Diaconis-Ylvisaker conjugate prior, and the uniform is the prior distribution on the space of models. Under these conditions, the Bayes factor between two models is a function of the normalizing constants of the prior and posterior distribution of the loglinear parameters. These constants are functions of the hyperparameters (m, α) which can be interpreted, respectively, as the marginal counts and total count of a fictive contingency table. We study the behavior of the Bayes factor when a tends to zero. In this study, the most important tool is the characteristic function J C of the interior C of the convex hull C of the convex hull $\bar C$ of the support of the multinomial distribution for a given hierarchical loglinear model. If h C is the support function of C, the function J C is the Laplace transform of exp(–h C ). We show that, when α tends to 0, if the data lies on a face F i of C i , i = 1,2, of dimension k i , the Bayes factor behaves like ${\alpha ^{k1 - k2}}$ . This implies in particular that when the data is in C₁ and in C₂, that is, when k i equals the dimension of model J i , the sparser model is favored, thus confirming the idea of Bayesian regularization. In order to find the faces of $\bar C$ , we need to know its facets. We show that since here C is a polytope, the denominator of the rational function J C is m e product of the equations of the facets. We also identify a category of facets common to all hierarchical models for discrete variables, not necessarily binary. Finally, we show that these facets are the only facets of $\bar C$ when the model is graphical with respect to a decomposable graph.|BAYES FACTORS AND THE GEOMETRY OF DISCRETE HIERARCHICAL LOGLINEAR MODELS|http://www.jstor.org/stable/41713658|41713658|2012-04-01|2012|['eng']|['Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
Our daily visual experiences are inevitably linked to recognizing the rich variety of textures. However, how the brain encodes and differentiates a plethora of natural textures remains poorly understood. Here, we show that many neurons in macaque V4 selectively encode sparse combinations of higher-order image statistics to represent natural textures. We systematically explored neural selectivity in a high-dimensional texture space by combining texture synthesis and efficient-sampling techniques. This yielded parameterized models for individual texture-selective neurons. The models provided parsimonious but powerful predictors for each neuron’s preferred textures using a sparse combination of image statistics. As a whole population, the neuronal tuning was distributed in a way suitable for categorizing textures and quantitatively predicts human ability to discriminate textures. Together, we suggest that the collective representation of visual image statistics in V4 plays a key role in organizing the natural texture perception.|Image statistics underlying natural texture selectivity of neurons in macaque V4|http://www.jstor.org/stable/26454269|26454269|2015-01-27|2015|['eng']|['Biological sciences - Biology']|['Science & Mathematics', 'Biological Sciences', 'General Science']
"Objectives—To describe the small area system developed in Finland. To illustrate the use of the system with analyses of incidence of lung cancer around an asbestos mine. To compare the performance of different spatial statistical models when applied to sparse data. Methods—In the small area system, cancer and population data are available by ses, age, and socioeconomic status in adjacent ""pixels"", squares of size 0.5 km × 0.5 km. The study area was partitioned into sub-areas based on estimated exposure. The original data at the pixel level were used in a spatial random field model. For comparison, standardised incidence ratios were estimated, and full bayesian and empirical bayesian models were fitted to aggregated data. Incidence of lung cancer around a former asbestos mine was used as an illustration. Results—The spatial random field model, which has been used in former small area studies, did not converge with present fine resolution data. The number of neighbouring pixels used in smoothing had to be enlarged, and informative distributions for hyperparameters were used to stabilise the unobserved random field. The ordered spatial random field model gave lower estimates than the Poisson model. When one of the three effects of area were fixed, the model gave similar estimates with a narrower interval than the Poisson model. Conclusions—The use of fine resolution data and socioeconomic status as a means of controlling for confounding related to lifestyle is useful when estimating risk of cancer around point sources. However, better statistical methods are needed for spatial modelling of fine resolution data."|Small Area Estimation of Incidence of Cancer around a Known Source of Exposure with fine Resolution Data|http://www.jstor.org/stable/27731496|27731496|2001-05-01|2001|['eng']|['Health sciences - Medical specialties', 'Health sciences - Health and wellness', 'Physical sciences - Astronomy']|['Health Sciences', 'Medicine and Allied Health']
In the present study, we consider the problem of classifying spatial data distorted by a linear transformation or convolution and contaminated by additive random noise. In this setting, we show that classifier performance can be improved if we carefully invert the data before the classifier is applied. However, the inverse transformation is not constructed so as to recover the original signal, and in fact, we show that taking the latter approach is generally inadvisable. We introduce a fully data-driven procedure based on cross-validation, and use several classifiers to illustrate numerical properties of our approach. Theoretical arguments are given in support of our claims. Our procedure is applied to data generated by light detection and ranging (Lidar) technology, where we improve on earlier approaches to classifying aerosols. This article has supplementary materials online.|Deconvolution When Classifying Noisy Data Involving Transformations|http://www.jstor.org/stable/23427422|23427422|2012-09-01|2012|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Die bisher getrennte empirische Literatur zu Zentralbankglaubwürdigkeit und Insider Macht auf Arbeitsmärkten wird kritisch im Licht eines einheitlichen Modells untersucht, das den Interessenkonflikt von Gewerkschaften und Zentralbank bei der Bestimmung von Beschäftigung und Inflation berücksichtigt. Die Ergebnisse einer Schätzung von Zentralbankblaubwürdigkeit und Insider-Einfluß in Europa mit Hilfe eines neuen Kalman Filter-Verfahrens werden zusammenfassend dargestellt. The empirical literature on insider power in wage determination and central bank credibility largely neglects the strategical interaction of central banks and trade unions. The paper provides a critical survey of this literature using a unifying model which considers the conflict of interests of wage-setters and monetary policy makers. The results of an estimation of central bank credibility and insider-power in Europe with Bomhoff's Kalman Filter method are reported.|Zentralbankglaubwürdigkeit und Insider-Macht: Empirische Evidenz / Central Bank Credibility and Insider Power: Empirical Evidence|http://www.jstor.org/stable/23811833|23811833|1995-07-01|1995|['ger']||['Business & Economics', 'Science & Mathematics', 'Statistics', 'Economics']
Global positioning systems (GPSs) and geographical information systems (GISs) have been widely used to collect and synthesize spatial data from a variety of sources. New advances in satellite imagery and remote sensing now permit scientists to access spatial data at several different resolutions. The Internet facilitates fast and easy data acquisition. In any one study, several different types of data may be collected at differing scales and resolutions, at different spatial locations, and in different dimensions. Many statistical issues are associated with combining such data for modeling and inference. This article gives an overview of these issues and the approaches for integrating such disparate data, drawing on work from geography, ecology, agriculture, geology, and statistics. Emphasis is on state-of-the-art statistical solutions to this complex and important problem.|Combining Incompatible Spatial Data|http://www.jstor.org/stable/3085677|3085677|2002-06-01|2002|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
We develop a Bayesian hierarchical model for the analysis of ordinal data from multirater ranking studies. The model for a rater's score includes four latent factors: one is a latent item trait determining the true order of items and the other three are the rater's performance characteristics, including bias, discrimination, and measurement error in the ratings. The proposed approach aims at three goals. First, three Bayesian estimators are introduced to estimate the ranks of items. They all show a substantial improvement over the widely used score sums by using the information on the variable skill of the raters. Second, rater performance can be compared based on rater bias, discrimination, and measurement error. Third, a simulation-based decisiontheoretic approach is described to determine the number of raters to employ. A simulation study and an analysis based on a grant review data set are presented.|<strong>A Bayesian Approach to Ranking and Rater Evaluation: An Application to Grant Reviews</strong>|http://www.jstor.org/stable/40785161|40785161|2010-04-01|2010|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Education', 'Statistics', 'Social Sciences']
In this article we present a technique for implementing large-scale optimal portfolio selection. We use high-frequency daily data to capture valuable statistical information in asset returns. We describe several statistical issues involved in quantitative approaches to portfolio selection. Our methodology applies to large-scale portfolio-selection problems in which the number of possible holdings is large relative to the estimation period provided by historical data. We illustrate our approach on an equity database that consists of stocks from the Standard and Poor's index, and we compare our portfolios to this benchmark index. Our methodology differs from the usual quadratic programming approach to portfolio selection in three ways: (1) We employ informative priors on the expected returns and variance-covariance matrices, (2) we use daily data for estimation purposes, with upper and lower holding limits for individual securities, and (3) we use a dynamic asset-allocation approach that is based on reestimating and then rebalancing the portfolio weights on a prespecified time window. The key inputs to the optimization process are the predictive distributions of expected returns and the predictive variance-covariance matrix. We describe the statistical issues involved in modeling these inputs for high-dimensional portfolio problems in which our data frequency is daily. In our application, we find that our optimal portfolio outperforms the underlying benchmark.|Bayesian Portfolio Selection: An Empirical Analysis of the S&P 500 Index 1970-1996|http://www.jstor.org/stable/1392554|1392554|2000-04-01|2000|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
An important aspect of marketing practice is the targeting of consumer segments for differential promotional activity. The premise of this activity is that there exist distinct segments of homogeneous consumers who can be identified by readily available demographic information. The increased availability of individual consumer panel data open the possibility of direct targeting of individual households. The goal of this paper is to assess the information content of various information sets available for direct marketing purposes. Information on the consumer is obtained from the current and past purchase history as well as demographic characteristics. We consider the situation in which the marketer may have access to a reasonably long purchase history which includes both the products purchased and information on the causal environment. Short of this complete purchase history, we also consider more limited information sets which consist of only the current purchase occasion or only information on past product choice without causal variables. Proper evaluation of this information requires a flexible model of heterogeneity which can accommodate observable and unobservable heterogeneity as well as produce household level inferences for targeting purposes. We develop new econometric methods to implement a random coefficient choice model in which the heterogeneity distribution is related to observable demographics. We couple this approach to modeling heterogeneity with a target couponing problem in which coupons are customized to specific households on the basis of various information sets. The couponing problem allows us to place a monetary value on the information sets. Our results indicate there exists a tremendous potential for improving the profitability of direct marketing efforts by more fully utilizing household purchase histories. Even rather short purchase histories can produce a net gain in revenue from target couponing which is 2.5 times the gain from blanket couponing. The most popular current electronic couponing trigger strategy uses only one observation to customize the delivery of coupons. Surprisingly, even the information contained in observing one purchase occasion boasts net couponing revenue by 50% more than that which would be gained by the blanket strategy. This result, coupled with increased competitive pressures, will force targeted marketing strategies to become much more prevalent in the future than they are today.|The Value of Purchase History Data in Target Marketing|http://www.jstor.org/stable/184168|184168|1996-01-01|1996|['eng']|['Mathematics - Applied mathematics']|['Marketing & Advertising', 'Business & Economics', 'Business']
Rank aggregation, that is, combining several ranking functions (called base rankers) to get aggregated, usually stronger rankings of a given set of items, is encountered in many disciplines. Most methods in the literature assume that base rankers of interest are equally reliable. It is very common in practice, however, that some rankers are more informative and reliable than others. It is desirable to distinguish high quality base rankers from low quality ones and treat them differently. Some methods achieve this by assigning prespecified weights to base rankers. But there are no systematic and principled strategies for designing a proper weighting scheme for a practical problem. In this article, we propose a Bayesian approach, called Bayesian aggregation of rank data (BARD), to overcome this limitation. By attaching a quality parameter to each base ranker and estimating these parameters along with the aggregation process, BARD measures reliabilities of base rankers in a quantitative way and makes use of this information to improve the aggregated ranking. In addition, we design a method to detect highly correlated rankers and to account for their information redundancy appropriately. Both simulation studies and real data applications show that BARD significantly outperforms existing methods when equality of base rankers varies greatly.|Bayesian Aggregation of Order-Based Rank Data|http://www.jstor.org/stable/24247433|24247433|2014-09-01|2014|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
We propose a general Bayesian criterion for model assessment for categorical data called the weighted L measure, which is constructed from the posterior predictive distribution of the data. The measure is based on weighting the observations according to the sampling variance of their future response vector. The weight component in the weighted L measure plays the role of a penalty term in the criterion, in which a greater weight assigned to covariate values implies a greater penalty term on the dimension of the model. A detailed justification is provided for such a weighting procedure and several theoretical properties of the weighted L measure are presented for a wide variety of discrete data models. For these models, we examine properties of the weighted L measure, and show that it can perform better than the unweighted L measure in a variety of settings. In addition, we show that the weighted quadratic loss L measure is more attractive than the unweighted L measure and the deviance loss L measure for categorical data. Moreover, a calibration for the weighted L measure is motivated and proposed, which allows us to compare formally the L measure values of competing models. A detailed simulation study is presented to examine the performance of the weighted L measure, and it is compared to other established model-selection methods. Finally, the method is applied to a real dataset using a bivariate ordinal response model.|Bayesian Criterion Based Model Assessment for Categorical Data|http://www.jstor.org/stable/20441078|20441078|2004-03-01|2004|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
The randomized response technique ensures that individual item responses, denoted as true item responses, are randomized before observing them and so-called randomized item responses are observed. A relationship is specified between randomized item response data and true item response data. True item response data are modeled with a (non)linear mixed effects and/or item response theory model. Although the individual true item responses are masked through randomizing the responses, the model extension enables the computation of individual true item response probabilities and estimates of individuals' sensitive behavior/attitude and their relationships with background variables taking into account any clustering of respondents. Results are presented from a College Alcohol Problem Scale (CAPS) where students were interviewed via direct questioning or via a randomized response technique. A Markov Chain Monte Carlo algorithm is given for estimating simultaneously all model parameters given hierarchical structured binary or polytomous randomized item response data and background variables.|A Mixed Effects Randomized Item Response Model|http://www.jstor.org/stable/20172126|20172126|2008-12-01|2008|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Education', 'Statistics', 'Social Sciences']
Structured additive regression models are perhaps the most commonly used class of models in statistical applications. It includes, among others, (generalized) linear models, (generalized) additive models, smoothing spline models, state space models, semiparametric regression, spatial and spatiotemporal models, log-Gaussian Cox processes and geostatistical and geoadditive models. We consider approximate Bayesian inference in a popular subset of structured additive regression models, latent Gaussian models, where the latent field is Gaussian, controlled by a few hyperparameters and with non-Gaussian response variables. The posterior marginals are not available in closed form owing to the non-Gaussian response variables. For such models, Markov chain Monte Carlo methods can be implemented, but they are not without problems, in terms of both convergence and computational time. In some practical applications, the extent of these problems is such that Markov chain Monte Carlo sampling is simply not an appropriate tool for routine analysis. We show that, by using an integrated nested Laplace approximation and its simplified version, we can directly compute very accurate approximations to the posterior marginals. The main benefit of these approximations is computational: where Markov chain Monte Carlo algorithms need hours or days to run, our approximations provide more precise estimates in seconds or minutes. Another advantage with our approach is its generality, which makes it possible to perform Bayesian analysis in an automatic, streamlined way, and to compute model comparison criteria and various predictive measures so that models can be compared and the model under study can be challenged.|Approximate Bayesian Inference for Latent Gaussian Models by Using Integrated Nested Laplace Approximations|http://www.jstor.org/stable/40247579|40247579|2009-04-01|2009|['eng']|['Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
Studies of the predictive ability of the Federal Reserve's Beige Book for aggregate output and employment have proven inconclusive. This might be attributed, in part, to its irregular release schedule. We use a model that allows for data sampling at mixed frequencies to analyze the predictive power of the Beige Book. We find that the Beige Book's national summary and District reports predict GDP and aggregate employment and that most District reports provide information content for regional employment. In addition, there appears to be an asymmetry in the predictive content of the Beige Book language.|Measuring the Information Content of the Beige Book: A Mixed Data Sampling Approach|http://www.jstor.org/stable/25483476|25483476|2009-02-01|2009|['eng']|['Information science - Informetrics', 'Education - Educational resources', 'Applied sciences - Research methods', 'Physical sciences - Earth sciences', 'Mathematics - Applied mathematics', 'Physical sciences - Astronomy', 'Biological sciences - Ecology', 'Education - Academic communities']|['Business & Economics', 'Business', 'Economics', 'Finance']
Bayesian methods have been used quite extensively in recent years for solving small-area estimation problems. Particularly effective in this regard has been the hierarchical or empirical Bayes approach, which is especially suitable for a systematic connection of local areas through models. However, the development to date has mainly concentrated on continuous-valued variates. Often the survey data are discrete or categorical, so that hierarchical or empirical Bayes techniques designed for continuous variates are inappropriate. This article considers hierarchical Bayes generalized linear models for a unified analysis of both discrete and continuous data. A general theorem is provided that ensures the propriety of posteriors under diffuse priors. This result is then extended to the case of spatial generalized linear models. The hierarchical Bayes procedure is implemented via Markov chain Monte Carlo integration techniques. Two examples (one featuring spatial correlation structure) are given to illustrate the general method.|Generalized Linear Models for Small-Area Estimation|http://www.jstor.org/stable/2669623|2669623|1998-03-01|1998|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
This paper estimates energy demand in the British domestic and industrial sectors and analyzes the extent to which energy-saving technological change is exogenous, or induced by the energy price. The paper implements models with a linear trend, models making use of the price decomposition of Dargay and Gately (1995a) and the Structural Time Series Models (STSMs) of Harvey (1989). Stochastic trends have been found to be rather important while in neither of the sectors assessed in this study could the hypothesis of symmetric price effects be rejected. Following Hunt and Judge (2005), stochastic trend and asymmetric price effects are found to be substitutes in the industrial sector. In particular we conclude that asymmetric price effects can substitute for the slope in the stochastic trend. Finally, energy consumption in the industrial sector is strongly influenced by price while the effect of price in the domestic sector is markedly smaller.|Stochastic Trends and Technical Change: The Case of Energy Consumption in the British Industrial and Domestic Sectors|http://www.jstor.org/stable/41323381|41323381|2010-01-01|2010|['eng']|['Mathematics - Applied mathematics', 'Information science - Information analysis']|['Business & Economics', 'Business', 'Economics']
For the problem of variable selection for the normal linear model, selection criteria such as AIC, Cp, BIC and RIC have fixed dimensionality penalties. Such criteria are shown to correspond to selection of maximum posterior models under implicit hyperparameter choices for a particular hierarchical Bayes formulation. Based on this calibration, we propose empirical Bayes selection criteria that use hyperparameter estimates instead of fixed choices. For obtaining these estimates, both marginal and conditional maximum likelihood methods are considered. As opposed to traditional fixed penalty criteria, these empirical Bayes criteria have dimensionality penalties that depend on the data. Their performance is seen to approximate adaptively the performance of the best fixed-penalty criterion across a variety of orthogonal and nonorthogonal set-ups, including wavelet regression. Empirical Bayes shrinkage estimators of the selected coefficients are also proposed.|Calibration and Empirical Bayes Variable Selection|http://www.jstor.org/stable/2673607|2673607|2000-12-01|2000|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Many ecological studies require analysis of collections of estimates. For example, population change is routinely estimated for many species from surveys such as the North American Breeding Bird Survey (BBS), and the species are grouped and used in comparative analyses. We developed a hierarchical model for estimation of group attributes from a collection of estimates of population trend. The model uses information from predefined groups of species to provide a context and to supplement data for individual species; summaries of group attributes are improved by statistical methods that simultaneously analyze collections of trend estimates. The model is Bayesian; trends are treated as random variables rather than fixed parameters. We use Markov Chain Monte Carlo (MCMC) methods to fit the model. Standard assessments of population stability cannot distinguish magnitude of trend and statistical significance of trend estimates, but the hierarchical model allows us to legitimately describe the probability that a trend is within given bounds. Thus we define population stability in terms of the probability that the magnitude of population change for a species is less than or equal to a predefined threshold. We applied the model to estimates of trend for 399 species from the BBS to estimate the proportion of species with increasing populations and to identify species with unstable populations. Analyses are presented for the collection of all species and for 12 species groups commonly used in BBS summaries. Overall, we estimated that 49% of species in the BBS have positive trends and 33 species have unstable populations. However, the proportion of species with increasing trends differs among habitat groups, with grassland birds having only 19% of species with positive trend estimates and wetland birds having 68% of species with positive trend estimates.|Hierarchical Modeling of Population Stability and Species Group Attributes from Survey Data|http://www.jstor.org/stable/3071992|3071992|2002-06-01|2002|['eng']|['Applied sciences - Engineering', 'Biological sciences - Biology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
Background/Aims: Gene network analysis can be a very valuable approach for elucidating complex dependence between functional SNPs in a candidate genetic pathway and for assessing their association with a disease of interest. Even when the number of SNPs evaluated is relatively small (&lt;20), the number of potential gene networks induced by the SNPs can be very large and the contingency tables representing their joint distribution very sparse. Methods: In this paper, we propose a Bayesian model determination for gene network analysis using decomposable discrete graphical models combined with Reversible Jump Markov chain Monte Carlo. We show the application of this approach in a study of 13 SNPs in the DNA repair pathway and their association with breast cancer from a case-control study conducted in Ontario, Canada. Results: The strength of associations among the SNPs and between the SNPs and the disease status is evaluated by computing the posterior probability of any pair of variables. The corresponding gene network is reconstructed by retaining pair-wise associations with the highest posterior probabilities. In our real data analysis, we found evidence for a particular association between one SNP in the gene POLL and the disease status and also several interesting patterns of association between the SNPs themselves. Conclusion: This general statistical framework could serve as a basis for prioritizing genes and SNPs that play a major role in breast cancer etiology and to better understand their complex interactions in a specific genetic pathway.|Inferring Gene Network from Candidate SNP Association Studies Using a Bayesian Graphical Model|http://www.jstor.org/stable/48513754|48513754|2014-01-01|2014|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Biological Sciences', 'Medicine & Allied Health', 'Health Sciences']
Models for multiple-test screening data generally require the assumption that the tests are independent conditional on disease state. This assumption may be unreasonable, especially when the biological basis of the tests is the same. We propose a model that allows for correlation between two diagnostic test results. Since models that incorporate test correlation involve more parameters than can be estimated with the available data, posterior inferences will depend more heavily on prior distributions, even with large sample sizes. If we have reasonably accurate information about one of the two screening tests (perhaps the standard currently used test) or the prevalences of the populations tested, accurate inferences about all the parameters, including the test correlation, are possible. We present a model for evaluating dependent diagnostic tests and analyse real and simulated data sets. Our analysis shows that, when the tests are correlated, a model that assumes conditional independence can perform very poorly. We recommend that, if the tests are only moderately accurate and measure the same biological responses, researchers use the dependence model for their analyses.|Correlation-Adjusted Estimation of Sensitivity and Specificity of Two Diagnostic Tests|http://www.jstor.org/stable/3592632|3592632|2003-01-01|2003|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Over recent years, the use of homogeneous Gibbs prior models in image processing has become widely accepted. There has been, however, much discussion over precisely which models are most appropriate. For most applications, the simplest Gaussian model tends to oversmooth reconstructions, so it has been rejected in favor of various edge-preserving alternatives. We claim that the problem is not with the Gaussian family, but rather with the assumption of homogeneity. In this article we propose an inhomogeneous Gaussian random field as a general prior model for many image-processing applications. The simplicity of the Gaussian model allows rapid calculation, and the flexibility of the spatially varying prior parameter allows varying degrees of spatial smoothing. This approach is in the spirit of adaptive kernel density methods where only the choice of the variable window width is important. The analysis of real single-photon emission computed tomography data is used to illustrate the methods, and simulated data are used to demonstrate that the proposed procedures lead to more accurate reconstruction than edge-preserving homogeneous alternatives. The inhomogeneous model allows greater flexibility; small features are not masked by the smoothing, and constant regions obtain sufficient smoothing to remove the effects of noise.|Inhomogeneous Prior Models for Image Reconstruction|http://www.jstor.org/stable/2670008|2670008|1999-09-01|1999|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
This article proposes new Bayesian models for software reliability based on a piecewise constant failure rate. A martingale process prior is assumed on the failure rate. Three different hyperprior models on the martingale process are considered. The first two models assume that the conditional variance is dependent on the mean, whereas the third model assumes constant conditional variance. Markov chain sampling-based posterior analysis and prequential predictions are developed for these models and are illustrated in a software failure dataset. In addition, model comparison is studied via the Bayes factor criterion. General techniques are described for estimating the marginal likelihood of the proposed models as well as of many existing software reliability models, and these are illustrated in two datasets.|Bayesian Software Reliability Models Based on Martingale Processes|http://www.jstor.org/stable/25047012|25047012|2003-05-01|2003|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Many hormones are secreted in pulses. The pulsatile relationship between hormones regulates many biological processes. To understand endocrine system regulation, time series of hormone concentrations are collected. The goal is to characterize pulsatile patterns and associations between hormones. Currently each hormone on each subject is fitted univariately. This leads to estimates of the number of pulses and estimates of the amount of hormone secreted; however, when the signal-to-noise ratio is small, pulse detection and parameter estimation remains difficult with existing approaches. In this article, we present a bivariate deconvolution model of pulsatile hormone data focusing on incorporating pulsatile associations. Through simulation, we exhibit that using the underlying pulsatile association between two hormones improves the estimation of the number of pulses and the other parameters defining each hormone. We develop the one-to-one, driver-response case and show how birth-death Markov chain Monte Carlo can be used for estimation. We exhibit these features through a simulation study and apply the method to luteinizing and follicle stimulating hormones.|A Bayesian Approach to Modeling Associations between Pulsatile Hormones|http://www.jstor.org/stable/25502329|25502329|2009-06-01|2009|['eng']|['Biological sciences - Biology']|['Science & Mathematics', 'Statistics']
We model the baseline distribution in the accelerated failure-time (AFT) model as a mixture of Dirichlet processes for interval-censored data. This mixture is distinct from Dirichlet process mixtures, and can be viewed as a simple extension of existing parametric models, which we believe is an advantage in the practical modeling of data. We introduce a novel MCMC scheme for the purpose of making posterior inferences for the AFT regression model and illustrate our methods with several real examples.|A Bayesian Semiparametric AFT Model for Interval-Censored Data|http://www.jstor.org/stable/1391180|1391180|2004-06-01|2004|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Computer Science', 'Statistics']
State space models are considered for observations which have non-Gaussian distributions. We obtain accurate approximations to the loglikelihood for such models by Monte Carlo simulation. Devices are introduced which improve the accuracy of the approximations and which increase computational efficiency. The loglikelihood function is maximised numerically to obtain estimates of the unknown hyperparameters. Standard errors of the estimates due to simulation are calculated. Details are given for the important special cases where the observations come from an exponential family distribution and where the observation equation is linear but the observation errors are non-Gaussian. The techniques are illustrated with a series for which the observations have a Poisson distribution and a series for which the observation have a t-distribution.|Monte Carlo Maximum Likelihood Estimation for Non-Gaussian State Space Models|http://www.jstor.org/stable/2337587|2337587|1997-09-01|1997|['eng']|['Mathematics - Mathematical analysis', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Many biomedical studios collect data on times occurrence for a health event that can occur repeatedly, such as infection, hospitalization, recurrence of disease, or tumor onset. To analyze such data, it is necessary to account for within-subject dependency in the multiple event times. Motivated by data from studies of palpable tumors, this article proposes a dynamic frailty model and Bayesian semiparametric approach to inference. The widely used shared frailty proportional hazards model is generalized to allow subject-specific frailties to change dynamically with age while also accommodating nonproportional hazards. Parametric assumptions on the frailty distribution are avoided by using Dirichlet process priors for a shared frailty and for multiplicative innovations on this frailty. By centering the semiparametric model on a conditionally conjugate dynamic gamma model, we facilitate posterior computation and lack-of-fit assessments of the parametric model. Our proposed method is demonstrated using data from a cancer chemoprevention study.|Bayesian Semiparametric Dynamic Frailty Models for Multiple Event Time Data|http://www.jstor.org/stable/4124525|4124525|2006-12-01|2006|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Functional data are increasingly encountered in scientific studies, and their high dimensionality and complexity lead to many analytical challenges. Various methods for functional data analysis have been developed, including functional response regression methods that involve regression of a functional response on univariate/multivariate predictors with nonparametrically represented functional coefficients. In existing methods, however, the functional regression can be sensitive to outlying curves and outlying regions of curves, so is not robust. In this article, we introduce a new Bayesian method, robust functional mixed models (R-FMM), for performing robust functional regression within the general functional mixed model framework, which includes multiple continuous or categorical predictors and random effect functions accommodating potential between-function correlation induced by the experimental design. The underlying model involves a hierarchical scale mixture model for the fixed effects, random effect, and residual error functions. These modeling assumptions across curves result in robust nonparametric estimators of the fixed and random effect functions which down-weight outlying curves and regions of curves, and produce statistics that can be used to flag global and local outliers. These assumptions also lead to distributions across wavelet coefficients that have outstanding sparsity and adaptive shrinkage properties, with great flexibility for the data to determine the sparsity and the heaviness of the tails. Together with the down-weighting of outliers, these within-curve properties lead to fixed and random effect function estimates that appear in our simulations to be remarkably adaptive in their ability to remove spurious features yet retain true features of the functions. We have developed general code to implement this fully Bayesian method that is automatic, requiring the user to only provide the functional data and design matrices. It is efficient enough to handle large datasets, and yields posterior samples of all model parameters that can be used to perform desired Bayesian estimation and inference. Although we present details for a specific implementation of the R-FMM using specific distributional choices in the hierarchical model, 1D functions, and wavelet transforms, the method can be applied more generally using other heavy-tailed distributions, higher dimensional functions (e.g., images), and using other invertible transformations as alternatives to wavelets. Supplementary materials for this article are available online.|Robust, Adaptive Functional Regression in Functional Mixed Model Framework|http://www.jstor.org/stable/23427582|23427582|2011-09-01|2011|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
A multivariate structural time series model made up of unobserved components such as trends and seasonals is formulated. A homogeneous system, in which any linear combination of the observations follows the same time series process, is shown to correspond to a multivariate structural model in which the covariance matrices of the disturbances are proportional. A homogeneous model is considerably easier to estimate than the more general model and a score test of homogeneity can be constructed in the frequency domain. The finite-sample properties of this test are evaluated in a series of Monte Carlo experiments. Finally, a test of serial correlation for use in homogeneous systems is described.|Seemingly Unrelated Time Series Equations and a Test for Homogeneity|http://www.jstor.org/stable/1391754|1391754|1990-01-01|1990|['eng']|['Mathematics - Mathematical objects']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
There has been no attention to circular (purely cyclical) data in political science research. We show that such data exist and are mishandled by models that do not take into account the inherently recycling nature of some phenomenon. Clock and calendar effects are the obvious cases, but directional data are observed as well. We describe a standard maximum likelihood regression modeling framework based on the von Mises distribution, then develop a general Bayesian regression procedure for the first time, providing an easy-to-use Metropolis-Hastings sampler for this approach. Applications include a chronographic analysis of U.S. domestic terrorism and directional party preferences in a two-dimensional ideological space for German Bundestag elections. The results demonstrate the importance of circular models to handle periodic and directional data in political science.|Circular Data in Political Science and How to Handle It|http://www.jstor.org/stable/25792015|25792015|2010-07-01|2010|['eng']|['Information science - Informetrics']|['Political Science', 'Social Sciences']
We consider the problem of speaker diarization, the problem of segmenting an audio recording of a meeting into temporal segments corresponding to individual speakers. The problem is rendered particularly difficult by the fact that we are not allowed to assume knowledge of the number of people participating in the meeting. To address this problem, we take a Bayesian nonparametric approach to speaker diarization that builds on the hierarchical Dirichlet process hidden Markov model (HDP-HMM) of Teh et al. [J. Amer. Statist. Assoc. 101 (2006) 1566-1581]. Although the basic HDP-HMM tends to over-segment the audio data—creating redundant states and rapidly switching among them—we describe an augmented HDP-HMM that provides effective control over the switching rate. We also show that this augmentation makes it possible to treat emission distributions nonparametrically. To scale the resulting architecture to realistic diarization problems, we develop a sampling algorithm that employs a truncated approximation of the Dirichlet process to jointly resample the full state sequence, greatly improving mixing rates. Working with a benchmark NIST data set, we show that our Bayesian nonparametric architecture yields state-of-the-art speaker diarization results.|A STICKY HDP-HMM WITH APPLICATION TO SPEAKER DIARIZATION|http://www.jstor.org/stable/23024915|23024915|2011-06-01|2011|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
This article introduces a new model of trend inflation. In contrast to many earlier approaches, which allow for trend inflation to evolve according to a random walk, ours is a bounded model which ensures that trend inflation is constrained to lie in an interval. The bounds of this interval can either be fixed or estimated from the data. Our model also allows for a time-varying degree of persistence in the transitory component of inflation. In an empirical exercise with CPI inflation, we find the model to work well, yielding more sensible measures of trend inflation and forecasting better than popular alternatives such as the unobserved components stochastic volatility model. This article has supplementary materials online.|A New Model of Trend Inflation|http://www.jstor.org/stable/41810022|41810022|2013-01-01|2013|['eng']|['Physical sciences - Astronomy']|['Business', 'Business & Economics Collection', 'Economics', 'Science and Mathematics', 'Statistics']
This study investigates the pattern of knowledge spillovers arising from patent activity between European regions. A Bayesian hierarchical model is developed that specifies region-specific latent effects parameters modeled using a connectivity structure between regions that can reflect geographical proximity in conjunction with technological and other types of proximity. This approach exploits the fact that interregional relationships may exhibit industry-specific technological linkages or transportation network linkages, which is in contrast to traditional studies relying exclusively on geographical proximity. We also allow for both symmetric and asymmetric knowledge spillovers between regions, and for heterogeneity across the regional sample. A series of formal Bayesian model comparisons provides support for a model based on technological proximity combined with spatial proximity, asymmetric knowledge spillovers, and heterogeneity in the disturbances. Estimates of region-specific latent effects parameters structured in this fashion are produced by the model and used to draw inferences regarding the character of knowledge spillovers across the regions. The method is illustrated using sample data on patent activity covering 323 regions in nine European countries.|Using the Variance Structure of the Conditional Autoregressive Spatial Specification to Model Knowledge Spillovers|http://www.jstor.org/stable/25144543|25144543|2008-03-01|2008|['eng']|['Applied sciences - Engineering']|['Business & Economics', 'Business', 'Economics']
Collaborative filtering algorithms learn from the ratings of a group of users on a set of items to find personalized recommendations for each user. Traditionally they have been designed to work with one-dimensional ratings. With interest growing in recommendations based on multiple aspects of items, we present an algorithm for using multicomponent rating data. The presented mixture model-based algorithm uses the component rating dependency structure discovered by a structure learning algorithm. The structure is supported by the psychometric literature on the halo effect. This algorithm is compared with a set of model-based and instance-based algorithms for single-component ratings and their variations for multicomponent ratings. We evaluate the algorithms using data from Yahoo! Movies. Use of multiple components leads to significant improvements in recommendations. However, we find that the choice of algorithm depends on the sparsity of the training data. It also depends on whether the task of the algorithm is to accurately predict ratings or to retrieve relevant items. In our experiments a model-based multicomponent rating algorithm is able to better retrieve items when training data are sparse. However, if the training data are not sparse, or if we are trying to predict the rating values accurately, then the instance-based multicomponent rating collaborative filtering algorithms perform better. Beyond generating recommendations we show that the proposed model can fill in missing rating components. Theories in psychometric literature and the empirical evidence suggest that rating specific aspects of a subject is difficult. Hence, filling in the missing component values leads to the possibility of a rater support system to facilitate gathering of multicomponent ratings.|Research Note: The Halo Effect in Multicomponent Ratings and Its Implications for Recommender Systems: The Case of Yahoo! Movies|http://www.jstor.org/stable/23207883|23207883|2012-03-01|2012|['eng']|['Applied sciences - Engineering', 'Information science - Data products']|['Business', 'Business & Economics Collection']
The paper develops mixture models for spatially indexed data. We confine attention to the case of finite, typically irregular, patterns of points or regions with prescribed spatial relationships, and to problems where it is only the weights in the mixture that vary from one location to another. Our specific focus is on Poisson-distributed data, and applications in disease mapping. We work in a Bayesian framework, with the Poisson parameters drawn from gamma priors, and an unknown number of components. We propose two alternative models for spatially dependent weights, based on transformations of autoregressive Gaussian processes: in one (the logistic normal model), the mixture component labels are exchangeable; in the other (the grouped continuous model), they are ordered. Reversible jump Markov chain Monte Carlo algorithms for posterior inference are developed. Finally, the performances of both of these formulations are examined on synthetic data and real data on mortality from a rare disease.|Modelling Spatially Correlated Data via Mixtures: A Bayesian Approach|http://www.jstor.org/stable/3088815|3088815|2002-01-01|2002|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Recent studies have shown that grassland birds are declining more rapidly than any other group of terrestrial birds. Current methods of estimating avian age-specific nest survival rates require knowing the ages of nests, assuming homogeneous nests in terms of nest survival rates, or treating the hazard function as a piecewise step function. In this article, we propose a Bayesian hierarchical model with nest-specific covariates to estimate age-specific daily survival probabilities without the above requirements. The model provides a smooth estimate of the nest survival curve and identifies the factors that are related to the nest survival. The model can handle irregular visiting schedules and it has the least restrictive assumptions compared to existing methods. Without assuming proportional hazards, we use a multinomial semiparametric logit model to specify a direct relation between age-specific nest failure probability and nest-specific covariates. An intrinsic autoregressive prior is employed for the nest age effect. This nonparametric prior provides a more flexible alternative to the parametric assumptions. The Bayesian computation is efficient because the full conditional posterior distributions either have closed forms or are log concave. We use the method to analyze a Missouri dickcissel dataset and find that (1) nest survival is not homogeneous during the nesting period, and it reaches its lowest at the transition from incubation to nestling; and (2) nest survival is related to grass cover and vegetation height in the study area.|Modeling Age and Nest-Specific Survival Using a Hierarchical Bayesian Approach|http://www.jstor.org/stable/20640625|20640625|2009-12-01|2009|['eng']|['Applied sciences - Engineering', 'Biological sciences - Paleontology']|['Science & Mathematics', 'Statistics']
The analysis of genomics alterations that may occur in nature when segments of chromosomes are copied (known as copy number alterations) has been a focus of research to identify genetic markers of cancer. One high throughput technique that has recently been adopted is the use of molecular inversion probes to measure probe copy number changes. The resulting data consist of high dimensional copy number profiles that can be used to ascertain probe-specific copy number alterations in correlative studies with patient outcomes to guide risk stratification and future treatment. We propose a novel Bayesian variable selection method, the hierarchical structured variable selection method, which accounts for the natural gene and probe-within-gene architecture to identify important genes and probes associated with clinically relevant outcomes. We propose the hierarchical structured variable selection model for grouped variable selection, where simultaneous selection of both groups and within-group variables is of interest. The hierarchical structured variable selection model utilizes a discrete mixture prior distribution for group selection and group-specific Bayesian lasso hierarchies for variable selection within groups. We provide methods for accounting for serial correlations within groups that incorporate Bayesian fused lasso methods for within-group selection. Through simulations we establish that our method results in lower model errors than other methods when a natural grouping structure exists. We apply our method to a molecular inversion probe study of breast cancer and show that it identifies genes and probes that are significantly associated with clinically relevant subtypes of breast cancer.|Bayesian hierarchical structured variable selection methods with application to molecular inversion probe studies in breast cancer|http://www.jstor.org/stable/24771488|24771488|2014-08-01|2014|['eng']|['Biological sciences - Biology']|['Science & Mathematics', 'Statistics']
DNA microarrays in conjunction with statistical models may help gain a deeper understanding of the molecular basis for specific diseases. An intense area of research is concerned with the identification of genes related to particular phenotypes. The technology, however, is subject to various sources of error that may lead to expression readings that are substantially different from the true transcript levels. Few methods for microarray data analysis have accounted for measurement error in a substantial way and that is the purpose of this investigation. We describe a Bayesian error-in-variable model for the analysis of microarray data from a clinical study of patients with acute lymphoblastic leukemia. We focus in particular on the problem of identifying genes whose expression patterns are associated with duration of remission. This is a question of great practical interest since relapse is a major concern in the treatment of this disease. We explore the effects of ignoring the uncertainty in the expression estimates on the selection and ranking of genes.|Bayesian Error-in-Variable Survival Model for the Analysis of GeneChip Arrays|http://www.jstor.org/stable/3695969|3695969|2005-06-01|2005|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
"A set of unknown normal means (treatment effects, say) {θ1, θ2, ..., θk} is to be investigated. Two common questions in analysis of variance and ranking and selection are as follows: (a) What is the strength of evidence against the hypothesis H0 of equality of means? (b) If H0 is false, which mean is the largest (or smallest)? A Bayesian approach to the problem is taken, leading to calculation of the posterior probability of H0 and the posterior probability that each mean is the largest, conditional on H0 being false. A variety of exchangeable, nonexchangeable, informative, and noninformative prior assumptions are considered. Calculations involve, at worst, only low-dimensional numerical integration, in spite of the fact that the dimension k can be arbitrarily large. As an example, Table 1 presents, for each baseball team in the National League in 1984, the highest batting average obtained by any player on the team with at least 150 at bats. The observed batting averages are treated as sample proportions from binomial distributions with parameters θi = true probability of getting a hit for the given player, and it is desired to select the best hitter from the group, namely the player with the largest θi. Calculated, using a Bayesian model of exchangeability for the θi, are quantities such as the posterior probabilities that each θi is the largest. Such posterior probabilities give very easy to understand and useful measures to assist in selection and ranking. Of substantial interest is that, in unbalanced examples such as the baseball example (the players all had different numbers of ""at bats,"" and hence different variances), it need not be the case that the treatment with the largest sample mean is judged to have the largest true mean. Thus Player 1's observed batting average was higher than that of Player 2, but Player 2 had a substantially smaller variance and was determined (by the hierarchical Bayes method) to have a larger probability of being the best true hitter. An interesting sidelight to the development is the presentation of a closed-form solution for testing H0: θ1 = θ2 versus $H_1: \theta_1 &lt; \theta_2$ versus $H_2: \theta_1 &gt; \theta_2$, when the treatments are judged to be a priori exchangeable."|A Bayesian Approach to Ranking and Selection of Related Means With Alternatives to Analysis-of-Variance Methodology|http://www.jstor.org/stable/2288851|2288851|1988-06-01|1988|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
PNAS article classification is rooted in long-standing disciplinary divisions that do not necessarily reflect the structure of modern scientific research. We reevaluate that structure using latent pattern models from statistical machine learning, also known as mixed-membership models, that identify semantic structure in co-occurrence of words in the abstracts and references. Our findings suggest that the latent dimensionality of patterns underlying PNAS research articles in the Biological Sciences is only slightly larger than the number of categories currently in use, but it differs substantially in the content of the categories. Further, the number of articles that are listed under multiple categories is only a small fraction of what it should be. These findings together with the sensitivity analyses suggest ways to reconceptualize the organization of papers published in PNAS.|Reconceptualizing the classification of PNAS articles|http://www.jstor.org/stable/25756804|25756804|2010-12-07|2010|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Biological Sciences', 'General Science']
We address the problem of selecting which variables should be included in the fixed and random components of logistic mixed effects models for correlated data. A fully Bayesian variable selection is implemented using a stochastic search Gibbs sampler to estimate the exact model-averaged posterior distribution. This approach automatically identifies subsets of predictors having nonzero fixed effect coefficients or nonzero random effects variance, while allowing uncertainty in the model selection process. Default priors are proposed for the variance components and an efficient parameter expansion Gibbs sampler is developed for posterior computation. The approach is illustrated using simulated data and an epidemiologic example.|Fixed and Random Effects Selection in Linear and Logistic Models|http://www.jstor.org/stable/4541400|4541400|2007-09-01|2007|['eng']|['Applied sciences - Engineering', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
ABSTRACT: Resource selection studies often use analytical techniques that provide information at either a population or an individual level. We applied a Bayesian hierarchical model that simultaneously estimates population- and individual-level habitat selection to explore how varying levels of dietary specialisation affect resource requirements of 2 species of sea snakes that occupy the same coastal environment. We used passive acoustic telemetry to monitor the movements of the 2 species—a dietary generalist, Hydrophis (Lapemis) curtus, and a dietary specialist, H. elegans— and investigated how individuals select habitats based on habitat type, depth and proximity to sources of freshwater within a nearshore environment. Composition of diets in both species was also assessed using regurgitated material from captured individuals. Selection of habitats by the 2 species differed, with H. elegans displaying an affinity for mudflat and seagrass habitats &lt;4 km from sources of freshwater and depths &lt;3 m. H. curtus selected for slightly deeper seagrass habitats (1–4 m) further from freshwater sources (2–5 km). Data from regurgitated material showed that the diet of H. curtus comprised at least 4 families of fish and displayed some level of intraspecific predation, whereas H. elegans preyed solely on eels. Both species predominantly selected seagrass areas, indicating that these habitats provide key resources for sea snakes within nearshore environments. The results illustrated the utility of Bayesian hierarchical models when analysing passive acoustic monitoring data to provide population-level habitat selection metrics and incorporate individual-level variability in selection, both of which are necessary to inform targeted management and conservation practices.|Exploring habitat selection in sea snakes using passive acoustic monitoring and Bayesian hierarchical models|http://www.jstor.org/stable/24896891|24896891|2016-03-21|2016|['eng']|['Biological sciences - Ecology']|['Ecology & Evolutionary Biology', 'Aquatic Sciences', 'Science & Mathematics', 'Biological Sciences']
This article reviews the principle of minimum description length (MDL) for problems of model selection. By viewing statistical modeling as a means of generating descriptions of observed data, the MDL framework discriminates between competing models based on the complexity of each description. This approach began with Kolmogorov's theory of algorithmic complexity, matured in the literature on information theory, and has recently received renewed attention within the statistics community. Here we review both the practical and the theoretical aspects of MDL as a tool for model selection, emphasizing the rich connections between information theory and statistics. At the boundary between these two disciplines we find many interesting interpretations of popular frequentist and Bayesian procedures. As we show, MDL provides an objective umbrella under which rather disparate approaches to statistical modeling can coexist and be compared. We illustrate the MDL principle by considering problems in regression, nonparametric curve estimation, cluster analysis, and time series analysis. Because model selection in linear regression is an extremely common problem that arises in many applications, we present detailed derivations of several MDL criteria in this context and discuss their properties through a number of examples. Our emphasis is on the practical application of MDL, and hence we make extensive use of real datasets. In writing this review, we tried to make the descriptive philosophy of MDL natural to a statistics audience by examining classical problems in model selection. In the engineering literature, however, MDL is being applied to ever more exotic modeling situations. As a principle for statistical modeling in general, one strength of MDL is that it can be intuitively extended to provide useful tools for new problems.|Model Selection and the Principle of Minimum Description Length|http://www.jstor.org/stable/2670311|2670311|2001-06-01|2001|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
We demonstrate and critique the new Bayesian inference package Infer.NET in terms of its capacity for statistical analyses. Infer.NET differs from the well-known BUGS Bayesian inference packages in that its main engine is the variational Bayes family of deterministic approximation algorithms rather than Markov chain Monte Carlo. The underlying rationale is that such deterministic algorithms can handle bigger problems due to their increased speed, despite some loss of accuracy. We find that Infer.NET is a well-designed computational framework and offers significant speed advantages over BUGS. Nevertheless, the current release is limited in terms of the breadth of models it can handle, and its inference is sometimes inaccurate. Supplemental materials accompany the online version of this article.|Using Infer.NET for Statistical Analyses|http://www.jstor.org/stable/23020505|23020505|2011-05-01|2011|['eng']|['Information science - Coding theory', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
When assessing system reliability using system, subsystem, and component-level data, assumptions are required about the form of the system structure in order to utilize the lower-level data. We consider model forms which allow for the assessment and modeling of possible discrepancies between reliability estimates based on different levels of data. By understanding these potential conflicts between data, we can more realistically represent the true uncertainty of the estimates and gain understanding about inconsistencies which might guide further improvements to the system model. The new methodology is illustrated with several examples.|Reliability Models for Almost-Series and Almost-Parallel Systems|http://www.jstor.org/stable/27867221|27867221|2010-05-01|2010|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
This study finds that the set of policies that favor liberalization in credit markets (regulatory quality) are negatively correlated with countries' resilience to the recent recession as measured by output growth in 2008 and 2009. The Global nature of the recession and the cross-country heterogeneity of its depth provide a unique opportunity to examine the link between the structural characteristics of economic and social systems before and after the crisis.|Market Freedom and the Global Recession|http://www.jstor.org/stable/41290954|41290954|2011-01-01|2011|['eng']|['Economics - Economic disciplines']|['Business & Economics', 'Business', 'Economics']
"Agricultural expansion is the largest threat to global biodiversity. In particular, the rapid spread of tree plantations is a primary driver of deforestation in hyperdiverse tropical regions. Plantations tend to support considerably lower biodiversity than native forest, but it remains unclear whether plantation traits affect their ability to sustain native wildlife populations, particularly for threatened taxa. If animal diversity varies across plantations with different characteristics, these traits could be manipulated to make plantations more ""wildlife friendly."" The degree to which plantations create edge effects that degrade habitat quality in adjacent forest also remains unclear, limiting our ability to predict wildlife persistence in mixed-use landscapes. We used systematic camera trapping to investigate mammal occurrence and diversity in oil palm plantations and adjacent forest in Sabah, Malaysian Borneo. Mammals within plantations were largely constrained to locations near native forest; the occurrence of most species and overall species richness declined abruptly with decreasing forest proximity from an estimated 14 species at the forest ecotone to ∼1 species 2 km into the plantation. Neither tree height nor canopy cover within plantations strongly affected mammal diversity or occurrence, suggesting that manipulating tree spacing or planting cycles might not make plantations more wildlife friendly. Plantations did not appear to generate strong edge effects; mammal richness within forest remained high and consistent up to the plantation ecotone. Our results suggest that land-sparing strategies, as opposed to efforts to make plantations more wildlife-friendly, are required for regional wildlife conservation in biodiverse tropical ecosystems."|Oil palm plantations fail to support mammal diversity|http://www.jstor.org/stable/24700694|24700694|2015-12-01|2015|['eng']|['Biological sciences - Ecology', 'Biological sciences - Biogeography', 'Physical sciences - Earth sciences']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
The problem of estimating the mean of a multivariate normal distribution when the parameter space allows an orthogonal decomposition is discussed. Risk functions and lower bounds for a class of shrinkage estimators that includes Stein's estimator are derived, and an improvement on Stein's estimator that takes advantage of the orthogonal decomposition is introduced. Uniform asymptotics related to Pinsker's minimax risk is derived and we give conditions for attaining the lower risk bound. Special cases including regression and analysis of variance are discussed.|Shrinkage and Orthogonal Decomposition|http://www.jstor.org/stable/4616537|4616537|1999-03-01|1999|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
We examine box-office sales in the context of a market share model. This is accomplished by developing a combination of a sliding-window logit model and a gamma diffusion pattern in a hierarchical Bayes framework. We show that accounting for the full choice set available every week not only increases the fit of weekly movie sales but also leads to parameter estimates that depict a richer picture of the movie industry. We show that movie studios appear to have a good understanding of the products they produce, knowing when to support them and when not to. We also show that the effect of the number of opening week screens is overestimated in traditional models. Our research indicates that actors have a direct and directors an indirect effect on consumers' movie choice. Releasing a movie contemporaneously with other movies of the same genre adversely affects box-office performance all around. Releasing a movie against movies of the same Motion Picture Association of America (MPAA) rating hurts its sales in the beginning, but there is a displacement effect, which leads to a less severe sales loss in the long run.|Modeling Movie Life Cycles and Market Share|http://www.jstor.org/stable/40056978|40056978|2005-07-01|2005|['eng']|['Information science - Informetrics', 'Information science - Data products']|['Marketing & Advertising', 'Business & Economics', 'Business']
This paper considers parametric statistical decision problems conducted within a Bayesian nonparametric context. Our work was motivated by the realisation that typical parametric model selection procedures are essentially incoherent. We argue that one solution to this problem is to use a flexible enough model in the first place, a model that will not be checked no matter what data arrive. Ideally, one would use a nonparametric model to describe all the uncertainty about the density function generating the data. However, parametric models are the preferred choice for many statisticians, despite the incoherence involved in model checking, incoherence that is quite often ignored for pragmatic reasons. In this paper we show how coherent parametric inference can be carried out via decision theory and Bayesian nonparametrics. None of the ingredients discussed here are new, but our main point only becomes evident when one sees all priors-even parametric ones-as measures on sets of densities as opposed to measures on finite-dimensional parameter spaces. /// Ce travail considère des problèmes de décision statistique conduisent dans le cadre Bayesien non paramétrique. Notre travail a été motivé par le fait que les procédés typiques paramétriques du sélection de modèle sont essentiellement incohérent. Notre argument pour trouver une solution a ce problème est l'utilisation tout d'abord d'un modèle suffisamment flexible, un modèle qui ne sera pas vérifier indépendamment des donnés reçues. Idéalement on devrait utiliser un modèle non paramétrique pour décrire toute l'incertitude sur la fonction de densité qui produit les donnés. Néanmoins, les modèles paramétriques sont le choix préféré pour plusieurs statisticiens, en dépis de l'incohérences associée pour vérifier le modèle, incohérence qui est souvent ignorée pour des raisons pragmatiques. Dans ce travail nous montrons comment la cohérence de l'inférence paramétrique peut être utilisées dans le cadre de la théorie des décisions et dans le cadre Bayesien non paramétrique. Aucun des ingrédients utilisés ici est nouveau, mais le point essentiel devient évident lorsque l'on voit toutes les priors-mêmes les paramétriques-comme des mesures de densité en contre partie aux mesures dans des espaces paramétriques de dimension finie.|Statistical Decision Problems and Bayesian Nonparametric Methods|http://www.jstor.org/stable/25472678|25472678|2005-12-01|2005|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
To improve operational efficiencies while providing state of the art healthcare services, hospitals rely on information technology enabled physician referral systems (IT-PRS). This study examines learning curves in an IT-PRS setting to determine whether agents achieve performance improvements from cumulative experience at different rates and how information technologies transform the learning dynamics in this setting. We present a hierarchical Bayes model that accounts for different agent skills (domain and system) and estimate learning rates for three types of referral requests: emergency (EM), nonemergency (NE), and nonemergency out of network (NO). Furthermore, the model accounts for learning spillovers among the three referral request types and the impact of system upgrade on learning rates. We estimate this model using data from more than 80,000 referral requests to a large IT-PRS. We find that: (1) The IT-PRS exhibits a learning rate of 4.5% for EM referrals, 7.2% for NE referrals, and 12.3% for NO referrals. This is slower than the learning rate of manufacturing (on average 20%) and more comparable to other service settings (on average, 8%). (2) Domain and system experts are found to exhibit significantly different learning behaviors. (3) Significant and varying learning spillovers among the three referral request types are also observed. (4) The performance of domain experts is affected more adversely in comparison to system experts immediately after system upgrade. (5) Finally, the learning rate change subsequent to system upgrade is also higher for system experts in comparison to domain experts. Overall, system upgrades are found to have a long-term positive impact on the performance of all agents. This study contributes to the development of theoretically grounded understanding of learning behaviors of domain and system experts in an IT-enabled critical healthcare service setting.|Learning Curves of Agents with Diverse Skills in Information Technology-Enabled Physician Referral Systems|http://www.jstor.org/stable/23015596|23015596|2011-09-01|2011|['eng']|['Health sciences - Medical treatment', 'Health sciences - Medical sciences', 'Business - Industry', 'Information science - Data products', 'Information science - Coding theory']|['Business & Economics', 'Business']
We investigate inflation predictability in the United States across the monetary regimes of the twentieth century. The forecasts based on money growth and output growth were significantly more accurate than the forecasts based on past inflation only during the regimes associated with neither a clear nominal anchor nor a credible commitment to fight inflation. These include the years from the outbreak of World War II in 1939 to the implementation of the Bretton Woods Agreements in 1951 and from Nixon's closure of the gold window in 1971 to the end of Volcker's disinflation in 1983.|A CENTURY OF INFLATION FORECASTS|http://www.jstor.org/stable/23355343|23355343|2012-11-01|2012|['eng']|['Mathematics - Applied mathematics']|['Business', 'Business & Economics Collection', 'Economics']
This paper informs a statistical readership about Artificial Neural Networks (ANNs), points out some of the links with statistical methodology and encourages cross-disciplinary research in the directions most likely to bear fruit. The areas of statistical interest are briefly outlined, and a series of examples indicates the flavor of ANN models. We then treat various topics in more depth. In each case, we describe the neural network architectures and training rules and provide a statistical commentary. The topics treated in this way are perceptrons (from single-unit to multilayer versions), Hopfield-type recurrent networks (including probabilistic versions strongly related to statistical physics and Gibbs distributions) and associative memory networks trained by so-called unsuperviszd learning rules. Perceptrons are shown to have strong associations with discriminant analysis and regression, and unsupervized networks with cluster analysis. The paper concludes with some thoughts on the future of the interface between neural networks and statistics.|Neural Networks: A Review from a Statistical Perspective|http://www.jstor.org/stable/2246275|2246275|1994-02-01|1994|['eng']|['Mathematics - Mathematical logic', 'Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
"A characterization of the priors, for classification problems with binary data, which are invariant under permutations of tests is here derived. A particular invariant prior is then studied and shown that it permits positive correlation between cell chances that correspond to ""similar"" strings. The classification performance with the new prior is then compared with those of a symmetrical Dirichlet prior and of the familiar INDEP 1 model."|A New Look at the Problem of Classification with Binary Data|http://www.jstor.org/stable/2987605|2987605|1983-03-01|1983|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
In stochastic frontier analysis, firm-specific efficiencies and their distribution are often main variables of interest. If firms fall into several groups, it is natural to allow each group to have its own distribution. This paper considers a method for nonparametrically modelling these distributions using Dirichlet processes. A common problem when applying nonparametric methods to grouped data is small sample sizes for some groups which can lead to poor inference. Methods that allow dependence between each group's distribution are one set of solutions. The proposed model clusters the groups and assumes that the unknown distribution for each group in a cluster are the same. These clusters are inferred from the data. Markov chain Monte Carlo methods are necessary for model-fitting and efficient methods are described. The model is illustrated on a cost frontier application to US hospitals.|Bayesian clustering of distributions in stochastic frontier analysis|http://www.jstor.org/stable/23883803|23883803|2011-12-01|2011|['eng']|['Applied sciences - Engineering']|['Business', 'Business & Economics Collection']
We propose and test a new approach for modeling consumer heterogeneity in conjoint estimation based on convex optimization and statistical machine learning. We develop methods both for metric and choice data. Like hierarchical Bayes (HB), our methods shrink individual-level partworth estimates towards a population mean. However, while HB samples from a posterior distribution that is influenced by exogenous parameters (the parameters of the second-stage priors), we minimize a convex loss function that depends only on endogenous parameters. As a result, the amounts of shrinkage differ between the two approaches, leading to different estimation accuracies. In our comparisons, based on simulations as well as empirical data sets, the new approach overall outperforms standard HB (i.e., with relatively diffuse second-stage priors) both with metric and choice data.|A Convex Optimization Approach to Modeling Consumer Heterogeneity in Conjoint Estimation|http://www.jstor.org/stable/40057227|40057227|2007-11-01|2007|['eng']|['Applied sciences - Engineering']|['Marketing & Advertising', 'Business & Economics', 'Business']
This work focuses on combining observations from field experiments with detailed computer simulations of a physical process to carry out statistical inference. Of particular interest here is determining uncertainty in resulting predictions. This typically involves calibration of parameters in the computer simulator as well as accounting for inadequate physics in the simulator. The problem is complicated by the fact that simulation code is sufficiently demanding that only a limited number of simulations can be carried out. We consier applications in characterizing material properties for which the field data and the simulator output are highly multivariate. For example, the experimental data and simulation output may be an image or may describe the shape of a physical object. We make use of the basic framework of Kennedy and O'Hagan. However, the size and multivariate nature of the data lead to computational challenges in implementing the framework. To overcome these challenges, we make use of basis representations (e.g., principal components) to reduce the dimenstionality of the problem and speed up the computations required for exploring the posterior distribution. This methodology is applied to applications, both ongoing and historical, at Los Alamos National Laboratory.|Computer Model Calibration Using High-Dimensional Output|http://www.jstor.org/stable/27640080|27640080|2008-06-01|2008|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Multivariate quality characteristics are often monitored using a single statistic or a few statistics. However, it is difficult to determine the causes of an out-of-control signal based on a few summary statistics. Therefore, if a control chart for the mean detects a change in the mean, the quality engineer needs to determine which means shifted and the directions of the shifts to facilitate identification of root causes. We propose a Bayesian approach that gives a direct answer to this question. For each mean, an indicator variable that indicates whether the mean shifted upward, shifted downward, or remained unchanged is introduced. Prior distributions for the means and indicators capture prior knowledge about mean shifts and allow for asymmetry in upward and downward shifts. The mode of the posterior distribution of the vector of indicators or the mode of the marginal posterior distribution of each indicator gives the most likely scenario for each mean. Evaluation of the posterior probabilities of all possible values of the indicators is avoided by employing Gibbs sampling. This renders the computational cost more affordable for high-dimensional problems. This article has supplementary materials online.|A Bayesian Approach for Interpreting Mean Shifts in Multivariate Quality Control|http://www.jstor.org/stable/41714897|41714897|2012-08-01|2012|['eng']|['Mathematics - Applied mathematics']|['Science and Mathematics', 'Statistics']
Functional data often exhibit a common shape, but with variations in amplitude and phase across curves. The analysis often proceeds by synchronization of the data through curve registration. In this article we propose a Bayesian hierarchical model for curve registration. Our hierarchical model provides a formal account of amplitude and phase variability while borrowing strength from the data across curves in the estimation of the model parameters. We discuss extensions of the model by using penalized B-splines in the representation of the shape and time-transformation functions, and by allowing temporal misalignment of the curves. We discuss applications of our model to simulated data, as well as to two data sets. In particular, we use our model in a nonstandard analysis aimed at investigating regulatory network in time course microarray data.|Bayesian Hierarchical Curve Registration|http://www.jstor.org/stable/27640043|27640043|2008-03-01|2008|['eng']|['Applied sciences - Engineering', 'Biological sciences - Biology']|['Science & Mathematics', 'Statistics']
Estimation of the mean of a multivariate normal distribution is considered. The components of the mean vector θ are assumed to be exchangeable; this is modelled in a hierarchical fashion with independent Cauchy distributions as the first-stage prior. The resulting generalized Bayes estimator is calculated and shown to be robust with respect to the presence of outlying means. Alternative estimators that have similar behaviour but are cheaper to compute are also derived. /// Nous étudierons l'estimation de la moyenne d'une loi normale multivariée. Nous assumerons que les composantes du vecteur moyenne, θ, sont échangeables. Cette information a priori sera représentée par un modèle hiérarchique avec des lois Cauchy indépendantes comme distribution a priori de premier niveau. L'estimateur de Bayes généralisé correspondant à ce modèle sera calculé et nous montrerons qu'il est robuste par rapport à la présence de valeurs aberrantes dans le vecteur observation. D'autres estimateurs possédant cette propriété mais plus économiques à calculer seront aussi développés.|Robust Hierarchical Bayes Estimation of Exchangeable Means|http://www.jstor.org/stable/3315535|3315535|1991-03-01|1991|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Modeling the probability of use of land units characterized by discrete and continuous measures, we present a Bayesian random-effects model to assess resource selection. This model provides simultaneous estimation of both individual- and population-level selection. Deviance information criterion (DIC), a Bayesian alternative to AIC that is sample-size specific, is used for model selection. Aerial radiolocation data from 76 adult female caribou (Rangifer tarandus) and calf pairs during 1 year on an Arctic coastal plain calving ground were used to illustrate models and assess population-level selection of landscape attributes, as well as individual heterogeneity of selection. Landscape attributes included elevation, NDVI (a measure of forage greenness), and land cover-type classification. Results from the first of a 2-stage model-selection procedure indicated that there is substantial heterogeneity among cow-calf pairs with respect to selection of the landscape attributes. In the second stage, selection of models with heterogeneity included indicated that at the population-level, NDVI and land cover class were significant attributes for selection of different landscapes by pairs on the calving ground. Population-level selection coefficients indicate that the pairs generally select landscapes with higher levels of NDVI, but the relationship is quadratic. The highest rate of selection occurs at values of NDVI less than the maximum observed. Results for land cover-class selections coefficients indicate that wet sedge, moist sedge, herbaceous tussock tundra, and shrub tussock tundra are selected at approximately the same rate, while alpine and sparsely vegetated landscapes are selected at a lower rate. Furthermore, the variability in selection by individual caribou for moist sedge and sparsely vegetated landscapes is large relative to the variability in selection of other land cover types. The example analysis illustrates that, while sometimes computationally intense, a Bayesian hierarchical discrete-choice model for resource selection can provide managers with 2 components of population-level inference: average population selection and variability of selection. Both components are necessary to make sound management decisions based on animal selection.|A Bayesian Random Effects Discrete-Choice Model for Resource Selection: Population-Level Selection Inference|http://www.jstor.org/stable/3803686|3803686|2006-01-01|2006|['eng']|['Biological sciences - Biology']|['Science & Mathematics', 'Biological Sciences', 'Zoology']
Common factors for seasonal multivariate time series are usually obtained by first filtering the series to eliminate the seasonal component and then extracting the nonseasonal common factors. This approach has two drawbacks. First, we cannot detect common factors with seasonal structure; second, it is well known that a deseasonalized time series may exhibit spurious cycles that the original data do not contain, which can make more difficult the detection of the nonseasonal factors. In this paper we propose a procedure using the original data to estimate the dynamic common factors when some, or all, of the time series are seasonal. We assume that the factor may be stationary or nonstationary and seasonal or not. The procedure is based on the asymptotic behavior of the sequence of the socalled sample generalized autocovariance matrices and of the sequence of canonical correlation matrices, and it includes a statistical test for detecting the total number of common factors. The model is estimated by the Kalman Filter. The procedure is illustrated with an environmental example where two interesting seasonal common factors are found.|COMMON SEASONALITY IN MULTIVARIATE TIME SERIES|http://www.jstor.org/stable/44114339|44114339|2016-10-01|2016|['eng']|['Mathematics - Mathematical objects']|['Mathematics', 'Science & Mathematics', 'Statistics']
Empirical Bayes modeling has a long and celebrated history in statistical theory and applications. After a brief review of the literature, we propose a new dynamic empirical Bayes modeling approach that provides flexible and computationally efficient methods for the analysis and prediction of longitudinal data from many individuals. This approach pools the cross-sectional information over individual time series to replace an inherently complicated hidden Markov model by a considerably simpler generalized linear mixed model. We apply this approach to modeling default probabilities of firms that are jointly exposed to some unobservable dynamic risk factor, and to the well-known statistical problem of predicting baseball batting averages studied by Efron and Morris and recently by Brown.|DYNAMIC EMPIRICAL BAYES MODELS AND THEIR APPLICATIONS TO LONGITUDINAL DATA ANALYSIS AND PREDICTION|http://www.jstor.org/stable/24310956|24310956|2014-10-01|2014|['eng']|['Mathematics - Applied mathematics']|['Mathematics', 'Science & Mathematics', 'Statistics']
This paper examines necessary and sufficient conditions for the propriety of the posterior distribution in hierarchical linear mixed effects models for a collection of improper prior distributions. In addition to the flat prior for the fixed effects, the collection includes various limiting forms of the invariant gamma distribution for the variance components, including cases considered previously by Datta and Ghosh (1991), and Hobert and Casella (1996). Previous work is extended by considering a family of correlated random effects which include as special cases the intrinsic autoregressive models of Besag, York and Mollié (1991), the Autoregressive (AR) Model of Ord (1975), and the Conditional Autoregressive (CAR) Models of Clayton and Kaldor (1987), which have been found useful in the analysis of spatial effects. Conditions are then presented for the propriety of the posterior distribution for a generalized linear mixed model, where the first stage distribution belongs to an exponential family.|PROPRIETY OF POSTERIORS WITH IMPROPER PRIORS IN HIERARCHICAL LINEAR MIXED MODELS|http://www.jstor.org/stable/24306811|24306811|2001-01-01|2001|['eng']|['Mathematics - Mathematical logic']|['Mathematics', 'Science and Mathematics', 'Statistics']
The author extends to the Bayesian nonparametric context the multinomial goodness-of-fit tests due to Cressie &amp; Read (1984). Her approach is suitable when the model of interest is a discrete distribution. She provides an explicit form for the tests, which are based on power-divergence measures between a prior Dirichlet process that is highly concentrated around the model of interest and the corresponding posterior Dirichlet process. In addition to providing interesting special cases and useful approximations, she discusses calibration and the choice of test through examples. /// L'auteure étend au cadre bayésien non paramétrique les tests d'ajustement multinomial de Cressie &amp; Read (1984). Son approche est indiquée quand le modèle d'intérêt est une loi discrète. Elle précise la forme des tests, qui s'expriment en terme de puissances de la divergence entre un processus de Dirichlet a priori très concentré autour du modèle d'intérêt et le processus de Dirichlet a posteriori correspondant. En plus de décrire quelques cas spéciaux et certaines approximations utiles, elle aborde la question de la calibration et du choix du test au moyen d'exemples.|A Family of Power-Divergence Diagnostics for Goodness-of-Fit|http://www.jstor.org/stable/20445277|20445277|2007-12-01|2007|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
If one formulates Helmholtz's ideas about perception in terms of modern-day theories one arrives at a model of perceptual inference and learning that can explain a remarkable range of neurobiological facts. Using constructs from statistical physics it can be shown that the problems of inferring what cause our sensory inputs and learning causal regularities in the sensorium can be resolved using exactly the same principles. Furthermore, inference and learning can proceed in a biologically plausible fashion. The ensuing scheme rests on Empirical Bayes and hierarchical models of how sensory information is generated. The use of hierarchical models enables the brain to construct prior expectations in a dynamic and context-sensitive fashion. This scheme provides a principled way to understand many aspects of the brain's organisation and responses. In this paper, we suggest that these perceptual processes are just one emergent property of systems that conform to a free-energy principle. The free-energy considered here represents a bound on the surprise inherent in any exchange with the environment, under expectations encoded by its state or configuration. A system can minimise free-energy by changing its configuration to change the way it samples the environment, or to change its expectations. These changes correspond to action and perception, respectively, and lead to an adaptive exchange with the environment that is characteristic of biological systems. This treatment implies that the system's state and structure encode an implicit and probabilistic model of the environment. We will look at models entailed by the brain and how minimisation of free-energy can explain its dynamics and structure.|Free-Energy and the Brain|http://www.jstor.org/stable/27653633|27653633|2007-12-01|2007|['eng']|['Biological sciences - Biology']|['Science & Mathematics', 'Science & Technology Studies', 'Philosophy', 'Humanities']
This article introduces a new method for the estimation of the intensity of an inhomogeneous one-dimensional Poisson process. The Haar-Fisz transformation transforms a vector of binned Poisson counts to approximate normality with variance one. Hence we can use any suitable Gaussian wavelet shrinkage method to estimate the Poisson intensity. Since the Haar-Fisz operator does not commute with the shift operator we can dramatically improve accuracy by always cycle spinning before the Haar-Fisz transform as well as optionally after. Extensive simulations show that our approach usually significantly out-performed state-of-the-art competitors but was occasionally comparable. Our method is fast, simple, automatic, and easy to code. Our technique is applied to the estimation of the intensity of earthquakes in northern California. We show that our technique gives visually similar results to the current state-of-the-art.|A Haar-Fisz Algorithm for Poisson Intensity Estimation|http://www.jstor.org/stable/1390996|1390996|2004-09-01|2004|['eng']|['Applied sciences - Engineering', 'Mathematics - Mathematical analysis']|['Science & Mathematics', 'Computer Science', 'Statistics']
A first order autoregressive non-Gaussian model for analysing panel data is proposed. The main feature is that the model is able to accommodate fat tails and also skewness, thus allowing for outliers and asymmetries. The modelling approach is designed to gain sufficient flexibility, without sacrificing interpretability and computational ease. The model incorporates individual effects and covariates and we pay specific attention to the elicitation of the prior. As the prior structure chosen is not proper, we derive conditions for the existence of the posterior. By considering a model with individual dynamic parameters we are also able to formally test whether the dynamic behaviour is common to all units in the panel. The methodology is illustrated with two applications involving earnings data and one on growth of countries.|NON-GAUSSIAN DYNAMIC BAYESIAN MODELLING FOR PANEL DATA|http://www.jstor.org/stable/40984770|40984770|2010-11-01|2010|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Business', 'Economics']
Comparative sociologists mostly ignore wide differences in criminality and incarceration rates among modern western societies; with notable exceptions, students of the prison take scant notice of research comparing political economies, welfare regimes, and patterns of inequality. This article outlines an opportunity structures model of imprisonment that bridges this gap by treating incarceration trends as byproducts of the institutional organization of opportunities over the life course. Using a sample of 15 rich democracies observed over four decades, empirical attention focuses on three levels of analysis: the capacities of alternative life course paths, the distribution of political power, and institutional differences in state structures and policy regimes. Hypothesized cross-level interactions call for the specification of a hierarchical model to be estimated within a Bayesian framework. Results conform to the expectations of the opportunity structures model and support many of its specific predictions.|Imprisonment and Opportunity Structures: A Bayesian Hierarchical Analysis|http://www.jstor.org/stable/41343467|41343467|2012-02-01|2012|['eng']|['Philosophy - Applied philosophy']|['Social Sciences', 'Sociology']
We address the problem of Markov chain Monte Carlo analysis of a complex ecological system by using a Bayesian inferential approach. We describe a complete likelihood framework for the life history of the wavyleaf thistle, including missing information and density dependence. We indicate how, to make inference on life history transitions involving both missing information and density dependence, the stochastic models underlying each component can be combined with each other and with priors to obtain expressions that can be directly sampled. This innovation and the principles described could be extended to other species featuring such missing stage information, with potential for improving inference relating to a range of ecological or evolutionary questions.|Using Bayesian Inference to Understand the Allocation of Resources between Sexual and Asexual Reproduction|http://www.jstor.org/stable/25578155|25578155|2009-05-01|2009|['eng']|['Biological sciences - Biology', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
In Bayesian hierarchical modeling, it is often appealing to allow the conditional density of an (observable or unobservable) random variable Y to change flexibly with categorical and continuous predictors X. A mixture of regression models is proposed, with the mixture distribution varying with X. Treating the smoothing parameters and number of mixture components as unknown, the MLE does not exist, motivating an empirical Bayes approach. The proposed method shrinks the spatially-adaptive mixture distributions to a common baseline, while penalizing rapid changes and large numbers of components. The discrete form of the mixture distribution facilitates flexible classification of subjects. A Gibbs sampling algorithm is developed, which embeds a Monte Carlo EM-type stage to estimate smoothing and hyper-parameters. The method is applied to simulated examples and data from an epidemiologic study.|EMPIRICAL BAYES DENSITY REGRESSION|http://www.jstor.org/stable/24307729|24307729|2007-04-01|2007|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Mathematics', 'Science and Mathematics', 'Statistics']
A major challenge facing pest-eradication efforts is determining when eradication has been achieved. When the pest can no longer be detected, managers have to decide whether the pest has actually been eliminated and hence to decide when to terminate the eradication program. For most eradication programs, this decision entails considerable risk and is the largest single issue facing managers of such programs. We addressed this issue for an eradication program of feral pigs (Sus scrofa) from Santa Cruz Island, California. Using a Bayesian approach, we estimated the degree of confidence in the success of the eradication program at the point when monitoring failed to detect any more pigs. Catch-effort modeling of the hunting effort required to dispatch pigs during the eradication program was used to determine the relationship between detection probability and searching effort for different hunting methods. We then used these relationships to estimate the amount of monitoring effort required to declare eradication successful with criteria that either set a threshold for the probability that pigs remained undetected (type I error) or minimized the net expected costs of the eradication program (cost of type I and II errors). For aerial and ground-based monitoring techniques, the amount of search effort required to declare eradication successful on the basis of either criterion was highly dependent on the prior belief in the success of the program unless monitoring intensities exceeded 30 km of searching effort per square kilometer of search area for aerial monitoring and, equivalently, 38 km for ground monitoring. Calculation of these criteria to gauge the success of eradication should form an essential component of any eradication program as it allows for a transparent assessment of the risks inherent in the decision to terminate the program. /// Un reto mayor de los esfuerzos de erradicación de plagas es la determinación de cuando se ha alcanzado la erradicación. Cuando una plaga ya no es detectada los manejadores tienen que decidir sí la plaga ha sido eliminada realmente y, por lo tanto, decidir cuándo terminar un programa de erradicación. Para la mayoría de los programas, esta decisión representa riesgo considerable y es el principal tema que enfrentan los manejadores de tales programas. Atendimos este tema para un programa de erradicación de cerdos (Sus scrofa) cimarrones de la Isla Santa Cruz, California. Utilizando un enfoque Bayesiano, estimamos el nivel de confianza del éxito del programa de erradicación en el punto cuando el monitoreo no detectó más cerdos. Utilizamos modelos del esfuerzo-captura del esfuerzo de cacería requerido para sacrificar cerdos durante el programa de erradicación para determinar la relación entre la probabilidad de detección y el esfuerzo de búsqueda para diferentes métodos de caza. Posteriormente utilizamos estas relaciones para estimar el esfuerzo de monitoreo requerido para declarar el éxito de la erradicación con criterios que definen un umbral para la probabilidad de que cerdos permanezcan sin detección (error tipo I) o minimizan los costos netos del programa de erradicación esperados (costo de errores tipo I y II). Para las técnicas de monitoreo aéreo y terrestre, el esfuerzo de búsqueda requerido para declarar el éxito de la erradicación con base en ambos criterios fue altamente dependiente de la creencia previa del éxito del programa a menos que las intensidades de monitoreo excedieran 30 km de esfuerzo de búsqueda por kilómetro cuadrado de área de búsqueda para el monitoreo aéreo y, equivalentemente, 38 km para el monitoreo terrestre. El cálculo de estos criterios para evaluar el éxito de la erradicación debería ser un componente esencial de cualquier programa de erradicación y, por lo tanto, debería permitir una evaluación transparente de los riesgos inherentes a la decisión de terminar el programa.|Quantifying Eradication Success: The Removal of Feral Pigs from Santa Cruz Island, California|http://www.jstor.org/stable/29738744|29738744|2009-04-01|2009|['eng']|['Physical sciences - Astronomy']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
Normal mixture models provide the most popular framework for modelling heterogeneity in a population with continuous outcomes arising in a variety of subclasses. In the last two decades, the skew normal distribution has been shown beneficial in dealing with asymmetric data in various theoretic and applied problems. In this article, we address the problem of analyzing a mixture of skew normal distributions from the likelihood-based and Bayesian perspectives, respectively. Computational techniques using EM-type algorithms are employed for iteratively computing maximum likelihood estimates. Also, a fully Bayesian approach using the Markov chain Monte Carlo method is developed to carry out posterior analyses. Numerical results are illustrated through two examples.|FINITE MIXTURE MODELLING USING THE SKEW NORMAL DISTRIBUTION|http://www.jstor.org/stable/24307705|24307705|2007-07-01|2007|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Mathematics', 'Science and Mathematics', 'Statistics']
Bayesian model comparison requires the specification of a prior distribution on the parameter space of each candidate model. In this connection two concerns arise: on the one hand the elicitation task rapidly becomes prohibitive as the number of models increases; on the other hand numerous prior specifications can only exacerbate the well-known sensitivity to prior assignments, thus producing less dependable conclusions. Within the subjective framework, both difficulties can be counteracted by linking priors across models in order to achieve simplification and compatibility; we discuss links with related objective approaches. Given an encompassing, or full, model together with a prior on its parameter space, we review and summarize a few procedures for deriving priors under a submodel, namely marginalization, conditioning, and Kullback–Leibler projection. These techniques are illustrated and discussed with reference to variable selection in linear models adopting a conventional g-prior; comparisons with existing standard approaches are provided. Finally, the relative merits of each procedure are evaluated through simulated and real data sets.|Compatibility of Prior Specifications Across Linear Models|http://www.jstor.org/stable/20697643|20697643|2008-08-01|2008|['eng']|['Mathematics - Mathematical logic']|['Science and Mathematics', 'Statistics']
Multiple imputation replaces an incomplete dataset with m &gt; 1 simulated complete versions that are analyzed separately by standard methods. We present a natural extension of multiple imputation for handling the dual problems of nonresponse and response error. This extension, which we call multiple edit/multiple imputation (MEMI), replaces an observed dataset containing missing values and errors with m &gt; 1 simulated versions of the ideal dataset that is complete and error-free. These ideal data sets are analyzed separately, and the results are combined using the same rules as for multiple imputation. The resulting inferences simultaneously reflect uncertainty due to nonresponse and response error. MEMI may be an attractive alternative to deterministic or quasi-statistical edit and imputation procedures used by many data-collecting agencies. Producing MEMI's requires assumptions about the distribution of the ideal data, the nature of nonresponse, and a model for the response error mechanism. However, fitting such a model does not necessarily require data from a follow-up study. In this article we develop and implement MEMI for preliminary data from the Third National Health and Nutrition Examination Survey, Phase I (1988-1991). Raw body measurements for 1,345 children age 2-3 years are imputed under a Bayesian model for intermittent or semicontinuous errors. The resulting population estimates are found to be quite insensitive to prior assumptions about the rates and magnitude of errors.|Multiple Edit/Multiple Imputation for Multivariate Continuous Data|http://www.jstor.org/stable/30045332|30045332|2003-12-01|2003|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
The environmental Kuznets curve (EKC) has been extensively criticized on theoretical and empirical grounds. In this article, the EKC is reformulated as the best practice technology frontier—countries' distances from the frontier reflect the degree to which they have adopted the best practice technology in emissions abatement. The Kaiman filter is used to model the state of sulfur emissions abatement technology in a panel of 15 mainly developed countries. The results are used to determine whether and how fast countries are converging to best practice throughout time and what variables affect the level of technology adopted. The results show that with the exception of Australia, countries are converging toward the frontier but have settled into low pollution abatement and high pollution abatement groups. Preabatement levels of pollution, income per capita, population density, and perhaps cultural factors might partly explain the level of abatement adopted.|Beyond the Environmental Kuznets Curve: Diffusion of Sulfur-Emissions-Abating Technology|http://www.jstor.org/stable/44319720|44319720|2005-03-01|2005|['eng']|['Biological sciences - Ecology']|['Business & Economics', 'Development Studies', 'Social Sciences', 'Environmental Studies']
This paper studies dynamic identification of parameters of a dynamic stochastic general equilibrium model from the first and second moments of the data. Classical results for dynamic simultaneous equations do not apply because the state space solution of the model does not constitute a standard reduced form. Full rank of the Jacobian matrix of derivatives of the solution parameters with respect to the parameters of interest is necessary but not sufficient for identification. We use restrictions implied by observational equivalence to obtain two sets of rank and order conditions: one for stochastically singular models and another for nonsingular models. Measurement errors, mean, long-run, and a priori restrictions can be accommodated. An example is considered to illustrate the results.|DYNAMIC IDENTIFICATION OF DYNAMIC STOCHASTIC GENERAL EQUILIBRIUM MODELS|http://www.jstor.org/stable/41336541|41336541|2011-11-01|2011|['eng']|['Mathematics - Mathematical logic']|['Business & Economics', 'Mathematics', 'Science & Mathematics', 'Business', 'Economics']
A framework for causal inference from two-level factorial designs is proposed, which uses potential outcomes to define causal effects. The paper explores the effect of non-additivity of unit level treatment effects on Neyman's repeated sampling approach for estimation of causal effects and on Fisher's randomization tests on sharp null hypotheses in these designs. The framework allows for statistical inference from a finite population, permits definition and estimation of estimands other than 'average factorial effects' and leads to more flexible inference procedures than those based on ordinary least squares estimation from a linear model.|"Causal inference from 2
          <sup>K</sup>
          factorial designs by using potential outcomes"|http://www.jstor.org/stable/24775307|24775307|2015-09-01|2015|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
An exact Bayesian analysis of finite mixture distributions is often computationally infeasible, because the number of terms in the posterior density grows exponentially with the sample size. A modification of the Laplace method is presented and applied to estimation of posterior functions in a Bayesian analysis of finite mixture distributions. The procedure, which involves computations similar to those required in maximum likelihood estimation, is shown to have high asymptotic accuracy for finite mixtures of certain exponential-family densities. For these mixture densities, the posterior density is also shown to be asymptotically normal. An approximation of the posterior density of the number of components is presented. The method is applied to Duncan's barley data and to a distribution of lake chemistry data for north-central Wisconsin.|An Application of the Laplace Method to Finite Mixture Distributions|http://www.jstor.org/stable/2291222|2291222|1994-03-01|1994|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Product offerings in many grocery product categories in supermarkets display varied branding structures built around a discernible branding hierarchy typically comprising brands, subbrands and stock keeping units. Firms often want to know what contribution each layer in the brand hierarchy brings to overall product value, and precisely how much of this contribution comes from unique branding associations (we term this value contribution the 'residual equity' of that branding layer). We make the economic argument that, in mature product categories, profit maximizing firms would retain the upper levels of the branding structure only if they were value enhancing. Using only aggregate sales and product data, we develop a semiparametric Bayesian method for a market response model to estimate jointly the residual equity of each layer in the branding structure while accommodating certain a priori restrictions on the equity values.Our proposed model is simple yet flexible and avoids common drawbacks in extant approaches.We implement our model on AC Nielsen beer category data from US supermarkets. We find that residual equity exists, is sizable in magnitude and sales impact, is heterogeneous in occurrence across the branding structure, yields realistic brand valuations and bears managerially relevant insights and implications.|A non-parametric model of residual brand equity in hierarchical branding structures with application to US beer data|http://www.jstor.org/stable/43965675|43965675|2014-01-01|2014|['eng']|['Information science - Coding theory']|['Science & Mathematics', 'Statistics']
Both evolutionary ecologists and wildlife managers make inference based on how fitness and demography vary in space. Spatial variation in survival can be difficult to assess in the wild because (1) multisite study designs are not well suited to populations that are continuously distributed across a large area and (2) available statistical models accounting for detectability less than 1.0 do not easily cope with geographical coordinates. Here we use penalized splines within a Bayesian state-space modeling framework to estimate and visualize survival probability in two dimensions. The approach is flexible in that no parametric form for the relationship between survival and coordinates need be specified a priori. To illustrate our method, we study a game species, the Eurasian Woodcock Scolopax rusticola, based on band recovery data (5000 individuals) collected over a &gt;50 000-km2 area in west-central France with contrasted habitats and hunting pressures. We find that spatial variation in survival probability matches an index of hunting pressure and creates a mosaic of population sources and sinks. Such analyses could provide guidance concerning the spatial management of hunting intensity or could be used to identify pathways of spatial variation in fitness, for example, to study adaptation to changing landscape and climate.|Nonparametric spatial regression of survival probability: visualization of population sinks in Eurasian Woodcock|http://www.jstor.org/stable/23034892|23034892|2011-08-01|2011|['eng']|['Physical sciences - Astronomy']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
Although discrete mixture modelling has formed the backbone of the literature on Bayesian density estimation, there are some well-known disadvantages. As an alternative to discrete mixtures, we propose a class of priors based on random nonlinear functions of a uniform latent variable with an additive residual. The induced prior for the density is shown to have desirable properties, including ease of centring on an initial guess, large support, posterior consistency and straightforward computation via Gibbs sampling. Some advantages over discrete mixtures, such as Dirichlet process mixtures of Gaussian kernels, are discussed and illustrated via simulations and an application.|Latent factor models for density estimation|http://www.jstor.org/stable/43304673|43304673|2014-09-01|2014|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
"We deal with general mixture or hierarchical models of the form m(x)=∫Θ f(x|θ )g(θ )dθ , where g(θ) and m(x) are called mixing and mixed or compound densities respectively, and θ is called the mixing parameter. The usual statistical application of these models emerges when we have data xi,i=1,... ,n with densities f(xi|θ i) for given θ i, and the θ i are independent with common density g(θ). For a certain well known class of densities f(x| θ), we present a sample-based approach to reconstruct g(θ). We first provide theoretical results and then we use, in an empirical Bayes spirit, the first four moments of the data to estimate the first four moments of g(θ). By using sampling techniques we proceed in a fully Bayesian fashion to obtain any posterior summaries of interest. Simulations which investigate the operating characteristics of our proposed methodology are presented. We illustrate our approach using data from mixed Poisson and mixed exponential densities. /// Nous considérons des modèles à combinaisons ou modèles hiérarchiques de la forme m(x)=∫Θ f(x|θ )g(θ )dθ , où g(θ) et m(x) sont appelées respectivement densité de combinaison et densité mixte ou combinée, et θ est appelé le paramètre de combinaison. L'application statistique habituelle de ces modèles apparaît pour des données xi,i=1,... ,n, avec des densités f(xi|θ i) pour des θ i donnés, les θ i étant indépendants et de densité commune g(θ). Pour une certaine classe bien connue de densités f(x|θ), nous présentons une approche par sondage pour reconstruire g(θ). Nous fournissons d'abord des résultats théoriques, puis nous utilisons, dans une démarche bayésienne empirique, les quatre premiers moments des données pour estimer les quatre premiers moments de g(θ). En utilisant des techniques d'echantillonage, nous procédons selon une approche bayésienne pour obtenir tout moment d'ordre supérieur. Des simulations sont présentées pour étudier les caractéristiques de l'utilisation de la méthodologie proposée. Nous illustrons notre approche par des données combinant densité de Poisson et densité exponentielle."|A Simulation Approach to Nonparametric Empirical Bayes Analysis|http://www.jstor.org/stable/1403530|1403530|2001-04-01|2001|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
In 2001, the U.S. Office of Personnel Management required all health plans participating in the Federal Employees Health Benefits Program to offer mental health and substance abuse benefits on par with general medical benefits. The initial evaluation found that, on average, parity did not result in either large spending increases or increased service use over the four-year observational period. However, some groups of enrollees may have benefited from parity more than others. To address this question, we propose a Bayesian two-part latent class model to characterize the effect of parity on mental health use and expenditures. Within each class, we fit a two-part random effects model to separately model the probability of mental health or substance abuse use and mean spending trajectories among those having used services. The regression coefficients and random effect covariances vary across classes, thus permitting class-varying correlation structures between the two components of the model. Our analysis identified three classes of subjects: a group of low spenders that tended to be male, had relatively rare use of services, and decreased their spending pattern over time; a group of moderate spenders, primarily female, that had an increase in both use and mean spending after the introduction of parity; and a group of high spenders that tended to have chronic service use and constant spending patterns. By examining the joint 95% highest probability density regions of expected changes in use and spending for each class, we confirmed that parity had an impact only on the moderate spender class.|A Bayesian Two-Part Latent Class Model for Longitudinal Medical Expenditure Data: Assessing the Impact of Mental Health and Substance Abuse Parity|http://www.jstor.org/stable/41242459|41242459|2011-03-01|2011|['eng']|['Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
We examine philosophical problems and sampling deficiencies that are associated with current Bayesian hypothesis testing methodology, paying particular attention to objective Bayes methodology. Because the prior densities that are used to define alternative hypotheses in many Bayesian tests assign non-negligible probability to regions of the parameter space that are consistent with null hypotheses, resulting tests provide exponential accumulation of evidence in favour of true alternative hypotheses, but only sublinear accumulation of evidence in favour of true null hypotheses. Thus, it is often impossible for such tests to provide strong evidence in favour of a true null hypothesis, even when moderately large sample sizes have been obtained. We review asymptotic convergence rates of Bayes factors in testing precise null hypotheses and propose two new classes of prior densities that ameliorate the imbalance in convergence rates that is inherited by most Bayesian tests. Using members of these classes, we obtain analytic expressions for Bayes factors in linear models and derive approximations to Bayes factors in large sample settings.|On the Use of Non-Local Prior Densities in Bayesian Hypothesis Tests|http://www.jstor.org/stable/40541581|40541581|2010-03-01|2010|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Viewing the observed data of a statistical model as incomplete and augmenting its missing parts are useful for clarifying concepts and central to the invention of two well-known statistical algorithms: expectation-maximization (EM) and data augmentation. Recently, Liu, Rubin, and Wu demonstrated that expanding the parameter space along with augmenting the missing data is useful for accelerating iterative computation in an EM algorithm. The main purpose of this article is to rigorously define a parameter expanded data augmentation (PX-DA) algorithm and to study its theoretical properties. The PX-DA is a special way of using auxiliary variables to accelerate Gibbs sampling algorithms and is closely related to reparameterization techniques. We obtain theoretical results concerning the convergence rate of the PX-DA algorithm and the choice of prior for the expansion parameter. To understand the role of the expansion parameter, we establish a new theory for iterative conditional sampling under the transformation group formulation, which generalizes the standard Gibbs sampler. Using the new theory, we show that the PX-DA algorithm with a Haar measure prior (often improper) for the expansion parameter is always proper and is optimal among a class of such algorithms including reparameterization.|Parameter Expansion for Data Augmentation|http://www.jstor.org/stable/2669940|2669940|1999-12-01|1999|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
The local specification of priors in non-decomposable graphical models does not necessarily yield a proper joint prior for all the parameters of the model. Using results concerning general exponential families with cuts, we derive specific results for the multivariate Gamma distribution (conjugate prior for Poisson counts) and the Wishart distribution (conjugate prior for Gaussian models). These results link the existence of a locally specified joint prior to the solvability of a related marginal problem over the cliques of the graph.|Functionally Compatible Local Characteristics for the Local Specification of Priors in Graphical Models|http://www.jstor.org/stable/41548583|41548583|2007-12-01|2007|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
We propose a hierarchical Bayesian model for analysing gene expression data to identify pathways differentiating between two biological states (e.g. cancer versus non-cancer). Finding significant pathways can improve our understanding of normal and pathological processes and can lead to more effective treatments. Our method, Bayasian gene set analysis, evaluates the statistical significance of a specific pathway by using the posterior distribution of its corresponding hyperparameter. We apply Bayesian gene set analysis to a gene expression microarray data set on 50 cancer cell lines, of which 33 have a known p53 mutation and the remaining are p53 wild type, to identify pathways that are associated with the mutational status in the gene p53. We identify several significant pathways with strong biological connections. We show that our approach provides a natural framework for incorporating prior biological information, and it produces the best overall performance in terms of correctly identifying significant pathways compared with several alternative methods.|Bayesian gene set analysis for identifying significant biological pathways|http://www.jstor.org/stable/41262291|41262291|2011-08-01|2011|['eng']|['Biological sciences - Biology', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
We construct non-Gaussian processes that vary continuously in space and time with nonseparable covariance functions. Starting from a general and flexible way of constructing valid nonseparable covariance functions through mixing over separable covariance functions, the resulting models are generalized by allowing for outliers as well as regions with larger variances. We induce this through scale mixing with separate positive-valued processes. Smooth mixing processes are applied to the underlying correlated processes in space and in time, thus leading to regions in space and time of increased spread. An uncorrelated mixing process on the nugget effect accommodates outliers. Posterior and predictive Bayesian inference with these models is implemented through a Markov chain Monte Carlo sampler. An application to temperature data in the Basque country illustrates the potential of this model in the identification of outliers and regions with inflated variance, and shows that this improves the predictive performance.|Non-Gaussian spatiotemporal modelling through scale mixing|http://www.jstor.org/stable/23076170|23076170|2011-12-01|2011|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical objects']|['Science and Mathematics', 'Statistics']
A mixture item response model is proposed for investigating individual differences in the selection of response categories in multiple-choice items. The model accounts for local dependence among response categories by assuming that examinees belong to discrete latent classes that have different propensities towards those responses. Varying response category propensities are captured by allowing the category intercept parameters in a nominal response model (Bock, 1972) to assume different values across classes. A Markov Chain Monte Carlo algorithm for the estimation of model parameters and classification of examinees is described. A real-data example illustrates how the model can be used to distinguish examinees that are disproportionately attracted to different types of distractors in a test of English usage. A simulation study evaluates item parameter recovery and classification accuracy in a hypothetical multiple-choice test designed to be diagnostic. Implications for test construction and the use of multiple-choice tests to perform cognitive diagnoses of item response patterns are discussed.|A Mixture Item Response Model for Multiple-Choice Data|http://www.jstor.org/stable/3648167|3648167|2001-12-01|2001|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics', 'Applied sciences - Engineering', 'Philosophy - Logic']|['Science & Mathematics', 'Education', 'Statistics', 'Social Sciences']
The paper provides a Bayesian analysis of a practical problem in auditing in which substantial prior information needs to be combined with limited sample data. The specific context of the paper is a multilocation audit in which auditors take a two-stage sample of transactions from different sites within an organization. The hierarchical model that we propose has the flexibility to deal with this sort of stratified population in which some strata are not sampled and in which strata are not exchangeable. A careful, thorough elicitation of auditors' prior information is another key feature of our approach. Inference from our model is by direct Monte Carlo simulation. The example data set is given in the context of an audit carried out by the UK's National Audit Office who funded this research.|A Hierarchical Bayes Model for Multilocation Auditing|http://www.jstor.org/stable/3650351|3650351|2002-01-01|2002|['eng']|['Applied sciences - Engineering', 'Information science - Data products', 'Information science - Coding theory']|['Science & Mathematics', 'Statistics']
Models for natural nonlinear processes, such as population dynamics, have been given much attention in applied mathematics. For example, species competition has been extensively modeled by differential equations. Often, the scientist has preferred to model the underlying dynamical processes (i.e., theoretical mechanisms) in continuous time. It is of both scientific and mathematical interest to implement such models in a statistical framework to quantify uncertainty associated with the models in the presence of observations. That is, given discrete observations arising from the underlying continuous process, the unobserved process can be formally described while accounting for multiple sources of uncertainty (e.g., measurement error, model choice, and inherent stochasticity of process parameters). In addition to continuity, natural processes are often bounded; specifically, they tend to have nonnegative support. Various techniques have been implemented to accommodate nonnegative processes, but such techniques are often limited or overly compromising. This article offers an alternative to common differential modeling practices by using a bias-corrected truncated normal distribution to model the observations and latent process, both having bounded support. Parameters of an underlying continuous process are characterized in a Bayesian hierarchical context, utilizing a fourth-order Runge—Kutta approximation.|Models for Bounded Systems with Continuous Dynamics|http://www.jstor.org/stable/20640583|20640583|2009-09-01|2009|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Nuclear magnetic resonance (NMR) spectra are widely used in metabolomics to obtain profiles of metabolites dissolved in biofluids such as cell supernatants. Methods for estimating metabolite concentrations from these spectra are presently confined to manual peak fitting and to binning procedures for integrating resonance peaks. Extensive information on the patterns of spectral resonance generated by human metabolites is now available in online databases. By incorporating this information into a Bayesian model, we can deconvolve resonance peaks from a spectrum and obtain explicit concentration estimates for the corresponding metabolites. Spectral resonances that cannot be deconvolved in this way may also be of scientific interest; so, we model them jointly using wavelets. We describe a Markov chain Monte Carlo algorithm that allows us to sample from the joint posterior distribution of the model parameters, using specifically designed block updates to improve mixing. The strong prior on resonance patterns allows the algorithm to identify peaks corresponding to particular metabolites automatically, eliminating the need for manual peak assignment. We assess our method for peak alignment and concentration estimation. Except in cases when the target resonance signal is very weak, alignment is unbiased and precise. We compare the Bayesian concentration estimates with those obtained from a conventional numerical integration method and find that our point estimates have six-fold lower mean squared error. Finally, we apply our method to a spectral dataset taken from an investigation of the metabolic response of yeast to recombinant protein expression. We estimate the concentrations of 26 metabolites and compare with manual quantification by five expert spectroscopists. We discuss the reason for discrepancies and the robustness of our method's concentration estimates. This article has supplementary materials online.|A Bayesian Model of NMR Spectra for the Deconvolution and Quantification of Metabolites in Complex Biological Mixtures|http://www.jstor.org/stable/23427332|23427332|2012-12-01|2012|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
The translation features typically used in Phrase-Based Statistical Machine Translation (PB-SMT) model dependencies between the source and target phrases, but not among the phrases in the source language themselves. A swathe of research has demonstrated that integrating source context modelling directly into log-linear PB-SMT can positively influence the weighting and selection of target phrases, and thus improve translation quality. In this contribution we present a revised, extended account of our previous work on using a range of contextual features, including lexical features of neighbouring words, supertags, and dependency information. We add a number of novel aspects, including the use of semantic roles as new contextual features in PB-SMT, adding new language pairs, and examining the scalability of our research to larger amounts of training data. While our results are mixed across feature selections, classifier hyperparameters, language pairs, and learning curves, we observe that including contextual features of the source sentence in general produces improvements. The most significant improvements involve the integration of long-distance contextual features, such as dependency relations in combination with part-of-speech tags in Dutch-to-English subtitle translation, the combination of dependency parse and semantic role information in English-to-Dutch parliamentary debate translation, or supertag features in English-to-Chinese translation.|Integrating source-language context into phrase-based statistical machine translation|http://www.jstor.org/stable/41487496|41487496|2011-09-01|2011|['eng']|['Linguistics - Grammar']|['Science & Mathematics', 'Linguistics', 'Computer Science', 'Social Sciences']
This article considers the development of objective prior distributions for discrete parameter spaces. Formal approaches to such development—such as the reference prior approach—often result in a constant prior for a discrete parameter, which is questionable for problems that exhibit certain types of structure. To take advantage of structue, this article proposes embedding the original problem in a continuous problem that preserves the structure, and then using standard reference prior theory to determine the appropriate objective prior. Four different possibilities for this embedding are explored, and applied to a population-size model, the hupergeometric distribution, the multivariate hypergeometric distribution, the binomial-beta distribution, and the binomial distribution. The recommended objective priors for the first, third, and fourth problems are new.|Objective Priors for Discrete Parameter Spaces|http://www.jstor.org/stable/23239599|23239599|2012-06-01|2012|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
"Multi-species distribution modeling, which relates the occurrence of multiple species to environmental variables, is an important tool used by ecologists for both predicting the distribution of species in a community and identifying the important variables driving species co-occurrences. Recently, Dunstan, Foster and Darnell [Ecol. Model. 222 (2011) 955–963] proposed using finite mixture of regression (FMR) models for multi-species distribution modeling, where species are clustered based on their environmental response to form a small number of ""archetypal responses."" As an illustrative example, they applied their mixture model approach to a presence–absence data set of 200 marine organisms, collected along the Great Barrier Reef in Australia. Little attention, however, was given to the problem of model selection—since the archetypes (mixture components) may depend on different but likely overlapping sets of covariates, a method is needed for performing variable selection on all components simultaneously. In this article, we consider using penalized likelihood functions for variable selection in FMR models. We propose two penalties which exploit the grouped structure of the covariates, that is, each covariate is represented by a group of coefficients, one for each component. This leads to an attractive form of shrinkage that allows a covariate to be removed from all components simultaneously. Both penalties are shown to possess specific forms of variable selection consistency, with simulations indicating they outperform other methods which do not take into account the grouped structure. When applied to the Great Barrier Reef data set, penalized FMR models offer more insight into the important variables driving species co-occurrence in the marine community (compared to previous results where no model selection was conducted), while offering a computationally stable method of modeling complex species–environment relationships (through regularization)."|MULTI-SPECIES DISTRIBUTION MODELING USING PENALIZED MIXTURE OF REGRESSIONS|http://www.jstor.org/stable/24522606|24522606|2015-06-01|2015|['eng']|['Applied sciences - Engineering', 'Mathematics - Mathematical logic']|['Mathematics', 'Science & Mathematics', 'Statistics']
An important problem in shape analysis is to match configurations of points in space after filtering out some geometrical transformation. In this paper we introduce hierarchical models for such tasks, in which the points in the configurations are either unlabelled or have at most a partial labelling constraining the matching, and in which some points may only appear in one of the configurations. We derive procedures for simultaneous inference about the matching and the transformation, using a Bayesian approach. Our hierarchical model is based on a Poisson process for hidden true point locations; this leads to considerable mathematical simplification and efficiency of implementation of EM and Markov chain Monte Carlo algorithms. We find a novel use for classical distributions from directional statistics in a conditionally conjugate specification for the case where the geometrical transformation includes an unknown rotation. Throughout, we focus on the case of affine or rigid motion transformations. Under a broad parametric family of loss functions, an optimal Bayesian point estimate of the matching matrix can be constructed that depends only on a single parameter of the family. Our methods are illustrated by two applications from bioinformatics. The first problem is of matching protein gels in two dimensions, and the second consists of aligning active sites of proteins in three dimensions. In the latter case, we also use information related to the grouping of the amino acids, as an example of a more general capability of our methodology to include partial labelling information. We discuss some open problems and suggest directions for future work.|Bayesian Alignment Using Hierarchical Models, with Applications in Protein Bioinformatics|http://www.jstor.org/stable/20441278|20441278|2006-06-01|2006|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
This article considers methodology for hierarchical functional data analysis, motivated by studies of reproductive hormone profiles in the menstrual cycle. Current methods standardize the cycle lengths and ignore the timing of ovulation within the cycle, both of which are biologically informative. Methods are needed that avoid standardization, while flexibly incorporating information on covariates and the timing of reference events, such as ovulation and onset of menses. In addition, it is necessary to account for within-woman dependency when data are collected for multiple cycles. We propose an approach based on a hierarchical generalization of Bayesian multivariate adaptive regression splines. Our formulation allows for an unknown set of basis functions characterizing the population-averaged and woman-specific trajectories in relation to covariates. A reversible jump Markov chain Monte Carlo algorithm is developed for posterior computation. Applying the methods to data from the North Carolina Early Pregnancy Study, we investigate differences in urinary progesterone profiles between conception and nonconception cycles.|Bayesian Adaptive Regression Splines for Hierarchical Data|http://www.jstor.org/stable/4541404|4541404|2007-09-01|2007|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
The empirical literature on the relationship between natural disaster risk and investment in education is inconclusive. Model averaging methods in a framework of crosscountry and panel regressions show an extremely robust negative partial correlation between secondary school enrollment and natural disaster risk. This result is driven exclusively by geologic disasters. Exposure to natural disaster risk is a robust determinant of differences in secondary school enrollment between countries but not necessarily within countries Natural disasters, human capital, education, school enrollment, Bayesian model averaging|Natural Disasters and Human Capital Accumulation|http://www.jstor.org/stable/40891368|40891368|2010-01-01|2010|['eng']|['Physical sciences - Astronomy']|['Business & Economics', 'Business', 'Development Studies', 'Economics']
"A fundamental problem for Bayesian mixture model analysis is label switching, which occurs as a result of the nonidentifiability of the mixture components under symmetric priors. We propose two labeling methods to solve this problem. The first method, denoted by PM(ALG), is based on the posterior modes and an ascending algorithm generically denoted ALG. We use each Markov chain Monte Carlo sample as the starting point in an ascending algorithm, and label the sample based on the mode of the posterior to which it converges. Our natural assumption here is that the samples converged to the same mode should have the same labels. The PM(ALG) labeling method has some computational advantages over other popular labeling methods. Additionally, it automatically matches the ""ideal"" labels in the highest posterior density credible regions. The second method does labeling by maximizing the normal likelihood of the labeled Gibbs samples. Using a Monte Carlo simulation study and a real dataset, we demonstrate the success of our new methods in dealing with the label switching problem."|Bayesian Mixture Labeling by Highest Posterior Density|http://www.jstor.org/stable/40592220|40592220|2009-06-01|2009|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
This paper examines the robustness of explanatory variables in cross-country economic growth regressions. It introduces and employs a novel approach, Bayesian Averaging of Classical Estimates (BACE), which constructs estimates by averaging OLS coefficients across models. The weights given to individual regressions have a Bayesian justification similar to the Schwarz model selection criterion. Of 67 explanatory variables we find 18 to be significantly and robustly partially correlated with long-term growth and another three variables to be marginally related. The strongest evidence is for the relative price of investment, primary school enrollment, and the initial level of real GDP per capita.|Determinants of Long-Term Growth: A Bayesian Averaging of Classical Estimates (BACE) Approach|http://www.jstor.org/stable/3592794|3592794|2004-09-01|2004|['eng']|['Physical sciences - Astronomy']|['Business & Economics', 'Business', 'Economics']
"We use a Bayesian time-varying parameters structural VAR with stochastic volatility for GDP deflator inflation, real GDP growth, a 3-month nominal rate, and the rate of growth of M4 to investigate the underlying causes of the Great Moderation in the United Kingdom. Our evidence points toward a dominant role played by good luck in fostering the more stable macroeconomic environment of the last two decades. Results from counterfactual simulations, in particular, show that (i) ""bringing the Monetary Policy Committee back in time"" would only have had a limited impact on the Great Inflation episode, at the cost of lower output growth; (ii) imposing the 1970s monetary rule over the entire sample period would have made almost no difference in terms of inflation and output growth outcomes; and (iii) the Great Inflation was due, to a dominant extent, to large demand non-policy shocks, and to a lesser extent-especially in 1973 and 1979-to supply shocks."|"The ""Great Moderation"" in the United Kingdom"|http://www.jstor.org/stable/25096242|25096242|2008-02-01|2008|['eng']|['Mathematics - Applied mathematics', 'Information science - Informetrics']|['Business & Economics', 'Business', 'Economics', 'Finance']
We propose a phase I clinical trial design that seeks to determine the cumulative safety of a series of administrations of a fixed dose of an investigational agent. In contrast with traditional phase I trials that are designed solely to find the maximum tolerated dose of the agent, our design instead identifies a maximum tolerated schedule that includes a maximum tolerated dose as well as a vector of recommended administration times. Our model is based on a non-mixture cure model that constrains the probability of dose limiting toxicity for all patients to increase monotonically with both dose and the number of administrations received. We assume a specific parametric hazard function for each administration and compute the total hazard of dose limiting toxicity for a schedule as a sum of individual administration hazards. Throughout a variety of settings motivated by an actual study in allogeneic bone marrow transplant recipients, we demonstrate that our approach has excellent operating characteristics and performs as well as the only other currently published design for schedule finding studies. We also present arguments for the preference of our non-mixture cure model over the existing model.|Parametric Non-Mixture Cure Models for Schedule Finding of Therapeutic Agents|http://www.jstor.org/stable/25578159|25578159|2009-05-01|2009|['eng']|['Applied sciences - Engineering', 'Health sciences - Health and wellness', 'Biological sciences - Biology']|['Science & Mathematics', 'Statistics']
Let X₁, ..., X n be independent and identically distributed random vectors with a (Lebesgue) density f. We first prove that, with probability 1, there is a unique log-concave maximum likelihood estimator f n of f. The use of this estimator is attractive because, unlike kernel density estimation, the method is fully automatic, with no smoothing parameters to choose. Although the existence proof is non-constructive, we can reformulate the issue of computing f n in terms of a non-differentiable convex optimization problem, and thus combine techniques of computational geometry with Shor's r-algorithm to produce a sequence that converges to f n . An R version of the algorithm is available in the package LogConcDEAD—log-concave density estimation in arbitrary dimensions. We demonstrate that the estimator has attractive theoretical properties both when the true density is log-concave and when this model is misspecified. For the moderate or large sample sizes in our simulations, f n is shown to have smaller mean integrated squared error compared with kernel-based methods, even when we allow the use of a theoretical, optimal fixed bandwidth for the kernel estimator that would not be available in practice. We also present a real data clustering example, which shows that our methodology can be used in conjunction with the expectation-maximization algorithm to fit finite mixtures of log-concave densities.|Maximum likelihood estimation of a multidimensional log-concave density|http://www.jstor.org/stable/40925417|40925417|2010-11-01|2010|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
The expectation-maximization (EM) algorithm is a popular tool for maximizing likelihood functions in the presence of missing data. Unfortunately, EM often requires the evaluation of analytically intractable and high dimensional integrals. The Monte Carlo EM (MCEM) algorithm is the natural extension of EM that employs Monte Carlo methods to estimate the relevant integrals. Typically, a very large Monte Carlo sample size is required to estimate these integrals within an acceptable tolerance when the algorithm is near convergence. Even if this sample size were known at the onset of implementation of MCEM, its use throughout all iterations is wasteful, especially when accurate starting values are not available. We propose a data-driven strategy for controlling Monte Carlo resources in MCEM. The algorithm proposed improves on similar existing methods by recovering EM's ascent (i.e. likelihood increasing) property with high probability, being more robust to the effect of user-defined inputs and handling classical Monte Carlo and Markov chain Monte Carlo methods within a common framework. Because of the first of these properties we refer to the algorithm as 'ascent-based MCEM'. We apply ascent-based MCEM to a variety of examples, including one where it is used to accelerate the convergence of deterministic EM dramatically.|Ascent-Based Monte Carlo Expectation-Maximization|http://www.jstor.org/stable/3647576|3647576|2005-01-01|2005|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Abstract: The size and distribution of colonies of burrow-nesting petrels is thought to be limited partly by the availability of suitable breeding habitat and partly by predation. Historically, the availability of safe nesting habitat was restricted in New Zealand, due to the introduction of rats by humans. More recently, however, habitat has been restored by rat eradication. Petrel colony growth is mediated by both positive and negative density dependence, although it is unclear if, or how, density dependence will affect patterns in post-eradication colony recovery. Here, using burrow density as a proxy for relative abundance, we tested whether petrel colonies increase in density or area after rat eradication by sampling along a chronosequence of (1) five islands from which rats were eliminated 1 to 26 years ago, (2) two islands that never had rats, and (3) an island with rats still present, while controlling for habitat availability. We also measured a time series of burrow densities in plots on each island to compare temporal changes after rat eradication. Using Bayesian hierarchical modelling, after controlling for nesting habitat, we found that mean burrow density increased with time since rat eradication. Burrows remained clustered (i.e. spatially structured), but became more randomly distributed on islands with more time since eradication. Point density mapping indicated that colony extent increased with time since rat eradication, with colonies filling over 70% of surveyed areas on islands by 25 years after eradication. Increases in burrow density and colony area, but maintenance of clustered distribution, suggest both positive and negative density dependence may operate during colony expansion. Understanding patterns in petrel colony recovery is important, not only due to the indispensable role of petrels as island ecosystem engineers, reflecting the recovery of ecosystem functioning, but also to help guide post-eradication monitoring strategies.|Spatio-temporal changes in density and distribution of burrow-nesting seabird colonies after rat eradication|http://www.jstor.org/stable/26198737|26198737|2016-01-01|2016|['eng']|['Biological sciences - Ecology']|['Ecology & Evolutionary Biology', 'Science & Mathematics']
The minimum description length (MDL) principle articulated in the last decade by Rissanen and his co-workers yields new criteria for statistical model selection. MDL criteria permit data-based choices from among alternative statistical descriptions of data without necessarily assuming that the data were sampled randomly. This article explains the MDL principle informally, indicates the criteria it yields in the common cases of multinomial distributions and Gaussian regression, and illustrates MDL's use with numerical examples. We hope thereby to stimulate experimentation and debate about the pedagogical and practical implications of the MDL approach.|Model Selection Using the Minimum Description Length Principle|http://www.jstor.org/stable/2685777|2685777|2000-11-01|2000|['eng']|['Information science - Coding theory', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Short-term projections of the acquired immune deficiency syndrome (AIDS) epidemic in England and Wales have been regularly updated since the publication of the Cox report in 1988. The key approach for those updates has been the back-calculation method, which has been informally adapted to acknowledge various sources of uncertainty as well as to incorporate increasingly available information on the spread of the human immunodeficiency virus (HIV) in the population. We propose a Bayesian formulation of the back-calculation method which allows a formal treatment of uncertainty and the inclusion of extra information, within a single coherent composite model. Estimation of the variably dimensioned model is carried out by using reversible jump Markov chain Monte Carlo methods. Application of the model to data for homosexual and bisexual males in England and Wales is presented, and the role of the various sources of information and model assumptions is appraised. Our results show a massive peak in HIV infections around 1983 and suggest that the incidence of AIDS has now reached a plateau, although there is still substantial uncertainty about the future.|Bayesian Projection of the Acquired Immune Deficiency Syndrome Epidemic|http://www.jstor.org/stable/2986077|2986077|1998-01-01|1998|['eng']|['Biological sciences - Biology', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
A fundamental problem in many disciplines, including political science, sociology and epidemiology, is the examination of the association between two binary variables across a series of 2 x 2 tables, when only the margins are observed, and one of the margins is fixed. Two unobserved fractions are of interest, with only a single response per table, and it is this non-identifiability that is the inherent difficulty lying at the heart of ecological inference. Many methods have been suggested for ecological inference, often without a probabilistic model; we clarify the form of the sampling distribution and critique previous approaches within a formal statistical framework, thus allowing clarification and examination of the assumptions that are required under all approaches. A particularly difficult problem is choosing between models with and without contextual effects. Various Bayesian hierarchical modelling approaches are proposed to allow the formal inclusion of supplementary data, and/or prior information, without which ecological inference is unreliable. Careful choice of the prior within such models is required, however, since there may be considerable sensitivity to this choice, even when the model assumed is correct and there are no contextual effects. This sensitivity is shown to be a function of the number of areas and the distribution of the proportions in the fixed margin across areas. By explicitly providing a likelihood for each table, the combination of individual level survey data and aggregate level data is straightforward and we illustrate that survey data can be highly informative, particularly if these data are from a survey of the minority population within each area. This strategy is related to designs that are used in survey sampling and in epidemiology. An approximation to the suggested likelihood is discussed, and various computational approaches are described. Some extensions are outlined including the consideration of multiway tables, spatial dependence and area-specific (contextual) variables. Voter registration-race data from 64 counties in the US state of Louisiana are used to illustrate the methods.|Ecological Inference for 2 x 2 Tables|http://www.jstor.org/stable/3559772|3559772|2004-01-01|2004|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
"Fisher's linear discriminant analysis is a valuable tool for multigroup classification. With a large number of predictors, one can find a reduced number of discriminant coordinate functions that are ""optimal"" for separating the groups. With two such functions, one can produce a classification map that partitions the reduced space into regions that are identified with group membership, and the decision boundaries are linear. This article is about richer nonlinear classification schemes. Linear discriminant analysis is equivalent to multiresponse linear regression using optimal scorings to represent the groups. In this paper, we obtain nonparametric versions of discriminant analysis by replacing linear regression by any nonparametric regression method. In this way, any multiresponse regression technique (such as MARS or neural networks) can be postprocessed to improve its classification performance"|Flexible Discriminant Analysis by Optimal Scoring|http://www.jstor.org/stable/2290989|2290989|1994-12-01|1994|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Landscape features may serve as either barriers or gateways to the spread of certain infectious diseases, and understanding the way geographic structure impacts disease spread could lead to improved containment strategies. Here, we focus on the space-time diffusion of a raccoon rabies outbreak across several states in the Eastern United States. While focusing on pattern, we move toward closer links between pattern and process by considering statistical estimation of local pattern features to gain insight on landscape influences on the underlying process. Specifically, we quantify the impact that landscape features, such as mountains and rivers, have on the speed of infectious disease diffusion. This work combines statistical modeling with operations in a geographic information system (GIS) to link observed patterns of disease diffusion with local landscape values. We explore three analytic approaches. First, we use spatial prediction (kriging) to provide a descriptive pattern of the spread of the virus. Second, we use Bayesian areal wombling to detect barriers for infectious disease transmission and examine spatial coincidence with potential features. Finally, we input landscape variables into a hierarchical Bayesian model with spatially varying coefficients to obtain model-based estimates of their local impacts on transmission time in counties.|Mountains, Valleys, and Rivers: The Transmission of Raccoon Rabies over a Heterogeneous Landscape|http://www.jstor.org/stable/27595696|27595696|2008-12-01|2008|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Agriculture', 'Statistics']
The tail of a bivariate distribution function in the domain of attraction of a bivariate extreme value distribution may be approximated by that of its extreme value attractor. The extreme value attractor has margins that belong to a three-parameter family and a dependence structure which is characterized by a probability measure on the unit interval with mean equal to ½, which is called the spectral measure. Inference is done in a Bayesian framework using a censored likelihood approach. A prior distribution is constructed on an infinite dimensional model for this measure, the model being at the same time dense and computationally manageable. A trans-dimensional Markov chain Monte Carlo algorithm is developed and convergence to the posterior distribution is established. In simulations, the Bayes estimator for the spectral measure is shown to compare favourably with frequentist non-parametric estimators. An application to a data set of Danish fire insurance claims is provided.|Non-parametric Bayesian inference on bivariate extremes|http://www.jstor.org/stable/41262676|41262676|2011-06-01|2011|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
ABSTRACT: Seascape genetics, a term coined in 2006, is a fast growing area of population genetics that draws on ecology, oceanography and geography to address challenges in basic understanding of marine connectivity and applications to management. We provide an accessible overview of the latest developments in seascape genetics that merge exciting new ideas from the field of marine population connectivity with statistical and technical advances in population genetics. After summarizing the historical context leading to the emergence of seascape genetics, we detail questions and methodological approaches that are evolving the discipline, highlight applications to conservation and management, and conclude with a summary of the field’s transition to seascape genomics. From 100 seascape genetic studies, we assess trends in taxonomic and geographic coverage, sampling and statistical design, and dominant seascape drivers. Notably, temperature, oceanography and geography show equal prevalence of influence on spatial genetic patterns, and tests of over 20 other seascape factors suggest that a variety of forces impact connectivity at distinct spatio-temporal scales. A new level of rigor in statistical analysis is critical for disentangling multiple drivers and spurious effects. Coupled with GIS data and genomic scale sequencing methods, this rigor is taking seascape genetics beyond an initial focus on identifying correlations to hypothesis-driven insights into patterns and processes of population connectivity and adaptation. The latest studies are illuminating differences between demographic, functional and neutral genetic connectivity, and informing applications to marine reserve design, fisheries science and strategies to assess resilience to climate change and other anthropogenic impacts.|A decade of seascape genetics:|http://www.jstor.org/stable/24897812|24897812|2016-07-28|2016|['eng']|['Biological sciences - Ecology']|['Ecology & Evolutionary Biology', 'Aquatic Sciences', 'Science & Mathematics', 'Biological Sciences']
This paper examines penalized likelihood estimation in the context of general regression problems, characterized as probability models with composite likelihood functions. The emphasis is on the common situation where a parametric model is considered satisfactory but for inhomogeneity with respect to a few extra variables. A finite-dimensional formulation is adopted, using a suitable set of basis functions. Appropriate definitions of deviance, degrees of freedom, and residual are provided, and the method of cross-validation for choice of the tuning constant is discussed. Quadratic approximations are derived for all the required statistics. /// On examine ici l'estimation par la vraisemblance pénalisée dans le contexte des problèmes généraux de régression, caractérisés comme des modèles avec des fonctions composites de vraisemblance. On accentue la situation fréquente quand on trouve un modèle paramétrique comme utile sauf pour la nonhomogénéité à l'égard de quelques variables supplémentaires. Une formulation de dimension finie est adoptée avec une base convenable de fonctions. Des définitions appropriées de la déviation, des dégrés de liberté, et de résidu sont examinées, et la méthode de validation croisée pour un choix du paramètre d'ajustement est discutée. Des approximations quadratiques sont présentées pour toutes les statistiques nécessaires.|Penalized Likelihood for General Semi-Parametric Regression Models|http://www.jstor.org/stable/1403404|1403404|1987-12-01|1987|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical analysis']|['Science & Mathematics', 'Statistics']
We develop a Bayesian analysis based on two different Jeffreys priors for the Student-t regression model with unknown degrees of freedom. It is typically difficult to estimate the number of degrees of freedom: improper prior distributions may lead to improper posterior distributions, whereas proper prior distributions may dominate the analysis. We show that Bayesian analysis with either of the two considered Jeffreys priors provides a proper posterior distribution. Finally, we show that Bayesian estimators based on Jeffreys analysis compare favourably to other Bayesian estimators based on priors previously proposed in the literature.|Objective Bayesian Analysis for the Student-T Regression Model|http://www.jstor.org/stable/20441467|20441467|2008-06-01|2008|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
The need to identify a few important variables that affect a certain outcome of interest commonly arises in various industrial engineering applications. The genetic algorithm (GA) appears to be a natural tool for solving such a problem. In this article we first demonstrate that the GA is actually not a particularly effective variable selection tool, and then propose a very simple modification. Our idea is to run a number of GAs in parallel without allowing each GA to fully converge, and to consolidate the information from all the individual GAs in the end. We call the resulting algorithm the parallel genetic algorithm (PGA). Using a number of both simulated and real examples, we show that the PGA is an interesting as well as highly competitive and easy-to-use variable selection tool.|Darwinian Evolution in Parallel Universes: A Parallel Genetic Algorithm for Variable Selection|http://www.jstor.org/stable/25471241|25471241|2006-11-01|2006|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
This work is concerned with the vulnerability of spaceborne microelectronics to single-event upset, which is a change of state caused by high-energy charged particles in the solar wind or the cosmic ray environment striking a sensitive node. To measure the susceptibility of a semiconductor device to single-event upsets, testing is conducted by exposing it to high-energy heavy ions or protons produced in a particle accelerator. The number of upsets is characterized by the interaction cross-section, which is an increasing function of linear energy transfer. The prediction of the on-orbit upset rate is made by combining the device geometry and cross-section versus linear energy transfer curve with a model for the orbit-specific radiation environment. We develop a semiparametric isotonic regression method for the upset count responses, based on a Dirichlet process prior for the cross-section curve. The methodology proposed allows the data to drive the shape of the cross-section versus linear energy transfer relationship, resulting in more robust predictive inference for the on-orbit upset rate than conventional models based on Weibull or log-normal parametric forms for the cross-section curve. We illustrate the modelling approach with data from two particle accelerator experiments.|An application of semiparametric Bayesian isotonic regression to the study of radiation effects in spaceborne microelectronics|http://www.jstor.org/stable/23360974|23360974|2013-01-01|2013|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Applied mathematics']|['Science and Mathematics', 'Statistics']
Many studies comparing new treatments to standard treatments consist of parallel randomized experiments. In the example considered here, randomized experiments were conducted in eight schools to determine the effectiveness of special coaching programs for the SAT. The purpose here is to illustrate Bayesian and empirical Bayesian techniques that can be used to help summarize the evidence in such data about differences among treatments, thereby obtaining improved estimates of the treatment effect in each experiment, including the one having the largest observed effect. Three main tools are illustrated: 1) graphical techniques for displaying sensitivity within an empirical Bayes framework, 2) simple simulation techniques for generating Bayesian posterior distributions of individual effects and the largest effect, and 3) methods for monitoring the adequacy of the Bayesian model specification by simulating the posterior predictive distribution in hypothetical replications of the same treatments in the same eight schools.|Estimation in Parallel Randomized Experiments|http://www.jstor.org/stable/1164617|1164617|1981-12-01|1981|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Education', 'Statistics', 'Social Sciences']
One of the key ingredients in drug discovery is the derivation of conceptual templates called pharmacophores. A pharmacophore model characterizes the physicochemical properties common to all active molecules, called ligands, bound to a particular protein receptor, together with their relative spatial arrangement. Motivated by this important application, we develop a Bayesian hierarchical model for the derivation of pharmacophore templates from multiple configurations of point sets, partially labeled by the atom type of each point. The model is implemented through a multistage template hunting algorithm that produces a series of templates that capture the geometrical relationship of atoms matched across multiple configurations. Chemical information is incorporated by distinguishing between atoms of different elements, whereby different elements are less likely to be matched than atoms of the same element. We illustrate our method through examples of deriving templates from sets of ligands that all bind structurally related protein active sites and show that the model is able to retrieve the key pharmacophore features in two test cases.|Hierarchical Bayesian Modeling of Pharmacophores in Bioinformatics|http://www.jstor.org/stable/41242499|41242499|2011-06-01|2011|['eng']|['Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
We describe an integrated methodology for analysing dynamic magnetic resonance images of the breast. The problems that motivate this methodology arise from a collaborative study with a tumour institute. The methods are developed within the Bayesian framework and comprise image restoration and classification steps. Two different approaches are proposed for the restoration. Bayesian inference is performed by means of Markov chain Monte Carlo algorithms. We make use of a Metropolis algorithm with a specially chosen proposal distribution that performs better than more commonly used proposals. The classification step is based on a few attribute images yielded by the restoration step that describe the essential features of the contrast agent variation over time. Procedures for hyperparameter estimation are provided, so making our method automatic. The results show the potential of the methodology to extract useful information from acquired dynamic magnetic resonance imaging data about tumour morphology and internal pathophysiological features.|Bayesian Analysis of Dynamic Magnetic Resonance Breast Images|http://www.jstor.org/stable/3592566|3592566|2004-01-01|2004|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
"This paper shows that the commonly encountered volatility persistence in fitting GARCH models to financial time series can arise if the possibility of structural changes is not incorporated in the time series model. To avoid spurious long memory in modeling volatilities of econometric time series, we consider two time-scales and use the ""short"" time-scale to define GARCH dynamics and the ""long"" time-scale to incorporate parameter jumps. This leads to a Bayesian change-point ARX-GARCH model, whose unknown parameters can undergo occasional changes at unspecified times and can be estimated by explicit recursive formulas when the hyperparameters of the Bayesian model are specified. Efficient estimators of the hyperparameters of the Bayesian model are developed, yielding empirical Bayes estimates of the piecewise constant parameters in the stochastic change-point model. The empirical Bayes approach is applied to the frequentist problem of partitioning the time series into segments under sparsity assumptions on the change-points. Simulation and empirical studies of its performance are also given."|STOCHASTIC CHANGE-POINT ARX-GARCH MODELS AND THEIR APPLICATIONS TO ECONOMETRIC TIME SERIES|http://www.jstor.org/stable/24310813|24310813|2013-10-01|2013|['eng']|['Applied sciences - Engineering', 'Mathematics - Mathematical logic']|['Mathematics', 'Science & Mathematics', 'Statistics']
"Let $X|\mu -N_{p}(\mu,\nu _{x}I)$ and $Y|\mu -N_{p}(\mu,\nu _{y}I)$ be independent p-dimensional multivariate normal vectors with common unknown mean μ. Based on only observing X ≡ x, we consider the problem of obtaining a predictive density p̂(y|x) for Y that is close to p(y|μ) as measured by expected Kullback-Leibler loss. A natural procedure for this problem is the (formal) Bayes predictive density $\hat{p}_{{\rm U}}(y|x)$ under the uniform prior $\hat{p}_{{\rm U}}(y|x)$, which is best invariant and minimax. We show that any Bayes predictive density will be minimax if it is obtained by a prior yielding a marginal that is superharmonic or whose square root is superharmonic. This yields wide classes of minimax procedures that dominate $\hat{p}_{{\rm U}}(y|x)$, including Bayes predictive densities under superharmonic priors. Fundamental similarities and differences with the parallel theory of estimating a multivariate normal mean under quadratic loss are described."|Improved Minimax Predictive Densities under Kullback-Leibler Loss|http://www.jstor.org/stable/25463408|25463408|2006-02-01|2006|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
We propose a Bayesian nonparametric instrumental variable approach under additive separability that allows us to correct for endogeneity bias in regression models where the covariate effects enter with unknown functional form. Bias correction relies on a simultaneous equations specification with flexible modeling of the joint error distribution implemented via a Dirichlet process mixture prior. Both the structural and instrumental variable equation are specified in terms of additive predictors comprising penalized splines for nonlinear effects of continuous covariates. Inference is fully Bayesian, employing efficient Markov chain Monte Carlo simulation techniques. The resulting posterior samples do not only provide us with point estimates, but allow us to construct simultaneous credible bands for the nonparametric effects, including data-driven smoothing parameter selection. In addition, improved robustness properties are achieved due to the flexible error distribution specification. Both these features are challenging in the classical framework, making the Bayesian one advantageous. In simulations, we investigate small sample properties and an investigation of the effect of class size on student performance in Israel provides an illustration of the proposed approach which is implemented in an R package bayesIV. Supplementary materials for this article are available online.|Bayesian Nonparametric Instrumental Variables Regression Based on Penalized Splines and Dirichlet Process Mixtures|http://www.jstor.org/stable/43702774|43702774|2014-07-01|2014|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
Mourning doves (Zenaida macroura) are surveyed in North America with a Call-Count Survey (CCS) and the North American Breeding Bird Survey (BBS). Analyses in recent years have identified inconsistencies in results between surveys, and a need exists to analyze the surveys using modern methods and examine possible causes of differences in survey results. Call-Count Survey observers collect separate information on number of doves heard and number of doves seen during counting, whereas BBS observers record one index containing all doves observed. We used hierarchical log-linear models to estimate trend and annual indices of abundance for 1966-2007 from BBS data, CCS-heard data, and CCS-seen data. Trend estimates from analyses provided inconsistent results for several states and for eastern and central dovemanagement units. We examined differential effects of change in land use and noise-related disturbance on the CCS indices. Changes in noiserelated disturbance along CCS routes had a larger influence on the heard index than on the seen index, but association analyses among states of changes in temperature and of amounts of developed land suggest that CCS indices are differentially influenced by changes in these environmental features. Our hierarchical model should be used to estimate population change from dove surveys, because it provides an efficient framework for estimating population trends from dove indices while controlling for environmental features that differentially influence the indices.|Comparative Analysis of Mourning Dove Population Change in North America|http://www.jstor.org/stable/40665180|40665180|2010-07-01|2010|['eng']|['Biological sciences - Ecology', 'Biological sciences - Biogeography']|['Science & Mathematics', 'Biological Sciences', 'Zoology']
State-space or dynamic approaches to discrete or grouped duration data with competing risks or multiple terminating events allow simultaneous modeling and smooth estimation of hazard functions and time-varying effects in a flexible way. Full Bayesian or posterior mean estimation, using numerical integration techniques or Monte Carlo methods, can become computationally rather demanding or even infeasible for higher dimensions and larger datasets. Therefore, based on previous work on filtering and smoothing for multicategorical time series and longitudinal data, our approach uses posterior mode estimation. Thus we have to maximize posterior densities or, equivalently, a penalized likelihood, which enforces smoothness of hazard functions and time-varying effects by a roughness penalty. Dropping the Bayesian smoothness prior and adopting a nonparametric viewpoint, one might also start directly from maximizing this penalized likelihood. We show how Fisher scoring smoothing iterations can be carried out efficiently by iteratively applying linear Kalman filtering and smoothing to a working model. This algorithm can be combined with an EM-type procedure to estimate unknown smoothing parameters or hyperparameters. The methods are applied to a larger set of unemployment duration data with one terminating event and, in a further analysis, multiple terminating events from the German socioeconomic panel GSOEP.|Smoothing Hazard Functions and Time-Varying Effects in Discrete Duration and Competing Risks Models|http://www.jstor.org/stable/2291584|2291584|1996-12-01|1996|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics', 'Mathematics - Mathematical analysis']|['Science & Mathematics', 'Statistics']
The empirical support for features of a Dynamic Stochastic General Equilibrium model with two technology shocks is evaluated using Bayesian model averaging over vector autoregressions. The model features include equilibria, restrictions on long-run responses, a structural break of unknown date, and a range of lags and deterministic processes. We find support for a number of features implied by the economic model, and the evidence suggests a break in the entire model structure around 1984, after which technology shocks appear to account for all stochastic trends. Business cycle volatility seems more due to investment-specific technology shocks than neutral technology shocks.|EVIDENCE ON FEATURES OF A DSGE BUSINESS CYCLE MODEL FROM BAYESIAN MODEL AVERAGING|http://www.jstor.org/stable/23352330|23352330|2013-02-01|2013|['eng']|['Mathematics - Applied mathematics']|['Business', 'Business & Economics Collection', 'Economics']
"We consider applying Bayesian Variable Selection Regression, or BVSR, to genome-wide association studies and similar large-scale regression problems. Currently, typical genome-wide association studies measure hundreds of thousands, or millions, of genetic variants (SNPs), in thousands or tens of thousands of individuals, and attempt to identify regions harboring SNPs that affect some phenotype or outcome of interest. This goal can naturally be cast as a variable selection regression problem, with the SNPs as the covariates in the regression. Characteristic features of genome-wide association studies include the following: (i) a focus primarily on identifying relevant variables, rather than on prediction; and (ii) many relevant covariates may have tiny effects, making it effectively impossible to confidently identify the complete ""correct"" subset of variables. Taken together, these factors put a premium on having interpretable measures of confidence for individual covariates being included in the model, which we argue is a strength of BVSR compared with alternatives such as penalized regression methods. Here we focus primarily on analysis of quantitative phenotypes, and on appropriate prior specification for BVSR in this setting, emphasizing the idea of considering what the priors imply about the total proportion of variance in outcome explained by relevant covariates. We also emphasize the potential for BVSR to estimate this proportion of variance explained, and hence shed light on the issue of ""missing heritability"" in genome-wide association studies. More generally, we demonstrate that, despite the apparent computational challenges, BVSR can provide useful inferences in these large-scale problems, and in our simulations produces better power and predictive performance compared with standard single-SNP analyses and the penalized regression method LASSO. Methods described here are implemented in a software package, pi-MASS, available from the Guan Lab website http://bcm.edu/cnrc/mcmcmc/pimass."|BAYESIAN VARIABLE SELECTION REGRESSION FOR GENOME-WIDE ASSOCIATION STUDIES AND OTHER LARGE-SCALE PROBLEMS|http://www.jstor.org/stable/23069354|23069354|2011-09-01|2011|['eng']|['Applied sciences - Engineering', 'Biological sciences - Biology']|['Mathematics', 'Science & Mathematics', 'Statistics']
Healthcare resource allocation decisions are commonly informed by computer model predictions of population mean costs and health effects. It is common to quantify the uncertainty in the prediction due to uncertain model inputs, but methods for quantifying uncertainty due to inadequacies in model structure are less well developed. We introduce an example of a model that aims to predict the costs and health effects of a physical activity promoting intervention. Our goal is to develop a framework in which we can manage our uncertainty about the costs and health effects due to deficiencies in the model structure. We describe the concept of 'model discrepancy': the difference between the model evaluated at its true inputs, and the true costs and health effects. We then propose a method for quantifying discrepancy based on decomposing the cost-effectiveness model into a series of subfunctions, and considering potential error at each subfunction. We use a variance-based sensitivity analysis to locate important sources of discrepancy within the model to guide model refinement. The resulting improved model is judged to contain less structural error, and the distribution on the model output better reflects our true uncertainty about the costs and effects of the intervention.|Managing structural uncertainty in health economic decision models: a discrepancy approach|http://www.jstor.org/stable/41430947|41430947|2012-01-01|2012|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Science and Mathematics', 'Statistics']
A Bayesian model averaging approach to the estimation of lag structures is introduced and applied to assess the impact of R&amp;D on agricultural productivity in the United States from 1889 to 1990. Lag and structural break coefficients are estimated using a reversible jump algorithm that traverses the model space. In addition to producing estimates and standard deviations for the coefficients, the probability that a given lag (or break) enters the model is estimated. The approach is extended to select models populated with gamma distributed lags of different frequencies. Results are consistent with the hypothesis that R&amp;D positively drives productivity. Gamma lags are found to retain their usefulness in imposing a plausible structure on lag coefficients, and their role is enhanced through the use of model averaging.|An Analysis of The Impact of Research and Development on Productivity Using Bayesian Model Averaging with a Reversible Jump Algorithm|http://www.jstor.org/stable/40931061|40931061|2010-07-01|2010|['eng']|['Applied sciences - Engineering']|['Business & Economics', 'Science & Mathematics', 'Agriculture', 'Business', 'Economics']
Sample size criteria are often expressed in terms of the concentration of the posterior density, as controlled by some sort of error bound. Since this is done pre-experimentally, one can regard the posterior density as a function of the data. Thus, when a sample size criterion is formalized in terms of a functional of the posterior, its value is a random variable. Generally, such functionals have means under the true distribution. We give asymptotic expressions for the expected value, under a fixed parameter, for certain types of functionals of the posterior density in a Bayesian analysis. The generality of our treatment permits us to choose functionals that encapsulate a variety of inference criteria and large ranges of error bounds. Consequently, we get simple inequalities which can be solved to give minimal sample sizes needed for various estimation goals. In several parametric examples, we verify that our asymptotic bounds give good approximations to the expected values of the functionals they approximate. Also, our numerical computations suggest our treatment gives reasonable results.|Closed Form Expressions for Bayesian Sample Size|http://www.jstor.org/stable/25463459|25463459|2006-06-01|2006|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Mathematical analysis']|['Science & Mathematics', 'Statistics']
One of the challenges with emulating the response of a multivariate function to its inputs is the quantity of data that must be assimilated, which is the product of the number of model evaluations and the number of outputs. This article shows how even large calculations can be made tractable. It is already appreciated that gains can be made when the emulator residual covariance function is treated as separable in the model-inputs and model-outputs. Here, an additional simplification on the structure of the regressors in the emulator mean function allows very substantial further gains. The result is that it is now possible to emulate rapidly—on a desktop computer—models with hundreds of evaluations and hundreds of outputs. This is demonstrated through calculating costs in floating-point operations, and in an illustration. Even larger sets of outputs are possible if they have additional structure, for example, spatial-temporal.|Efficient Emulators for Multivariate Deterministic Functions|http://www.jstor.org/stable/25651230|25651230|2008-12-01|2008|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Mathematical analysis']|['Science & Mathematics', 'Computer Science', 'Statistics']
We introduce new applications of the skew-probit IRT model by considering a flexible skew-normal distribution for the latent variables and by extending this model to include an additional random effect for modeling dependence between items within the same testlet. A Bayesian hierarchical structure is presented using a double data augmentation approach. This can be easily implemented in WinBUGS or SAS by considering MCMC algorithms. Several Bayesian model selection criteria, such as DIC, EAIC and EBIC, have been considered; in addition, we use posterior sum of squares of the latent residuals as a global discrepancy measure to model comparison. Two applications illustrate the methodology, one data set related to a mathematical test and another related to reading comprehension, both applied to Peruvian students. Results indicate better performance of the more flexible models proposed in this paper.|Extensions of the skew-normal ogive item response model|http://www.jstor.org/stable/43601359|43601359|2014-02-01|2014|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Mathematics', 'Statistics']
Consider a multinomial regression model where the response, which indicates a unit's membership in one of several possible unordered classes, is associated with a set of predictor variables. Such models typically involve a matrix of regression coefficients, with the (j, k) element of this matrix modulating the effect of the kth predictor on the propensity of the unit to belong to the jth class. Thus, a supposition that only a subset of the available predictors are associated with the response corresponds to some of the columns of the coefficient matrix being zero. Under the Bayesian paradigm, the subset of predictors which are associated with the response can be treated as an unknown parameter, leading to typical Bayesian model selection and model averaging procedures. As an alternative, we investigate model selection and averaging, whereby a subset of individual elements of the coefficient matrix are zero. That is, the subset of predictors associated with the propensity to belong to a class varies with the class. We refer to this as class-specific predictor selection. We argue that such a scheme can be attractive on both conceptual and computational grounds.|Bayesian Multinomial Regression with Class-Specific Predictor Selection|http://www.jstor.org/stable/30245144|30245144|2008-12-01|2008|['eng']|['Applied sciences - Engineering', 'Information science - Coding theory', 'Mathematics - Applied mathematics']|['Mathematics', 'Science & Mathematics', 'Statistics']
An array of regularly spaced seismic stations can estimate the location of a distant earthquake using arrival times at the stations of seismic waves generated by the earthquakes. However, the accuracy decreases as the distance to the epicentre of the earthquake from the array increases. This paper is concerned with the modification of the estimated location by removing its bias which is locally systematic but globally complex, reflecting the structure of the Earth's interior. Spline surfaces are used to model such biases. Then a Bayesian procedure is carried out not only to tune the smoothness constraints but also to select the best combination among various sums of squares of differently weighted residuals and various roughness penalties for the smoothing. Using the estimated splines of the posterior mode, the newly determined epicentre locations are transformed to confirm its practical utility. Residual distributions show that our procedure improves the modification by the conventional procedure. A spatial pattern of the residuals reveals some geophysical characteristics.|Correction of Earthquake Location Estimation in a Small-Seismic-Array System|http://www.jstor.org/stable/3318749|3318749|1998-06-01|1998|['eng']|['Physical sciences - Astronomy']|['Mathematics', 'Science and Mathematics', 'Statistics']
"We generalize the well-known mixtures of Gaussians approach to density estimation and the accompanying Expectation—Maximization technique for finding the maximum likelihood parameters of the mixture to the case where each data point carries an individual d-dimensional uncertainty covariance and has unique missing data properties. This algorithm reconstructs the errordeconvolved or ""underlying"" distribution function common to all samples, even when the individual data points are samples from different distributions, obtained by convolving the underlying distribution with the heteroskedastic uncertainty distribution of the data point and projecting out the missing data directions. We show how this basic algorithm can be extended with conjugate priors on all of the model parameters and a ""split-and-merge"" procedure designed to avoid local maxima of the likelihood. We demonstrate the full method by applying it to the problem of inferring the three-dimensional velocity distribution of stars near the Sun from noisy two-dimensional, transverse velocity measurements from the Hipparcos satellite."|EXTREME DECONVOLUTION: INFERRING COMPLETE DISTRIBUTION FUNCTIONS FROM NOISY, HETEROGENEOUS AND INCOMPLETE OBSERVATIONS|http://www.jstor.org/stable/23024867|23024867|2011-06-01|2011|['eng']|['Physical sciences - Astronomy']|['Mathematics', 'Science & Mathematics', 'Statistics']
Marginalized models (Heagerty, 1999, Biometrics 55, 688-698) permit likelihood-based inference when interest lies in marginal regression models for longitudinal binary response data. Two such models are the marginalized transition and marginalized latent variable models. The former captures within-subject serial dependence among repeated measurements with transition model terms while the latter assumes exchangeable or nondiminishing response dependence using random intercepts. In this article, we extend the class of marginalized models by proposing a single unifying model that describes both serial and long-range dependence. This model will be particularly useful in longitudinal analyses with a moderate to large number of repeated measurements per subject, where both serial and exchangeable forms of response correlation can be identified. We describe maximum likelihood and Bayesian approaches toward parameter estimation and inference, and we study the large sample operating characteristics under two types of dependence model misspecification. Data from the Madras Longitudinal Schizophrenia Study (Thara et al., 1994, Acta Psychiatrica Scandinavica 90, 329-336) are analyzed. /// Les modèles marginalisés (Heagerty, 1999) permettent l'inférence basée sur la vraisemblance lorsqu'on s'intéresse à des modèles marginaux de régression pour des données longitudinales binaires. Deux modèles de ce type sont le modèle de transition marginalisée, et le modèle à variable latente marginalisée. Le premier reflète la dépendance sérielle intra-sujet au sein des mesures répétées avec des termes de transition dans le modèle, tandis que le deuxième suppose l'échangeabilité ou la non-décroissance de la dépendance de la réponse avec des intercepts aléatoires. Dans cet article, nous etendèns la classe des modèles marginalisés en proposant un modèle simple unifié décrivant à la fois la dépendance sérielle et la dépendance au long terme. Ce modèle est particulièrement utile dans des analyses longitudinales avec un nombre de mesures répétées par sujet modéré ou large, où aussi bien les formes sérielles et échangeables de la corrélation de la réponse peuvent être identifiées. Nous décrivons des approches bayésiennes et au maximum de vraisemblance pour l'estimation des paramètres et pour l'inférence, et nous étudions les caractéristiques opérationnelles pour de grands échantillons, sous deux types de mauvaise spécification du modèle de dépendance. On analyse les données de l'étude longitudinale de schizophrenie de Madras (Thara, 1994).|Marginalized Models for Moderate to Long Series of Longitudinal Binary Response Data|http://www.jstor.org/stable/4541342|4541342|2007-06-01|2007|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
The life testing of items that exhibit a distribution of times to failure is undertaken for making decisions such as design qualification and reliability demonstration. In such contexts, procedures based on the Bayesian paradigm have assumed a common prior distribution of item reliability by both the consumer and the manufacturer. In this paper we consider the adversarial situation wherein both parties agree on a statistical model for lifetimes but use different prior distributions. We require that the consumer's criteria for acceptance and rejection be known to the manufacturer. We illustrate our approach via the case of exponentially distributed life lengths and relate it to the approach specified in standard MIL STD 781C.|Adversarial Life Testing|http://www.jstor.org/stable/2345997|2345997|1993-01-01|1993|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Bayesian analysis of macroevolutionary mixtures (BAMM) has recently taken the study of lineage diversification by storm. BAMM estimates the diversification-rate parameters (speciation and extinction) for every branch of a study phylogeny and infers the number and location of diversification-rate shifts across branches of a tree. Our evaluation of BAMM reveals two major theoretical errors: (i) the likelihood function (which estimates the model parameters from the data) is incorrect, and (ii) the compound Poisson process prior model (which describes the prior distribution of diversification-rate shifts across branches) is incoherent. Using simulation, we demonstrate that these theoretical issues cause statistical pathologies; posterior estimates of the number of diversification-rate shifts are strongly influenced by the assumed prior, and estimates of diversification-rate parameters are unreliable. Moreover, the inability to correctly compute the likelihood or to correctly specify the prior for rate-variable trees precludes the use of Bayesian approaches for testing hypotheses regarding the number and location of diversification-rate shifts using BAMM.|Critically evaluating the theory and performance of Bayesian analysis of macroevolutionary mixtures|http://www.jstor.org/stable/26471498|26471498|2016-08-23|2016|['eng']|['Applied sciences - Engineering', 'Biological sciences - Biology']|['Science & Mathematics', 'Biological Sciences', 'General Science']
The evolution of the U.S. economy over the past 55 years is examined through the lens of a microfounded model that allows for changes in the behaviour of the Federal Reserve and in the volatility of structural shocks. Agents are aware of the possibility of regime changes and their beliefs matter for the law of motion underlying the macroeconomy. Monetary policy is identified by repeated fluctuations between a Hawk and a Dove regime, with the latter prevailing in the 1970s and during the recent crisis. To explore the role of agents' beliefs I introduce a new class of counterfactual simulations: beliefs counterf actuals. If, in the 1970s, agents had anticipated the appointment of an extremely conservative Chairman, inflation would have been lower and the inflation-output trade-off more favourable. The large drop in inflation and output at the end of 2008 would have been mitigated if agents had expected the Federal Reserve to be exceptionally active in the near future.|Regime Switches, Agents' Beliefs, and Post-World War II U.S. Macroeconomic Dynamics|http://www.jstor.org/stable/43551492|43551492|2013-04-01|2013|['eng']|['Economics - Economic disciplines']|['Business & Economics', 'Business', 'Economics']
The authors develop a methodology for predicting unobserved values in a conditionally lognormal random spatial field like those commonly encountered in environmental risk analysis. These unobserved values are of two types. The first come from spatial locations where the field has never been monitored, the second, from currently monitored sites which have been only recently installed. Thus the monitoring data exhibit a monotone pattern, resembling a staircase whose highest step comes from the oldest monitoring sites. The authors propose a hierarchical Bayesian approach using the lognormal sampling distribution, in conjunction with a conjugate generalized Wishart distribution. This prior distribution allows different degrees of freedom to be fitted for individual steps, taking into account the differential amounts of information available from sites at the different steps in the staircase. The resulting hierarchical model is a predictive distribution for the unobserved values of the field. The method is demonstrated by application to the ambient ozone field for the southwestern region of British Columbia. /// Les auteurs développent une méthode permettant de faire de la prévision pour des valeurs non-observées dans un champ spatial aléatoire conditionnellement lognormal comme ceux que l'on rencontre fréquemment dans l'analyse des risques environnementaux. Les valeurs non-observées correspondent soit à des sites où le champ n'a jamais été mesuré, soit à des stations d'observation en activité mais qui n'ont été installées que récemment. On peut donc distinguer dans les données un patron monotone semblable à un escalier dont la plus haute marche correspond aux sites les plus anciens. Les auteurs proposent une approche bayésienne hiérarchique s'appuyant sur la loi lognormale et une loi a priori de Wishart généralisée conjuguée. Cette dernière autorise l'emploi de degrés de liberté différents pour chacune des marches, ce qui permet de refléter la quantité d'information disponible à chacun des sites le long de l'escalier. On peut alors déduire de ce modèle hiérarchique une loi prévisionnelle pour les valeurs non-observées du champ. L'approche est illustrée à l'aide de données sur le niveau ambiant d'ozone dans le sud-ouest de la Colombie-Britannique.|Spatial Prediction and Temporal Backcasting for Environmental Fields Having Monotone Data Patterns|http://www.jstor.org/stable/3316006|3316006|2001-12-01|2001|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
The conventional view of fiscal zoning is that communities make land use accommodations only for nuisance entities that generate sufficient net fiscal benefits. State reforms that redistribute property tax revenues have been accused of undermining these decisions ex-post. This paper tests this critique by examining communities with nuclear power plants sited before the school finance equalization reforms of the 1970s and 1980s. Hedonic regressions reveal these reforms to disproportionately reduce housing prices in nuclear plant communities but increase them among neighboring areas. This pattern of interjurisdictional capitalization is consistent with the concerns raised by critics of these reforms.|Are Community-Nuisance Fiscal Zoning Arrangements Undermined by State Property Tax Reforms? Evidence from Nuclear Power Plants and School Finance Equalization|http://www.jstor.org/stable/24243719|24243719|2013-08-01|2013|['eng']|['Economics - Economic disciplines']|['Business & Economics', 'Business', 'Economics']
The paper deals with the problem of determining the number of components in a mixture model. We take a Bayesian non-parametric approach and adopt a hierarchical model with a suitable non-parametric prior for the latent structure. A commonly used model for such a problem is the mixture of Dirichlet process model. Here, we replace the Dirichlet process with a more general non-parametric prior obtained from a generalized gamma process. The basic feature of this model is that it yields a partition structure for the latent variables which is of Gibbs type. This relates to the well-known (exchangeable) product partition models. If compared with the usual mixture of Dirichlet process model the advantage of the generalization that we are examining relies on the availability of an additional parameter σ belonging to the interval (0,1): it is shown that such a parameter greatly influences the clustering behaviour of the model. A value of σ that is close to 1 generates a large number of clusters, most of which are of small size. Then, a reinforcement mechanism which is driven by σ acts on the mass allocation by penalizing clusters of small size and favouring those few groups containing a large number of elements. These features turn out to be very useful in the context of mixture modelling. Since it is difficult to specify a priori the reinforcement rate, it is reasonable to specify a prior for σ. Hence, the strength of the reinforcement mechanism is controlled by the data.|Controlling the Reinforcement in Bayesian Non-Parametric Mixture Models|http://www.jstor.org/stable/4623292|4623292|2007-01-01|2007|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Class prediction based on high-dimensional features has received a great deal of attention in many areas of application. For example, biologists are interested in using microarray gene expression profiles for diagnosis or prognosis of a certain disease (e.g., cancer). For computational and other reasons, it is necessary to select a subset of features before fitting a statistical model, by evaluating how strongly the features are related to the response. However, such a feature selection procedure will result in overconfident predictive probabilities for future cases, because the signal-to-noise ratio in the retained features is exacerbated by the feature selection. In this article we develop a hierarchical Bayesian classification method that can correct for this feature selection bias. Our method, which we term bias-corrected Bayesian classification with selected features (BCBCSF), uses the partial information from the feature selection procedure, in addition to the retained features, to form a correct (unbiased) posterior distribution of certain hyperparameters in the hierarchical Bayesian model that control the signal-to-noise ratio of the dataset. We take a Markov chain Monte Carlo (MCMC) approach to inferring the model parameters. We then use MCMC samples to make predictions for future cases. Because of the simplicity of the models, the inferred parameters from MCMC are easy to interpret, and the computation is very fast. Simulation studies and tests with two real microarray datasets related to complex human diseases show that our BCBCSF method provides better predictions than two widely used high-dimensional classification methods, prediction analysis for microarrays and diagonal linear discriminant analysis. The R package BCBCSF for the method described here is available from http://math.usask.ca/~longhai/software/BCBCSF and CRAN.|Bias-Corrected Hierarchical Bayesian Classification With a Selected Subset of High-Dimensional Features|http://www.jstor.org/stable/23239656|23239656|2012-03-01|2012|['eng']|['Applied sciences - Engineering', 'Information science - Coding theory']|['Science & Mathematics', 'Statistics']
Correlated count data arise often in practice, especially in repeated measures situations or instances in which observations are collected over time. In this paper, we consider a parametric model for a time series of counts by constructing a likelihood-based version of a model similar to that of Zeger (1988, Biometrika 75, 621-629). The model has the advantage of incorporating both overdispersion and autocorrelation. We consider a Bayesian approach and propose a class of informative prior distributions for the model parameters that are useful for prediction. The prior specification is motivated from the notion of the existence of data from similar previous studies, called historical data, which is then quantified into a prior distribution for the current study. We derive the Bayesian predictive distribution and use a Bayesian criterion, called the predictive L measure, for assessing the predictions for a given time series model. The distribution of the predictive L measure is also derived, which will enable us to compare the predictive ability for each model under consideration. Our methodology is motivated by a real data set involving yearly pollen counts, which is examined in some detail.|Bayesian Predictive Inference for Time Series Count Data|http://www.jstor.org/stable/2676908|2676908|2000-09-01|2000|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
An important problem in econometrics and marketing is to infer the causal impact that a designed market intervention has exerted on an outcome metric over time. This paper proposes to infer causal impact on the basis of a diffusion-regression state-space model that predicts the counterfactual market response in a synthetic control that would have occurred had no intervention taken place. In contrast to classical difference-in-differences schemes, state-space models make it possible to (i) infer the temporal evolution of attributable impact, (ii) incorporate empirical priors on the parameters in a fully Bayesian treatment, and (iii) flexibly accommodate multiple sources of variation, including local trends, seasonality and the time-varying influence of contemporaneous covariates. Using a Markov chain Monte Carlo algorithm for posterior inference, we illustrate the statistical properties of our approach on simulated data. We then demonstrate its practical utility by estimating the causal effect of an online advertising campaign on search-related site visits. We discuss the strengths and limitations of state-space models in enabling causal attribution in those settings where a randomised experiment is unavailable. The CausalImpact R package provides an implementation of our approach.|INFERRING CAUSAL IMPACT USING BAYESIAN STRUCTURAL TIME-SERIES MODELS|http://www.jstor.org/stable/24522418|24522418|2015-03-01|2015|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
The paper estimates an index of coincident economic indicators for the US economy by using time series with different frequencies of observation (monthly and quarterly, possibly with missing values). The model that is considered is the dynamic factor model that was proposed by Stock and Watson, specified in the logarithms of the original variables and at the monthly frequency, which poses a problem of temporal aggregation with a non-linear observational constraint when quarterly time series are included. Our main methodological contribution is to provide an exact solution to this problem that hinges on conditional mode estimation by iteration of the extended Kalman filtering and smoothing equations. On the empirical side the contribution of the paper is to provide monthly estimates of quarterly indicators, among which is the gross domestic product, that are consistent with the quarterly totals. Two applications are considered: the first dealing with the construction of a coincident index for the US economy, whereas the second does the same with reference to the euro area.|Dynamic Factor Analysis with Non-Linear Temporal Aggregation Constraints|http://www.jstor.org/stable/3592667|3592667|2006-01-01|2006|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
We study several theoretical properties of Jeffreys's prior for binomial regression models. We show that Jeffreys's prior is symmetric and unimodal for a class of binomial regression models. We characterize the tail behavior of Jeffreys's prior by comparing it with the multivariate t and normal distributions under the commonly used logistic, probit, and complementary log-log regression models. We also show that the prior and posterior normalizing constants under Jeffreys's prior are linear transformation-invariant in the covariates. We further establish an interesting theoretical connection between the Bayes information criterion and the induced dimension penalty term using Jeffreys's prior for binomial regression models with general links in variable selection problems. Moreover, we develop an importance sampling algorithm for carrying out prior and posterior computations under Jeffreys's prior. We analyze a real data set to illustrate the proposed methodology.|Properties and Implementation of Jeffreys's Prior in Binomial Regression Models|http://www.jstor.org/stable/27640213|27640213|2008-12-01|2008|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Questions that use a discrete ratings scale are commonplace in survey research. Examples in marketing include customer satisfaction measurement and purchase intention. Survey research practitioners have long commented that respondents vary in their usage of the scale: Common patterns include using only the middle of the scale or using the upper or lower end. These differences in scale usage can impart biases to correlation and regression analyses. To capture scale usage differences, we developed a new model with individual scale and location effects and a discrete outcome variable. We model the joint distribution of all ratings scale responses rather than specific univariate conditional distributions as in the ordinal probit model. We apply our model to a customer satisfaction survey and show that the correlation inferences are much different once proper adjustments are made for the discreteness of the data and scale usage. We also show that our adjusted or latent ratings scale is more closely related to actual purchase behavior.|Overcoming Scale Usage Heterogeneity: A Bayesian Hierarchical Approach|http://www.jstor.org/stable/2670337|2670337|2001-03-01|2001|['eng']|['Applied sciences - Engineering', 'Information science - Coding theory']|['Science & Mathematics', 'Statistics']
We use saddlepoint approximation to derive credible intervals for Bayesian wavelet regression estimates. Simulations show that the resulting intervals perform better than the best existing method.|Posterior Probability Intervals in Bayesian Wavelet Estimation|http://www.jstor.org/stable/20441116|20441116|2004-06-01|2004|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Likelihood inference for discretely observed Markov jump processes with finite state space is investigated. The existence and uniqueness of the maximum likelihood estimator of the intensity matrix are investigated. This topic is closely related to the imbedding problem for Markov chains. It is demonstrated that the maximum likelihood estimator can be found either by the EM algorithm or by a Markov chain Monte Carlo procedure. When the maximum likelihood estimator does not exist, an estimator can be obtained by using a penalized likelihood function or by the Markov chain Monte Carlo procedure with a suitable prior. The methodology and its implementation are illustrated by examples and simulation studies.|Statistical Inference for Discretely Observed Markov Jump Processes|http://www.jstor.org/stable/3647667|3647667|2005-01-01|2005|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
It is very challenging to select informative features from tens of thousands of measured features in high-throughput data analysis. Recently, several parametric/regression models have been developed utilizing the gene network information to select genes or pathways strongly associated with a clinical/biological outcome. Alternatively, in this paper, we propose a non-parametric Bayesian model for gene selection incorporating network information. In addition to identifying genes that have a strong association with a clinical outcome, our model can select genes with particular expressional behavior, in which case the regression models are not directly applicable. We show that our proposed model is equivalent to an infinity mixture model for which we develop a posterior computation algorithm based on Markov chain Monte Carlo (MCMC) methods. We also propose two fast computing algorithms that approximate the posterior simulation with good accuracy but relatively low computational cost. We illustrate our methods on simulation studies and the analysis of Spellman yeast cell cycle microarray data.|A BAYESIAN NONPARAMETRIC MIXTURE MODEL FOR SELECTING GENES AND GENE SUBNETWORKS|http://www.jstor.org/stable/24522085|24522085|2014-06-01|2014|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
"In order to make a missing at random (MAR) or ignorability assumption realistic, auxiliary covariates are often required. However, the auxiliary covariates are not desired in the model for inference. Typical multiple imputation approaches do not assume that the imputation model marginalizes to the inference model. This has been termed ""uncongenial"" [Meng (1994, Statistical Science 9, 538–558)]. In order to make the two models congenial (or compatible), we would rather not assume a parametric model for the marginal distribution of the auxiliary covariates, but we typically do not have enough data to estimate the joint distribution well non-parametrically. In addition, when the imputation model uses a non-linear link function (e.g., the logistic link for a binary response), the marginalization over the auxiliary covariates to derive the inference model typically results in a difficult to interpret form for the effect of covariates. In this article, we propose a fully Bayesian approach to ensure that the models are compatible for incomplete longitudinal data by embedding an interpretable inference model within an imputation model and that also addresses the two complications described above. We evaluate the approach via simulations and implement it on a recent clinical trial."|Fully Bayesian Inference under Ignorable Missingness in the Presence of Auxiliary Covariates|http://www.jstor.org/stable/24537888|24537888|2014-03-01|2014|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Item response theory models for roll-call voting data provide political scientists with parsimonious descriptions of political actors' relative preferences. However, models using only voting data tend to obscure variation in preferences across different issues due to identification and labeling problems that arise in multidimensional scaling models. We propose a new approach to using sources of metadata about votes to estimate the degree to which those votes are about common issues. We demonstrate our approach with votes and opinion texts from the U.S. Supreme Court, using latent Dirichlet allocation to discover the extent to which different issues were at stake in different cases and estimating justice preferences within each of those issues. This approach can be applied using a variety of unsupervised and supervised topic models for text, community detection models for networks, or any other tool capable of generating discrete or mixture categorization of subject matter from relevant vote-specific metadata.|Scaling Politically Meaningful Dimensions Using Texts and Votes|http://www.jstor.org/stable/24363519|24363519|2014-07-01|2014|['eng']|['Mathematics - Applied mathematics', 'Philosophy - Logic']|['Political Science', 'American Studies', 'Social Sciences', 'Area Studies']
The elastic net procedure is a form of regularized optimization for linear regression that provides a bridge between ridge regression and the lasso. The estimate that it produces can be viewed as a Bayesian posterior mode under a prior distribution implied by the form of the elastic net penalty. This article broadens the scope of the Bayesian connection by providing a complete characterization of a class of prior distributions that generate the elastic net estimate as the posterior mode. The resulting model-based framework allows for methodology that moves beyond exclusive use of the posterior mode by considering inference based on the full posterior distribution. Two characterizations of the class of prior distributions are introduced: a properly normalized, direct characterization, which is shown to be conjugate for linear regression models, and an alternate representation as a scale mixture of normal distributions. Prior distributions are proposed for the regularization parameters, resulting in an infinite mixture of elastic net regression models that allows for adaptive, data-based shrinkage of the regression coefficients. Posterior inference is easily achieved using Markov chain Monte Carlo (MCMC) methods. Uncertainty about model specification is addressed from a Bayesian perspective by assigning prior probabilities to all possible models. Corresponding computational approaches are described. Software for implementing the MCMC methods described in this article, written in C++ with an R package interface, is available at http://www.stat.osu.edu/~hans/software/.|Elastic Net Regression Modeling With the Orthant Normal Prior|http://www.jstor.org/stable/23239545|23239545|2011-12-01|2011|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Much politico-economic research on individuals' preferences is cross-sectional and does not model dynamic aspects of preference or attitude formation. I present a Bayesian dynamic panel model, which facilitates the analysis of repeated preferences using individual-level panel data. My model deals with three problems. First, I explicitly include feedback from previous preferences taking into account that available survey measures of preferences are categorical. Second, I model individuals' initial conditions when entering the panel as resulting from observed and unobserved individual attributes. Third, I capture unobserved individual preference heterogeneity both via standard parametric random effects and a robust alternative based on Bayesian nonparametric density estimation. I use this model to analyze the impact of income and wealth on preferences for government intervention using the British Household Panel Study from 1991 to 2007.|Modeling Dynamic Preferences: A Bayesian Robust Dynamic Latent Ordered Probit Model|http://www.jstor.org/stable/24572664|24572664|2013-07-01|2013|['eng']|['Mathematics - Applied mathematics']|['Political Science', 'Social Sciences']
"Bayesian methods furnish an attractive approach to inference in generalized linear mixed models. In the absence of subjective prior information for the random-effect variance components, these analyses are typically conducted using either the standard invariant prior for normal responses or diffuse conjugate priors. Previous work has pointed out serious difficulties with both strategies, and we show here that as in normal mixed models, the standard invariant prior leads to an improper posterior distribution for generalized linear mixed models. This article proposes and investigates two alternative reference (i.e., ""objective"" or ""noninformative"") priors: an approximate uniform shrinkage prior and an approximate Jeffreys's prior. We give conditions for the existence of the posterior distribution under any prior for the variance components in conjunction with a uniform prior for the fixed effects. The approximate uniform shrinkage prior is shown to satisfy these conditions for several families of distributions, in some cases under mild constraints on the data. Simulation studies conducted using a logit-normal model reveal that the approximate uniform shrinkage prior improves substantially on a plug-in empirical Bayes rule and fully Bayesian methods using diffuse conjugate specifications. The methodology is illustrated on a seizure dataset."|Reference Bayesian Methods for Generalized Linear Mixed Models|http://www.jstor.org/stable/2669540|2669540|2000-03-01|2000|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
We demonstrate how case influence analysis, commonly used in regression, can be applied to Bayesian hierarchical models. Draws from the joint posterior distribution of parameters are importance weighted to reflect the effect of deleting each observation in turn; the ensuing changes in the posterior distribution of each parameter are displayed graphically. The procedure is particularly useful when drawing a sample from the posterior distribution requires extensive calculations (as with a Markov Chain Monte Carlo sampler). The structure of hierarchical models, and other models with local dependence, makes the importance weights inexpensive to calculate with little additional programming. Some new alternative weighting schemes are described that extend the range of problems in which reweighting can be used to assess influence. Applications to a growth curve model and a complex hierarchical model for opinion data are described. Our focus on case influence on parameters is complementary to other work that measures influence by distances between posterior or predictive distributions.|Case Influence Analysis in Bayesian Inference|http://www.jstor.org/stable/1390736|1390736|1997-09-01|1997|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Science & Mathematics', 'Computer Science', 'Statistics']
A discrete data augmentation scheme together with two different parameterizations yields two Gibbs samplers for sampling from the posterior distribution of the hyperparameters of the Dirichlet-multinomial hierarchical model under a default prior distribution. The finite-state space nature of this data augmentation permits us to construct two perfect samplers using bounding chains that take advantage of monotonicity and anti-monotonicity in the target posterior distribution, but both are impractically slow. We demonstrate that a composite algorithm that strategically alternates between the two samplers' updates can be substantially faster than either individually. The speed gains come because the composite algorithm takes a divide-and-conquer approach in which one update quickly shrinks the bounding set for the augmented data, and the other update immediately coalesces on the parameter, once the augmented-data bounding set is a singleton. We theoretically bound the expected time until coalescence for the composite algorithm, and show via simulation that the theoretical bounds can be close to actual performance.|Practical perfect sampling using composite bounding chains: the Dirichlet-multinomial model|http://www.jstor.org/stable/43305572|43305572|2013-12-01|2013|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
The authors propose a general model that includes the effects of discrete and continuous heterogeneity as well as self-stated and derived attribute importance in hybrid conjoint studies. Rather than use the self-stated importances as prior information, as has been done in several previous approaches, the authors consider them data and therefore include them in the formulation of the likelihood, which helps investigate the relationship of self-stated and derived importances at the individual level. The authors formulate several special cases of the model and estimate them using the Gibbs sampler. The authors reanalyze Srinivasan and Park's (1997) data and show that the current model predicts real choices better than competing models do. The posterior credible intervals of the predictions of models with the different heterogeneity specifications overlap, so there is no clear superior specification of heterogeneity. However, when different sources of data are used-that is, full profile evaluations, self-stated importances, or both-clear differences arise in the accuracy of predictions. Moreover, the authors find that including the self-stated importances in the likelihood leads to much better predictions than does considering them prior information.|Bayesian Prediction in Hybrid Conjoint Analysis|http://www.jstor.org/stable/1558490|1558490|2002-05-01|2002|['eng']|['Applied sciences - Engineering']|['Business', 'Business & Economics Collection', 'Marketing & Advertising']
Research on methods of meta-analysis (the synthesis of related study results) has dealt with many simple study indices, but less attention has been paid to the issue of summarizing regression slopes. In part this is because of the many complications that arise when real sets of regression models are accumulated. We outline the complexities involved in synthesizing slopes, describe existing methods of analysis and present a multivariate generalized least squares approach to the synthesis of regression slopes.|The Synthesis of Regression Slopes in Meta-Analysis|http://www.jstor.org/stable/27645847|27645847|2007-08-01|2007|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science and Mathematics', 'Statistics']
We propose new Metropolis-Hastings algorithms for sampling from multimodal distributions on ${\cal R}^n$. Tjelmeland and Hegstad have obtained direct mode jumping proposals by optimization within Metropolis-Hastings updates and different proposals for 'forward' and 'backward' steps. We generalize their scheme by allowing the probability distribution for forward and backward kernels to depend on the current state. We use the new setting to combine mode jumping proposals and proposals from a prior approximation. We obtain that the frequency of proposals from the different proposal kernels is automatically adjusted to their quality. Mode jumping proposals include local optimizations. When combining this with a prior approximation it is tempting to use local optimization results not only for mode jumping proposals but also to improve the prior approximation. We show how this idea can be implemented. The resulting algorithm is adaptive but has a Markov structure. We evaluate the effectiveness of the proposed algorithms in two simulation examples.|On the Use of Local Optimizations within Metropolis-Hastings Updates|http://www.jstor.org/stable/3647534|3647534|2004-01-01|2004|['eng']|['Applied sciences - Engineering', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
We consider the problem of fitting a statistical model to 30 years of sea surface temperature records collected over a large portion of the Northern Atlantic. The observations were collected sparsely in space and time with different levels of accuracy. The purpose of the model is to produce an atlas of oceanic properties, including climatological mean fields, estimates of historical trends, and a spatio-temporal reconstruction of the anomalies, i.e., the transient deviations from the climatological mean. These products are of interest to climate change and climate variability research, numerical modeling, and remote sensing analyses. Our model improves upon the current tools used by oceanographers in that it constructs instantaneous temperature fields before averaging them into the climatology, thus giving equal weight to all years in the time frame, regardless of the temporal distribution of data. It also accounts for nonisotropic and nonstationary space and time dependencies, owing to its use of discrete process convolutions. Particular attention is given to the handling of massive datasets such as the one under study. This is achieved by considering compact support kernels that allow an efficient parallelization of the Markov chain Monte Carlo method used in the estimation of the model parameters. Resulting monthly climatologies are compared with those of the World Ocean Atlas 2001, version 2. Different water masses appear better separated in our climatology, and a close link emerges between the kernels' shape and the dominating patterns of ocean currents. The subpolar and the temperate North Atlantic display opposite trends, with the former mainly cooling over the years and the latter mainly warming, especially in the Gulf Stream region. Long-term changes in annual cycles are also detected. As in any hierarchical Bayesian model, parameter estimates come with credibility intervals, which are useful to compare results with other approaches and detect areas where sampling campaigns are needed the most.|A Spatio-Temporal Model for Mean, Anomaly, and Trend Fields of North Atlantic Sea Surface Temperature [with Comments]|http://www.jstor.org/stable/40591896|40591896|2009-03-01|2009|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
This paper proposes a Bayesian approach for estimating and smoothing the baseline hazard in a discrete time hazard model. The hazard model is specified as a multiperiod probit model and estimated using a Gibbs sampler with data augmentation. The baseline hazard specification is smoothed using the smoothness priors introduced by Shiller (1973). The methods proposed in this paper are then used to study the effect of Canadian Unemployment Insurance eligibility rules on employment durations from New Brunswick, Canada.|Bayesian Estimation and Smoothing of the Baseline Hazard in Discrete Time Duration Models|http://www.jstor.org/stable/2646662|2646662|2000-11-01|2000|['eng']|['Mathematics - Applied mathematics']|['Business', 'Business & Economics Collection', 'Economics']
This paper considers the interaction between place of residence and maternal characteristics for infant health outcomes. It illustrates how indices of performance specified in terms of the varying incidence of adverse events or the effect of maternal risk on such outcomes may be sensitive to sampling unreliability, and how this may be clarified in a Bayesian analysis. An adjustment for sampling errors, together with a correction for the interplay of various risk factors and the effect of area deprivation, may considerably modify impressions of relative performance gained from comparing simple perinatal mortality rates.|A Multilevel Model for Infant Health Outcomes: Maternal Risk Factors and Geographic Variation|http://www.jstor.org/stable/2988432|2988432|1998-01-01|1998|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
This paper presents estimates of delayed childbearing and permanent childlessness in the United States and the determinants of those phenomena. The estimates are derived by fitting the Coale-McNeil marriage model to survey data on age at first birth and by letting the parameters of the model depend on covariates. Substantively, the results provide evidence that the low first birth fertility rates experienced in the 1970s were due to both delayed childbearing and to increasing levels of permanent childlessness. The results also indicate that (a) delayed childbearing is less prevalent among black women than among nonblack women; (b) education is an important determinant of delayed childbearing whose influence on this phenomenon seems to be increasing across cohorts; (c) education is positively associated with heterogeneity among women in their age at first birth; (d) the dispersion of age at first birth is increasing across cohorts; (e) race has an insignificant effect on childlessness; and (f) education is positively associated with childlessness, with the effect of education increasing and reaching strikingly high levels for the most recent cohorts.|What are the Determinants of Delayed Childbearing and Permanent Childlessness in the United States?|http://www.jstor.org/stable/2060917|2060917|1984-11-01|1984|['eng']|['Mathematics - Mathematical logic']|['Population Studies', 'Social Sciences']
A bivariate and unimodal distribution is introduced to model an unconventionally distributed data set collected by the Forensic Science Service. This family of distributions allows for a different kurtosis in each orthogonal direction and has a constructive rather than probability density function definition, making conventional inference impossible. However, the construction and inference work well with a Bayesian Markov chain Monte Carlo analysis.|Bivariate Kurtotic Distributions of Garment Fibre Data|http://www.jstor.org/stable/3592555|3592555|2003-01-01|2003|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
1. Most wild animal populations are subjected to many perturbations, including environmental forcing and anthropogenic mortality. How population size varies in response to these perturbations largely depends on life-history strategy and density regulation. 2. Using the mid-continent population of redhead Aythya americana (a North American diving duck), we investigated the population response to two major perturbations, changes in breeding habitat availability (number of ponds in the study landscape) and changes in harvest regulations directed at managing mortality patterns (bag limit). We used three types of data collected at the continental scale (capture-recovery, population surveys and age-and sex ratios in the harvest) and combined them into integrated population models to assess the interaction between density dependence and the effect of perturbations. 3. We observed a two-way interaction between the effects on fecundity of pond number and population density. Hatch-year female survival was also density dependent. Matrix modelling showed that population booms could occur after especially wet years. However, the effect of moderate variation in pond number was generally offset by density dependence the following year. 4. Mortality patterns were insensitive to changes in harvest regulations and, in males at least, insensitive to density dependence as well. We discuss potential mechanisms for compensation of hunting mortality as well as possible confounding factors. 5. Our results illustrate the interplay of density dependence and environmental variation both shaping population dynamics in a harvested species, which could be generalized to help guide the dual management of habitat and harvest regulations.|Demographic response to perturbations: the role of compensatory density dependence in a North American duck under variable harvest regulations and changing habitat|http://www.jstor.org/stable/41682512|41682512|2012-09-01|2012|['eng']|['Biological sciences - Ecology']|['Biological Sciences', 'Ecology & Evolutionary Biology', 'Science and Mathematics']
A hierarchical logistic regression model is proposed for studying data with group structure and a binary response variable. The group structure is defined by the presence of micro observations embedded within contexts (macro observations), and the specification is at both of these levels. At the first (micro) level, the usual logistic regression model is defined for each context. The same regressors are used in each context, but the micro regression coefficients are free to vary over contexts. At the second level, the micro coefficients are treated as functions of macro regressors. An empirical Bayes estimation procedure is proposed for estimating the micro and macro coefficients. Explicit formulas are provided that are computationally feasible for large-scale data analyses; these include an algorithm for finding the maximum likelihood estimates of the covariance components representing within- and between-macro-equation error variability. The methodology is applied to World Fertility Survey data, with individuals viewed as micro observations and countries as macro observations.|The Hierarchical Logistic Regression Model for Multilevel Analysis|http://www.jstor.org/stable/2288464|2288464|1985-09-01|1985|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Let X has a p-dimensional normal distribution with mean vector θ and identity covariance matrix I. In a compound decision problem consisting of squared-error estimation of θ, Strawderman (1971) placed a Beta(α, 1) prior distribution on a normal class of priors to produce a family of Bayes minimax estimators. We propose an incomplete Gamma (α, β) prior distribution on the same normal class of priors to produce a larger family of Bayes minimax estimators. We present the results of a Monte Carlo study to demonstrate the reduced risk of our estimators in comparison with the Strawderman estimators when θ is away from the zero vector. /// Soit X une variable aléatoire de Laplace-Gauss de moyenne θ et de covariance I dans l'espace à p dimensions. Afin d'estimer θ en minimisant l'erreur quadratique moyenne, Strawderman (1971) a imposé une loi Beta(α, 1) sur la classe des lois normales a priori, ce qui conduit à une famille d'estimateurs de Bayes minimax. On peut élargir davantage cette famille en se servant plutôt d'une loi Gamma(α, β) incomplète. Lorsque θ est suffisamment éloigné de l'origine, il en résulte une réduction du risque qu'une étude de Monte Carlo nous permet ici d'évaluer numériquement.|A Family of Admissible Minimax Estimators of the Mean of a Multivariate, Normal Distribution|http://www.jstor.org/stable/3314801|3314801|1986-09-01|1986|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
Many animal life cycles involve movements among different habitats to fulfill varying resource demands. There are inherent costs associated with such movements, and the decision to leave or stay at a given location ought to be motivated by the benefits associated with potential target habitats. Because moVement patterns, especially those associated with reproduction, can have important implications for the success (survival, reproduction) of individual animals, and therefore a population's dynamics, it is important to identify and understand their sources of variation (environmental and individual). Here, using a markrecapture, multistate modeling approach, we investigated a set of a priori hypotheses regarding sources and patterns of variation in breeding-colony attendance for Weddell seal (Leptony chotes weddellii) females on sabbatical from pup production. For such females, colony attendance might be motivated by prédation avoidance and positive social interactions related to reproduction, but some costs, such as reduced foraging opportunities or aggressive interactions with conspecifics, might also exist. We expected these benefits and costs to vary with a female's condition and the environment. Results revealed that the probability of being absent from colonies was higher (1) in years when the extent of local sea ice was larger, (2) for the youngest and oldest individuals, and (3) for females with less reproductive experience. We also found substantial levels of residual individual heterogeneity in these rates. Based on our a priori predictions, we postulate that the decision to attend breeding colonies or not is directly influenced by an individual's physiological condition, as well as by the ice-covered distance to good foraging areas, availability of predator-free haul-out sites, and the level of negative interactions with conspecifics inside colonies. Our results support the idea that in iteroparous species, and colonial animals in particular, seasonal and temporary movements from/to reproductive sites represent flexible behavioral strategies that can play an important role in coping with environmental variability.|Female Weddell seals show flexible strategies of colony attendance related to varying environmental conditions|http://www.jstor.org/stable/43495088|43495088|2015-02-01|2015|['eng']|['Biological sciences - Biology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
The paper provides a unifying famework for conducting Bayesian inference on the presence of seasonal and zero frequency unit roots in quarterly data. The main technique used is the analysis of posterior odds ratios. A new parameterisation is provided fo the model and the prior distributions implemented are discusses and justified. The analysis relies heavily on the application of a Gibbs sampling algorithm. Such techniques render the Bayesian approach more flexible and implementable, giving the applied researcher the possibility of specifying a vast range of prior distribution. The methods are applied to a set of United Kingdom quarterly series. Compared to previous studies, less evidence is found to support seasonal integration hypotheses.|BAYESIAN ANALYSIS OF INTEGRATION AT DIFFERENT FREQUENCIES IN QUARTERLY DATA|http://www.jstor.org/stable/23247792|23247792|1995-07-01|1995|['eng']|['Mathematics - Mathematical logic']|['Business & Economics', 'Business', 'Economics']
A Bayesian intensity model is presented for studying a bioassay problem involving interval-censored tumour onset times, and without discretization of times of death. Both tumour lethality and base-line hazard rates are estimated in the absence of cause-of-death information. Markov chain Monte Carlo methods are used in the numerical estimation, and sophisticated group updating algorithms are applied to achieve reasonable convergence properties. This method was tried on the rat tumorigenicity data that have previously been analysed by Ahn, Moon and Kodell, and our results seem to be more realistic.|Tumour Incidence, Prevalence and Lethality Estimation in the Absence of Cause-of-Death Information|http://www.jstor.org/stable/3592577|3592577|2004-01-01|2004|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
For testing nested hypotheses from a Bayesian standpoint, a desirable condition is that the prior for the alternative model concentrates mass around the smaller, or null, model. For testing independence in contingency tables, the intrinsic priors satisfy this requirement. Furthermore, the degree of concentration of the priors is controlled by a discrete parameter, t, the training sample size, which plays an important role in the resulting answer. In this article we report on the robustness of the tests of independence for small or moderate sample sizes in contingency tables with respect to intrinsic priors with different degrees of concentration around the null. We compare these tests to frequentist tests and other robust Bayes tests. For large sample sizes, robustness is achieved because the intrinsic Bayesian tests are consistent. Examples using real and simulated data are given. Supplemental materials (technical details and data sets) are available online.|Assessing Robustness of Intrinsic Tests of Independence in Two-Way Contingency Tables|http://www.jstor.org/stable/40592293|40592293|2009-09-01|2009|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
For many consumer packaged goods products, researchers have documented inertia in brand choice, a form of persistence whereby consumers have a higher probability of choosing a product that they have purchased in the past. We show that the finding of inertia is robust to flexible controls for preference heterogeneity and not due to autocorrelated taste shocks. We explore three economic explanations for the observed structural state dependence: preference changes due to past purchases or consumption experiences which induce a form of loyalty, search, and learning. Our data are consistent with loyalty, but not with search or learning. This distinction is important for policy analysis, because the alternative sources of inertia imply qualitative differences in firm's pricing incentives and lead to quantitatively different equilibrium pricing outcomes.|State dependence and alternative explanations for consumer inertia|http://www.jstor.org/stable/25746036|25746036|2010-10-01|2010|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Business', 'Economics']
The usual linear statistical model is reanalyzed using Bayesian methods and the concept of exchangeability. The general method is illustrated by applications to two-factor experimental designs and multiple regression.|Bayes Estimates for the Linear Model|http://www.jstor.org/stable/2985048|2985048|1972-01-01|1972|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
A Bayesian multivariate adaptive regression spline fitting approach is used to model univariate and multivariate survival data with censoring. The possible models contain the proportional hazards model as a subclass and automatically detect departures from this. A reversible jump Markov chain Monte Carlo algorithm is described to obtain the estimate of the hazard function as well as the survival curve.|Bayesian Survival Analysis Using a MARS Model|http://www.jstor.org/stable/2533722|2533722|1999-12-01|1999|['eng']|['Mathematics - Mathematical analysis', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Our focus is on realistically modeling and forecasting dynamic networks of face-to-face contacts among individuals. Important aspects of such data that lead to problems with current methods include the tendency of the contacts to move between periods of slow and rapid changes, and the dynamic heterogeneity in the actors' connectivity behaviors. Motivated by this application, we develop a novel method for Locally Adaptive DYnamic (LADY) network inference. The proposed model relies on a dynamic latent space representation in which each actor's position evolves in time via stochastic differential equations. Using a state-space representation for these stochastic processes and Pólya-gamma data augmentation, we develop an efficient MCMC algorithm for posterior inference along with tractable procedures for online updating and forecasting of future networks. We evaluate performance in simulation studies, and consider an application to face-to-face contacts among individuals in a primary school.|LOCALLY ADAPTIVE DYNAMIC NETWORKS|http://www.jstor.org/stable/44252232|44252232|2016-12-01|2016|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical objects']|['Mathematics', 'Science & Mathematics', 'Statistics']
"This paper is a continuation of a paper in the Annals of Statistics (1976), 4 1159-1189 where, among other things, a Bayesian approach to testing independence in contingency tables was developed. Our first purpose now, after allowing for an improvement in the previous theory (which also has repercussions on earlier work on the multinomial), is to give extensive numerical results for two-dimensional tables, both sparse and nonsparse. We deal with the statistics X2, Λ (the likelihood-ratio statistic), a slight transformation G of the Type II likelihood ratio, and the number of repeats within cells. The latter has approximately a Poisson distribution for sparse tables. Some of the ""asymptotic"" distributions are surprisingly good down to exceedingly small tail-area probabilities, as in the previous ""mixed Dirichlet"" approach to multinomial distributions (J. Roy. Statist. Soc. B. 1967, 29 399-431; J. Amer. Statist. Assoc. 1974, 69 711-720). The approach leads to a quantitative measure of the amount of evidence concerning independence provided by the marginal totals, and this amount is found to be small when neither the row totals nor the column totals are very ""rough"" and the two sets of totals are not both very flat. For Model 3 (all margins fixed), the relationship is examined between the Bayes factor against independence and its tail-area probability."|On the Application of Symmetric Dirichlet Distributions and Their Mixtures to Contingency Tables, Part II|http://www.jstor.org/stable/2240935|2240935|1980-11-01|1980|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
We review the main identification strategies and empirical evidence on the role of expectations in the New Keynesian Phillips curve, paying particular attention to the issue of weak identification. Our goal is to provide a clear understanding of the role of expectations that integrates across the different papers and specifications in the literature. We discuss the properties of the various limited-information econometric methods used in the literature and provide explanations of why they produce conflicting results. Using a common dataset and a flexible empirical approach, we find that researchers are faced with substantial specification uncertainty, as different combinations of various a priori reasonable specification choices give rise to a vast set of point estimates. Moreover, given a specification, estimation is subject to considerable sampling uncertainty due to weak identification. We highlight the assumptions that seem to matter most for identification and the configuration of point estimates. We conclude that the literature has reached a limit on how much can be learned about the New Keynesian Phillips curve from aggregate macroeconomic time series. New identification approaches and new datasets are needed to reach an empirical consensus.|Empirical Evidence on Inflation Expectations in the New Keynesian Phillips Curve|http://www.jstor.org/stable/24433859|24433859|2014-03-01|2014|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Business', 'Economics']
The conditional intensity function of a point process is a useful tool for generating probability forecasts of earthquakes. The epidemic-type aftershock sequence (ETAS) model is defined by a conditional intensity function, and the corresponding point process is equivalent to a branching process, assuming that an earthquake generates a cluster of offspring earthquakes (triggered earthquakes or so-called aftershocks). Further, the size of the first-generation cluster depends on the magnitude of the triggering (parent) earthquake. The ETAS model provides a good fit to standard earthquake occurrences. However, there are nonstandard earthquake series that appear under transient stress changes caused by aseismic forces such as volcanic magma or fluid intrusions. These events trigger transient nonstandard earthquake swarms, and they are poorly fitted by the stationary ETAS model. In this study, we examine nonstationary extensions of the ETAS model that cover nonstandard cases. These models allow the parameters to be time-dependent and can be estimated by the empirical Bayes method. The best model is selected among the competing models to provide the inversion solutions of nonstationary changes. To address issues of the uniqueness and robustness of the inversion procedure, this method is demonstrated on an inland swarm activity induced by the 2011 Tohoku-Oki, Japan earthquake of magnitude 9.0.|NONSTATIONARY ETAS MODELS FOR NONSTANDARD EARTHQUAKES|http://www.jstor.org/stable/24522286|24522286|2014-09-01|2014|['eng']|['Physical sciences - Astronomy']|['Mathematics', 'Science & Mathematics', 'Statistics']
This article uses a Bayesian hierarchical model to quantify the adverse health effects associated with in-utero exposure to methylmercury. By allowing for study-to-study as well as outcome-to-outcome variability, the approach provides a useful meta-analytic tool for multi-outcome, multi-study environmental risk assessments. The analysis presented here expands on the findings of a National Academy of Sciences (NAS) committee, charged with advising the United States Environmental Protection Agency (EPA) on an appropriate approach to conducting a risk assessment for methylmercury. The NAS committee, for which the senior author (Ryan) was a committee member, reviewed the findings from several conflicting studies and reported the results from a Bayesian hierarchical model that synthesized information across several studies and for several outcomes. Although the NAS committee did not suggest that the hierarchical model be used as the actual basis for a methylmercury risk assessment, the results from the model were used to justify and support the final recommendation that the risk analysis be based on data from a study conducted in the Faroe Islands, which had found an association between in-utero exposure to methylmercury and impaired neurological development. We consider a variety of statistical issues, but particularly sensitivity to model specification.|A Bayesian Hierarchical Model for Risk Assessment of Methylmercury|http://www.jstor.org/stable/1400546|1400546|2003-09-01|2003|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Agriculture', 'Statistics']
Penalized splines have become an increasingly popular tool for nonparametric smoothing because of their use of low-rank spline bases, which makes computations tractable while maintaining accuracy as good as smoothing splines. This article extends penalized spline methodology by both modeling the variance function nonparametrically and using a spatially adaptive smoothing parameter. This combination is needed for satisfactory inference and can be implemented effectively by Bayesian MCMC. The variance process controlling the spatially adaptive shrinkage of the mean and the variance of the heteroscedastic error process are modeled as log-penalized splines. We discuss the choice of priors and extensions of the methodology, in particular, to multivariate smoothing. A fully Bayesian approach provides the joint posterior distribution of all parameters, in particular, of the error standard deviation and penalty functions. MATLAB, C, and FORTRAN programs implementing our methodology are publicly available.|Spatially Adaptive Bayesian Penalized Splines with Heteroscedastic Errors|http://www.jstor.org/stable/27594243|27594243|2007-06-01|2007|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Computer Science', 'Statistics']
Density regression models allow the conditional distribution of the response given predictors to change flexibly over the predictor space. Such models are much more flexible than nonparametric mean regression models with nonparametric residual distributions, and are well supported in many applications. A rich variety of Bayesian methods have been proposed for density regression, but it is not clear whether such priors have full support so that any true data-generating model can be accurately approximated. This article develops a new class of density regression models that incorporate stochastic-ordering constraints which are natural when a response tends to increase or decrease monotonely with a predictor. Theory is developed showing large support. Methods are developed for hypothesis testing, with posterior computation relying on a simple Gibbs sampler. Frequentist properties are illustrated in a simulation study, and an epidemiology application is considered.|Bayesian isotonic density regression|http://www.jstor.org/stable/23076130|23076130|2011-09-01|2011|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics']|['Science and Mathematics', 'Statistics']
We propose a Bayesian framework in which the uncertainty about the half-life of deviations from purchasing power parity can be quantified. Based on the responses to a survey study, we propose a prior probability distribution for the half-life under the recent float intended to capture widely held views among economists. We derive the posterior probability distribution of the half-life under this consensus prior and confirm the presence of substantial uncertainty about the half-life. We provide for the first time a comprehensive formal evaluation of several nonnested hypotheses of economic interest, including Rogoff's (1996) claim that the half-life is contained in the range of 3 to 5 years. We find that no hypothesis receives strong support from the data.|Quantifying the Uncertainty about the Half-Life of Deviations from PPP|http://www.jstor.org/stable/4129246|4129246|2002-03-01|2002|['eng']|['Physical sciences - Astronomy', 'Mathematics - Mathematical logic']|['Business & Economics', 'Business', 'Economics']
In many chronic conditions, subjects alternate between an active and an inactive state, and sojourns into the active state may involve multiple lesions, infections, or other recurrences with different times of onset and resolution. We present a biologically interpretable model of such chronic recurrent conditions based on a queueing process. The model has a birth-death process describing recurrences and a semi-Markov process describing the alternation between active and inactive states, and can be fit to panel data that provide only a binary assessment of the active or inactive state at a series of discrete time points using a hidden Markov approach. We accommodate individual heterogeneity and covariates using a random effects model, and simulate the posterior distribution of unknowns using a Markov chain Monte Carlo algorithm. Application to a clinical trial of genital herpes shows how the method can characterize the biology of the disease and estimate treatment efficacy.|A Queueing Model for Chronic Recurrent Conditions under Panel Observation|http://www.jstor.org/stable/3695662|3695662|2005-03-01|2005|['eng']|['Health sciences - Medical conditions', 'Health sciences - Medical sciences']|['Science & Mathematics', 'Statistics']
This paper compares two methods of analyzing aggregate data that is classified by period and age. Because there is a linear relationship among age, period, and cohort, it is not possible to distinguish the separate effects without employing an identifying assumption. The first method, which is applied in the economics literature, assumes that period effects are orthogonal to a linear time trend. The second method, which is applied in the statistics literature, assumes that the effect parameters change gradually. Simulation results suggest that the performances of both methods are comparable. The results of applying the second method to household saving rates suggest that period effects had a negligible influence in the United States but considerable influence in Japan.|Age-Period-Cohort Decomposition of Aggregate Data: An Application to US and Japanese Household Saving Rates|http://www.jstor.org/stable/25146480|25146480|2006-11-01|2006|['eng']|['Applied sciences - Engineering', 'Physical sciences - Astronomy']|['Business & Economics', 'Business', 'Economics']
Studies of time-to-event are often conducted using follow-up sessions with subjects at risk. When these sessions must be widely spaced, their timing can significantly affect the efficiency of a study design. In this article we analyze the optimal timing of follow-up from a Bayesian decision theoretic standpoint. The article has two goals: (1) to develop the necessary distributional theory and computational approaches to determine optimal sequential and nonsequential follow-up schedules in the exponential case and (2) to demonstrate that unusually large gains in efficiency (more than threefold over the standard approach in the examples considered) can be achieved using our group sequential timing of follow-up. We hope that this striking illustration will encourage more systematic consideration of follow-up times in study designs. Specifically, we consider time-independent hazard rates. We derive posterior and predictive distributions in three scenarios: single follow-up time for the estimation of a single hazard rate, group-specific follow-up time for the comparison of hazard rates in two treatment groups or cohorts, and multiple follow-up times for a single hazard rate. We encounter a novel family of mixtures of gamma functions and characterize its moments, which play a critical role in the determination of optima. We then provide a solution to the optimal follow-up time in the single follow-up problem. We develop a practical and accurate approximation to the optimal solution as a function of prior hyperparameters that can be used to implement real time calculations and more complex sequential strategies. Finally, we consider the sequential choice of follow-up times. We discuss the general dynamic programming solution and illustrate it in the setting of a two-stage design.|Designing Follow-Up Times|http://www.jstor.org/stable/3085726|3085726|2002-09-01|2002|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
This study offers a simultaneous equations model of the birth process with seven endogenous variables: four birth inputs (maternal smoking, maternal drinking, first trimester prenatal care, and maternal weight gain) and three birth outputs (gestational age, birth length, and birth weight). The data are taken from the National Longitudinal Survey of Youth. Our analysis conditions on twenty-nine exogenous variables including four racial dummies to account for the widely cited racial differences in birth outputs. We find that there is sizeable correlation between the disturbances in the four input and three output equations and among output disturbances, and that results from our simultaneous equations model are substantially different from those using the single-equation approach. It appears that the High/Low Risk Birth Weight Puzzle remains unresolved under our modeling framework.|Bayesian Analysis of an Econometric Model of Birth Inputs and Outputs|http://www.jstor.org/stable/20007873|20007873|2003-08-01|2003|['eng']|['Physical sciences - Astronomy']|['Business', 'Business & Economics Collection', 'Economics', 'Population Studies', 'Social Sciences']
This article focuses on the widely studied question of whether the inclusion of indicators of real economic activity lowers the prediction mean squared error of forecasting models of U.S. consumer price inflation. We propose three variants of the bagging algorithm specifically designed for this type of forecasting problem and evaluate their empirical performance. Although bagging predictors in our application are clearly more accurate than equally weighted forecasts, median forecasts, ARM forecasts, AFTER forecasts, or Bayesian forecast averages based on one extra predictor at a time, they are generally about as accurate as the Bayesian shrinkage predictor, the ridge regression predictor, the iterated LASSO predictor, or the Bayesian model average predictor based on random subsets of extra predictors. Our results show that bagging can achieve large reductions in prediction mean-squared errors even in such challenging applications as inflation forecasting; however, bagging is not the only method capable of achieving such gains.|How Useful Is Bagging in Forecasting Economic Time Series? A Case Study of U.S. Consumer Price Inflation|http://www.jstor.org/stable/27640075|27640075|2008-06-01|2008|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
We propose a regression model for binary response data that places no structural restrictions on the link function except monotonicity and known location and scale. Predictors enter linearly. We demonstrate Bayesian inference calculations in this model. By modifying the Dirichlet process, we obtain a natural prior measure over this semiparametric model, and we use Polya sequence theory to formulate this measure in terms of a finite number of unobserved variables. We design a Markov chain Monte Carlo algorithm for posterior simulation and apply the methodology to data on radiotherapy treatments for cancer.|Bayesian Inference for Semiparametric Binary Regression|http://www.jstor.org/stable/2291390|2291390|1996-03-01|1996|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
This article examines the evidence of vote tampering in a District Justice election in Beaver County, Pennsylvania. An informal exploratory data analysis and a legal history are followed by a formal Bayesian model of the data from the vote count on election night and the recount completed 2 months later. The evidence suggests that persons unknown could have gained access to the boxes containing the paper ballots, and surprising patterns of changes in the counts support the inference that certain boxes were tampered with. Three methods are compared not only with respect to the overall matter of whether tampering occurred, but also with respect to which precincts were likely to have been tampered with, and to what extent. The results are generally consistent across methods. The Bayesian model is validated by using it on the data for a race (for Superior Court) in the same election in which vote tampering is not suspected. The results show that the model gives a predictive distribution of just a few votes uncertainty for the Superior Court race but of around 60 votes in the District Justice race, enough to swing the election. Technically, the computations involve a Markov chain Monte Carlo. Because it is not possible to observe how each individual ballot was counted each time, data augmentation is required to fill in a Markov matrix given both margins. The fact that both margins are given restricts the kinds of proposals that the chain considers.|Vote Tampering in a District Justice Election in Beaver County, PA|http://www.jstor.org/stable/2670289|2670289|2001-06-01|2001|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
In this article, we demonstrate how Gaussian Markov random field properties give large computational benefits and new opportunities for the Bayesian animal model. We make inference by computing the posteriors for important quantitative genetic variables. For the single-trait animal model, a nonsampling-based approximation is presented. For the mult it rait model, we set up a robust and fast Markov chain Monte Carlo algorithm. The proposed methodology was used to analyze quantitative genetic properties of morphological traits of a wild house sparrow population. Results for single-and multitrait models were compared.|Utilizing Gaussian Markov Random Field Properties of Bayesian Animal Models|http://www.jstor.org/stable/40962447|40962447|2010-09-01|2010|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Sampling units that do not answer a survey may dramatically affect the estimation results of interest. The response may even be conditional on the outcome of interest in the survey. If estimates are found using only those who responded, the estimate may be biased, known as nonresponse bias. We are interested in finding estimates of success rates from a survey. We begin by looking at two current Bayesian approaches to treating nonresponse in a hierarchical model. However, these approaches do not consider possible spatial correlations between domains for either success rate or response rate. We build a Bayesian hierarchical spatial model to explicitly estimate the success rate, response rate given success, and response rate given failure. The success rates in the domains of the survey are allowed to be spatially correlated. We also allow spatial dependence between domains in both response rate given success and response rate given failure. Spatial dependence is induced by a common latent spatial structure between the two conditional response rates. We use the 1998 Missouri Turkey Hunting Survey to illustrate this methodology. We find significant spatial correlation in the success rates and incorporating nonrespondents has an impact on the success rate estimates.|Hierarchical Bayesian Modeling in Dichotomous Processes in the Presence of Nonresponse|http://www.jstor.org/stable/3695551|3695551|2004-03-01|2004|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
In hierarchical Bayesian modeling of normal means, it is common to complete the prior specification by choosing a constant prior density for unmodeled hyperparameters (e.g., variances and highest-level means). This common practice often results in an inadequate overall prior, inadequate in the sense that estimators resulting from its use can be inadmissible under quadratic loss. In this paper, hierarchical priors for normal means are categorized in terms of admissibility and inadmissibility of resulting estimators for a quite general scenario. The Jeffreys prior for the hyper-variance and a shrinkage prior for the hypermeans are recommended as admissible alternatives. Incidental to this analysis is presentation of the conditions under which the (generally improper) priors result in proper posteriors.|Choice of Hierarchical Priors: Admissibility in Estimation of Normal Means|http://www.jstor.org/stable/2242575|2242575|1996-06-01|1996|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Bayesian priors are often used to restrain the otherwise highly over-parametrized vector autoregressive (VAR) models. The currently available Bayesian VAR methodology does not allow the user to specify prior beliefs about the unconditional mean, or steady state, of the system. This is unfortunate as the steady state is something that economists usually claim to know relatively well. This paper develops easily implemented methods for analyzing both stationary and cointegrated VARs, in reduced or structural form, with an informative prior on the steady state. We document that prior information on the steady state leads to substantial gains in forecasting accuracy on Swedish macro data. A second example illustrates the use of informative steady-state priors in a cointegration model of the consumption-wealth relationship in the USA.|Steady-State Priors for Vector Autoregressions|http://www.jstor.org/stable/40206295|40206295|2009-06-01|2009|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering', 'Mathematics - Mathematical logic']|['Business & Economics', 'Business', 'Economics']
Fingerprint individuality refers to the extent of uniqueness of fingerprints and is governed by the distribution of fingerprint features, termed minutiae, in a population. This article develops a flexible class of marked point processes for minutiae and associated methodology for assessing fingerprint individuality. Inference is carried out in a Bayesian MCMC framework. The flexibility of the model fit to different kinds of minutiae patterns is demonstrated using real fingerprints. Evidence of a Paired Impostor Correspondence (EPIC) is developed as a measure of fingerprint individuality and its value is obtained using a simulation procedure based on the fitted models. This article has supplementary material online.|Assessing Fingerprint Individuality Using EPIC: A Case Study in the Analysis of Spatially Dependent Marked Processes|http://www.jstor.org/stable/23209724|23209724|2011-05-01|2011|['eng']|['Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
Exposure to high levels of air pollution during the pregnancy is associated with increased probability of preterm birth (PTB), a major cause of infant morbidity and mortality. New statistical methodology is required to specifically determine when a particular pollutant impacts the PTB outcome, to determine the role of different pollutants, and to characterize the spatial variability in these results. We develop a new Bayesian spatial model for PTB which identifies susceptible windows throughout the pregnancy jointly for multiple pollutants (PM₂ . ₅, ozone) while allowing these windows to vary continuously across space and time. We geo-code vital record birth data from Texas (2002-2004) and link them with standard pollution monitoring data and a newly introduced EPA product of calibrated air pollution model output. We apply the fully spatial model to a region of 13 counties in eastern Texas consisting of highly urban as well as rural areas. Our results indicate significant signal in the first two trimesters of pregnancy with different pollutants leading to different critical windows. Introducing the spatial aspect uncovers critical windows previously unidentified when space is ignored. A proper inference procedure is introduced to correctly analyze these windows.|Spatial-Temporal Modeling of the Association between Air Pollution Exposure and Preterm Birth: Identifying Critical Windows of Exposure|http://www.jstor.org/stable/41806034|41806034|2012-12-01|2012|['eng']|['Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
Elicitation of expert opinion is becoming increasingly important in the elicitation of prior distributions. In this paper, the psychology of elicitation and the currently available methods are briefly reviewed, but the primary discussion is on the distinction between 'general' elicitation methods for a class of problems and 'application-specific' methods which are useful only once. Examples of both types of elicitation are given, along with a discussion about general versus application-specific methods, and predictive versus structural elicitation.|Experiences in Elicitation|http://www.jstor.org/stable/2988424|2988424|1998-01-01|1998|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
When two nested models are compared, using a Bayes factor, from an objective standpoint, two seemingly conflicting issues emerge at the time of choosing parameter priors under the two models. On the one hand, for moderate sample sizes, the evidence in favor of the smaller model can be inflated by diffuseness of the prior under the larger model. On the other hand, asymptotically, the evidence in favor of the smaller model typically accumulates at a slower rate. With reference to finitely discrete data models, we show that these two issues can be dealt with jointly, by combining intrinsic priors and nonlocal priors in a new unified class of priors. We illustrate our ideas in a running Bernoulli example, then we apply them to test the equality of two proportions, and finally we deal with the more general case of logistic regression models.|The Whetstone and the Alum Block: Balanced Objective Bayesian Comparison of Nested Models for Discrete Data|http://www.jstor.org/stable/43288424|43288424|2013-08-01|2013|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
We use Bayesian time-varying parameter structural vector autoregressions with stochastic volatility to investigate changes in reduced-form and structural correlations between inventories and either sales growth or the real interest rate in the USA during both the inter-war and post-World War II periods. We identify four structural shocks by combining a single long-run restriction to identify a permanent output shock with three sign restrictions to identify demand- and supply-side transitory shocks. We show that during both the inter-war and post-war periods the structural correlation between inventories and real interest rate conditional on identified interest rate shocks is systematically positive; the reduced-form correlation between the two series is positive during the post-war period, but in line with the predictions of theory it is robustly negative during the inter-war era; during that era the correlations between inventories and either of the two other series exhibit a remarkably strong co-movement with output at business cycle frequencies.|SALES, INVENTORIES AND REAL INTEREST RATES|http://www.jstor.org/stable/26609016|26609016|2014-11-01|2014|['eng']|['Economics - Economic disciplines']|['Business & Economics', 'Business', 'Economics']
"Let X be a random vector distributed according to an exponential family with natural parameter θ ∈ Θ. We characterize conjugate prior measures on Θ through the property of linear posterior expectation of the mean parameter of X : E{E(X|θ)|X = x} = ax + b. We also delineate which hyperparameters permit such conjugate priors to be proper."|Conjugate Priors for Exponential Families|http://www.jstor.org/stable/2958808|2958808|1979-03-01|1979|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
A Bayesian and an empirical Bayes approach to shrinkage estimation of regression coefficients and the uses of these in prediction are investigated. The methods, along with least squares and least absolute deviations, are applied to data sets of different sizes and cross-validated with observations not in the data sets. The fully Bayes and empirical Bayes methods are seen to perform consistently better in predicting the response variable than either of least squares or least absolute deviations.|Bayes and Empirical Bayes Shrinkage Estimation of Regression Coefficients: A Cross-Validation Study|http://www.jstor.org/stable/1164651|1164651|1988-10-01|1988|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Science & Mathematics', 'Education', 'Statistics', 'Social Sciences']
The analysis of fuel economy data results in estimates of the technology utilization by manufacturer and vehicle line. The analysis employs a hierarchical Bayesian regression model with random components representing vehicle lines and manufacturers. The model includes predictor variables which describe vehicle features, such as type of transmission, and vehicle line specific measurements, such as compression ratio. Non-informative priors with novel modifications are used and the Bayes estimates are obtained by use of Gibbs sampling. The results show there is substantial variability among manufacturers in efficiently utilizing technology for fuel economy.|Bayesian Estimation of Manufacturing Effects in a Fuel Economy Model|http://www.jstor.org/stable/2285072|2285072|1993-12-01|1993|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Business', 'Economics']
• Background and Aims Sexual reproduction is one of the most important moments in a life cycle, determining the genetic composition of individual offspring. Controlled pollination experiments often show high variation in the mating system at the individual level, suggesting a persistence of individual variation in natural populations. Individual variation in mating patterns may have significant adaptive implications for a population and for the entire species. Nevertheless, field data rarely address individual differences in mating patterns, focusing rather on averages. This study aimed to quantify individual variation in the different components of mating patterns. • Methods Microsatellite data were used from 421 adult trees and 1911 seeds, structured in 72 half-sib families collected in a single mixed stand of Quercus robur and Q. petraea in northern Poland. Using a Bayesian approach, mating patterns were investigated, taking into account pollen dispersal, male fecundity, possible hybridization and heterogeneity in immigrant pollen pools. • Key Results Pollen dispersal followed a heavy-tailed distribution (283 m on average). In spite of high pollen mobility, immigrant pollen pools showed strong genetic structuring among mothers. At the individual level, immigrant pollen pools showed highly variable divergence rates, revealing that sources of immigrant pollen can vary greatly among particular trees. Within the stand, the distribution of male fecundity appeared highly skewed, with a small number of dominant males, resulting in a ratio of census to effective density of pollen donors of 5·3. Male fecundity was not correlated with tree diameter but showed strong cline-like spatial variation. This pattern can be attributed to environmental variation. Quercus petraea revealed a greater preference (74 %) towards intraspecific mating than Q. robur (36 %), although mating preferences varied among trees. • Conclusions Mating patterns can reveal great variation among individuals, even within a single even-age stand. The results show that trees can mate assortatively, with little respect for spatial proximity. Such selective mating may be a result of variable combining compatibility among trees due to genetic and/or environmental factors.|Seeing the forest through the trees: comprehensive inference on individual mating patterns in a mixed stand of Quercus robur and Q. petraea|http://www.jstor.org/stable/42797891|42797891|2013-08-01|2013|['eng']|['Biological sciences - Biology']|['Biological Sciences', 'Botany & Plant Sciences', 'Science and Mathematics']
Mixture models, or convex combinations of a countable number of probability distributions, offer an elegant framework for inference when the population of interest can be subdivided into latent clusters having random characteristics that are heterogeneous between, but homogeneous within, the clusters. Traditionally, the different kinds of mixture models have been motivated and analyzed from very different perspectives, and their common characteristics have not been fully appreciated. The inferential techniques developed for these models usually necessitate heavy computational burdens that make them difficult, if not impossible, to apply to the massive data sets increasingly encountered in real world studies. This paper introduces a flexible class of models called generalized Pólya urn (GPU) processes. Many common mixture models, including finite mixtures, hidden Markov models, and Dirichlet processes, are obtained as special cases of GPU processes. Other important special cases include finite-dimensional Dirichlet priors, infinite hidden Markov models, analysis of densities models, nested Chinese restaurant processes, hierarchical DP models, nonparametric density models, spatial Dirichlet processes, weighted mixtures of DP priors, and nested Dirichlet processes. An investigation of the theoretical properties of GPU processes offers new insight into asymptotics that form the basis of cost-effective Markov chain Monte Carlo (MCMC) strategies for large datasets. These MCMC techniques have the advantage of providing inferences from the posterior of interest, rather than an approximation, and are applicable to different mixture models. The versatility and impressive gains of the methodology are demonstrated by simulation studies and by a semiparametric Bayesian analysis of high-resolution comparative genomic hybridization data on lung cancer. The appendixes are available online as supplemental material.|Posterior Simulation in Countable Mixture Models for Large Datasets|http://www.jstor.org/stable/29747082|29747082|2010-06-01|2010|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Governments try to discourage risky health behaviours, yet such behaviours are bewilderingly persistent. We suggest a new conceptual approach to this puzzle. We show that expected utility theory predicts that unhappy people will be attracted to risk-taking. Using US seatbelt data, we document evidence strongly consistent with that prediction. We exploit various methodological approaches, including Bayesian model selection and instrumental variable estimation. Using road accident data, we find strongly corroborative longitudinal evidence. Government policy may thus have to change. It may need to improve the underlying happiness of individuals instead of, or in addition to, its traditional concern with society's risk-taking symptoms.|Happiness as a Driver of Risk-avoiding Behaviour: Theory and an Empirical Study of Seatbelt Wearing and Automobile Accidents|http://www.jstor.org/stable/24029701|24029701|2014-10-01|2014|['eng']|['Health sciences - Health and wellness', 'Education - Educational psychology', 'Social sciences - Psychology', 'Physical sciences - Physics']|['Business', 'Business & Economics Collection', 'Economics']
Motivated by the need to smooth and to summarize multiple simultaneous time series arising from networks of environmental monitors, we propose a hierarchical wavelet model for which estimation of hyperparameters can be performed by marginal maximum likelihood. The result is an empirical Bayes thresholding procedure whose results improve on those of wavethresh in terms of mean square error. We apply the approach to data from the SensorScope environmental modelling system, and briefly discuss issues that arise concerning variance estimation in this context.|Hierarchical wavelet modelling of environmental sensor data|http://www.jstor.org/stable/43601377|43601377|2011-11-01|2011|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Mathematics', 'Statistics']
We propose a non-linear density estimator, which is locally adaptive, like wavelet estimators, and positive everywhere, without a log-or root-transform. This estimator is based on maximizing a non-parametric log-likelihood function regularized by a total variation penalty. The smoothness is driven by a single penalty parameter, and to avoid cross-validation, we derive an information criterion based on the idea of universal penalty. The penalized log-likelihood maximization is reformulated as an l₁ -penalized strictly convex programme whose unique solution is the density estimate. A Newton-type method cannot be applied to calculate the estimate because the l₁ -penalty is non-differentiable. Instead, we use a dual block coordinate relaxation method that exploits the problem structure. By comparing with kernel, spline and taut string estimators on a Monte Carlo simulation, and by investigating the sensitivity to ties on two real data sets, we observe that the new estimator achieves good L₁ and L₂ risk for densities with sharp features, and behaves well with ties.|Density Estimation by Total Variation Penalized Likelihood Driven by the Sparsity l₁ Information Criterion|http://www.jstor.org/stable/41000882|41000882|2010-06-01|2010|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
In recent years, marketing researchers have become increasingly interested in under-and overreporting. However, there are few suitable approaches to operationalize deviations from the truth, particularly in behavioral domains in which self-reports are usually the only viable method of choice to measure behavior or attitudes. An especially difficult situation arises if some people underreport while others overreport. This article proposes a Bayesian item response theory model to quantify under-and overreporting in surveys. The method utilizes within-person differences between answers obtained under direct questioning (no privacy protection) and randomized-response questioning (which ensures item-level privacy protection). This method has the important features of incorporating behavioral response-mode effects (e.g., privacy loss when switching from direct to randomized-response questioning, response-mode inertia effects) and allowing the direction of bias to differ across respondents. The authors provide an empirical application for excessive alcohol consumption involving 1,408 respondents from a commercial web panel. The results show that respondents are averse to decreases in privacy and that randomized response is less effective if respondents provide biased responses to earlier direct questions.|Quantifying Under- and Overreporting in Surveys Through a Dual-Questioning-Technique Design|http://www.jstor.org/stable/43832399|43832399|2015-12-01|2015|['eng']|['Health sciences - Health and wellness']|['Business & Economics', 'Marketing & Advertising', 'Business']
With recent advances in genetic analysis, it has become feasible to classify a pathogen into genetically distinct variants even though they apparently cause an infected subject similar symptoms. The availability of such data opens up the interesting problem of studying the spatiotemporal variation in the diversity of variants of a pathogen. Data on pathogen variants often suffer the problems of (i) low cell counts, (ii) incomplete classification due to laboratory problems (e.g., contamination), and (iii) unseen variants. Shannon's entropy may be used as a measure of variant diversity. A Bayesian approach can be used to deal with the problems of low cell counts and unseen variants. Bayesian analysis of incomplete multinomial data may be carried out by Markov chain Monte Carlo techniques. However, for pathogen-variant data, there often is only one source of missingness—namely, some subjects are known to be infected by some unidentified pathogen variant. We point out that for incomplete data with disjoint sources of missingness, Bayesian analysis can be done more efficiently using an iid sampling scheme from the posterior distribution. We illustrate the method by analyzing a data set on the prevalence of bartonella infection among individual colonies of prairie dogs at the study site in Colorado between 2003 and 2006. We compare the result from the proposed Monte Carlo method with the results from other methods, including a model that entertains within-variant spatial correlation but no between-variant spatial correlation. This article has supplementary material online.|Bayesian Inference With Incomplete Multinomial Data: A Problem in Pathogen Diversity|http://www.jstor.org/stable/29747067|29747067|2010-06-01|2010|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Objective: Region-specific maps of cancer incidence, mortality, late detection rates, and screening rates can be very helpful in the planning, targeting, and coordination of cancer control activities. Unfortunately, past efforts in this area have been few, and have not used appropriate statistical models that account for the correlation of rates across both neighboring regions and different cancer types. In this article we develop such models, and apply them to the problem of cancer control in the counties of Minnesota during the period 1993-1997. Methods: We use hierarchical Bayesian spatial statistical methods, implemented using modern Markov chain Monte Carlo computing techniques and software. Results: Our approach results in spatially smoothed maps emphasizing either cancer prevention or cancer outcome for breast, colorectal, and lung cancer, as well as an overall map which combines results from these three individual cancers. Conclusions: Our methods enable us to produce a more statistically accurate picture of the geographic distribution of important cancer prevention and outcome variables in Minnesota, and appear useful for making decisions regarding targeting cancer control resources within the state.|Using Hierarchical Spatial Models for Cancer Control Planning in Minnesota (United States)|http://www.jstor.org/stable/3554027|3554027|2002-12-01|2002|['eng']|['Applied sciences - Engineering']|['Health Sciences', 'Medicine and Allied Health']
This article presents a method to estimate the coefficients, to test specification hypotheses, and to conduct policy exercises in multicountry Vector Autoregressive (VAR) models with cross-unit interdependencies, unit-specific dynamics, and time variations in the coefficients. The framework of analysis is Bayesian: A prior flexibly reduces the dimensionality of the model and puts structure on the time variations, Markov chain Monte Carlo (MCMC) methods are used to obtain posterior distributions, and marginal likelihoods to check the fit of various specifications. Impulse responses and conditional forecasts are obtained with the output of an MCMC routine. The transmission of certain shocks across countries is analyzed.|Estimating Multicountry VAR Models|http://www.jstor.org/stable/25621493|25621493|2009-08-01|2009|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Business', 'Economics']
We introduce a new, hierarchical, model for single-nucleotide polymorphism allele frequencies in a structured population, which is naturally fitted via Markov chain Monte Carlo methods. There is one parameter for each population, closely analogous to a population-specific version of Wright's FST, which can be interpreted as measuring how isolated the relevant population has been. Our model includes the effects of single-nucleotide polymorphism ascertainment and is motivated by population genetics considerations, explicitly in the transient setting after divergence of populations, rather than as the equilibrium of a stochastic model, as is traditionally the case. For the sizes of data set that we consider the method provides good parameter estimates and considerably outperforms estimation methods analogous to those currently used in practice. We apply the method to one new and one existing human data set, each with rather different characteristics-the first consisting of three rather close European populations; the second of four populations taken from across the globe. A novelty of our framework is that the fit of the underlying model can be assessed easily, and these results are encouraging for both data sets analysed. Our analysis suggests that Iceland is more differentiated than the other two European populations (France and Utah), a finding which is consistent with the historical record, but not obvious from comparisons of simple summary statistics.|Assessing Population Differentiation and Isolation from Single-Nucleotide Polymorphism Data|http://www.jstor.org/stable/3088810|3088810|2002-01-01|2002|['eng']|['Biological sciences - Biology', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
We present an approach to model selection for a time series of data on a fine time scale. The underlying process generating the data is modelled as a continuous time stochastic process. The underlying continuous processes are assumed to be diffusions with time varying drift and diffusion coefficient. Several approaches to modelling the diffusion coefficient are described. To perform model selection, we propose an approximation to the Bayes factor that uses only the discrete data. We illustrate our approach for several well-known processes including: Brownian motion with drift, the Ornstein-Uhlenbeck process, a mean reversion process with drift, exponential Brownian motion, and a logistic growth model. Finally, we apply our technique to data from the Standard &amp; Poor's 500 stock index by comparing a random walk to a mean reversion model.|Bayes Factors for Discrete Observations from Diffusion Processes|http://www.jstor.org/stable/2337046|2337046|1994-03-01|1994|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Standard assumptions incorporated into Bayesian model selection procedures result in procedures that are not competitive with commonly used penalized likelihood methods. We propose modifications of these methods by imposing nonlocal prior densities on model parameters. We show that the resulting model selection procedures are consistent in linear model settings when the number of possible covariates p is bounded by the number of observations n, a property that has not been extended to other model selection procedures. In addition to consistently identifying the true model, the proposed procedures provide accurate estimates of the posterior probability that each identified model is correct. Through simulation studies, we demonstrate that these model selection procedures perform as well or better than commonly used penalized likelihood methods in a range of simulation settings. Proofs of the primary theorems are provided in the Supplementary Material that is available online.|Bayesian Model Selection in High-Dimensional Settings|http://www.jstor.org/stable/23239600|23239600|2012-06-01|2012|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
A mixture experiment is characterized by having two or more inputs that are specified as a percentage contribution to a total amount of material. In such situations, the input variables are correlated because they must sum to one. Consequently, additional care must be taken when fitting statistical models or visualizing the effect of one or more inputs on the response. In this article, we consider the use of a Gaussian process to model the output from a computer simulator taking a mixture input. We introduce a procedure to perform global sensitivity analysis of the code output providing main effects and revealing interactions. The resulting methodology is illustrated using a function with analytically tractable results for comparison, a chemical compositional simulator, and a physical experiment. Supplementary materials providing assistance with implementing this methodology are available online.|Global Sensitivity Analysis for Mixture Experiments|http://www.jstor.org/stable/24587389|24587389|2013-02-01|2013|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
For many neurological disorders, prediction of disease state is an important clinical aim. Neuroimaging provides detailed information about brain structure and function from which such predictions may be statistically derived. A multinomial logit model with Gaussian process priors is proposed to: (i) predict disease state based on whole-brain neuroimaging data and (ii) analyze the relative informativeness of different image modalities and brain regions. Advanced Markov chain Monte Carlo methods are employed to perform posterior inference over the model. This paper reports a statistical assessment of multiple neuroimaging modalities applied to the discrimination of three Parkinsonian neurological disorders from one another and healthy controls, showing promising predictive performance of disease states when compared to nonprobabilistic classifiers based on multiple modalities. The statistical analysis also quantifies the relative importance of different neuroimaging measures and brain regions in discriminating between these diseases and suggests that for prediction there is little benefit in acquiring multiple neuroimaging sequences. Finally, the predictive capability of different brain regions is found to be in accordance with the regional pathology of the diseases as reported in the clinical literature.|PROBABILISTIC PREDICTION OF NEUROLOGICAL DISORDERS WITH A STATISTICAL ASSESSMENT OF NEUROIMAGING DATA MODALITIES|http://www.jstor.org/stable/41713499|41713499|2012-12-01|2012|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
The potential of untargeted metabolomics to answer important questions across the life sciences is hindered because of a paucity of computational tools that enable extraction of key biochemically relevant information. Available tools focus on using mass spectrometry fragmentation spectra to identify molecules whose behavior suggests they are relevant to the system under study. Unfortunately, fragmentation spectra cannot identify molecules in isolation but require authentic standards or databases of known fragmented molecules. Fragmentation spectra are, however, replete with information pertaining to the biochemical processes present, much of which is currently neglected. Here, we present an analytical workflow that exploits all fragmentation data from a given experiment to extract biochemically relevant features in an unsupervised manner. We demonstrate that an algorithm originally used for text mining, latent Dirichlet allocation, can be adapted to handle metabolomics datasets. Our approach extracts biochemically relevant molecular substructures (“Mass2Motifs”) from spectra as sets of co-occurring molecular fragments and neutral losses. The analysis allows us to isolate molecular substructures, whose presence allows molecules to be grouped based on shared substructures regardless of classical spectral similarity. These substructures, in turn, support putative de novo structural annotation of molecules. Combining this spectral connectivity to orthogonal correlations (e.g., common abundance changes under system perturbation) significantly enhances our ability to provide mechanistic explanations for biological behavior.|Topic modeling for untargeted substructure exploration in metabolomics|http://www.jstor.org/stable/26472669|26472669|2016-11-29|2016|['eng']|['Applied sciences - Engineering', 'Biological sciences - Biology']|['Science & Mathematics', 'Biological Sciences', 'General Science']
"On May 4, 1990, the Students' Seminar Series of the Department of Statistics at the University of Connecticut organized a ""Research Panel Discussion."" The students' committee consisting of Tumulesh Solanky (Chair), Tai-Ming Lee and Saibal Chattopadhyay had invited Professors Peter Kempthorne, Pranab K. Sen and Shelemyahu Zacks to serve as guest panelists. Professor Nitis Mukhopadhyay served as the moderator. The informal discussion touched upon many interesting aspects of statistical research and education. The varied opinions and comments of these expert panelists were undoubtedly most informative to the audience present that day, and it is hoped that such comments will also prove to be useful in the future for the general audience. What follows is a slightly edited version of the proceedings of that lively panel discussion."|Research--How to Do It: A Panel Discussion|http://www.jstor.org/stable/2245590|2245590|1991-05-01|1991|['eng']|['Philosophy - Logic', 'Philosophy - Epistemology']|['Science and Mathematics', 'Statistics']
In this article, we present a likelihood-based framework for modeling site dependencies. Our approach builds upon standard evolutionary models but incorporates site dependencies across the entire tree by letting the evolutionary parameters in these models depend upon the ancestral states at the neighboring sites. It thus avoids the need for introducing new and high-dimensional evolutionary models for site-dependent evolution. We propose a Markov chain Monte Carlo approach with data augmentation to infer the evolutionary parameters under our model. Although our approach allows for wide-ranging site dependencies, we illustrate its use, in two non-coding datasets, in the case of nearest-neighbor dependencies (i.e., evolution directly depending only upon the immediate flanking sites). The results reveal that the general time-reversible model with nearest-neighbor dependencies substantially improves the fit to the data as compared to the corresponding model with site independence. Using the parameter estimates from our model, we elaborate on the importance of the 5-methylcytosine deamination process (i.e., the CpG effect) and show that this process also depends upon the 5′ neighboring base identity. We hint at the possibility of a so-called TpA effect and show that the observed substitution behavior is very complex in the light of dinucleotide estimates. We also discuss the presence of CpG effects in a nuclear small subunit dataset and find significant evidence that evolutionary models incorporating context-dependent effects perform substantially better than independent-site models and in some cases even outperform models that incorporate varying rates across sites.|A Model-Based Approach to Study Nearest-Neighbor Influences Reveals Complex Substitution Patterns in Non-Coding Sequences|http://www.jstor.org/stable/27756389|27756389|2008-10-01|2008|['eng']|['Biological sciences - Biology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
Given data from L experiments or observational studies initially believed to be similar, it is desired to estimate the mean corresponding to an experiment or observational study, Ej, of particular interest. It is often profitable to use the data from related studies to sharpen the estimate corresponding to experiment Ej. However, it is essential that all of the data that are combined be concordant with the data from Ej. We improve the methodology first proposed by Malec &amp; Sedransk (1992) which uses the observed data to determine the nature and amount of the pooling of the data. We do this by eliminating the need to specify a scale parameter, and by showing how the technique can accommodate unknown variance components. We show the efficacy of the method by presenting an asymptotic result about the posterior probability function associated with all partitions of the experiment means, μ1...,μL, IL, into subsets, and by carrying out a numerical investigation. The latter study shows that our method provides sensible estimates, in contrast to some alternatives in common use, and exhibits the large gains in precision that are possible. We also analyse a dataset from six clinical trials that studied the effect of using aspirin following a myocardial infarction. Our analysis is useful because it has a perspective that is different from other published analyses of these data.|Combining Data from Experiments That May Be Similar|http://www.jstor.org/stable/2673436|2673436|2001-09-01|2001|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Progress in statistical computation often leads to advances in statistical modeling. For example, it is surprisingly common that an existing model is reparameterized, solely for computational purposes, but then this new configuration motivates a new family of models that is useful in applied statistics. One reason why this phenomenon may not have been noticed in statistics is that reparameterizations do not change the likelihood. In a Bayesian framework, however, a transformation of parameters typically suggests a new family of prior distributions. We discuss examples in censored and truncated data, mixture modeling, multivariate imputation, stochastic processes, and multilevel models.|Parameterization and Bayesian Modeling|http://www.jstor.org/stable/27590408|27590408|2004-04-01|2004|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
The expectation maximization (EM) algorithm is a popular, and often remarkably simple, method for maximum likelihood estimation in incomplete-data problems. One criticism of EM in practice is that asymptotic variance-covariance matrices for parameters (e.g., standard errors) are not automatic byproducts, as they are when using some other methods, such as Newton-Raphson. In this article we define and illustrate a procedure that obtains numerically stable asymptotic variance-covariance matrices using only the code for computing the complete-data variance-covariance matrix, the code for EM itself, and code for standard matrix operations. The basic idea is to use the fact that the rate of convergence of EM is governed by the fractions of missing information to find the increased variability due to missing information to add to the complete-data variance-covariance matrix. We call this supplemented EM algorithm the SEM algorithm. Theory and particular examples reinforce the conclusion that the SEM algorithm can be a practically important supplement to EM in many problems. SEM is especially useful in multiparameter problems where only a subset of the parameters are affected by missing information and in parallel computing environments. SEM can also be used as a tool for monitoring whether EM has converged to a (local) maximum.|Using EM to Obtain Asymptotic Variance-Covariance Matrices: The SEM Algorithm|http://www.jstor.org/stable/2290503|2290503|1991-12-01|1991|['eng']|['Mathematics - Pure mathematics', 'Mathematics - Mathematical logic', 'Mathematics - Mathematical analysis', 'Information science - Coding theory']|['Science & Mathematics', 'Statistics']
Grouped survival data with possible interval censoring arise in a variety of settings. This paper presents a penalized likelihood method (also called posterior likelihood by Leonard, 1978) for the analysis of such interval-censored survival data. A penalty function based on the motivation from the auto-correlated prior process is used to incorporate the available prior information on smoothness of the hazard. A version of the EM algorithm (Dempster et al., 1977) for the maximization of the penalized likelihood is used to obtain smooth estimates of the hazard function. We also discuss different methods to estimate the hyperparameter of smoothing. The methodology developed in this article is exemplified with the data for the times to cosmetic deterioration of breast cancer patients.|Bayesian Analysis of Interval-Censored Survival Data Using Penalized Likelihood|http://www.jstor.org/stable/25051336|25051336|2001-02-01|2001|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical analysis']|['Science and Mathematics', 'Statistics']
Reference growth curves estimate the distribution of a measurement as it changes according to some covariate, often age. We present a new methodology to estimate growth curves based on mixture models and splines. We model the distribution of the measurement with a mixture of normal distributions with an unknown number of components, and model dependence on the covariate through the weights, using smooth functions based on B-splines. In this way the growth curves respect the continuity of the covariate and there is no need for arbitrary grouping of the observations. The method is illustrated with data on triceps skinfold in Gambian girls and women.|Bayesian Growth Curves Using Normal Mixtures with Nonparametric Weights|http://www.jstor.org/stable/1391197|1391197|2003-06-01|2003|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Computer Science', 'Statistics']
We consider the choice of explanatory variables in multivariate linear regression. Our approach balances prediction accuracy against costs attached to variables in a multivariate version of a decision theory approach pioneered by Lindley (1968). We also employ a non-conjugate proper prior distribution for the parameters of the regression model, extending the standard normal-inverse Wishart by adding a component of error which is unexplainable by any number of predictor variables, thus avoiding the determinism identified by Dawid (1988). Simulated annealing and fast updating algorithms are used to search for good subsets when there are very many regressors. The technique is illustrated on a near infrared spectroscopy example involving 39 observations and 300 explanatory variables. This demonstrates the effectiveness of multivariate regression as opposed to separate univariate regressions. It also emphasises that within a Bayesian framework more variables than observations can be utilised.|The Choice of Variables in Multivariate Regression: A Non-Conjugate Bayesian Decision Theory Approach|http://www.jstor.org/stable/2673659|2673659|1999-09-01|1999|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
The analysis of non-Gaussian time series using state space models is considered from both classical and Bayesian perspectives. The treatment in both cases is based on simulation using importance sampling and antithetic variables; Markov chain Monte Carlo methods are not employed. Non-Gaussian disturbances for the state equation as well as for the observation equation are considered. Methods for estimating conditional and posterior means of functions of the state vector given the observations, and the mean-square errors of their estimates, are developed. These methods are extended to cover the estimation of conditional and posterior densities and distribution functions. Choice of importance sampling densities and antithetic variables is discussed. The techniques work well in practice and are computationally efficient. Their use is illustrated by applying them to a univariate discrete time series, a series with outliers and a volatility series.|Time Series Analysis of Non-Gaussian Observations Based on State Space Models from Both Classical and Bayesian Perspectives|http://www.jstor.org/stable/2680676|2680676|2000-01-01|2000|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical analysis', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Many problems arising in applications result in the need to probe a probability distribution for functions. Examples include Bayesian nonparametric statistics and conditioned diffusion processes. Standard MCMC algorithms typically become arbitrarily slow under the mesh refinement dictated by nonparametric description of the unknown function. We describe an approach to modifying a whole range of MCMC methods, applicable whenever the target measure has density with respect to a Gaussian process or Gaussian random field reference measure, which ensures that their speed of convergence is robust under mesh refinement. Gaussian processes or random fields are fields whose marginal distributions, when evaluated at any finite set of N points, are RN -valued Gaussians. The algorithmic approach that we describe is applicable not only when the desired probability measure has density with respect to a Gaussian process or Gaussian random field reference measure, but also to some useful non-Gaussian reference measures constructed through random truncation. In the applications of interest the data is often sparse and the prior specification is an essential part of the overall modelling strategy. These Gaussian-based reference measures are a very flexible modelling tool, finding wide-ranging application. Examples are shown in density estimation, data assimilation in fluid mechanics, subsurface geophysics and image registration. The key design principle is to formulate the MCMC method so that it is, in principle, applicable for functions; this may be achieved by use of proposals based on carefully chosen time-discretizations of stochastic dynamical systems which exactly preserve the Gaussian reference measure. Taking this approach leads to many new algorithms which can be implemented via minor modification of existing algorithms, yet which show enormous speed-up on a wide range of applied problems.|MCMC Methods for Functions: Modifying Old Algorithms to Make Them Faster|http://www.jstor.org/stable/43288425|43288425|2013-08-01|2013|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
The authors discuss a general class of hierarchical ordinal regression models that includes both location and scale parameters, allows link functions to be selected adaptively as finite mixtures of normal cumulative distribution functions, and incorporates flexible correlation structures for the latent scale variables. Exploiting the well-known correspondence between ordinal regression models and parametric ROC (Receiver Operating Characteristic) curves makes it possible to use a hierarchical ROC (HROC) analysis to study multilevel clustered data in diagnostic imaging studies. The authors present a Bayesian approach to model fitting using Markov chain Monte Carlo methods and discuss HROC applications to the analysis of data from two diagnostic radiology studies involving multiple interpreters. /// Les auteurs s'intéressent à une classe assez vaste de modèles de régression ordinale avec paramètres de localisation et d'échelle, laquelle permet la sélection adaptative de fonctions de lien s'exprimant comme mélanges finis de fonctions de répartition normales et fournit des structures de corrélation flexibles pour les variables d'échelle latentes. En exploitant la correspondance bien connue entre les modèles de régression ordinale et les courbes d'efficacité paramétriques (CEP) des tests diagnostiques, il est possible d'analyser des données d'imagerie médicale diagnostique regroupées à plusieurs niveaux au moyen d'une CEP hiérarchique. Les auteurs décrivent une approche bayésienne pour l'ajustement de tels modèles au moyen des méthodes de Monte Carlo à chaîne de Markov et présentent deux applications concrètes concernant l'interprétation de clichés radiologiques.|A General Class of Hierarchical Ordinal Regression Models with Applications to Correlated ROC Analysis|http://www.jstor.org/stable/3315913|3315913|2000-12-01|2000|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
"Poverty maps are used to aid important political decisions such as allocation of development funds by governments and international organizations. Those decisions should be based on the most accurate poverty figures. However, often reliable poverty figures are not available at fine geographical levels or for particular risk population subgroups due to the sample size limitation of current national surveys. These surveys cannot cover adequately all the desired areas or population subgroups and, therefore, models relating the different areas are needed to ""borrow strength"" from area to area. In particular, the Spanish Survey on Income and Living Conditions (SILC) produces national poverty estimates but cannot provide poverty estimates by Spanish provinces due to the poor precision of direct estimates, which use only the province specific data. It also raises the ethical question of whether poverty is more severe for women than for men in a given province. We develop a hierarchical Bayes (HB) approach for poverty mapping in Spanish provinces by gender that overcomes the small province sample size problem of the SILC. The proposed approach has a wide scope of application because it can be used to estimate general nonlinear parameters. We use a Bayesian version of the nested error regression model in which Markov chain Monte Carlo procedures and the convergence monitoring therein are avoided. A simulation study reveals good frequentist properties of the HB approach. The resulting poverty maps indicate that poverty, both in frequency and intensity, is localized mostly in the southern and western provinces and it is more acute for women than for men in most of the provinces."|SMALL AREA ESTIMATION OF GENERAL PARAMETERS WITH APPLICATION TO POVERTY INDICATORS: A HIERARCHICAL BAYES APPROACH|http://www.jstor.org/stable/24522079|24522079|2014-06-01|2014|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Applied mathematics']|['Mathematics', 'Science & Mathematics', 'Statistics']
The generalized linear mixed model (GLMM), which extends the generalized linear model (GLM) to incorporate random effects characterizing heterogeneity among subjects, is widely used in analyzing correlated and longitudinal data. Although there is often interest in identifying the subset of predictors that have random effects, random effects selection can be challenging, particularly when outcome distributions are nonnormal. This article proposes a fully Bayesian approach to the problem of simultaneous selection of fixed and random effects in GLMMs. Integrating out the random effects induces a covariance structure on the multivariate outcome data, and an important problem that we also consider is that of covariance selection. Our approach relies on variable selection-type mixture priors for the components in a special Cholesky decomposition of the random effects covariance. A stochastic search MCMC algorithm is developed, which relies on Gibbs sampling, with Taylor series expansions used to approximate intractable integrals. Simulated data examples are presented for different exponential family distributions, and the approach is applied to discrete survival data from a time-to-pregnancy study.|Bayesian Covariance Selection in Generalized Linear Mixed Models|http://www.jstor.org/stable/3695863|3695863|2006-06-01|2006|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
We present a class of sparse generalized linear models that include probit and logistic regression as special cases and offer some extra flexibility. We provide an EM algorithm for learning the parameters of these models from data. We apply our method in text classification and in simulated data and show that our method outperforms the logistic and probit models and also the elastic net, in general by a substantial margin.|A Flexible Bayesian Generalized Linear Model for Dichotomous Response Data with an Application to Text Categorization|http://www.jstor.org/stable/20461460|20461460|2007-01-01|2007|['eng']|['Mathematics - Applied mathematics']|['Mathematics', 'Science & Mathematics', 'Statistics']
In applied sciences, generalized linear mixed models have become one of the preferred tools to analyze a variety of longitudinal and clustered data. Due to software limitations, the analyses are often restricted to the setting in which the random effects terms follow a multivariate normal distribution. However, this assumption may be unrealistic, obscuring important features of among-unit variation. This work describes a widely applicable semiparametric Bayesian approach that relaxes the normality assumption by using a novel mixture of multivariate Polya trees prior to define a flexible nonparametric model for the random effects distribution. The nonparametric prior is centered on the commonly used parametric normal family. We allow this parametric family to hold only approximately, thereby providing a robust alternative for modeling. We discuss and implement practical procedures for addressing the computational challenges that arise under this approach. We illustrate the methodology by applying it to real-life examples. Supplemental materials for this paper are available online.|Robustifying Generalized Linear Mixed Models Using a New Class of Mixtures of Multivariate Polya Trees|http://www.jstor.org/stable/25651281|25651281|2009-12-01|2009|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Computer Science', 'Statistics']
This paper provides a practical simulation-based Bayesian and non-Bayesian analysis of correlated binary data using the multivariate probit model. The posterior distribution is simulated by Markov chain Monte Carlo methods and maximum likelihood estimates are obtained by a Monte Carlo version of the EM algorithm. A practical approach for the computation of Bayes factors from the simulation output is also developed. The methods are applied to a dataset with a bivariate binary response, to a four-year longitudinal dataset from the Six Cities study of the health effects of air pollution and to a seven-variate binary response dataset on the labour supply of married women from the Panel Survey of Income Dynamics.|Analysis of Multivariate Probit Models|http://www.jstor.org/stable/2337362|2337362|1998-06-01|1998|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
We develop an empirical Bayesian thresholding rule for the normal mean problem that adapts well to the sparsity of the signal. An key element is the use of a mixture loss function that combines both the Lp loss and the 0–1 loss function. The Bayes procedures under this loss are explicitly given as thresholding rules and are easy to compute. The prior on each mean is a mixture of an atom of probability at zero, and a Laplace or normal density for the nonzero part. The mixing probability as well as the spread of the non-zero part are hyperparameters that are estimated by the empirical Bayes procedure. Our simulation experiments demonstrate that the proposed method performs better than the other competing methods for a wide range of scenarios. We also apply our proposed method for feature selection to four data sets.|EMPIRICAL BAYESIAN THRESHOLDING FOR SPARSE SIGNALS USING MIXTURE LOSS FUNCTIONS|http://www.jstor.org/stable/24309280|24309280|2011-01-01|2011|['eng']|['Mathematics - Applied mathematics', 'Information science - Coding theory']|['Mathematics', 'Science and Mathematics', 'Statistics']
This paper considers estimation of the unknown size N of a population based on multiple capture-recapture samples. We extend the Bayesian multiple recapture model to accommodate possible heterogeneity and dependence among the samples and possible heterogeneity within the samples. In the dependent model, we show that posterior inference for N is independent of almost all the nuisance parameters. We develop a flexible Bayesian model for heterogeneity within samples and demonstrate how Gibbs sampling can be used to calculate the Bayesian estimator for N and other quantities of interest. The performance of the proposed estimators is evaluated by simulation under both correct and incorrect model specifications, and we illustrate our methods in two examples about software review and estimation of a cottontail rabbit population.|Bayesian Capture-Recapture Methods for Error Detection and Estimation of Population Size: Heterogeneity and Dependence|http://www.jstor.org/stable/2673684|2673684|2001-03-01|2001|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Certain conditionally specified joint distributions prove to be convenient conjugate prior families for classical multiparameter problems. Although the number of hyperparameters is large, assessment is shown to be reasonably straightforward, often involving the use of routine regression programs. Examples are provided involving both informative and diffuse prior information.|Bayesian Analysis for Classical Distributions Using Conditionally Specified Priors|http://www.jstor.org/stable/25053036|25053036|1998-08-01|1998|['eng']|['Mathematics - Mathematical logic']|['Science and Mathematics', 'Statistics']
We are interested in predicting one or more continuous forest variables (e. g., biomass, volume, age) at a fine resolution (e. g., pixel level) across a specified domain. Given a definition of forest/nonforest, this prediction is typically a two-step process. The first step predicts which locations are forested. The second step predicts the value of the variable for only those forested locations. Rarely is the forest/nonforest status predicted without error. However, the uncertainty in this prediction is typically not propagated through to the subsequent prediction of the forest variable of interest. Failure to acknowledge this error can result in biased estimates of forest variable totals within a domain. In response to this problem, we offer a modeling framework that will allow propagation of this uncertainty. Here we envision two latent processes generating the data. The first is a continuous spatial process while the second is a binary spatial process. The continuous spatial process controls the spatial association structure of the forest variable of interest, while the binary process indicates presence of a possible nonzero value for the forest variable at a given location. The proposed models are applied to georeferenced National Forest Inventory (NFI) data and spatially coinciding remotely sensed predictor variables. Due to the large number of observed locations in this dataset we seek dimension reduction not just in the likelihood, but also for unobserved stochastic processes. We demonstrate how a low-rank predictive process can be adapted to our setting and reduce the dimensionality of the data and ease the computational burden.|A Hierarchical Model for Quantifying Forest Variables Over Large Heterogeneous Landscapes With Uncertain Forest Areas|http://www.jstor.org/stable/41415531|41415531|2011-03-01|2011|['eng']|['Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
The accuracy of population estimates strongly interferes with our ability to obtain unbiased estimates of population parameters based on analyses of time series of population fluctuations. Here we use long-term data on fluctuations in the size of Mallard populations collected as part of the May Breeding Waterfowl Survey covering a large section of North America. We assume a log-linear model of density dependence and use a hierarchical Bayesian state-space approach in which all parameters are assumed to be realizations from a common underlying distribution. Thus, parameters for different populations are not allowed to vary independently of each other. We then simulated independent time series of aerial counts, using the estimated parameters and adding various levels of observation error. These simulations showed that the estimates of stochastic population growth rate and strength of density dependence were biased even when moderate sampling errors were present. In contrast, the estimates of the environmental stochasticity and the carrying capacity were unbiased even for short time series and large observation error. Our results underline the importance of reducing the magnitude of sampling error in the design of large-scale monitoring programs of population fluctuations.|Estimation of Population Parameters from Aerial Counts of North American Mallards: A Cautionary Tale|http://www.jstor.org/stable/40062121|40062121|2008-01-01|2008|['eng']|['Biological sciences - Biology', 'Physical sciences - Earth sciences']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
Objective: Genetic association studies based on haplotypes are powerful in the discovery and characterization of the genetic basis of complex human diseases. However, statistical methods for detecting haplotype-haplotype and haplotype-environment interactions have not yet been fully developed owing to the difficulties encountered: large numbers of potential haplotypes and unknown haplotype pairs. Furthermore, methods for detecting the association between rare haplotypes and disease have not kept pace with their counterpart of common haplotypes. Methods/Results: We herein propose an efficient and robust method to tackle these problems based on a Bayesian hierarchical generalized linear model. Our model simultaneously fits environmental effects, main effects of numerous common and rare haplotypes, and haplotype-haplotype and haplotype-environment interactions. The key to the approach is the use of a continuous prior distribution on coefficients that favors sparseness in the fitted model and facilitates computation. We develop a fast expectation-maximization algorithm to fit models by estimating posterior modes of coefficients. We incorporate our algorithm into the iteratively weighted least squares for classical generalized linear models as implemented in the R package glm. We evaluate the proposed method and compare its performance to existing methods on extensive simulated data. Conclusion: The results show that the proposed method performs well under all situations and is more powerful than existing approaches.|A Bayesian Hierarchical Model for Detecting Haplotype-Haplotype and Haplotype-Environment Interactions in Genetic Association Studies|http://www.jstor.org/stable/48513314|48513314|2011-01-01|2011|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Biological Sciences', 'Medicine & Allied Health', 'Health Sciences']
Bayesian methods have become widespread in marketing literature. We review the essence of the Bayesian approach and explain why it is particularly useful for marketing problems. While the appeal of the Bayesian approach has long been noted by researchers, recent developments in computational methods and expanded availability of detailed marketplace data has fueled the growth in application of Bayesian methods in marketing. We emphasize the modularity and flexibility of modern Bayesian approaches. The usefulness of Bayesian methods in situations in which there is limited information about a large number of units or where the information comes from different sources is noted. We include an extensive discussion of open issues and directions for future research.|Bayesian Statistics and Marketing|http://www.jstor.org/stable/4129743|4129743|2003-07-01|2003|['eng']|['Mathematics - Applied mathematics']|['Marketing & Advertising', 'Business & Economics', 'Business']
A general methodology is presented for finding suitable Poisson log-linear models with applications to multiway contingency tables. Mixtures of multivariate normal distributions are used to model prior opinion when a subset of the regression vector is believed to be nonzero. This prior distribution is studied for two- and three-way contingency tables, in which the regression coefficients are interpretable in terms of odds ratios in the table. Efficient and accurate schemes are proposed for calculating the posterior model probabilities. The methods are illustrated for a large number of two-way simulated tables and for two three-way tables. These methods appear to be useful in selecting the best log-linear model and in estimating parameters of interest that reflect uncertainty in the true model. /// Nous présentons une méthodologie générale pour trouver des modèles logarithmiques linéaires Poisson comportant des applications aux tableaux de contingence à entrées multiples. Nous utilisons des mélanges de distributions normales à plusieurs variables pour modéliser l'opinion a priori lorsqu'un sous-ensemble du vecteur de régression est considéré différent de zéro. Nous étudions cette distribution a priori pour les tableaux de contingence à deux et trois entrées, dans lesquels les coefficients de régression peuvent être interprétés en termes de rapports impairs dans le tableau. Nous proposons des schèmes efficaces et précis pour calculer les probabilités du modèle a posteriori. Nous illustrons les méthodes dans un grand nombre de tableaux simulés à deux entrées et dans deux tableaux à trois entrées. Ces méthodes semblent être utiles lors de la sélection du meilleur modèle logarithmique linéaire et lors de l'estimation de paramètres d'intérêt qui reflètent l'incertitude inhérente au vrai modèle.|Bayesian Selection of Log-Linear Models|http://www.jstor.org/stable/3315743|3315743|1996-09-01|1996|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Bayesian networks (BNs) have attained widespread use in data analysis and decision making. Well-studied topics include efficient inference, evidence propagation, parameter learning from data for complete and incomplete data scenarios, expert elicitation for calibrating BN probabilities, and structure learning. It is common for the researcher to assume the structure of the BN or to glean the structure from expert elicitation or domain knowledge. In this scenario, the model may be calibrated through learning the parameters from relevant data. There is a lack of work on model diagnostics for fitted BNs; this is the contribution of this article. We key on the definition of (conditional) independence to develop a graphical diagnostic that indicates whether the conditional independence assumptions imposed, when one assumes the structure of the BN, are supported by the data. We develop the approach theoretically and describe a Monte Carlo method to generate uncertainty measures for the consistency of the data with conditional independence assumptions under the model structure. We describe how this theoretical information and the data are presented in a graphical diagnostic tool. We demonstrate the approach through data simulated from BNs under different conditional independence assumptions. We also apply the diagnostic to a real-world dataset. The results presented in this article show that this approach is most feasible for smaller BNs—this is not peculiar to the proposed diagnostic graphic, but rather is related to the general difficulty of combining large BNs with data in any manner (such as through parameter estimation). It is the authors' hope that this article helps highlight the need for more research into BN model diagnostics. This article has supplementary materials online.|A Graphical Approach to Diagnosing the Validity of the Conditional Independence Assumptions of a Bayesian Network Given Data|http://www.jstor.org/stable/41739831|41739831|2012-12-01|2012|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Computer Science', 'Statistics']
We study a discriminant direction vector that generally exists only in high-dimension, low sample size settings. Projections of data onto this direction vector take on only two distinct values, one for each class. There exist infinitely many such directions in the subspace generated by the data; but the maximal data piling vector has the longest distance between the projections. This paper investigates mathematical properties and classification performance of this discrimination method.|The maximal data piling direction for discrimination|http://www.jstor.org/stable/27798914|27798914|2010-03-01|2010|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
"A Bayesian analysis of a nonidentified model is always possible if a proper prior on all the parameters is specified. There is, however, no Bayesian free lunch. The ""price"" is that there exist quantities about which the data are uninformative, i.e., their marginal prior and posterior distributions are identical. In the case of improper priors the analysis is problematic--resulting posteriors can be improper. This study investigates both proper and improper cases through a series of examples."|Revising Beliefs in Nonidentified Models|http://www.jstor.org/stable/3533214|3533214|1998-08-01|1998|['eng']|['Mathematics - Mathematical logic']|['Business', 'Business & Economics Collection', 'Economics']
The polymerase chain reaction (PCR) is a procedure by which the DNA in a single cell can be made to replicate many times in a test tube. By amplifying the DNA from individual sperm cells and typing the results, estimates of male recombination fractions can be made, which are valuable for creating genetic maps and locating regions of unusually intense crossover activity on the human genome. Because PCR typing results are subject to random error, stochastic models must be constructed to obtain accurate results. In practice, to obtain enough information to accurately estimate small recombination fractions, it is necessary to combine data from several PCR experiments. Stochastic models in common use assume that PCR error rates are constant across experiments. We show by analysis of a dataset that PCR error rates can vary considerably from experiment to experiment, and that models that fail to take this heterogeneity into account can produce biased estimators. We present two new estimators and show with simulation studies that they perform better than conventional methods under realistic conditions. These estimators may be appropriate whenever PCR data from several experiments are combined.|Combining Data from Polymerase Chain Reaction DNA Typing Experiments: Applications to Sperm Typing Data|http://www.jstor.org/stable/2669985|2669985|1999-09-01|1999|['eng']|['Biological sciences - Biology']|['Science & Mathematics', 'Statistics']
Evidence for severe declines in large predatory fishes is increasing around the world. Because of its long history of intense fishing, the Mediterranean Sea offers a unique perspective on fish population declines over historical timescales. We used a diverse set of records dating back to the early 19th and mid 20th century to reconstruct long-term population trends of large predatory sharks in the northwestern Mediterranean Sea. We complied 9 time series of abundance indices from commercial and recreational fishery landings, scientific surveys, and sighting records. Generalized linear models were used to extract instantaneous rates of change from each data set, and a meta-analysis was conducted to compare population trends. Only 5 of the 20 species we considered had sufficient records for analysis. Hammerhead (Sphyrna spp.), blue (Prionace glauca), mackerel (Isurus oxyrinchus and Lamna nasus), and thresher sharks (Alopias vulpinus) declined between 96 and 99.99% relative to their former abundance. According to World Conservation Union (IUCN) criteria, these species would be considered critically endangered. So far, the lack of quantitative population assessments has impeded shark conservation in the Mediterranean Sea. Our study fills this critical information gap, suggesting that current levels of exploitation put large sharks at risk of extinction in the Mediterranean Sea. Possible ecosystem effects of these losses involve a disruption of top-down control and a release of midlevel consumers. /// La evidencia de declinaciones severas de peces depredadores grandes está incrementando alrededor del mundo. Debido a su larga historia de pesca intensiva, el Mar Mediterráneo ofrece una perspectiva única de las declinaciones de poblaciones de peces en escalas de tiempo histórico. Utilizamos un conjunto diverso de registros que datan de inicios del siglo XIX hasta mediados del siglo XX para reconstruir las tendencias poblacionales de largo plazo de tiburones depredadores en el noroeste del Mar Mediterráneo. Compilamos 9 series de tiempo de índices de abundancia de capturas comerciales y recreativas, muestreos científicos y registros visuales. Usamos modelos lineales generalizados para extraer tasas de cambio instantáneas de cada conjunto de datos, y realizamos un meta-análisis para comparar tendencias poblacionales. Solo 5 de las 20 especies consideradas tuvieron suficientes datos para el análisis. Sphyma spp., Prionace glauca, Isurus oxyrinchus, Lamna nasu y Alopias vulpinus declinaron entre 96 y 99.99% en relación con su abundancia anterior. De acuerdo con criterios de la Unión Mundial de Conservación (IUCN), estas especies serían consideradas en peligro crítico. Hasta ahora, la falta de evaluaciones poblacionales cuantitativas ha impedido la conservación de tiburones en el Mar Mediterráneo. Nuestro estudio llena este vacío de información crítico, lo cual sugiere que los niveles actuales de explotación han puesto en riesgo de extinción a los tiburones grandes en el Mar Mediterráneo. Los posibles efectos de estas pérdidas a nivel ecosistema implican una interferencia del control arriba-abajo y la detonación de consumidores primarios.|Loss of Large Predatory Sharks from the Mediterranean Sea|http://www.jstor.org/stable/20183477|20183477|2008-08-01|2008|['eng']|['Physical sciences - Astronomy']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
Many bacterial pathogens are specialized, infecting one or few hosts, and this is often associated with more acute disease presentation. Specific genomes show markers of this specialization, which often reflect a balance between gene acquisition and functional gene loss. Within Salmonella enterica subspecies enterica, a single lineage exists that includes human and animal pathogens adapted to cause infection in different hosts, including S. enterica serovar Enteritidis (multiple hosts), S. Gallinarum (birds), and S. Dublin (cattle). This provides an excellent evolutionary context in which differences between these pathogen genomes can be related to host range. Genome sequences were obtained from ∼60 isolates selected to represent the known diversity of this lineage. Examination and comparison of the clades within the phylogeny of this lineage revealed signs of host restriction as well as evolutionary events that mark a path to host generalism. We have identified the nature and order of events for both evolutionary trajectories. The impact of functional gene loss was predicted based upon position within metabolic pathways and confirmed with phenotyping assays. The structure of S. Enteritidis is more complex than previously known, as a second clade of S. Enteritidis was revealed that is distinct from those commonly seen to cause disease in humans or animals, and that is more closely related to S. Gallinarum. Isolates from this second clade were tested in a chick model of infection and exhibited a reduced colonization phenotype, which we postulate represents an intermediate stage in pathogen–host adaptation.|Patterns of genome evolution that have accompanied host adaptation in <em>Salmonella</em>|http://www.jstor.org/stable/26459408|26459408|2015-01-20|2015|['eng']|['Biological sciences - Biology']|['Science & Mathematics', 'Biological Sciences', 'General Science']
This article uses a Bayesian procedure based on obtaining posterior odds to assess the evidence about the existence of multiple changes of variance in a time series. The approach is developed for sequences of independent observations. An extension to consider autoregressive models is also discussed. The information on the data about the location of the change points and the magnitude of the variances at the different pieces of the series is summarized through posterior distributions. The procedure is illustrated with a well-known financial series.|Detection of Multiple Changes of Variance Using Posterior Odds|http://www.jstor.org/stable/1391953|1391953|1993-07-01|1993|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
Willingness-to-pay (WTP) responses from dichotomous choice contingent valuation studies are often modeled using logistic regression, from which estimates of mean or median WTP are calculated. However, a great many factors influence an individual's WTP, some of which may be unobserved. Hence, the regression model may have inadequate explanatory power, and parameter estimates may be biased and their significance overestimated. The effects of this overdispersion are examined within a generalized linear mixed modeling framework, and an example given using published data from Cooper and Loomis (1992).|Using a Generalized Linear Mixed Model to Analyze Dichotomous Choice Contingent Valuation Data|http://www.jstor.org/stable/3146644|3146644|1994-11-01|1994|['eng']|['Economics - Economic disciplines']|['Business & Economics', 'Business', 'Economics']
"Analyses of multivariate ordinal probit models typically use data augmentation to link the observed (discrete) data to latent (continuous) data via a censoring mechanism defined by a collection of ""cutpoints."" Most standard models, for which effective Markov chain Monte Carlo (MCMC) sampling algorithms have been developed, use a separate (and independent) set of cutpoints for each element of the multivariate response. Motivated by the analysis of ratings data, we describe a particular class of multivariate ordinal probit models where it is desirable to use a common set of cutpoints. While this approach is attractive from a data-analytic perspective, we show that the existing efficient MCMC algorithms can no longer be accurately applied. Moreover, we show that attempts to implement these algorithms by numerically approximating required multivariate normal integrals over high-dimensional rectangular regions can result in severely degraded estimates of the posterior distribution. We propose a new data augmentation that is based on a covariance decomposition and that admits a simple and accurate MCMC algorithm. Our data augmentation requires only that univariate normal integrals be evaluated, which can be done quickly and with high accuracy. We provide theoretical results that suggest optimal decompositions within this class of data augmentations, and, based on the theory, recommend default decompositions that we demonstrate work well in practice. This article has supplementary material online."|Covariance Decompositions for Accurate Computation in Bayesian Scale-Usage Models|http://www.jstor.org/stable/41739803|41739803|2012-06-01|2012|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Computer Science', 'Statistics']
Intervention analysis has been recently the subject of several studies, mainly because real time series present a wide variety of phenomena that are caused by external and/or unexpected events. In this work, transfer functions are used to model different forms of invention to the mean level of a time series. This is performed in the framework of state-space models. Two canonical forms of intervention are considered: pulse and step functions. Static and dynamic explanation of the intervention effects, normal and non-normal time series, detection of intervention, and study of the effect of outliers are also discussed. The performance of the two approaches is compared in terms of point and interval estimation through Monte Carlo Simulation. The methodology was applied to real time series and showed satisfactory results for the intervention models used. L'analyse d'invention a récemment fait l'object de plusieurs études, principalement parce que les séries chronologiques réel présentent une grande variéte de phénomènes qui sont causés par des événements extrnes et/ou inattendus. Dans ce travail, les fonctions de transfer sont employées pour modéliser différentes formes d'intervention au niveau moyen d'une série temporelle. Cette tâche est réalisée dans le cadre de modèles de état-space. Deux formes canoniques d'intervention sont prises en considération: les fonctions d'impulsion et d'étape. Les modèles considérés tiennent ègalement compte de l'explication statique et dynamique des effets d'intervention, séries chronologiques normales et non-normales, détection de l'intervention et l'étude des effect des valeurs aberrantes. La comparasion entre les deux approches est effectuée en termes d'estimation ponctuelle et d'intervalle par simulation de Monte Carlo. La méthodologie a été appliquée à séries chronologiques réel a donné des résultats satisfaisants pour les modèles d'intervention utilisés.|Comparison of Classical and Bayesian Approaches For Intervention Analysis|http://www.jstor.org/stable/27919834|27919834|2010-08-01|2010|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
Demographic data of rare and endangered species are often too sparse to estimate vital rates and population size with sufficient precision for understanding population growth and decline. Yet, the combination of different sources of demographic data into one statistical model holds promise. We applied Bayesian integrated population modeling to demographic data from a colony of the endangered greater horseshoe bats (Rhinolophus ferrumequinum). Available data were the number of subadults and adults emerging from the colony roost at dusk, the number of newborns from 1991 to 2005, and recapture data of subadults and adults from 2004 and 2005. Survival rates did not differ between sexes, and demographic rates remained constant across time. The greater horseshoe bat is a long-lived species with high survival rates (first year: 0.49 [SD 0.06]; adults: 0.91 [SD 0.02]) and lowfecundity (0.74 [SD 0.12]). The yearly average population growth was 4.4% (SD 0.1%) and there were 92 (SD 10) adults in the colony in year 2005. Had we analyzed each data set separately, we would not have been able to estimate fecundity, the estimates of survival would have been less precise, and the estimate of population growth biased. Our results demonstrate that integrated models are suitable for obtaining crucial demographic information from limited data.|Use of Integrated Modeling to Enhance Estimates of Population Dynamics Obtained from Limited Data|http://www.jstor.org/stable/4620908|4620908|2007-08-01|2007|['eng']|['Biological sciences - Biology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
"Many analyses in epidemiological and prognostic studies and in studies of event history data require methods that allow for unobserved covariates or ""frailties."" Clayton and Cuzick (1985, Journal of the Royal Statistical Society, Series A 148, 82-117) proposed a generalization of the proportional hazards model that implemented such random effects, but the proof of the asymptotic properties of the method remains elusive, and practical experience suggests that the likelihoods may be markedly nonquadratic. This paper sets out a Bayesian representation of the model in the spirit of Kalbfleisch (1978, Journal of the Royal Statistical Society, Series B 40, 214-221) and discusses inference using Monte Carlo methods."|A Monte Carlo Method for Bayesian Inference in Frailty Models|http://www.jstor.org/stable/2532139|2532139|1991-06-01|1991|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
"An empirical Bayes approach to the estimation of possibly sparse sequences observed in Gaussian white noise is set out and investigated. The prior considered is a mixture of an atom of probability at zero and a heavy-tailed density γ, with the mixing weight chosen by marginal maximum likelihood, in the hope of adapting between sparse and dense sequences. If estimation is then carried out using the posterior median, this is a random thresholding procedure. Other thresholding rules employing the same threshold can also be used. Probability bounds on the threshold chosen by the marginal maximum likelihood approach lead to overall risk bounds over classes of signal sequences of length n, allowing for sparsity of various kinds and degrees. The signal classes considered are ""nearly black"" sequences where only a proportion η is allowed to be nonzero, and sequences with normalized ℓp norm bounded by η, for $\eta &gt; 0$ and $0 &lt; p \leq 2$. Estimation error is measured by mean qth power loss, for $0 &lt; q \leq 2$. For all the classes considered, and for all q in (0, 2], the method achieves the optimal estimation rate as n → ∞ and η → 0 at various rates, and in this sense adapts automatically to the sparseness or otherwise of the underlying signal. In addition the risk is uniformly bounded over all signals. If the posterior mean is used as the estimator, the results still hold for q &gt; 1. Simulations show excellent performance. For appropriately chosen functions γ, the method is computationally tractable and software is available. The extension to a modified thresholding method relevant to the estimation of very sparse sequences is also considered."|Needles and Straw in Haystacks: Empirical Bayes Estimates of Possibly Sparse Sequences|http://www.jstor.org/stable/3448545|3448545|2004-08-01|2004|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
In the classical biased sampling problem, we have k densities π1(·),..., πk(·), each known up to a normalizing constant, i.e., for l = 1,..., k, πl(·) = νl(·)/ml, where νl(·) is a known function and ml is an unknown constant. For each l, we have an independent and identically distributed sample from πl, and the problem is to estimate the ratios ml/ms for all l and all s. This problem arises frequently in several situations in both frequentist and Bayesian inference. An estimate of the ratios was developed and studied by Vardi and his co-workers over two decades ago, and there has been much subsequent work on this problem from many perspectives. In spite of this, there are no rigorous results in the literature on how to estimate the standard error of the estimate. We present a class of estimates of the ratios of normalizing constants that are appropriate for the case where the samples from the πls are not necessarily independent and identically distributed sequences but are Markov chains. We also develop an approach based on regenerative simulation for obtaining standard errors for the estimates of ratios of normalizing constants. These standard error estimates are valid for both the independent and identically distributed samples case and the Markov chain case.|Estimates and standard errors for ratios of normalizing constants from multiple Markov chains via regeneration|http://www.jstor.org/stable/24774564|24774564|2014-09-01|2014|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
Human mortality data sets can be expressed as multiway data arrays, the dimensions of which correspond to categories by which mortality rates are reported, such as age, sex, country and year. Regression models for such data typically assume an independent error distribution or an error model that allows for dependence along at most one or two dimensions of the data array. However, failing to account for other dependencies can lead to inefficient estimates of regression parameters, inaccurate standard errors and poor predictions. An alternative to assuming independent errors is to allow for dependence along each dimension of the array using a separable covariance model. However, the number of parameters in this model increases rapidly with the dimensions of the array and, for many arrays, maximum likelihood estimates of the covariance parameters do not exist. In this paper, we propose a submodel of the separable covariance model that estimates the covariance matrix for each dimension as having factor analytic structure. This model can be viewed as an extension of factor analysis to array-valued data, as it uses a factor model to estimate the covariance along each dimension of the array. We discuss properties of this model as they relate to ordinary factor analysis, describe maximum likelihood and Bayesian estimation methods, and provide a likelihood ratio testing procedure for selecting the factor model ranks. We apply this methodology to the analysis of data from the Human Mortality Database, and show in a cross-validation experiment how it outperforms simpler methods. Additionally, we use this model to impute mortality rates for countries that have no mortality data for several years. Unlike other approaches, our methodology is able to estimate similarities between the mortality rates of countries, time periods and sexes, and use this information to assist with the imputations.|SEPARABLE FACTOR ANALYSIS WITH APPLICATIONS TO MORTALITY DATA|http://www.jstor.org/stable/24521728|24521728|2014-03-01|2014|['eng']|['Mathematics - Applied mathematics']|['Mathematics', 'Science & Mathematics', 'Statistics']
Relationships between functional traits and average or potential demographic rates have provided insight into the functional constraints and trade-offs underlying life-history strategies of tropical tree species. We have extended this framework by decomposing growth rates of ~130 000 trees of 171 Neotropical tree species into intrinsic growth and the response of growth to light and size. We related these growth characteristics to multiple functional traits (wood density, adult stature, seed mass, leaf traits) in a hierarchical Bayesian model that accounted for measurement error and intraspecific variability of functional traits. Wood density was the most important trait determining all three growth characteristics. Intrinsic growth rates were additionally strongly related to adult stature, while all traits contributed to light response. Our analysis yielded a predictive model that allows estimation of growth characteristics for rare species on the basis of a few easily measurable morphological traits.|Functional traits explain light and size response of growth rates in tropical tree species|http://www.jstor.org/stable/41739620|41739620|2012-12-01|2012|['eng']|['Biological sciences - Biology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
"Estimates of uncertainty are the basis for inference of population risk. Uncertainty is estimated from models fitted to data that typically include a deterministic model (e.g., population growth) and stochastic elements, which should accommodate errors in sampling and any sources of variability that affect observations. Prediction from fitted models (of, say, demography) to new variables (say, population growth) requires propagation of these stochastic elements. Ecological models ignore most forms of variability, because they make statistical models complex, and they pose computational challenges. Variability associated with space, time, and among individuals that is not accommodated by demographic models can make parameter estimates and growth rate predictions unrealistic. I adapt a hierarchical approach to the problem of estimating population growth rates and their uncertainties when individuals vary and that variability cannot be assigned to specific causes. In contrast to an overfitted model that would assign a different parameter value to each individual, hierarchical models accommodate individual differences, but assume that those differences derive from an underlying distribution-they belong to a ""population."" The hierarchical model can be implemented in classical (frequentist) and Bayesian frameworks (I demonstrate both) and analyzed using Markov chain Monte Carlo simulation. Results show that population growth models that rely on standard propagation of estimation error but ignore variability among individuals can misrepresent uncertainties in ways that erode credibility."|Uncertainty and Variability in Demography and Population Growth: A Hierarchical Approach|http://www.jstor.org/stable/3107955|3107955|2003-06-01|2003|['eng']|['Biological sciences - Biology', 'Applied sciences - Engineering']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
Information-theoretic methodologies are increasingly being used in various disciplines. Frequently an information measure is adapted for a problem, yet the perspective of information as the unifying notion is overlooked. We set forth this perspective through presenting information-theoretic methodologies for a set of problems in probability and statistics. Our focal measures are Shannon entropy and Kullback–Leibler information. The background topics for these measures include notions of uncertainty and information, their axiomatic foundation, interpretations, properties, and generalizations. Topics with broad methodological applications include discrepancy between distributions, derivation of probability models, dependence between variables, and Bayesian analysis. More specific methodological topics include model selection, limiting distributions, optimal prior distribution and design of experiment, modeling duration variables, order statistics, data disclosure, and relative importance of predictors. Illustrations range from very basic to highly technical ones that draw attention to subtle points. Les méthodologies issues de la Théorie de l'Information sont de plus en plus fréquemment appliquées dans des disciplines diverses. Très souvent, cependant, lorsqu'une mesure d'information est adaptée à un problème donné, les perspectives unificatrices de la théorie dont elle est issue sont négligées. Nous exposons ici ces perspectives pour un ensemble de problèmes rencontrés en probabilité et en statistique. L'accent est mis sur l'entropie de Shannon et l'information de Kullback-Leibler. Les notions de base sur lesquelles s'appuient ces deux mesures comprennent les notions d'incertitude et d'information, leurs fondements axiomatiques, leur interprétation, leurs propriétés et leurs généralisations. Les applications visées sont, entre autres, les discordances entre lois de probabilités, la construction de modèles probabilistes, les mesures de dépendance entre variables, et l'analyse bayésienne. De façon plus spécifique, ces applications se rencontrent en sélection de variables, l'étude de lois-limites, la caracterisation de lois a priori et de plans d'expériences optimaux, la modélisation des durées de vie, l'utilisation des statistiques d'ordre, et les performances relatives de prédicteurs. Les illustrations vont des problèmes les plus simples aux plus techniques, et permettent d'attirer l'attention sur certains points particulièrement subtils.|Information Measures in Perspective|http://www.jstor.org/stable/27919860|27919860|2010-12-01|2010|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
"We propose a test for comparing the out-of-sample accuracy of competing density forecasts of a variable. The test is valid under general conditions: The data can be heterogeneous and the forecasts can be based on (nested or nonnested) parametric models or produced by semiparametric, nonparametric, or Bayesian estimation techniques. The evaluation is based on scoring rules, which are loss functions defined over the density forecast and the realizations of the variable. We restrict attention to the logarithmic scoring rule and propose an out-of-sample ""weighted likelihood ratio"" test that compares weighted averages of the scores for the competing forecasts. The user-defined weights are a way to focus attention on different regions of the distribution of the variable. For a uniform weight function, the test can be interpreted as an extension of Vuong's likelihood ratio test to time series data and to an out-of-sample testing framework. We apply the tests to evaluate density forecasts of U.S. inflation produced by linear and Markov-switching Phillips curve models estimated by either maximum likelihood or Bayesian methods. We conclude that a Markov-switching Phillips curve estimated by maximum likelihood produces the best density forecasts of inflation."|Comparing Density Forecasts via Weighted Likelihood Ratio Tests|http://www.jstor.org/stable/27638923|27638923|2007-04-01|2007|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
The International Badminton Federation recently introduced rule changes to make the game faster and more entertaining, by influencing how players score points and win games. We assess the fairness of both systems by applying combinatorics, probability theory and simulation to extrapolate known probabilities of winning individual rallies into probabilities of winning games and matches. We also measure how effective the rule changes are by comparing the numbers of rallies per game and the scoring patterns within each game, using data from the 2006 Commonwealth Games to demonstrate our results. We then develop subjective Bayesian methods for specifying the probabilities of winning. Finally, we describe how to propagate this information with observed data to determine posterior predictive distributions that enable us to predict match outcomes before and during play.|A Mathematical Analysis of Badminton Scoring Systems|http://www.jstor.org/stable/20202266|20202266|2009-01-01|2009|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Business & Economics', 'Business']
Predicting changes in individual customer behavior is an important element for success in any direct marketing activity. In this article we develop a hierarchical Bayes model of customer interpurchase times based on the generalized gamma distribution. The model allows for both cross-sectional and temporal heterogeneity, with the latter introduced through the component mixture model dependent on lagged covariates. The model is applied to personal investment data to predict when and if a specific customer will likely increase time between purchases. This prediction can be used managerially as a signal for the firm to use some type of intervention to keep that customer.|A Dynamic Model of Purchase Timing with Application to Direct Marketing|http://www.jstor.org/stable/2670153|2670153|1999-06-01|1999|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
A nonlinear wavelet shrinkage estimator was proposed in an earlier article by Huang and Lu. Such an estimator combined the asymptotic equivalence to the best linear unbiased prediction and the Bayesian estimation in nonparametric mixed-effects models. In this article, a data-driven GCV method is proposed to select hyperparameters. The proposed GCV method has low computational cost and can be applied to one or higher dimensional data. It can be used for selecting hyperparameters for either level independent or level dependent shrinkage. It can also be used for selecting the primary resolution level and the number of vanishing moments in the wavelet basis. The strong consistency of the GCV method is proved.|Generalized Cross-Validation for Wavelet Shrinkage in Nonparametric Mixed Effects Models|http://www.jstor.org/stable/1391046|1391046|2003-09-01|2003|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Computer Science', 'Statistics']
We propose a methodology for estimating the cell probabilities in a multiway contingency table by combining partial information from a number of studies when not all of the variables are recorded in all studies. We jointly model the full set of categorical variables recorded in at least one of the studies, and we treat the variables that are not reported as missing dimensions of the study-specific contingency table. For example, we might be interested in combining several cohort studies in which the incidence in the exposed and nonexposed groups is not reported for all risk factors in all studies while the overall numbers of cases and cohort size is always available. To account for study-to-study variability, we adopt a Bayesian hierarchical model. At the first stage of the model, the observation stage, data are modeled by a multinomial distribution with fixed total number of observations. At the second stage, we use the logistic normal (LN) distribution to model variability in the study-specific cells' probabilities. Using this model and data augmentation techniques, we reconstruct the contingency table for each study regardless of which dimensions are missing, and we estimate population parameters of interest. Our hierarchical procedure borrows strength from all the studies and accounts for correlations among the cells' probabilities. The main difficulty in combining studies recording different variables is in maintaining a consistent interpretation of parameters across studies. The approach proposed here overcomes this difficulty and at the same time addresses the uncertainty arising from the missing dimensions. We apply our modeling strategy to analyze data on air pollution and mortality from 1987 to 1994 for six U.S. cities by combining six cross-classifications of low, medium, and high levels of mortality counts, particulate matter, ozone, and carbon monoxide with the complication that four of the six cities do not report all the air pollution variables. Our goals are to investigate the association between air pollution and mortality by reconstructing the tables with missing dimensions, to determine the most harmful pollutant combinations, and to make predictions about these key issues for a city other than the six sampled. We find that, for high levels of ozone and carbon monoxide, the number of cases with a high number of deaths increases as the levels of particulate matter, PM10, increases and that the most harmful combinations corresponds to high levels of PM10, confirming prior findings that levels of PM10 higher than the NAAQS standard are harmful.|Combining Contingency Tables with Missing Dimensions|http://www.jstor.org/stable/2676999|2676999|2000-06-01|2000|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Since their emergence in the 1990s, the support vector machine and the AdaBoost algorithm have spawned a wave of research in statistical machine learning. Much of this new research falls into one of two broad categories: kernel methods and ensemble methods. In this expository article, I discuss the main ideas behind these two types of methods, namely how to transform linear algorithms into nonlinear ones by using kernel functions, and how to make predictions with an ensemble or a collection of models rather than a single model. I also share my personal perspectives on how these ideas have influenced and shaped my own research. In particular, I present two recent algorithms that I have invented with my collaborators: LAGO, a fast kernel algorithm for unbalanced classification and rare target detection; and Darwinian evolution in parallel universes, an ensemble method for variable selection.|Kernels and Ensembles: Perspectives on Statistical Learning|http://www.jstor.org/stable/27643986|27643986|2008-05-01|2008|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
We propose an empirical Bayes method for variable selection and coefficient estimation in linear regression models. The method is based on a particular hierarchical Bayes formulation, and the empirical Bayes estimator is shown to be closely related to the LASSO estimator. Such a connection allows us to take advantage of the recently developed quick LASSO algorithm to compute the empirical Bayes estimate, and provides a new way to select the tuning parameter in the LASSO method. Unlike previous empirical Bayes variable selection methods, which in most practical situations can be implemented only through a greedy stepwise algorithm, our method gives a global solution efficiently. Simulations and real examples show that the proposed method is very competitive in terms of variable selection, estimation accuracy, and computation speed compared with other variable selection and estimation methods.|Efficient Empirical Bayes Variable Selection and Estimation in Linear Models|http://www.jstor.org/stable/27590666|27590666|2005-12-01|2005|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Among the fundamental evolutionary forces, recombination arguably has the largest impact on the practical work of plant breeders. Varying over 1,000-fold across the maize genome, the local meiotic recombination rate limits the resolving power of quantitative trait mapping and the precision of favorable allele introgression. The consequences of low recombination also theoretically extend to the species-wide scale by decreasing the power of selection relative to genetic drift, and thereby hindering the purging of deleterious mutations. In this study, we used genotyping-by-sequencing (GBS) to identify 136,000 recombination breakpoints at high resolution within US and Chinese maize nested association mapping populations. We find that the pattern of cross-overs is highly predictable on the broad scale, following the distribution of gene density and CpG methylation. Several large inversions also suppress recombination in distinct regions of several families. We also identify recombination hotspots ranging in size from 1 kb to 30 kb. We find these hotspots to be historically stable and, compared with similar regions with low recombination, to have strongly differentiated patterns of DNA methylation and GC content. We also provide evidence for the historical action of GC-biased gene conversion in recombination hotspots. Finally, using genomic evolutionary rate profiling (GERP) to identify putative deleterious polymorphisms, we find evidence for reduced genetic load in hotspot regions, a phenomenon that may have considerable practical importance for breeding programs worldwide.|Recombination in diverse maize is stable, predictable, and associated with genetic load|http://www.jstor.org/stable/26462162|26462162|2015-03-24|2015|['eng']|['Biological sciences - Biology']|['Science & Mathematics', 'Biological Sciences', 'General Science']
"The thermosphere-ionosphere electrodynamics general circulation model (TIE-GCM) of the upper atmosphere has a number of features that are a challenge to standard approaches to emulation, including a long run time, multivariate output, periodicity, and strong constraints on the interrelationship between inputs and outputs. These kinds of features are not unusual in models of complex systems. We show how they can be handled in an emulator and demonstrate the use of the outer product emulator for efficient calculation, with an emphasis on predictive diagnostics for model choice and model validation. We use our emulator to ""verify"" the underlying computer code and to quantify our qualitative physical understanding."|Expert Knowledge and Multivariate Emulation: The Thermosphere-lonosphere Electrodynamics General Circulation Model (TIE-GCM)|http://www.jstor.org/stable/40586651|40586651|2009-11-01|2009|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
The genetic architecture of fluctuating asymmetry (FA) as an estimate of developmental instability (DI) has received much attention in the recent literature. Although some studies report significant heritabilities of FA (h2FA) and DI, generally heritability estimates are low. Summarizing available estimates in a recently performed fixed effects meta-analysis has provoked a lot of discussion. One objection is that heritabilities in general are population and even trait specific, and that they are influenced by a number of stochastic processes. Summarizing available information by an average value has therefore only limited relevance. Meta-analyzes should in addition attempt to model the underlying stochasticity and mean values should be accompanied by a measure of variability (i.e., random effects model). In this paper, I explore and apply a Bayesian method, hierarchical modeling, to model between-population and between-trait heterogeneity in h2FA, taking estimation accuracy into account. The analysis confirms the low values of h2FA, with a 95% confidence interval ranging between 0.009 and 0.104. In addition, between-species and -population differences in heritabilities were much higher than between-trait heterogeneity, indicating that the weak genetic effects relative to environmental influences and sampling error affect different traits in a comparable way. Although at present it is difficult to analyze how different potential influential factors contribute to the variation in h2FA, Bayesian modeling can provide a valuable statistical tool to model the underlying stochasticity of genetic parameters in general.|The heritability of fluctuating asymmetry: a Bayesian hierarchical model|http://www.jstor.org/stable/23735666|23735666|2000-01-01|2000|['eng']|['Biological sciences - Biology']|['Biological Sciences', 'Botany & Plant Sciences', 'Science and Mathematics']
Bayesian inference in factor analytic models has received renewed attention in recent years, partly due to computational advances but also partly to applied focuses generating factor structures as exemplified by recent work in financial time series modeling. The focus of our current work is to investigate the commonly overlooked problem of prior specification and sensitivity in factor models. We accomplish that by implementing Pérez and Berger's (1999). Expected Posterior (EP) prior distributions. As opposed to alternative objective priors, such as Jeffreys' prior and Bernardo's prior, EP prior has several important theoretical and practical properties, with its straightforward computation through MCMC methods and coherence when comparing multiple models perhaps the most important ones.|Expected posterior priors in factor analysis|http://www.jstor.org/stable/43601025|43601025|2003-06-01|2003|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Mathematics', 'Statistics']
Noninvasive, radiological image-based detection and stratification of Gleason patterns can impact clinical outcomes, treatment selection, and the determination of disease status at diagnosis without subjecting patients to surgical biopsies. We present machine learning-based automatic classification of prostate cancer aggressiveness by combining apparent diffusion coefficient (ADC) and T2-weighted (T2-w) MRI-based texture features. Our approach achieved reasonably accurate classification of Gleason scores (GS) 6(3 + 3) vs. ≥7 and 7(3 + 4) vs. 7(4 + 3) despite the presence of highly unbalanced samples by using two different sample augmentation techniques followed by feature selection-based classification. Our method distinguished between GS 6(3 + 3) and ≥7 cancers with 93% accuracy for cancers occurring in both peripheral (PZ) and transition (TZ) zones and 92% for cancers occurring in the PZ alone. Our approach distinguished the GS 7(3 + 4) from GS 7(4 + 3) with 92% accuracy for cancers occurring in both the PZ and TZ and with 93% for cancers occurring in the PZ alone. In comparison, a classifier using only the ADC mean achieved a top accuracy of 58% for distinguishing GS 6(3 + 3) vs. GS ≥7 for cancers occurring in PZ and TZ and 63% for cancers occurring in PZ alone. The same classifier achieved an accuracy of 59% for distinguishing GS 7(3 + 4) from GS 7(4 + 3) occurring in the PZ and TZ and 60% for cancers occurring in PZ alone. Separate analysis of the cancers occurring in TZ alone was not performed owing to the limited number of samples. Our results suggest that texture features derived from ADC and T2-w MRI together with sample augmentation can help to obtain reasonably accurate classification of Gleason patterns.|Automatic classification of prostate cancer Gleason scores from multiparametric magnetic resonance images|http://www.jstor.org/stable/26466464|26466464|2015-11-17|2015|['eng']|['Applied sciences - Engineering', 'Biological sciences - Biology']|['Science & Mathematics', 'Biological Sciences', 'General Science']
Many theories of consumer behavior involve thresholds and discontinuities. In this paper, we investigate consumers' use of screening rules as part of a discrete-choice model. Alternatives that pass the screen are evaluated in a manner consistent with random utility theory; alternatives that do not pass the screen have a zero probability of being chosen. The proposed model accommodates conjunctive, disjunctive, and compensatory screening rules. We estimate a model that reflects a discontinuous decision process by employing the Bayesian technique of data augmentation and using Markov-chain Monte Carlo methods to integrate over the parameter space. The approach has minimal information requirements and can handle a large number of choice alternatives. The method is illustrated using a conjoint study of cameras. The results indicate that 92% of respondents screen alternatives on one or more attributes.|A Choice Model with Conjunctive, Disjunctive, and Compensatory Screening Rules|http://www.jstor.org/stable/30036705|30036705|2004-07-01|2004|['eng']|['Philosophy - Logic', 'Applied sciences - Engineering']|['Marketing & Advertising', 'Business & Economics', 'Business']
Movie producers and exhibitors make various decisions requiring an understanding of moviegoer's preferences at the local level. Two examples of such decisions are exhibitors' allocation of screens to movies and producers' allocation of advertising across different regions of the country. This study presents a predictive model of local demand for movies with two unique features. First, arguing that consumers' political tendencies have an unutilized predictive power for marketing models, we allow consumers' heterogeneity to depend on their voting tendencies. Second, instead of relying on the commonly used genre classifications to characterize movies, we estimate latent movie attributes. These attributes are not determined a priori by industry professionals but rather reflect consumers' perceptions, as revealed by their moviegoing behavior. Box-office data over five years from 25 counties in the U.S. Midwest provide support for this model. First, consumers' preferences are related to their political tendencies. For example, we find that counties that voted for congressional Republicans prefer movies starring young, Caucasian, female actors over those starring African American, male actors. Second, perceived attributes provide new insights into consumers' preferences. For example, one of these attributes is the movie's degree of seriousness. Finally, and most importantly, the two improvements proposed here have a meaningful impact on forecasting error, decreasing it by 12.6%.|When Kerry Met Sally: Politics and Perceptions in the Demand for Movies|http://www.jstor.org/stable/42919625|42919625|2014-07-01|2014|['eng']|['Information science - Informetrics']|['Business', 'Business & Economics Collection', 'Management & Organizational Behavior']
Multivariate count models are rare in political science despite the presence of many count time series. This article develops a new Bayesian Poisson vector autoregression model that can characterize endogenous dynamic counts with no restrictions on the contemporaneous correlations. Impulse responses, decomposition of the forecast errors, and dynamic multiplier methods for the effects of exogenous covariate shocks are illustrated for the model. Two full illustrations of the model, its interpretations, and results are presented. The first example is a dynamic model that reanalyzes the patterns and predictors of superpower rivalry events. The second example applies the model to analyze the dynamics of transnational terrorist targeting decisions between 1968 and 2008. The latter example's results have direct implications for contemporary policy about terrorists' targeting that are both novel and innovative in the study of terrorism.|A Bayesian Poisson Vector Autoregression Model|http://www.jstor.org/stable/23260319|23260319|2012-07-01|2012|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Political Science', 'Social Sciences']
We propose a prior probability model in the wavelet coefficient space. The proposed model implements wavelet coefficient thresholding by full posterior inference in a coherent probability model. We introduce a prior probability model with mixture priors for the wavelet coefficients. The prior includes a positive prior probability mass at zero which leads to a posteriori thresholding and generally to a posteriori shrinkage on the coefficients. We discuss an efficient posterior simulation scheme to implement inference in the proposed model. The discussion is focused on the density estimation problem. However, the introduced prior probability model on the wavelet coefficient space and the Markov chain Monte Carlo scheme are general.|Bayesian Inference with Wavelets: Density Estimation|http://www.jstor.org/stable/1390676|1390676|1998-12-01|1998|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Computer Science', 'Statistics']
Single-index models offer a flexible semiparametric regression framework for high-dimensional predictors. Bayesian methods have never been proposed for such models. We develop a Bayesian approach incorporating some frequentist methods: B-splines approximate the link function, the prior on the index vector is Fishervon Mises, and regularization with generalized cross validation is adopted to avoid over-fitting the link function. A random walk Metropolis algorithm is used to sample from the posterior. Simulation results indicate that our procedure provides some improvement over the best frequentist method available. Two data examples are included.|BAYESIAN ESTIMATION IN SINGLE-INDEX MODELS|http://www.jstor.org/stable/24307224|24307224|2004-10-01|2004|['eng']|['Mathematics - Mathematical objects']|['Mathematics', 'Science and Mathematics', 'Statistics']
In this work we apply the methodology of integral priors to deal with Bayesian model selection in nested binomial regression models with a general link function. These models are often used to investigate associations and risks in epidemiological studies where one goal is to find whether or not an exposure is a risk factor for developing a certain disease; the purpose of the current paper is to test the effect of specific exposure factors. We formulate the problem as a Bayesian model selection one and solve it using objective Bayes factors. To elicit prior distributions on the regression coefficients of the binomial regression models, we rely on the methodology of integral priors that is nearly automatic as it only requires the specification of estimation reference priors and it does not depend on tuning parameters or on hyperparameters.|OBJECTIVE BAYESIAN HYPOTHESIS TESTING IN BINOMIAL REGRESSION MODELS WITH INTEGRAL PRIOR DISTRIBUTIONS|http://www.jstor.org/stable/24721218|24721218|2015-07-01|2015|['eng']|['Applied sciences - Engineering', 'Mathematics - Mathematical logic']|['Mathematics', 'Science & Mathematics', 'Statistics']
Benehmark estimation is motivated by the goal of producing an approximation to a posterior distribution that is better than the empirical distribution function. This is accomplished by incorporating additional information into the construction of the approximation. We focus here on generalized poststratification, the most successful implementation of benchmark estimation in our experience. We develop generalized poststratification for settings where the source of the simulation differs from the posterior that is to be approximated. This allows us to use the techniques in settings where it is advantageous to draw from a distribution different than the posterior, whether for exploration of the data and/or model, for algorithmic simplicity, for improved convergence of the simulation, or for improved estimation of selected features of the posterior. We develop an asymptotic (in simulation size) theory for the estimators, providing conditions under which central limit theorems hold. The central limit theorems apply both to an importance sampling context and to direct sampling from the posterior distribution. The asymptotic results, coupled with large-sample (size of data) approximation results provide guidance on how to implement generalized poststratification. The theoretical results also explain the gains associated with generalized poststratification and the empirically observed robustness to cutpoints for the strata. We note that the results apply well beyond the setting of Markov chain Monte Carlo simulation. The technique is illustrated with an infinite-dimensional semiparametric Bayesian regression model and a low-dimensional, overdispersed hierarchical Bayesian model. In both cases, the technique shows substantial benefits.|Generalized Poststratification and Importance Sampling for Subsampled Markov Chain Monte Carlo Estimation|http://www.jstor.org/stable/27590793|27590793|2006-09-01|2006|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Bayesian estimation procedures based on a hierarchical model for estimating parameters in the Rasch model are described. Through simulation studies it is shown that the Bayesian procedure is superior to the maximum likelihood procedure in that the estimates are (a) more accurate, at least in small samples; and (b) meaningful in that parameters corresponding to perfect item and ability responses can be estimated.|Bayesian Estimation in the Rasch Model|http://www.jstor.org/stable/1164643|1164643|1982-10-01|1982|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Education', 'Statistics', 'Social Sciences']
Multi-resolution analysis is used here to derive a wavelet smoother as an estimated regression function for a given set of noisy data. The hierarchical Bayesian approach is employed to model the regression function using a wavelet basis and to perform the subsequent estimations. The Bayesian model selection tool of Bayes factor is used to select the optimal resolution level of the multi-resolution analysis. Error bands are provided as an index of estimation error. The methodology is illustrated with two examples and a simulation study.|Bayesian Nonparametric Regression Using Wavelets|http://www.jstor.org/stable/25051362|25051362|2001-10-01|2001|['eng']|['Mathematics - Mathematical analysis', 'Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science and Mathematics', 'Statistics']
Existing research on choice designs focuses exclusively on compensatory models that assume that all available alternatives are considered in the choice process. In this paper, we develop a method to construct efficient designs for a two-stage, consider-then-choose model that involves a noncompensatory screening process at the first stage and a compensatory choice process at the second stage. The method applies to both conjunctive and disjunctive screening rules. Under certain conditions, the method also applies to the subset conjunctive and disjunctions of conjunctions screening rules. Based on the local design criterion, we conduct a comparative study of compensatory and conjunctive designs—the former are optimized for a compensatory model and the latter for a two-stage model that uses conjunctive screening in its first stage. We find that conjunctive designs have higher level overlap than compensatory designs. This occurs because level overlap helps pinpoint screening behavior. Higher overlap of conjunctive designs is also accompanied by lower orthogonality, less level balance, and more utility balance. We find that compensatory designs have a significant loss of design efficiency when the true model involves conjunctive screening at the consideration stage. These designs also have much less power than conjunctive designs in identifying a true consider-then-choose process with conjunctive screening. In contrast, when the true model is compensatory, the efficiency loss from using a conjunctive design is lower. Also, conjunctive designs have about the same power as compensatory designs in identifying a true compensatory choice process. Our findings make a strong case for the use of conjunctive designs when there is prior evidence to support respondent screening.|Efficient Choice Designs for a Consider-Then-Choose Model|http://www.jstor.org/stable/23012003|23012003|2011-03-01|2011|['eng']|['Applied sciences - Engineering']|['Business', 'Business & Economics Collection', 'Marketing & Advertising']
Analyzing macro-political processes is complicated by four interrelated problems: model scale, endogeneity, persistence, and specification uncertainty. These problems are endemic in the study of political economy, public opinion, international relations, and other kinds of macro-political research. We show how a Bayesian structural time series approach addresses them. Our illustration is a structurally identified, nine-equation model of the U.S. political-economic system. It combines key features of the model of Erikson, MacKuen, and Stimson (2002) of the American macropolity with those of a leading macroeconomic model of the United States (Sims and Zha, 1998; Leeper, Sims, and Zha, 1996). This Bayesian structural model, with a loosely informed prior, yields the best performance in terms of model fit and dynamics. This model 1) confirms existing results about the countercyclical nature of monetary policy (Williams 1990); 2) reveals informational sources of approval dynamics: innovations in information variables affect consumer sentiment and approval and the impacts on consumer sentiment feed-forward into subsequent approval changes; 3) finds that the real economy does not have any major impacts on key macropolity variables; and 4) concludes, contrary to Erikson, MacKuen, and Stimson (2002), that macropartisanship does not depend on the evolution of the real economy in the short or medium term and only very weakly on informational variables in the long term.|Modeling Macro-Political Dynamics|http://www.jstor.org/stable/25791964|25791964|2009-04-01|2009|['eng']|['Mathematics - Applied mathematics']|['Political Science', 'Social Sciences']
Spatial gene expression patterns enable the detection of local covariability and are extremely useful for identifying local gene interactions during normal development. The abundance of spatial expression data in recent years has led to the modeling and analysis of regulatory networks. The inherent complexity of such data makes it a challenge to extract biological information. We developed staNMF, a method that combines a scalable implementation of nonnegative matrix factorization (NMF) with a new stability-driven model selection criterion. When applied to a set of Drosophila early embryonic spatial gene expression images, one of the largest datasets of its kind, staNMF identified 21 principal patterns (PP). Providing a compact yet biologically interpretable representation of Drosophila expression patterns, PP are comparable to a fate map generated experimentally by laser ablation and show exceptional promise as a data-driven alternative to manual annotations. Our analysis mapped genes to cell-fate programs and assigned putative biological roles to uncharacterized genes. Finally, we used the PP to generate local transcription factor regulatory networks. Spatially local correlation networks were constructed for six PP that span along the embryonic anterior–posterior axis. Using a two-tail 5% cutoff on correlation, we reproduced 10 of the 11 links in the well-studied gap gene network. The performance of PP with the Drosophila data suggests that staNMF provides informative decompositions and constitutes a useful computational lens through which to extract biological insight from complex and often noisy gene expression data.|Stability-driven nonnegative matrix factorization to interpret spatial gene expression and build local gene networks|http://www.jstor.org/stable/26469318|26469318|2016-04-19|2016|['eng']|['Biological sciences - Biology']|['Science & Mathematics', 'Biological Sciences', 'General Science']
We examine three sets of established behavioral hypotheses about consumers’ in‐store behavior using field data on grocery store shopping paths and purchases. Our results provide field evidence for the following empirical regularities. First, as consumers spend more time in the store, they become more purposeful—they are less likely to spend time on exploration and more likely to shop/buy. Second, consistent with “licensing” behavior, after purchasing virtue categories, consumers are more likely to shop at locations that carry vice categories. Third, the presence of other shoppers attracts consumers toward a store zone but reduces consumers’ tendency to shop there.|Testing Behavioral Hypotheses Using an Integrated Model of Grocery Store Shopping Path and Purchase Behavior|http://www.jstor.org/stable/10.1086/599046|10.1086/599046|2009-10-01|2009|['eng']|['Mathematics - Mathematical objects']|['Marketing & Advertising', 'Business & Economics', 'Business']
"Ambulatory cardiovascular (CV) measurements provide valuable insights into individuals' health conditions in ""real-life,"" everyday settings. Current methods of modeling ambulatory CV data do not consider the dynamic characteristics of the full data set and their relationships with covariates such as caffeine use and stress. We propose a stochastic differential equation (SDE) in the form of a dual nonlinear Ornstein-Uhlenbeck (OU) model with person-specific covariates to capture the morning surge and nighttime dipping dynamics of ambulatory CV data. To circumvent the data analytic constraint that empirical measurements are typically collected at irregular and much larger time intervals than those evaluated in simulation studies of SDEs, we adopt a Bayesian approach with a regularized Brownian Bridge sampler (RBBS) and an efficient multiresolution (MR) algorithm to fit the proposed SDE. The MR algorithm can produce more efficient MCMC samples that is crucial for valid parameter estimation and inference. Using this model and algorithm to data from the Duke Behavioral Investigation of Hypertension Study, results indicate that age, caffeine intake, gender and race have effects on distinct dynamic characteristics of the participants' CV trajectories."|BAYESIAN ANALYSIS OF AMBULATORY BLOOD PRESSURE DYNAMICS WITH APPLICATION TO IRREGULARLY SPACED SPARSE DATA|http://www.jstor.org/stable/43826435|43826435|2015-09-01|2015|['eng']|['Applied sciences - Engineering', 'Biological sciences - Biology']|['Mathematics', 'Science & Mathematics', 'Statistics']
"We present two models for the short-term prediction of the number of deaths arising from common cancers in the United States. The first is a local linear model, in which the slope of the segment joining the number of deaths for any two consecutive time periods is assumed to be random with a nonparametric distribution, which has a Dirichlet process prior. For slightly longer prediction periods, we present a local quadratic model. This extension of the local linear model includes an additional ""Acceleration"" term that allows it to quickly adjust to sudden changes in the time series. The proposed models can be used to obtain the predictive distributions of the future number of deaths, as well their means and variances through Markov chain Monte Carlo techniques. We illustrate our methods by runs on data from selected cancer sites."|Prediction of U.S. Cancer Mortality Counts Using Semiparametric Bayesian Techniques|http://www.jstor.org/stable/27639815|27639815|2007-03-01|2007|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Within a decision-making group, such as a central bank's monetary-policy committee, group members often hold differing views about the future of key economic variables. Such differences of opinion can be thought of as reflecting differing sets of judgement. This paper suggests modelling each agent's judgement as one scenario in a macroeconomic model. Each judgement set has a specific dynamic impact on the system and, accordingly, a particular predictive density— or fan chart—associated with it. A weighted linear combination of the predictive densities yields a final predictive density that reflects the uncertainty perceived by the agents generating the forecast.|Incorporating Judgement in Fan Charts|http://www.jstor.org/stable/40254869|40254869|2009-06-01|2009|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Business', 'European Studies', 'Economics', 'Area Studies']
"In generalized linear regression problems with an abundant number of features, lasso-type regularization which imposes an 𝓁¹-constraint on the regression coefficients has become a widely established technique. Deficiencies of the lasso in certain scenarios, notably strongly correlated design, were unmasked when Zou and Hastie [J. Roy. Statist. Soc. Ser. B 67 (2005) 301—320] introduced the elastic net. In this paper we propose to extend the elastic net by admitting general nonnegative quadratic constraints as a second form of regularization. The generalized ridge-type constraint will typically make use of the known association structure of features, for example, by using temporalor spatial closeness. We study properties of the resulting ""structured elastic net"" regression estimation procedure, including basic asymptotics and the issue of model selection consistency. In this vein, we provide an analog to the so-called ""irrepresentable condition"" which holds for the lasso. Moreover, we outline algorithmic solutions for the structured elastic net within the generalized linear model family. The rationale and the performance of our approach is illustrated by means of simulated and real world data, with a focus on signal regression."|FEATURE SELECTION GUIDED BY STRUCTURAL INFORMATION|http://www.jstor.org/stable/29765542|29765542|2010-06-01|2010|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Mathematics', 'Science & Mathematics', 'Statistics']
We consider a Bayesian approach to nonlinear inverse problems in which the unknown quantity is a random field (spatial or temporal). The Bayesian approach contains a natural mechanism for regularization in the form of prior information, can incorporate information from heterogeneous sources and provide a quantitative assessment of uncertainty in the inverse solution. The Bayesian setting casts the inverse solution as a posterior probability distribution over the model parameters. The Karhunen-Loeve expansion is used for dimension reduction of the random field. Furthermore, we use a hierarchical Bayes' model to inject multiscale data in the modeling framework. In this Bayesian framework, we show that this inverse problem is well-posed by proving that the posterior measure is Lipschitz continuous with respect to the data in total variation norm. Computational challenges in this construction arise from the need for repeated evaluations of the forward model (e.g., in the context of MCMC) and are compounded by high dimensionality of the posterior. We develop two-stage reversible jump MCMC that has the ability to screen the bad proposals in the first inexpensive stage. Numerical results are presented by analyzing simulated as well as real data from hydrocarbon reservoir. This article has supplementary material available online.|Bayesian Uncertainty Quantification for Subsurface Inversion Using a Multiscale Hierarchical Model|http://www.jstor.org/stable/24587248|24587248|2014-08-01|2014|['eng']|['Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
"This article proposes a spatial dynamic structural equation model for the analysis of housing prices at the State level in the USA. The study contributes to the existing literature by extending the use of dynamic factor models to the econometric analysis of multivariate lattice data. One of the main advantages of our model formulation is that by modeling the spatial variation via spatially structured factor loadings, we entertain the possibility of identifying similarity ""regions' that share common time series components. The factor loadings are modeled as conditionally independent multivariate Gaussian Markov Random Fields, while the common components are modeled by latent dynamic factors. The general model is proposed in a state-space formulation where both stationary and nonstationary autoregressive distributed-lag processes for the latent factors are considered. For the latent factors which exhibit a common trend, and hence are cointegrated, an error correction specification of the (vector) autoregressive distributed-lag process is proposed. Full probabilistic inference for the model parameters is facilitated by adapting standard Markov chain Monte Carlo (MCMC) algorithms for dynamic linear models to our model formulation. The fit of the model is discussed for a data set of 48 States for which we model the relationship between housing prices and the macroeconomy, using State level unemployment and per capita personal income."|MODELING US HOUSING PRICES BY SPATIAL DYNAMIC STRUCTURAL EQUATION MODELS|http://www.jstor.org/stable/23566412|23566412|2013-06-01|2013|['eng']|['Mathematics - Mathematical objects']|['Mathematics', 'Science and Mathematics', 'Statistics']
This article is concerned with the simulation of one-day cricket matches. Given that only a finite number of outcomes can occur on each ball that is bowled, a discrete generator on a finite set is developed where the outcome probabilities are estimated from historical data involving one-day international cricket matches. The probabilities depend on the batsman, the bowler, the number of wickets lost, the number of balls bowled and the innings. The proposed simulator appears to do a reasonable job at producing realistic results. The simulator allows investigators to address complex questions involving one-day cricket matches. Cet article porte sur la simulation de matchs de cricket d'une seule journée. Étant donné qu'il y a un nombre fini d'événements possibles à chaque lancer de balle, un générateur discret. Sur un ensemble fini est développé où les probabilités de chacun des événements sont estimées à partir de données historiques provenant de matchs de cricket international d'une seule journée. Les probabilités dépendent du batteur, du lanceur, du nombre de guichets perdus, du nombre de balles lancées et des manches. Le simulateur proposé semble faire un travail raisonnable en produisant des résultats réalistes. Il permet aux chercheurs d'étudier des questions complexes concernant les matchs de cricket d'une seule journée.|Modelling and simulation for one-day cricket|http://www.jstor.org/stable/25653468|25653468|2009-06-01|2009|['eng']|['Health sciences - Health and wellness', 'Political science - Military science', 'Applied sciences - Research methods', 'Physical sciences - Astronomy', 'Health sciences - Medical treatment', 'Education - Formal education', 'Education - Specialized education', 'Health sciences - Medical specialties', 'Education - Educational administration']|['Science & Mathematics', 'Statistics']
The crucial step in Bayesian dating of phylogenies is the selection of prior probability curves for clade ages. In studies on regions derived from Gondwana, many authors have used steep priors, stipulating that clades can only be a little older than their oldest known fossil. These studies have ruled out vicariance associated with Gondwana breakup, but only because of the particular priors that were adopted. The use of non-flat priors for fossil-based ages is not justified and is unnecessary. Tectonic calibrations can be integrated with fossil calibrations that are used to give minimum clade ages only.|GUEST EDITORIAL: Bayesian transmogrification of clade divergence dates: a critique|http://www.jstor.org/stable/41687707|41687707|2012-10-01|2012|['eng']|['Biological sciences - Paleontology']|['Biological Sciences', 'Ecology & Evolutionary Biology', 'Science and Mathematics']
In this paper, we consider hierarchical Bayes generalized linear models for the analysis of longitudinal data. Specifically, we introduce the hierarchical Bayes random effects models and find sufficient conditions for the propriety of posteriors under noninformative priors. We discuss also implementation of the Bayes procedure via Markov Chain Monte Carlo integration techniques. The hierarchical Bayes method is illustrated with a real dataset and is compared with the corresponding empirical Bayes analysis of Waclawiw and Liang (1994).|Hierarchical Bayesian Analysis of Longitudinal Data|http://www.jstor.org/stable/25053008|25053008|1997-12-01|1997|['eng']|['Mathematics - Mathematical logic', 'Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
This paper considers several Bayesian classification methods for the analysis of the glioma cancer with microarray data based on reproducing kernel Hilbert space under the multiclass setup. We consider the multinomial logit likelihood as well as the likelihood related to the multiclass Support Vector Machine (SVM) model. It is shown that our proposed Bayesian classification models with multiple shrinkage parameters can produce more accurate classification scheme for the glioma cancer compared to several existing classical methods. We have also proposed a Bayesian variable selection scheme for selecting the differentially expressed genes integrated with our model. This integrated approach improves classifier design by yielding simultaneous gene selection.|Gene Expression-Based Glioma Classification Using Hierarchical Bayesian Vector Machines|http://www.jstor.org/stable/25664575|25664575|2007-08-01|2007|['eng']|['Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
Maximum likelihood estimation in random effects models for non-Gaussian data is a computationally challenging task that currently receives much attention. This article shows that the estimation process can be facilitated by the use of automatic differentiation, which is a technique for exact numerical differentiation of functions represented as computer programs. Automatic differentiation is applied to an approximation of the likelihood function, obtained by using either Laplace's method of integration or importance sampling. The approach is applied to generalized linear mixed models. The computational speed is high compared to the Monte Carlo EM algorithm and the Monte Carlo Newton-Raphson method.|Automatic Differentiation to Facilitate Maximum Likelihood Estimation in Nonlinear Random Effects Models|http://www.jstor.org/stable/1391062|1391062|2002-06-01|2002|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Computer Science', 'Statistics']
Advances in Geographical Information Systems (GIS) have led to the enormous recent burgeoning of spatial-temporal databases and associated statistical modeling. Here we depart from the rather rich literature in space—time modeling by considering the setting where space is discrete (e.g., aggregated data over regions), but time is continuous. Our major objective in this application is to carry out inference on gradients of a temporal process in our data set of monthly county level asthma hospitalization rates in the state of California, while at the same time accounting for spatial similarities of the temporal process across neighboring counties. Use of continuous time models here allows inference at a finer resolution than at which the data are sampled. Rather than use parametric forms to model time, we opt for a more flexible stochastic process embedded within a dynamic Markov random field framework. Through the matrix-valued covariance function we can ensure that the temporal process realizations are mean square differentiable, and may thus carry out inference on temporal gradients in a posterior predictive fashion. We use this approach to evaluate temporal gradients where we are concerned with temporal changes in the residual and fitted rate curves after accounting for seasonality, spatiotemporal ozone levels and several spatially-resolved important sociodemographic covariates.|MODELING TEMPORAL GRADIENTS IN REGIONALLY AGGREGATED CALIFORNIA ASTHMA HOSPITALIZATION DATA|http://www.jstor.org/stable/23566506|23566506|2013-03-01|2013|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science and Mathematics', 'Statistics']
We develop and implement a methodology for drawing inference about subgroup effects in a two-arm randomized trial when subgroup status is known only for a non-random sample in each of the trial arms. Since the subgroup effects are not point identified from the distribution of the observed data, we show how to compute bounds on these effects by using scientifically plausible assumptions. We characterize the uncertainty of our procedure by using the Bayesian paradigm. The methodology is developed in the context of the second Multicenter Automatic Defibrillator Intervention Trial, which was a randomized trial designed to evaluate the effectiveness of implantable defibrillators on survival.|Analysis of subgroup effects in randomized trials when subgroup membership is missing: application to the second Multicenter Automatic Defibrillator Intervention Trial|http://www.jstor.org/stable/41262295|41262295|2011-08-01|2011|['eng']|['Health sciences - Medical treatment']|['Science & Mathematics', 'Statistics']
"Limited dependent variable (LDV) data are common in political science, and political methodologists have given much good advice on dealing with them. We review some methods for LDV ""change point problems"" and demonstrate the use of Bayesian approaches for count, binary, and duration-type data. Our applications are drawn from American politics, Comparative politics, and International Political Economy. We discuss the tradeoffs both philosophically and computationally. We conclude with possibilities for multiple change point work."|Bayesian Approaches for Limited Dependent Variable Change Point Problems|http://www.jstor.org/stable/25791903|25791903|2007-10-01|2007|['eng']|['Philosophy - Logic', 'Mathematics - Mathematical logic', 'Mathematics - Applied mathematics', 'Physical sciences - Astronomy']|['Political Science', 'Social Sciences']
Gaussian Markov random field (GMRF) models are commonly used to model spatial correlation in disease mapping applications. For Bayesian inference by MCMC, so far mainly single-site updating algorithms have been considered. However, convergence and mixing properties of such algorithms can be extremely poor due to strong dependencies of parameters in the posterior distribution. In this paper, we propose various block sampling algorithms in order to improve the MCMC performance. The methodology is rather general, allows for non-standard full conditonals, and can be applied in a modular fashion in a large number of different scenarios. For illustration we consider three different applications: two formulations for spatial modelling of a single disease (with and without additional unstructured parameters respectively), and one formulation for the joint analysis of two diseases. The results indicate that the largest benefits are obtained if parameters and the corresponding hyperparameter are updated jointly in one large block. Implementation of such block algorithms is relatively easy using methods for fast sampling of Gaussian Markov random fields (Rue, 2001). By comparison, Monte Carlo estimates based on single-site updating can be rather misleading, even for very long runs. Our results may have wider relevance for efficient MCMC simulation in hierarchical models with Markov random field components.|On Block Updating in Markov Random Field Models for Disease Mapping|http://www.jstor.org/stable/4616737|4616737|2002-12-01|2002|['eng']|['Applied sciences - Engineering', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
The major implementational problem for reversible jump Markov chain Monte Carlo methods is that there is commonly no natural way to choose jump proposals since there is no Euclidean structure in the parameter space to guide our choice. We consider mechanisms for guiding the choice of proposal. The first group of methods is based on an analysis of acceptance probabilities for jumps. Essentially, these methods involve a Taylor series expansion of the acceptance probability around certain canonical jumps and turn out to have close connections to Langevin algorithms. The second group of methods generalizes the reversible jump algorithm by using the so-called saturated space approach. These allow the chain to retain some degree of memory so that, when proposing to move from a smaller to a larger model, information is borrowed from the last time that the reverse move was performed. The main motivation for this paper is that, in complex problems, the probability that the Markov chain moves between such spaces may be prohibitively small, as the probability mass can be very thinly spread across the space. Therefore, finding reasonable jump proposals becomes extremely important. We illustrate the procedure by using several examples of reversible jump Markov chain Monte Carlo applications including the analysis of autoregressive time series, graphical Gaussian modelling and mixture modelling.|Efficient Construction of Reversible Jump Markov Chain Monte Carlo Proposal Distributions|http://www.jstor.org/stable/3088825|3088825|2003-01-01|2003|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
AbstractThe genetic architecture of adaptive traits can reflect the evolutionary history of populations and also shape divergence among populations. Despite this central role in evolution, relatively little is known regarding the genetic architecture of adaptive traits in nature, particularly for traits subject to known selection intensities. Here we quantitatively describe the genetic architecture of traits that are subject to known intensities of differential selection between host plant species in Timema cristinae stick insects. Specifically, we used phenotypic measurements of 10 traits and 211,004 single-nucleotide polymorphisms (SNPs) to conduct multilocus genome-wide association mapping. We identified a modest number of SNPs that were associated with traits and sometimes explained a large proportion of trait variation. These SNPs varied in their strength of association with traits, and both major and minor effect loci were discovered. However, we found no relationship between variation in levels of divergence among traits in nature and variation in parameters describing the genetic architecture of those same traits. Our results provide a first step toward identifying loci underlying adaptation in T. cristinae. Future studies will examine the genomic location, population differentiation, and response to selection of the trait-associated SNPs described here.|Genome-Wide Association Mapping of Phenotypic Traits Subject to a Range of Intensities of Natural Selection in <em>Timema cristinae</em>|http://www.jstor.org/stable/10.1086/675497|10.1086/675497|2014-05-01|2014|['eng']|['Biological sciences - Biology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
Many clinical trials and other medical and reliability studies generate both longitudinal (repeated measurement) and survival (time to event) data. Many well-established methods exist for analyzing such data separately, but these may be inappropriate when the longitudinal variable is correlated with patient health status, hence the survival endpoint (as well as the possibility of study dropout). To remedy this, an earlier article proposed a joint model for longitudinal and survival data, obtaining maximum likelihood estimates via the EM algorithm. The longitudinal and survival responses are assumed independent given a linking latent bivariate Gaussian process and available covariates. We develop a fully Bayesian version of this approach, implemented via Markov chain Monte Carlo (MCMC) methods. We use the approach to jointly model the longitudinal and survival data from an AIDS clinical trial comparing two treatments, didanosine (ddI) and zalcitabine (ddC). Despite the complexity of the model, we find it to be relatively straightforward to implement and understand using the WinBUGS software. We compare our results to those obtained from readily available alternatives in SAS Procs MIXED, NLMIXED, PHREG, and LIFEREG, as well as Bayesian analogues of these traditional separate likelihood methods. The joint Bayesian approach appears to offer significantly improved and enhanced estimation of median survival times and other parameters of interest, as well as simpler coding and comparable runtimes.|Separate and Joint Modeling of Longitudinal and Event Time Data Using Standard Computer Packages|http://www.jstor.org/stable/27643494|27643494|2004-02-01|2004|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
"Performance characteristics of Bayes estimates are studied. More exactly, for each subject in a data set, let ξ be a vector of binary covariates and let Y be a normal response variable, with E{Y|ξ} = f(ξ) and var {Y|ξ} = 1. Here, f is an unknown function to be estimated from the data; the subjects are independent and identically distributed. Define a prior distribution on f as ∑kwkπ k/∑kwk, where π k is standard normal on the set of f which only depend on the first k covariates and $w_{k}&gt;0$ for infinitely many k. Bayes estimates are consistent for all f. On the other hand, if the π k are flat, inconsistency is the rule."|Consistency of Bayes Estimates for Nonparametric Regression: Normal Theory|http://www.jstor.org/stable/3318659|3318659|1998-12-01|1998|['eng']|['Mathematics - Mathematical logic']|['Mathematics', 'Science and Mathematics', 'Statistics']
In applications, statistical models are often restricted to what produces reasonable estimates based on the data at hand. In many cases, however, the principles that allow a model to be restricted can be derived theoretically, in the absence of any data and with minimal applied context. We illustrate this point with three well-known theoretical examples from spatial statistics and time series. First, we show that an autoregressive model for local averages violates a principle of invariance under scaling. Second, we show how the Bayesian estimate of a strictly-increasing time series, using a uniform prior distribution, depends on the scale of estimation. Third, we interpret local smoothing of spatial lattice data as Bayesian estimation and show why uniform local smoothing does not make sense. In various forms, the results presented here have been derived in previous work; our contribution is to draw out some principles that can be derived theoretically, even though in the past they may have been presented in detail in the context of specific examples.|BAYESIAN MODEL-BUILDING BY PURE THOUGHT: SOME PRINCIPLES AND EXAMPLES|http://www.jstor.org/stable/24306008|24306008|1996-01-01|1996|['eng']|['Mathematics - Mathematical objects']|['Mathematics', 'Science and Mathematics', 'Statistics']
We examine autoregressive time series models that are subject to regime switching. These shifts are determined by the outcome of an unobserved two-state indicator variable that follows a Markov process with unknown transition probabilities. A Bayesian framework is developed in which the unobserved states, one for each time point, are treated as missing data and then analyzed via the simulation tool of Gibbs sampling. This method is expedient because the conditional posterior distribution of the parameters, given the states, and the conditional posterior distribution of the states, given the parameters, all have a form amenable to Monte Carlo sampling. The approach is straightforward and generates marginal posterior distributions for all parameters of interest. Posterior distributions of the states, future observations, and the residuals, averaged over the parameter space are also obtained. Several examples with real and artificial data sets and weak prior information illustrate the usefulness of the methodology.|Bayes Inference via Gibbs Sampling of Autoregressive Time Series Subject to Markov Mean and Variance Shifts|http://www.jstor.org/stable/1391303|1391303|1993-01-01|1993|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering', 'Mathematics - Mathematical analysis', 'Mathematics - Mathematical logic']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
We analyze the temporal bipartite network of the leading Irish companies and their directors from 2003 to 2013, encompassing the end of the Celtic Tiger boom and the ensuing financial crisis in 2008. We focus on the evolution of company interlocks, whereby a company director simultaneously sits on two ormore boards. We develop a statistical model for this dataset by embedding the positions of companies and directors in a latent space. The temporal evolution of the network is modeled through three levels of Markovian dependence: one on the model parameters, one on the companies’ latent positions, and one on the edges themselves. The model is estimated using Bayesian inference. Our analysis reveals that the level of interlocking, as measured by a contraction of the latent space, increased before and during the crisis, reaching a peak in 2009, and has generally stabilized since then.|Interlocking directorates in Irish companies using a latent space model for bipartite networks|http://www.jstor.org/stable/26470266|26470266|2016-06-14|2016|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Biological Sciences', 'General Science']
We propose a method for analyzing data which consist of curves on multiple individuals, i.e., longitudinal or functional data. We use a Bayesian model where curves are expressed as linear combinations of B-splines with random coefficients. The curves are estimated as posterior means obtained via Markov chain Monte Carlo (MCMC) methods, which automatically select the local level of smoothing. The method is applicable to situations where curves are sampled sparsely and/or at irregular time points. We construct posterior credible intervals for the mean curve and for the individual curves. This methodology provides unified, efficient, and flexible means for smoothing functional data. /// Nous proposons une méthode d'analyse de données qui consistent en des courbes sur des individus multiples, i.e., des données fonctionnelles ou longitudinales. Nous utilisons un modèle bayésien où les courbes sont exprimées comme des combinaisons linéaires de B-splines avec des coefficients aléatoires. Les courbes sont estimées comme des moyennes de la loi a posteriori obtenue par des méthodes de Monte Carlo par Chaînes de Markov (MCMC), qui sélectionne automatiquement le niveau local de lissage. Cette méthode est applicable aux situations où les courbes sont échantillonnées de façon éparse et/ou à des temps irréguliers. Nous construisons des intervalles de crédibilité a posteriori pour la courbe moyenne et les courbes individuelles. Cette méthodologie fournit des moyennes unifiées, efficaces et flexibles pour des données fonctionnelles lissées.|A Bayesian Model for Sparse Functional Data|http://www.jstor.org/stable/25502021|25502021|2008-03-01|2008|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
How will the climate system respond to anthropogenic forcings? One approach to this question relies on climate model projections. Current climate projections are considerably uncertain. Characterizing and, if possible, reducing this uncertainty is an area of ongoing research. We consider the problem of making projections of the North Atlantic meridional overturning circulation (AMOC). Uncertainties about climate model parameters play a key role in uncertainties in AMOC projections. When the observational data and the climate model output are high-dimensional spatial data sets, the data are typically aggregated due to computational constraints. The effects of aggregation are unclear because statistically rigorous approaches for model parameter inference have been infeasible for high-resolution data. Here we develop a flexible and computationally efficient approach using principal components and basis expansions to study the effect of spatial data aggregation on parametric and projection uncertainties. Our Bayesian reduced-dimensional calibration approach allows us to study the effect of complicated error structures and data-model discrepancies on our ability to learn about climate model parameters from high-dimensional data. Considering high-dimensional spatial observations reduces the effect of deep uncertainty associated with prior specifications for the data-model discrepancy. Also, using the unaggregated data results in sharper projections based on our climate model. Our computationally efficient approach may be widely applicable to a variety of high-dimensional computer model calibration problems.|FAST DIMENSION-REDUCED CLIMATE MODEL CALIBRATION AND THE EFFECT OF DATA AGGREGATION|http://www.jstor.org/stable/24522071|24522071|2014-06-01|2014|['eng']|['Physical sciences - Astronomy']|['Mathematics', 'Science & Mathematics', 'Statistics']
We develop Bayesian methods of analysis for a new class of threshold autoregressive models: endogenous delay threshold. We apply our methods to the commonly used sunspot data set and find strong evidence in favor of the Endogenous Delay Threshold Autoregressive (EDTAR) model over linear and traditional threshold autoregressions.|Bayesian Analysis of Endogenous Delay Threshold Models|http://www.jstor.org/stable/1392355|1392355|2003-01-01|2003|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
The use of statistical methods to combine the results of independent empirical research studies (meta-analysis) has a long history. Meta-analytic work can be divided into two traditions: tests of the statistical significance of combined results and methods for combining estimates across studies. The principal classes of combined significance tests are reviewed, and the limitations of these tests are discussed. Fixed effects approaches treat the effect magnitude parameters to be estimated as a consequence of a model involving fixed but unknown constants. Random effects approaches treat effect magnitude parameters as if they were sampled from a universe of effects and attempt to estimate the mean and variance of the hyperpopulation of effects. Mixed models incorporate both fixed and random effects. Finally, areas of current research are summarized, including methods for handling missing data, models for publication selection, models to handle studies that are not independent, and distribution-free models for random effects.|Meta-Analysis|http://www.jstor.org/stable/1165125|1165125|1992-12-01|1992|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering', 'Philosophy - Logic', 'Biological sciences - Biology', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Education', 'Statistics', 'Social Sciences']
Atmospheric aerosols can cause serious damage to human health and reduce life expectancy. Using the radiances observed by NASA's Multi-angle Imaging SpectroRadiometer (MISR), the current MISR operational algorithm retrieves aerosol optical depth (AOD) at 17.6 km resolution. A systematic study of aerosols and their impact on public health, especially in highly populated urban areas, requires finer-resolution estimates of AOD's spatial distribution. We embed MISR's operational weighted least squares criterion and its forward calculations for AOD retrievals in a likelihood framework and further expand into a hierarchical Bayesian model to adapt to finer spatial resolution of 4.4 km. To take advantage of AOD's spatial smoothness, our method borrows strength from data at neighboring areas by postulating a Gaussian Markov random field prior for AOD. Our model considers AOD and aerosol mixing vectors as continuous variables, whose inference is carried out using Metropolis-within-Gibbs sampling methods. Retrieval uncertainties are quantified by posterior variabilities. We also develop a parallel Markov chain Monte Carlo (MCMC) algorithm to improve computational efficiency. We assess our retrieval performance using ground-based measurements from the AErosol RObotic NETwork (AERONET) and satellite images from Google Earth. Based on case studies in the greater Beijing area, China, we show that 4.4 km resolution can improve both the accuracy and coverage of remotely sensed aerosol retrievals, as well as our understanding of the spatial and seasonal behaviors of aerosols. This is particularly important during high-AOD events, which often indicate severe air pollution.|A Hierarchical Bayesian Approach for Aerosol Retrieval Using MISR Data|http://www.jstor.org/stable/24246457|24246457|2013-06-01|2013|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
In this article, we develop a fully integrated and dynamic Bayesian approach to forecast populations by age and sex. The approach embeds the Lee-Carter type models for forecasting the age patterns, with associated measures of uncertainty, of fertility, mortality, immigration, and emigration within a cohort projection model. The methodology may be adapted to handle different data types and sources of information. To illustrate, we analyze time series data for the United Kingdom and forecast the components of population change to the year 2024. We also compare the results obtained from different forecast models for age-specific fertility, mortality, and migration, hi doing so, we demonstrate the flexibility and advantages of adopting the Bayesian approach for population forecasting and highlight areas where this work could be extended.|Bayesian Population Forecasting: Extending the Lee-Carter Method|http://www.jstor.org/stable/43699175|43699175|2015-06-01|2015|['eng']|['Physical sciences - Astronomy']|['Population Studies', 'Social Sciences']
A multivariate regime-switching model for monetary policy is confronted with U.S. data. The best fit allows time variation in disturbance variances only. With coefficients allowed to change, the best fit is with change only in the monetary policy rule and there are three estimated regimes corresponding roughly to periods when most observers believe that monetary policy actually differed. But the differences among regimes are not large enough to account for the rise, then decline, in inflation of the 1970s and 1980s. Our estimates imply monetary targeting was central in the early 1980s, but also important sporadically in the 1970s.|Were There Regime Switches in U.S. Monetary Policy?|http://www.jstor.org/stable/30034354|30034354|2006-03-01|2006|['eng']|['Biological sciences - Biology']|['Business & Economics', 'Business', 'Economics']
We develop Markov chain Monte Carlo methodology for Bayesian inference for non-Gaussian Ornstein-Uhlenbeck stochastic volatility processes. The approach introduced involves expressing the unobserved stochastic volatility process in terms of a suitable marked Poisson process. We introduce two specific classes of Metropolis-Hastings algorithms which correspond to different ways of jointly parameterizing the marked point process and the model parameters. The performance of the methods is investigated for different types of simulated data. The approach is extended to consider the case where the volatility process is expressed as a superposition of Ornstein-Uhlenbeck processes. We apply our methodology to the US dollar-Deutschmark exchange rate.|Bayesian Inference for Non-Gaussian Ornstein-Uhlenbeck Stochastic Volatility Processes|http://www.jstor.org/stable/3647531|3647531|2004-01-01|2004|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics', 'Applied sciences - Engineering', 'Mathematics - Mathematical analysis']|['Science & Mathematics', 'Statistics']
"We study the asymptotic behaviour of the posterior distribution in a broad class of statistical models where the ""true"" solution occurs on the boundary of the parameter space. We show that in this case Bayesian inference is consistent, and that the posterior distribution has not only Gaussian components as in the case of regular models (the Bernstein-von Mises theorem) but also has Gamma distribution components whose form depends on the behaviour of the prior distribution near the boundary and have a faster rate of convergence. We also demonstrate a remarkable property of Bayesian inference, that for some models, there appears to be no bound on efficiency of estimating the unknown parameter if it is on the boundary of the parameter space. We illustrate the results on a problem from emission tomography."|THE BERNSTEIN-VON MISES THEOREM AND NONREGULAR MODELS|http://www.jstor.org/stable/43556346|43556346|2014-10-01|2014|['eng']|['Mathematics - Mathematical objects', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
The system equation of a hidden Markov model is rewritten to label the components by order of appearance, and to make explicit the random behaviour of the number of components, mt. We argue that this reformulation is often a good way to achieve identifiability, as it facilitates the interpretation of the posterior density, and the estimation of the number of components that have appeared in a given sample. We develop a sequential Monte Carlo algorithm for estimating the reformulated model, which relies on particle filtering and Gibbs sampling. Our algorithm has a computational cost that is similar to that of a Markov chain Monte Carlo sampler and is much less likely to be affected by label switching, i.e. the possibility of becoming trapped in a local mode of the posterior density. The extension to transdimensional priors is also considered. The approach is illustrated by two real data examples.|Inference and Model Choice for Sequentially Ordered Hidden Markov Models|http://www.jstor.org/stable/4623267|4623267|2007-01-01|2007|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
We describe a class of log-linear models for the detection of interactions in high-dimensional genomic data. This class of models leads to a Bayesian model selection algorithm that can be applied to data that have been reduced to contingency tables using ranks of observations within subjects, and discretization of these ranks within gene/network components. Many normalization issues associated with the analysis of genomic data are thereby avoided. A prior density based on Ewens' sampling distribution is used to restrict the number of interacting components assigned high posterior probability, and the calculation of posterior model probabilities is expedited by approximations based on the likelihood ratio statistic. Simulation studies are used to evaluate the efficiency of the resulting algorithm for known interaction structures. Finally, the algorithm is validated in a microarray study for which it was possible to obtain biological confirmation of detected interactions.|Log-Linear Models for Gene Association|http://www.jstor.org/stable/40592208|40592208|2009-06-01|2009|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Motivated by a longitudinal oral health study, the Signal—Tandmobiel® study, we propose a multivariate binary inhomogeneous Markov model in which unobserved correlated response variables are subject to an unconstrained misclassification process and have a monotone behavior. The multivariate baseline distributions and Markov transition matrices of the unobserved processes are defined as a function of covariates through the specification of compatible full conditional distributions. Distinct misclassification models are discussed. In all cases, the possibility that different examiners were involved in the scoring of the responses of a given subject across time is taken into account. A full Bayesian implementation of the model is described and its performance is evaluated using simulated data. We provide theoretical and empirical evidence that the parameters can be estimated without any external information about the misclassification parameters. Finally, the analyses of the motivating study are presented. Appendices 1—7 are available in the online supplementary materials.|Modeling of Multivariate Monotone Disease Processes in the Presence of Misclassification|http://www.jstor.org/stable/23427403|23427403|2012-09-01|2012|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
"Ultra-high energy cosmic rays (UHECRs) are atomic nuclei with energies over ten million times energies accessible to human-made particle accelerators. Evidence suggests that they originate from relatively nearby extragalactic sources, but the nature of the sources is unknown. We develop a multilevel Bayesian framework for assessing association of UHECRs and candidate source populations, and Markov chain Monte Carlo algorithms for estimating model parameters and comparing models by computing, via Chib's method, marginal likelihoods and Bayes factors. We demonstrate the framework by analyzing measurements of 69 UHECRs observed by the Pierre Auger Observatory (PAO) from 2004—2009, using a volume-complete catalog of 17 local active galactic nuclei (AGN) out to 15 megaparsecs as candidate sources. An early portion of the data (""period 1,"" with 14 events) was used by PAO to set an energy cut maximizing the anisotropy in period 1; the 69 measurements include this ""tuned"" subset, and subsequent ""untuned"" events with energies above the same cutoff. Also, measurement errors are approximately summarized. These factors are problematic for independent analyses of PAO data. Within the context of ""standard candle"" source models (i.e., with a common isotropic emission rate), and considering only the 55 untuned events, there is no significant evidence favoring association of UHECRs with local AGN vs. an isotropic background. The highest-probability associations are with the two nearest, adjacent AGN, Centaurus A and NGC 4945. If the association model is adopted, the fraction of UHECRs that may be associated is likely nonzero but is well below 50%. Our framework enables estimation of the angular scale for deflection of cosmic rays by cosmic magnetic fields; relatively modest scales of ≈3° to 30° are favored. Models that assign a large fraction of UHECRs to a single nearby source (e.g., Centaurus A) are ruled out unless very large deflection scales are specified a priori, and even then they are disfavored. However, including the period 1 data alters the conclusions significantly, and a simulation study supports the idea that the period 1 data are anomalous, presumably due to the tuning. Accurate and optimal analysis of future data will likely require more complete disclosure of the data."|MULTILEVEL BAYESIAN FRAMEWORK FOR MODELING THE PRODUCTION, PROPAGATION AND DETECTION OF ULTRA-HIGH ENERGY COSMIC RAYS|http://www.jstor.org/stable/23566473|23566473|2013-09-01|2013|['eng']|['Physical sciences - Astronomy']|['Mathematics', 'Science & Mathematics', 'Statistics']
There have been many recent suggestions as to how to build and estimate flexible Bayesian regression models, using constructs such as trees, neural networks, and Gaussian processes. Although there is much to commend these methods, their implementation and interpretation can be daunting for practitioners. This article presents a spline-based methodology for flexible Bayesian regression that is quite simple in terms of computation and interpretation. Smooth bivariate interactions are modeled in an economical and apparently novel way, and prior distributions that penalize complexity are used. Predictions can be based on either model selection or model averaging. Taking computation, interpretation, and predictive performance into account, the method is seen to perform well when applied to simulated and real data.|Bayesian Regression Modeling with Interactions and Smooth Effects|http://www.jstor.org/stable/2669463|2669463|2000-09-01|2000|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
This paper develops a framework for inferring common Markov-switching components in panel data sets with large cross-section and time series dimensions. We study similarities and differences across U.S. states in the timing of business cycles. We hypothesize that there exists a small number of cluster designations, with individual states in a given cluster sharing certain business cycle characteristics. We find that although oil-producing and agricultural states can sometimes experience a separate recession from the rest of the United States, for the most part, differences across states appear to be a matter of timing, with some states entering recession or recovering before others.|THE PROPAGATION OF REGIONAL RECESSIONS|http://www.jstor.org/stable/23355332|23355332|2012-11-01|2012|['eng']|['Mathematics - Mathematical logic']|['Business', 'Business & Economics Collection', 'Economics']
We describe a new approach to analyze chirp syllables of free-tailed bats from two regions of Texas in which they are predominant: Austin and College Station. Our goal is to characterize any systematic regional differences in the mating chirps and assess whether individual bats have signature chirps. The data are analyzed by modeling spectrograms of the chirps as responses in a Bayesian functional mixed model. Given the variable chirp lengths, we compute the spectrograms on a relative time scale interpretable as the relative chirp position, using a variable window overlap based on chirp length. We use two-dimensional wavelet transforms to capture correlation within the spectrogram in our modeling and obtain adaptive regularization of the estimates and inference for the regions-specific spectrograms. Our model includes random effect spectrograms at the bat level to account for correlation among chirps from the same bat and to assess relative variability in chirp spectrograms within and between bats. The modeling of spectrograms using functional mixed models is a general approach for the analysis of replicated nonstationary time series, such as our acoustical signals, to relate aspects of the signals to various predictors, while accounting for between-signal structure. This can be done on raw spectrograms when all signals are of the same length and can be done using spectrograms defined on a relative time scale for signals of variable length in settings where the idea of defining correspondence across signals based on relative position is sensible. Supplementary materials for this article are available online.|A Study of Mexican Free-Tailed Bat Chirp Syllables: Bayesian Functional Mixed Models for Nonstationary Acoustic Time Series|http://www.jstor.org/stable/24246460|24246460|2013-06-01|2013|['eng']|['Biological sciences - Biology', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
To study how a zygote develops into an embryo with different tissues, large-scale 4D confocal movies of C. elegans embryos have been produced recently by experimental biologists. However, the lack of principled statistical methods for the highly noisy data has hindered the comprehensive analysis of these data sets. We introduced a probabilistic change point model on the cell lineage tree to estimate the embryonic gene expression onset time. A Bayesian approach is used to fit the 4D confocal movies data to the model. Subsequent classification methods are used to decide a model selection threshold and further refine the expression onset time from the branch level to the specific cell time level. Extensive simulations have shown the high accuracy of our method. Its application on real data yields both previously known results and new findings.|"BAYESIAN DETECTION OF EMBRYONIC GENE EXPRESSION ONSET IN C. ""ELEGANS"""|http://www.jstor.org/stable/24522610|24522610|2015-06-01|2015|['eng']|['Biological sciences - Biology', 'Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
We develop two likelihood-based approaches to semiparametrically estimate a class of time-inhomogeneous diffusion processes: log penalized splines (P-splines) and the local log-linear method. Positive volatility is naturally embedded and this positivity is not guaranteed in most existing diffusion models. We investigate different smoothing parameter selections. Separate bandwidths are used for drift and volatility estimation. In the log P-splines approach, different smoothness for different time varying coefficients is feasible by assigning different penalty parameters. We also provide theorems for both approaches and report statistical inference results. Finally, we present a case study using the weekly three-month Treasury bill data from 1954 to 2004. We find that the log P-splines approach seems to capture the volatility dip in mid-1960s the best. We also present an application to calculate a financial market risk measure called Value at Risk (VaR) using statistical estimates from log P-splines.|SEMIPARAMETRIC ESTIMATION FOR A CLASS OF TIME-INHOMOGENEOUS DIFFUSION PROCESSES|http://www.jstor.org/stable/24308859|24308859|2009-04-01|2009|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
Hidden Markov models form an extension of mixture models which provides a flexible class of models exhibiting dependence and a possibly large degree of variability. We show how reversible jump Markov chain Monte Carlo techniques can be used to estimate the parameters as well as the number of components of a hidden Markov model in a Bayesian framework. We employ a mixture of zero-mean normal distributions as our main example and apply this model to three sets of data from finance, meteorology and geomagnetism.|Bayesian Inference in Hidden Markov Models through the Reversible Jump Markov Chain Monte Carlo Method|http://www.jstor.org/stable/2680677|2680677|2000-01-01|2000|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
In the absence of longitudinal data, the current presence and severity of disease can be measured for a sample of individuals to investigate factors related to disease incidence and progression. In this article, Bayesian discrete-time stochastic models are developed for inference from cross-sectional data consisting of the age at first diagnosis, the current presence of disease, and one or more surrogates of disease severity. Semiparametric models are used for the age-specific hazards of onset and diagnosis, and a normal underlying variable approach is proposed for modeling of changes with latency time in disease severity. The model accommodates multiple surrogates of disease severity having different measurement scales and heterogeneity among individuals in disease progression. A Markov chain Monte Carlo algorithm is described for posterior computation, and the methods are applied to data from a study of uterine leiomyoma.|Bayesian Modeling of Incidence and Progression of Disease from Cross-Sectional Data|http://www.jstor.org/stable/3068523|3068523|2002-12-01|2002|['eng']|['Biological sciences - Biology']|['Science & Mathematics', 'Statistics']
Many longitudinal studies involve relating an outcome process to a set of possibly time-varying covariates, giving rise to the usual regression models for longitudinal data. When the purpose of the study is to investigate the covariate effects when experimental environment undergoes abrupt changes or to locate the periods with different levels of covariate effects, a simple and easy-to-interpret approach is to introduce change-points in regression coefficients. In this connection, we propose a semiparametric change-point regression model, in which the error process (stochastic component) is nonparametric and the baseline mean function (functional part) is completely unspecified, the observation times are allowed to be subject specific, and the number, locations, and magnitudes of change-points are unknown and need to be estimated. We further develop an estimation procedure that combines the recent advance in semiparametric analysis based on counting process argument and multiple change-points inference and discuss its large sample properties, including consistency and asymptotic normality, under suitable regularity conditions. Simulation results show that the proposed methods work well under a variety of scenarios. An application to a real dataset is also given.|A Semiparametric Change-Point Regression Model for Longitudinal Observations|http://www.jstor.org/stable/23427360|23427360|2012-12-01|2012|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
The authors discuss prior distributions that are conjugate to the multivariate normal likelihood when some of the observations are incomplete. They present a general class of priors for incorporating information about unidentified parameters in the covariance matrix. They analyze the special case of monotone patterns of missing data, providing an explicit recursive form for the posterior distribution resulting from a conjugate prior distribution. They develop an importance sampling and a Gibbs sampling approach to sample from a general posterior distribution and compare the two methods. /// Les auteurs proposent des lois a priori conjuguées à la vraisemblance normale multivariée en présence d'observations incomplètes. Ils décrivent une classe générale de lois a priori permettant d'incorporer de l'information concernant des paramètres non identifiés dans la matrice de covariance. Ils analysent le cas spécial où le patron des données manquantes est monotone; ils montrent comment calculer explicitement, de façon récursive, la loi a posteriori conjuguée. Ils expliquent aussi comment échantillonner d'une loi a posteriori générale, soit par la méthode pondérée ou celle de Gibbs, et comparent les deux approches.|Conjugate Analysis of Multivariate Normal Data with Incomplete Observations|http://www.jstor.org/stable/3315963|3315963|2000-09-01|2000|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Feed-forward neural networks are now widely used in classification problems, whereas nonlinear methods of discrimination developed in the statistical field are much less widely known. A general framework for classification is set up within which methods from statistics, neural networks, pattern recognition and machine learning can be compared. Neural networks emerge as one of a class of flexible non-linear regression methods which can be used to classify via regression. Many interesting issues remain, including parameter estimation, the assessment of the classifiers and in algorithm development.|Neural Networks and Related Methods for Classification|http://www.jstor.org/stable/2346118|2346118|1994-01-01|1994|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Modeling high-dimensional functional responses utilizing multi-dimensional functional covariates is complicated by spatial and/or temporal dependence in the observations in addition to high-dimensional predictors. To utilize such rich sources of information we develop multi-dimensional spatial functional models that employ low-rank basis function expansions to facilitate model implementation. These models are developed within a hierarchical Bayesian framework that accounts for several sources of uncertainty, including the error that arises from truncating the infinite-dimensional basis function expansions, error in the observations, and uncertainty in the parameters. We illustrate the predictive ability of such a model through a simulation study and an application that considers spatial models of soil electrical conductivity depth profiles using spatially dependent near-infrared spectral images of electrical conductivity covariates.|BAYESIAN ANALYSIS OF SPATIALLY-DEPENDENT FUNCTIONAL RESPONSES WITH SPATIALLY-DEPENDENT MULTI-DIMENSIONAL FUNCTIONAL PREDICTORS|http://www.jstor.org/stable/24311012|24311012|2015-01-01|2015|['eng']|['Applied sciences - Engineering']|['Mathematics', 'Science & Mathematics', 'Statistics']
Non-Gaussian processes of Ornstein-Uhlenbeck (OU) type offer the possibility of capturing important distributional deviations from Gaussianity and for flexible modelling of dependence structures. This paper develops this potential, drawing on and extending powerful results from probability theory for applications in statistical analysis. Their power is illustrated by a sustained application of OU processes within the context of finance and econometrics. We construct continuous time stochastic volatility models for financial assets where the volatility processes are superpositions of positive OU processes, and we study these models in relation to financial data and theory.|Non-Gaussian Ornstein-Uhlenbeck-Based Models and Some of Their Uses in Financial Economics|http://www.jstor.org/stable/2680596|2680596|2001-01-01|2001|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
The geographic mapping of age-standardized, cause-specific death rates is a powerful tool for identifying possible etiologic factors, because the spatial distribution of mortality risks can be examined for correlations with the spatial distribution of disease-specific risk factors. This article presents a two-stage empirical Bayes procedure for calculating age-standardized cancer death rates, for use in mapping, which are adjusted for the stochasticity of rates in small area populations. Using the adjusted rates helps isolate and identify spatial patterns in the rates. The model is applied to sex-specific data on U.S. county cancer mortality in the white population for 15 cancer sites for three decades: 1950-1959, 1960-1969, and 1970-1979. Selected results are presented as maps of county death rates for white males.|Empirical Bayes Procedures for Stabilizing Maps of U.S. Cancer Mortality Rates|http://www.jstor.org/stable/2289644|2289644|1989-09-01|1989|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
Optimization for complex systems in engineering often involves the use of expensive computer simulation. By combining statistical emulation using treed Gaussian processes with pattern search optimization, we are able to perform robust local optimization more efficiently and effectively than when using either method alone. Our approach is based on the augmentation of local search patterns with location sets generated through improvement prediction over the input space. We further develop a computational framework for asynchronous parallel implementation of the optimization algorithm. We demonstrate our methods on two standard test problems and our motivating example of calibrating a circuit device simulator.|Bayesian Guided Pattern Search for Robust Local Optimization|http://www.jstor.org/stable/40586649|40586649|2009-11-01|2009|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
We describe a range of routine statistical problems in which marginal posterior distributions derived from improper prior measures are found to have an unBayesian property--one that could not occur if proper prior measures were employed. This paradoxical possibility is shown to have several facets that can be successfully analysed in the framework of a general group structure. The results cast a shadow on the uncritical use of improper prior measures. A separate examination of a particular application of Fraser's structural theory shows that it is intrinsically paradoxical under marginalization.|Marginalization Paradoxes in Bayesian and Structural Inference|http://www.jstor.org/stable/2984907|2984907|1973-01-01|1973|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
"Estimation of tree growth is based on sparse observations of tree diameter, ring widths, or increments read from a dendrometer. From annual measurements on a few trees (e.g., increment cores) or sporadic measurements from many trees (e.g., diameter censuses on mapped plots), relationships with resources, tree size, and climate are extrapolated to whole stands. There has been no way to formally integrate different types of data and problems of estimation that result from (1) multiple sources of observation error, which frequently result in impossible estimates of negative growth, (2) the fact that data are typically sparse (a few trees or a few years), whereas inference is needed broadly (many trees over many years), (3) the fact that some unknown fraction of the variance is shared across the population, and (4) the fact that growth rates of trees within competing stands are not independent. We develop a hierarchical Bayes state space model for tree growth that addresses all of these challenges, allowing for formal inference that is consistent with the available data and the assumption that growth is nonnegative. Prediction follows directly, incorporating the full uncertainty from inference with scenarios for ""filling the gaps"" for past growth rates and for future conditions affecting growth. An example involving multiple species and multiple stands with tree-ring data and up to 14 years of tree census data illustrates how different levels of information at the tree and stand level contribute to inference and prediction."|Tree Growth Inference and Prediction from Diameter Censuses and Ring Widths|http://www.jstor.org/stable/40062089|40062089|2007-10-01|2007|['eng']|['Applied sciences - Engineering', 'Biological sciences - Paleontology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
Several recent papers (e.g., Chen, Ibrahim, and Sinha, 1999, Journal of the American Statistical Association 94, 909-919; Ibrahim, Chen, and Sinha, 2001a, Biometrics 57, 383-388) have described statistical methods for use with time-to-event data featuring a surviving fraction (i.e., a proportion of the population that never experiences the event). Such cure rate models and their multivariate generalizations are quite useful in studies of multiple diseases to which an individual may never succumb, or from which an individual may reasonably be expected to recover following treatment (e.g., various types of cancer). In this article we extend these models to allow for spatial correlation (estimable via zip code identifiers for the subjects) as well as interval censoring. Our approach is Bayesian, where posterior summaries are obtained via a hybrid Markov chain Monte Carlo algorithm. We compare across a broad collection of rather high-dimensional hierarchical models using the deviance information criterion, a tool recently developed for just this purpose. We apply our approach to the analysis of a smoking cessation study where the subjects reside in 53 southeastern Minnesota zip codes. In addition to the usual posterior estimates, our approach yields smoothed zip code level maps of model parameters related to the relapse rates over time and the ultimate proportion of quitters (the cure rates).|Parametric Spatial Cure Rate Models for Interval-Censored Time-to-Relapse Data|http://www.jstor.org/stable/3695576|3695576|2004-03-01|2004|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
We develop in this article a data-analytic method to forecast the severity of next record insured loss to property caused by natural catastrophic events. The method requires and employs the knowledge of an expert and accounts for uncertainty in parameter estimation. Both considerations are essential for the task at hand because the available data are typically scarce in extreme value analysis. In addition, we consider three-parameter Gamma priors for the parameter in the model and thus provide simple analytical solutions to several key elements of interest, such as the predictive moments of record value. As a result, the model enables practitioners to gain insights into the behavior of such predictive moments without concerning themselves with the computational issues that are often associated with a complex Bayesian analysis. A data set consisting of catastrophe losses occurring in the United States between 1990 and 1999 is analyzed, and the forecasts of next record loss are made under various prior assumptions. We demonstrate that the proposed method provides more reliable and theoretically sound forecasts, whereas the conditional mean approach, which does not account for either prior information or uncertainty in parameter estimation, may provide inadmissible forecasts.|A Data-Analytic Method for Forecasting Next Record Catastrophe Loss|http://www.jstor.org/stable/3520036|3520036|2004-06-01|2004|['eng']|['Physical sciences - Astronomy']|['Business & Economics', 'Business', 'Economics']
We propose a fully inferential model-based approach to the problem of comparing the firing patterns of a neuron recorded under two distinct experimental conditions. The methodology is based on nonhomogeneous Poisson process models for the firing times of each condition with flexible nonparametric mixture prior models for the corresponding intensity functions. We demonstrate posterior inferences from a global analysis, which may be used to compare the two conditions over the entire experimental time window, as well as from a pointwise analysis at selected time points to detect local deviations of firing patterns from one condition to another. We apply our method on two neurons recorded from the primary motor cortex area of a monkey's brain while performing a sequence of reaching tasks.|<strong>Bayesian Nonparametric Modeling for Comparison of Single-Neuron Firing Intensities</strong>|http://www.jstor.org/stable/40663176|40663176|2010-03-01|2010|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
A practical impediment in adaptive clinical trials is that outcomes must be observed soon enough to apply decision rules to choose treatments for new patients. For example, if outcomes take up to six weeks to evaluate and the accrual rate is one patient per week, on average three new patients will be accrued while waiting to evaluate the outcomes of the previous three patients. The question is how to treat the new patients. This logistical problem persists throughout the trial. Various ad hoc practical solutions are used, none entirely satisfactory. We focus on this problem in phase I–II clinical trials that use binary toxicity and efficacy, defined in terms of event times, to choose doses adaptively for successive cohorts. We propose a general approach to this problem that treats late-onset outcomes as missing data, uses data augmentation to impute missing outcomes from posterior predictive distributions computed from partial follow-up times and complete outcome data, and applies the design's decision rules using the completed data. We illustrate the method with two cancer trials conducted using a phase I–II design based on efficacy–toxicity trade-offs, including a computer stimulation study. Supplementary materials for this article are available online.|Using Data Augmentation to Facilitate Conduct of Phase I–II Clinical Trials With Delayed Outcomes|http://www.jstor.org/stable/24247183|24247183|2014-06-01|2014|['eng']|['Physical sciences - Astronomy', 'Health sciences - Medical treatment']|['Science & Mathematics', 'Statistics']
DNA array technology is an important tool for genomic research due to its capacity of measuring simultaneously the expression levels of a great number of genes or fragments of genes in different experimental conditions. An important point in gene expression data analysis is to identify clusters of genes which present similar expression levels. We propose a new procedure for estimating the mixture model for clustering of gene expression data. The proposed method is a posterior split-merge-birth MCMC procedure which does not require the specification of the number of components, since it is estimated jointly with component parameters. The strategy for splitting is based on data and on posterior distribution from the previously allocated observations. This procedure defines a quick split proposal in contrary to other split procedures, which require substantial computational effort. The performance of the method is verified using real and simulated datasets.|Clustering Gene Expression Data using a Posterior Split-Merge-Birth Procedure|http://www.jstor.org/stable/41679803|41679803|2012-09-01|2012|['eng']|['Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
"Using the clickstream data recorded in Web server log files, the authors develop and estimate a model of the browsing behavior of visitors to a Web site. Two basic aspects of browsing behavior are examined: (1) the visitor's decisions to continue browsing (by submitting an additional page request) or to exit the site and (2) the length of time spent viewing each page. The authors propose a type II tobit model that captures both aspects of browsing behavior and handles the limitations of server log-file data. The authors fit the model to the individual-level browsing decisions of a random sample of 5000 visitors to the Web site of an Internet automotive company. Empirical results show that visitors' propensity to continue browsing changes dynamically as a function of the depth of a given site visit and the number of repeat visits to the site. The dynamics are consistent both with ""within-site lock-in"" or site ""stickiness"" and with learning that carries over repeat visits. In particular, repeat visits lead to reduced page-view propensities but not to reduced page-view durations. The results also reveal browsing patterns that may reflect visitors' timesaving strategies. Finally, the authors report that simple site metrics computed at the aggregate level diverge substantially from individual-level modeling results, which indicates the need for Web site analyses to control for cross-sectional heterogeneity."|A Model of Web Site Browsing Behavior Estimated on Clickstream Data|http://www.jstor.org/stable/30038857|30038857|2003-08-01|2003|['eng']|['Information science - Data products', 'Applied sciences - Engineering']|['Business', 'Business & Economics Collection', 'Marketing & Advertising']
Estimating the risks heat waves pose to human health is a critical part of assessing the future impact of climate change. In this article, we propose a flexible class of time series models to estimate the relative risk of mortality associated with heat waves and conduct Bayesian model averaging (BMA) to account for the multiplicity of potential models. Applying these methods to data from 105 U.S. cities for the period 1987—2005, we identify those cities having a high posterior probability of increased mortality risk during heat waves, examine the heterogeneity of the posterior distributions of mortality risk across cities, assess sensitivity of the results to the selection of prior distributions, and compare our BMA results to a model selection approach. Our results show that no single model best predicts risk across the majority of cities, and that for some cities heat-wave risk estimation is sensitive to model choice. Although model averaging leads to posterior distributions with increased variance as compared to statistical inference conditional on a model obtained through model selection, we find that the posterior mean of heat wave mortality risk is robust to accounting for model uncertainty over a broad class of models.|A Bayesian Model Averaging Approach for Estimating the Relative Risk of Mortality Associated with Heat Waves in 105 U.S. Cities|http://www.jstor.org/stable/41434466|41434466|2011-12-01|2011|['eng']|['Physical sciences - Astronomy', 'Health sciences - Health and wellness']|['Science and Mathematics', 'Statistics']
Long branches are potentially problematic in molecular dating because they can encompass a vast number of combinations of substitution rate and time. A long branch is suspected to have biased molecular clock estimates of the age of flowering plants (angiosperms) to be much older than their earliest fossils. This study explores the effect of the long branch subtending angiosperms in molecular dating and how different relaxed clocks react to it. Fossil angiosperm relatives, identified through a combined morphological and molecular phylogenetic analysis for living and fossil seed plants, were used to break the long angiosperm stem branch. Nucleotide sequences of angiosperm fossil relatives were simulated using a phylogeny and model parameters from living taxa and incorporated in molecular dating. Three relaxed clocks, which implement among-lineage rate heterogeneity differently, were used: penalized likelihood (using 2 different rate smoothing optimization criteria), a Bayesian rate-autocorrelated method, and a Bayesian uncorrelated method. Different clocks provided highly correlated ages across the tree. Breaking the angiosperm stem branch did not result in major age differences, except for a few sensitive nodes. Breaking the angiosperm stem branch resulted in a substantially younger age for crown angiosperms only with 1 of the 4 methods, but, nevertheless, the obtained age is considerably older than the oldest angiosperm fossils. The origin of crown angiosperms is estimated between the Upper Triassic and the early Permian. The difficulty in estimating crown angiosperm age probably lies in a combination of intrinsic and extrinsic complicating factors, including substantial molecular rate heterogeneity among lineages and through time. A more adequate molecular dating approach might combine moderate background rate heterogeneity with large changes in rate at particular points in the tree.|Using Fossils to Break Long Branches in Molecular Dating: A Comparison of Relaxed Clocks Applied to the Origin of Angiosperms|http://www.jstor.org/stable/20780355|20780355|2010-07-01|2010|['eng']|['Biological sciences - Biology']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
ABSTRACT Despite the advantage of global coverage at high spatiotemporal resolutions, satellite remotely sensed precipitation estimates still suffer from insufficient accuracy that needs to be improved for weather, climate, and hydrologic applications. This paper presents a framework of a deep neural network (DNN) that improves the accuracy of satellite precipitation products, focusing on reducing the bias and false alarms. The state-of-the-art deep learning techniques developed in the area of machine learning specialize in extracting structural information from a massive amount of image data, which fits nicely into the task of retrieving precipitation data from satellite cloud images. Stacked denoising autoencoder (SDAE), a widely used DNN, is applied to perform bias correction of satellite precipitation products. A case study is conducted on the Precipitation Estimation from Remotely Sensed Information Using Artificial Neural Networks Cloud Classification System (PERSIANN-CCS) with spatial resolution of 0.08° × 0.08° over the central United States, where SDAE is used to process satellite cloud imagery to extract information over a window of 15 × 15 pixels. In the study, the summer of 2012 (June–August) and the winter of 2012/13 (December–February) serve as the training periods, while the same seasons of the following year (summer of 2013 and winter of 2013/14) are used for validation purposes. To demonstrate the effectiveness of the methodology outside the study area, threemore regions are selected for additional validation. Significant improvements are achieved in both rain/no-rain (R/NR) detection and precipitation rate quantification: the results make 33% and 43% corrections on false alarm pixels and 98% and 78% bias reductions in precipitation rates over the validation periods of the summer and winter seasons, respectively.|A Deep Neural Network Modeling Framework to Reduce Bias in Satellite Precipitation Products|http://www.jstor.org/stable/24915609|24915609|2016-03-01|2016|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Environmental Science']
"Parametric empirical Bayes (EB) methods of point estimation date to the landmark paper by James and Stein (1961). Interval estimation through parametric empirical Bayes techniques has a somewhat shorter history, which was summarized by Laird and Louis (1987). In the exchangeable case, one obtains a ""naive"" EB confidence interval by simply taking appropriate percentiles of the estimated posterior distribution of the parameter, where the estimation of the prior parameters (""hyperparameters"") is accomplished through the marginal distribution of the data. Unfortunately, these ""naive"" intervals tend to be too short, since they fail to account for the variability in the estimation of the hyperparameters. That is, they do not attain the desired coverage probability in the EB sense defined by Morris (1983a,b). They also provide no statement of conditional calibration (Rubin 1984). In this article we propose a conditional bias correction method for developing EM intervals that corrects these deficiencies in the naive intervals. As an alternative, several authors have suggested use of the marginal posterior in this regard. We attempt to clarify its role in achieving EB coverage. Results of extensive simulation of coverage probability and interval length for these approaches are presented in the context of several illustrative examples."|Approaches for Empirical Bayes Confidence Intervals|http://www.jstor.org/stable/2289531|2289531|1990-03-01|1990|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Researchers often measure stress using questionnaire data on the occurrence of potentially stress-inducing life events and the strength of reaction to these events, characterized as negative or positive and assigned an ordinal ranking. In studying the health effects of stress, one needs to obtain measures of an individual's negative and positive stress levels to be used as predictors. Motivated by data of this type, we propose a latent variable model, which is characterized by event-specific negative and positive reaction scores. If the positive reaction score dominates the negative reaction score for an event, then the individual's reported response to that event will be positive, with an ordinal ranking determined by the value of the score. Measures of overall positive and negative stress can be obtained by summing the reactivity scores across the events that occur for an individual. By incorporating these measures as predictors in a regression model and fitting the stress and outcome models jointly using Bayesian methods, inferences can be conducted without the need to assume known weights for the different events. We propose an MCMC algorithm for posterior computation and apply the approach to study the effects of stress on preterm delivery.|Modeling the Effects of a Bidirectional Latent Predictor from Multivariate Questionnaire Data|http://www.jstor.org/stable/3695472|3695472|2004-12-01|2004|['eng']|['Biological sciences - Biology', 'Applied sciences - Engineering', 'Health sciences - Health and wellness']|['Science & Mathematics', 'Statistics']
In this paper, we describe Bayesian modeling of dependent multivariate survival data using positive stable frailty distributions. A flexible baseline hazard formulation using a piecewise exponential model with a correlated prior process is used. The estimation of the stable law parameter together with the parameters of the (conditional) proportional hazards model is facilitated by a modified Gibbs sampling procedure. The methodology is illustrated on kidney infection data (McGilchrist and Aisbett, 1991).|Multivariate Survival Analysis with Positive Stable Frailties|http://www.jstor.org/stable/2533819|2533819|1999-06-01|1999|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
1. Patterns of forest biodiversity are shaped by a complex set of processes operating over different spatial scales. Climate may largely determine species richness at regional scales, but biotic interactions and disturbance events are known to be important at local scales. The interactions between these local and regional processes are poorly understood, complicating efforts to manage for biodiversity. 2. In this study, we used Spanish forest inventory data, together with hierarchical Bayesian models, to analyse how different harvest intensities affect patterns of species richness in a 152 000 km² area in central Spain. Particularly, we quantified the interacting effects of locally applied silvicultural disturbances, of those applied in the surrounding landscape, and of the regional climate on native tree species richness. 3. Our study supports the overall hypothesis that a hierarchical set of processes influence species richness, with regional climate contributing to shape the impacts of local harvesting practices and other environmental variables (topography and productivity). 4. In particular, we found that partial harvesting in both coniferous and broad-leaved Mediterranean forests may support greater tree species richness than complete harvesting and no management. However, this effect depended on the ecosystem and the surrounding landscape, being much less likely in semi-arid regions under water stress conditions and in landscapes dominated by managed forests (and particularly by completely harvested stands). 5. In general, forest stands exhibited increased tree species richness when surrounded by species-rich riparian forests, probably due to metacommunity dynamics and/or ecological history (land uses) of the area. 6. Synthesis and applications. The effects of forest management on local species richness were shaped by coarse climate conditions and by the type and extent of other management practices in the surrounding landscapes. Therefore, to develop effective forestry management plans that optimize local diversity, we need to (i) apply regionally tailored practices with lower harvest intensities in areas of greater hydric stress; (ii) avoid the extensive application of a single silvicultural system over large areas and (iii) preserve a mosaic of species-rich forests that can act as sources of colonizers to enrich the regenerating stands nearby.|Effects of silviculture on native tree species richness: interactions between management, landscape context and regional climate|http://www.jstor.org/stable/24031543|24031543|2013-06-01|2013|['eng']|['Biological sciences - Ecology']|['Biological Sciences', 'Ecology & Evolutionary Biology', 'Science and Mathematics']
Cancer is the second leading cause of death in the United States. Cancer incidence and mortality rates measure the progress against cancer; these rates are obtained from the Surveillance, Epidemiology, and End Results (SEER) Program of the National Cancer Institute (NCI). Lung cancer has the highest mortality rate among all cancers, whereas prostate cancer has the highest number of new cases among males. In this article, we analyze the incidence rates of these two cancers, as well as colon and rectal cancer. The NCI reports trends in cancer age-adjusted mortality and incidence rates in its annual report to the nation and analyzes them using the Joinpoint software. The location of the joinpoints signifies changes in cancer trends, whereas changes in the regression slope measure the degree of change. The Joinpoint software uses a numerical search to detect the joinpoints, fits regression within two consecutive joinpoints by least squares, and finally selects the number of joinpoints by either a series of permutation tests or the Bayesian information criterion. We propose Bayesian joinpoint models and provide statistical estimates of the joinpoints and the regression slopes. While the Joinpoint software and other work in this area assumes that the joinpoints occur on the discrete time grid, we allow a continuous prior for the joinpoints induced by the Dirichlet distribution on the spacings in between. This prior further allows the user to impose prespecified minimum gaps in between two consecutive joinpoints. We develop parametric as well as semiparametric Bayesian joinpoint models; the semiparametric framework relaxes parametric distributional assumptions by modeling the distribution of regression slopes and error variances using Dirichlet process mixtures. These Bayesian models provide statistical inference with finite sample validity. Through a simulation study, we demonstrate the performance of the proposed parametric and semiparametric joinpoint models and compare the results with the ones from the Joinpoint software. We analyze ageadjusted cancer incidence rates from the SEER Program using these Bayesian models with different numbers of joinpoints by employing the deviance information criterion and the cross-validated predictive criterion. In addition, we model the lung cancer incidence rates and the smoking rates jointly and explore the relation between the two longitudinal processes.|Bayesian Analysis of Cancer Rates From SEER Program Using Parametric and Semiparametric Joinpoint Regression Models|http://www.jstor.org/stable/40592196|40592196|2009-06-01|2009|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
We consider the Bayesian statistical models in which the prior distribution of the parameter vector θ1 in the distribution of an observable random vector X is to be specified in a hierarchical fashion and one wants to learn about the hyperparameters at each level of this prior distribution. It is shown that for a wide class of information measures, based on the so-called f divergence, the information decreases as one moves to higher levels of hyperparameters. This result unifies all the theorems in Goel and DeGroot (1981) and provides several other information measures for which the above desirable property holds.|Information Measures and Bayesian Hierarchical Models|http://www.jstor.org/stable/2288648|2288648|1983-06-01|1983|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
Integral projection models (IPMs) have a number of advantages over matrixmodel approaches for analyzing size-structured population dynamics, because the latter require parameter estimates for each age or stage transition. However, IPMs still require appropriate data. Typically they are parameterized using individual-scale relationships between body size and demographic rates, but these are not always available. We present an alternative approach for estimating demographic parameters from time series of size-structured survey data using a Bayesian state-space IPM (SSIPM). By fitting an IPM in a state-space framework, we estimate unknown parameters and explicitly account for process and measurement error in a dataset to estimate the underlying process model dynamics. We tested our method by fitting SSIPMs to simulated data; the model fit the simulated size distributions well and estimated unknown demographic parameters accurately. We then illustrated our method using nine years of annual surveys of the density and size distribution of two fish species (blue rockfish, Sebastes mystinus, and gopher rockfish, S. camatus) at seven kelp forest sites in California. The SSIPM produced reasonable fits to the data, and estimated fishing rates for both species that were higher than our Bayesian prior estimates based on coast-wide stock assessment estimates of harvest. That improvement reinforces the value of being able to estimate demographic parameters from local-scale monitoring data. We highlight a number of key decision points in SSIPM development (e.g., open vs. closed demography, number of particles in the state-space filter) so that users can apply the method to their own datasets.|Fitting state-space integral projection models to size-structured time series data to estimate unknown parameters|http://www.jstor.org/stable/44132567|44132567|2016-12-01|2016|['eng']|['Biological sciences - Ecology', 'Applied sciences - Engineering']|['Ecology & Evolutionary Biology', 'Science & Mathematics', 'Biological Sciences']
Gaussian factor models have proven widely useful for parsimoniously characterizing dependence in multivariate data. There is rich literature on their extension to mixed categorical and continuous variables, using latent Gaussian variables or through generalized latent trait models accommodating measurements in the exponential family. However, when generalizing to non-Gaussian measured variables, the latent variables typically influence both the dependence structure and the form of the marginal distributions, complicating interpretation and introducing artifacts. To address this problem, we propose a novel class of Bayesian Gaussian copula factor models that decouple the latent factors from the marginal distributions. A semiparametric specification for the marginals based on the extended rank likelihood yields straightforward implementation and substantial computational gains. We provide new theoretical and empirical justifications for using this likelihood in Bayesian inference. We propose new default priors for the factor loadings and develop efficient parameter-expanded Gibbs sampling for posterior computation. The methods are evaluated through simulations and applied to a dataset in political science. The models in this article are implemented in the R package bfa (available from http://stat.duke.edu/jsm38/software/bfa). Supplementary materials for this article are available online.|Bayesian Gaussian Copula Factor Models for Mixed Data|http://www.jstor.org/stable/24246471|24246471|2013-06-01|2013|['eng']|['Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
This paper presents a Bayesian method to estimate the expected proportions in a three-way contingency table appropriate when prior knowledge about the main, first- and second-order interaction effects can be described by a particular kind of exchangeability assumption. The proposed Bayes estimates are calculated by finding those values of the effects which maximize the resulting posterior distribution and can be used to explore the possibility that a nonsaturated submodel, such as independence or conditional independence, fits the data. This extends the work of Leonard (1975) for two-dimensional tables. We discuss numerical strategies to solve the estimating equations and point out how an incorrect choice of values for the `indifference' case, as made by previous authors, can have serious effects on the convergence of the algorithms. The methods is exemplified by a survey on skin cancer and data on voting transitions in British elections.|Bayesian Log Linear Estimates for Three-Way Contingency Tables|http://www.jstor.org/stable/2336155|2336155|1987-06-01|1987|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
The increased collection of high-dimensional data in various fields has raised a strong interest in clustering algorithms and variable selection procedures. In this paper, we propose a model-based method that addresses the two problems simultaneously. We introduce a latent binary vector to identify discriminating variables and use Dirichlet process mixture models to define the cluster structure. We update the variable selection index using a Metropolis algorithm and obtain inference on the cluster structure via a split-merge Markov chain Monte Carlo technique. We explore the performance of the methodology on simulated data and illustrate an application with a DNA microarray study.|Variable Selection in Clustering via Dirichlet Process Mixture Models|http://www.jstor.org/stable/20441333|20441333|2006-12-01|2006|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
This paper proposes a Bayesian approach to a vector autoregression with stochastic volatility, where the multiplicative evolution of the precision matrix is driven by a multivariate beta variate. Exact updating formulas are given to the nonlinear filtering of the precision matrix. Estimation of the autoregressive parameters requires numerical methods: an importance-sampling based approach is explained here.|Bayesian Vector Autoregressions with Stochastic Volatility|http://www.jstor.org/stable/2171813|2171813|1997-01-01|1997|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Mathematics', 'Science & Mathematics', 'Business', 'Economics']
A methodology for the simultaneous Bayesian non-parametric modelling of several distributions is developed. Our approach uses normalized random measures with independent increments and builds dependence through the superposition of shared processes. The properties of the prior are described and the modelling possibilities of this framework are explored in detail. Efficient slice sampling methods are developed for inference. Various posterior summaries are introduced which allow better understanding of the differences between distributions. The methods are illustrated on simulated data and examples from survival analysis and stochastic frontier analysis.|Comparing distributions by using dependent normalized random-measure mixtures|http://www.jstor.org/stable/24772735|24772735|2013-06-01|2013|['eng']|['Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
Do public opinion dynamics play an important role in understanding conflict trajectories between democratic governments and other rival groups? The authors interpret several theories of opinion dynamics as competing clusters of contemporaneous causal links connoting reciprocity, accountability, and credibility. They translate these clusters into four distinct Bayesian structural time series models fit to events data from the Israeli-Palestinian conflict with variables for U.S. Intervention and Jewish public opinion about prospects for peace. A credibility model, allowing Jewish public opinion to influence U.S., Palestinian, and Israeli behavior within a given month, fits best. More pacific Israeli opinion leads to more immediate Palestinian hostility toward Israelis. This response's direction suggests a negative feedback mechanism in which low-level conflict is maintained and momentum toward either all-out war or dramatic peace is slowed. In addition, a forecasting model including Jewish public opinion is shown to forecast ex ante better than a model without this variable.|The Dynamics of Reciprocity, Accountability, and Credibility|http://www.jstor.org/stable/27638613|27638613|2008-06-01|2008|['eng']|['Political science - Politics', 'Political science - Government', 'Philosophy - Applied philosophy']|['Peace & Conflict Studies', 'Political Science', 'Social Sciences', 'International Relations']
Transcriptional reprogramming is integral to effective plant defense. Pathogen effectors act transcriptionally and posttranscriptionally to suppress defense responses. A major challenge to understanding disease and defense responses is discriminating between transcriptional reprogramming associated with microbial-associated molecular pattern (MAMP)-triggered immunity (MTI) and that orchestrated by effectors. A high-resolution time course of genome-wide expression changes following challenge with Pseudomonas syringae pv tomato DC3000 and the nonpathogenic mutant strain DC3000hrpA- allowed us to establish causal links between the activities of pathogen effectors and suppression of MTI and infer with high confidence a range of processes specifically targeted by effectors. Analysis of this information-rich data set with a range of computational tools provided insights into the earliest transcriptional events triggered by effector delivery, regulatory mechanisms recruited, and biological processes targeted. We show that the majority of genes contributing to disease or defense are induced within 6 h postinfection, significantly before pathogen multiplication. Suppression of chloroplast-associated genes is a rapid MAMP-triggered defense response, and suppression of genes involved in chromatin assembly and induction of ubiquitin-related genes coincide with pathogen-induced abscisic acid accumulation. Specific combinations of promoter motifs are engaged in fine-tuning the MTI response and active transcriptional suppression at specific promoter configurations by P. syringae.|Transcriptional Dynamics Driving MAMP-Triggered Immunity and Pathogen Effector-Mediated Immunosuppression in Arabidopsis Leaves Following Infection with <em>Pseudomonas syringae</em> pv tomato DC3000|http://www.jstor.org/stable/plantcell.27.11.3038|plantcell.27.11.3038|2015-11-01|2015|['eng']|['Biological sciences - Biology']|['Science & Mathematics', 'Botany & Plant Sciences', 'Biological Sciences']
We introduce the directionally dispersed class of multivariate distributions, a generalization of the elliptical class. By allowing dispersion of multivariate random variables to vary with direction it is possible to generate a very wide and flexible class of distributions. Directionally dispersed distributions have a simple form for their density, which extends a spherically symmetric density function by including a function D modelling directional dispersion. Under a mild condition, the class of distributions is shown to preserve both unimodality and moment existence. By adequately defining D, it is possible to generate skewed distributions. Using spline models on hyperspheres, we suggest a very flexible, yet practical, implementation for modelling directional dispersion in any dimension. Finally, we use the new class of distributions in a Bayesian regression set-up and analyse the distributions of a set of biomedical measurements and a sample of US manufacturing firms.|Modelling Directional Dispersion through Hyperspherical Log-Splines|http://www.jstor.org/stable/3647647|3647647|2005-01-01|2005|['eng']|['Mathematics - Mathematical objects']|['Science & Mathematics', 'Statistics']
The paper considers Bayesian methods for density regression, allowing a random probability distribution to change flexibly with multiple predictors. The conditional response distribution is expressed as a non-parametric mixture of regression models, with the mixture distribution changing with predictors. A class of weighted mixture of Dirichlet process priors is proposed for the uncountable collection of mixture distributions. It is shown that this specification results in a generalized Pólya urn scheme, which incorporates weights that are dependent on the distance between subjects' predictor values. To allow local dependence in the mixture distributions, we propose a kernel-based weighting scheme. A Gibbs sampling algorithm is developed for posterior computation. The methods are illustrated by using simulated data examples and an epidemiologic application.|Bayesian Density Regression|http://www.jstor.org/stable/4623261|4623261|2007-01-01|2007|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Smoothing of observed measures of healthcare provider performance is well known to lead to advantages in terms of predictive ability. However, with routinely collected longitudinal data there is the opportunity to smooth either between units, across time or both. Hierarchical generalized linear models with time as a covariate and hierarchical time series models each result in such two-way or 'bidirectional' smoothing. These models are increasingly being suggested in the literature, but their advantages relative to simpler alternatives have not been systematically investigated. With reference to two topical examples of performance data sets in the UK, we compare a range of models on the basis of their short-term predictive ability. Rather than focusing on point predictive accuracy alone, fully probabilistic comparisons are made, using proper scoring rules and tests for uniformity of predictive p-values. Hierarchical generalized linear models with time as a covariate were found to perform poorly for both data sets. In contrast, a hierarchical time series model with a latent AR(1) structure has attractive properties and was found to perform well. Of concern, however, is the large amount of time that is needed to fit this model using the WinBUGS software. We suggest that research into simpler and faster methods to fit models of a similar structure would be of much benefit.|Improved probabilistic prediction of healthcare performance indicators using bidirectional smoothing models|http://www.jstor.org/stable/23251171|23251171|2012-07-01|2012|['eng']|['Applied sciences - Engineering']|['Science and Mathematics', 'Statistics']
A recently developed statistical model, called Bayesian vector autoregression, has proven to be a useful tool for economic forecasting. Such a model today forecasts a strong resurgence of growth in the second half of 1985 and in 1986.|A Statistical Approach to Economic Forecasting|http://www.jstor.org/stable/1391378|1391378|1986-01-01|1986|['eng']|['Business - Business operations', 'Economics - Economic disciplines']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
Objectives: A steady increase in incidence of lymphoid neoplasms has been reported, especially for non-Hodgkin's lymphoma (NHL). Using high-quality incidence data from 1973-1992 in nine population-based cancer registries (Alberta, Bombay, Denmark, Israel, New Zealand, Osaka, Oxford, Slovenia, Utah), we have examined past increases in specific lymphoid neoplasms. Further, by using a Bayesian age-period-cohort approach, we have calculated 5-, 10- and 15-year projections for each group of lymphoid neoplasms. Results: NHL incidence increased in all centers by an average of 77% in men and 66% in women between 1973 and 1992. Fifteen-year projections of these rates to 2003-2007 indicate that they will increase by an average of 55% among men and 79% among women. High projected incidence rates above 15/100,000 in men and 10/100,000 in women are expected in Alberta, Denmark, Israel, New Zealand, Oxford, and Utah by 2003-2007. The one notable exception was among men from Osaka, where no increase was projected. Modest increases in leukemia and multiple myeloma rates were observed in most of the nine registries with further projected increases by 2007. Projected incidence rates of Hodgkin's disease indicated little change. Conclusion: Increases in NHL rates are occurring worldwide and provide no evidence of peaking. A key assumption in the projected rates is that the effect of environmental agents determining the trends during 1973-1992 will remain stable during the subsequent projection period.|Recent Trends and Future Projections of Lymphoid Neoplasms: A Bayesian Age-Period-Cohort Analysis|http://www.jstor.org/stable/3553770|3553770|2001-11-01|2001|['eng']|['Physical sciences - Astronomy']|['Health Sciences', 'Medicine and Allied Health']
The last remaining natural population of the critically endangered takahe (Porphyrio hochsletteri) is confined to the Murchison Mountains in Fiordland, New Zealand. This mainland population contains about half of the c. 300 remaining takahe and benefits from one of the costliest recovery programmes in the country. Management activities include deer culling, stoat trapping, nest manipulation (e.g. removal of infertile eggs) and captive rearing of chicks. To determine what effect this intensive management has had on the recovery of the Fiordland takahe population, we modelled 25 years of survival and breeding success data as as function of environmental factors (e.g. precipitation, temperature, beech seedfall, tussock flowering) and specific management activities (egg manipulation, captive rearing, stoat control). Annual adult survival, estimated at 78% (credibility interval (CI) = 75–81%), is significantly increased to 85% (76–92%CI) in presence of stoat trapping, but is still low relative to introduced takahe populations on offshore islands and other large New Zealand bird species in predator-free environments. This suggests that the harsh environment of Fiordland may be suboptimal habitat in terms of survival for takahe. On the other hand, reproductive output in Fiordland is similar to that for introduced island populations, and is improved even further by management. Number of chicks per pair fledged with nest manipulation and captive rearing is estimated at 0.66 compared with 0.43 in the absence of nest management. The difference is explained mainly by low fledging success in the wild, especially for double clutches, which justifies the practice of removing one of two viable eggs and transferring it to a captive-rearing facility. The results of this study indicate that current management activities such as stoat trapping and captive rearing have a strong positive effect on population growth of the Murchison Mountains takahe population.|Demography of takahe (Porphyrio hochstetteri) in Fiordland: environmental factors and management affect survival and breeding success|http://www.jstor.org/stable/24060877|24060877|2012-01-01|2012|['eng']|['Physical sciences - Astronomy']|['Ecology & Evolutionary Biology', 'Science & Mathematics']
The paper 'Modern statistics for spatial point processes' by Jesper Møller and Rasmus P. Waagepetersen is based on a special invited lecture given by the authors at the 21st Nordic Conference on Mathematical Statistics, held at Rebild, Denmark, in June 2006. At the conference, Antti Penttinen and Eva B. Vedel Jensen were invited to discuss the paper. We here present the comments from the two invited discussants and from a number of other scholars, as well as the authors' responses to these comments. Below Figure 1, Figure 2, etc., refer to figures in the paper under discussion, while Figure A, Figure B, etc., refer to figures in the current discussion. All numbered sections and formulas refer to the paper.|Discussion of 'Modern Statistics for Spatial Point Processes'|http://www.jstor.org/stable/41548577|41548577|2007-12-01|2007|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Two of the critical issues that arise when examining DNA microarray data are (1) determination of which genes best discriminate among the different types of tissue, and (2) characterization of expression patterns in tumor tissues. For (1), there are many genes that characterize DNA expression, and it is of critical importance to try and identify a small set of genes that best discriminate between normal and tumor tissues. For (2), it is critical to be able to characterize the DNA expression of the normal and tumor tissue samples and develop suitable models that explain patterns of DNA expression for these types of tissues. Toward this goal, we propose a novel Bayesian model for analyzing DNA microarray data and propose a model selection methodology for identifying subsets of genes that show different expression levels between normal and cancer tissues. In addition, we propose a novel class of hierarchical priors for the parameters that allow us to borrow strength across genes for making inference. The properties of the priors are examined in detail. We introduce a Bayesian model selection criterion for assessing the various models, and develop Markov chain Monte Carlo algorithms for sampling from the posterior distributions of the parameters and for computing the criterion. We present a detailed case study in endometrial cancer to demonstrate our proposed methodology.|Bayesian Models for Gene Expression with DNA Microarray Data|http://www.jstor.org/stable/3085761|3085761|2002-03-01|2002|['eng']|['Applied sciences - Engineering', 'Biological sciences - Biology']|['Science & Mathematics', 'Statistics']
This article asks a relatively simple question: does the creation of majority black electoral districts in the South aid Republicans? Given the post-1990 reapportionment of state legislative and congressional districts, and the zeal with which Sections Two and Five of the Voting Rights Act have been enforced by Republican Justice Departments, this is a timely question. The rise in the electoral fortunes of southern Republicans in the past decade also demands an answer to this question. Using electoral and district-level census data for the congressional districts of eight southern states in the 1990 and 1992 elections, this article found that, of the nine seats taken by the Republicans in 1992, four were due to the creation of majority black districts. Further, the following analysis finds that several previously uncompetitive Democratically held districts were made competitive by the creation of majority black districts, and can be expected to swing Republican in the next few years, especially given further acceleration of retirements by white Democratic incumbents.|Does the Creation of Majority Black Districts Aid Republicans? An Analysis of the 1992 Congressional Elections in Eight Southern States|http://www.jstor.org/stable/2960312|2960312|1995-05-01|1995|['eng']|['Political science - Politics']|['Political Science', 'Social Sciences']
Parametric empirical Bayes methods are discussed for estimating the mean proportion response from generalized linear regression models (GLiM's) based on the binomial distribution, including the well-known case of logistic regression. The GLiM's are extended via parametric families of link functions that embed specific links--such as the logit--within their parametric structure. The models may be viewed as members of a larger class of conditionally independent hierarchical models. An example from environmental mutagenesis, wherein a hierarchical effect is induced by similarities among responding units, motivates consideration of the hierarchical GLiM. Empirical Bayes estimation of the mean proportion response is addressed for this example, with emphasis directed at extensions of the logit model.|Empirical Bayes Estimation for Logistic Regression and Extended Parametric Regression Models|http://www.jstor.org/stable/1400367|1400367|1996-06-01|1996|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Agriculture', 'Statistics']
"Social scientists spend considerable energy constructing typologies and discussing their roles in measurement. Less discussed is the role of typologies in evaluating and revising theoretical arguments. We argue that unsupervised machine learning tools can be profitably applied to the development and testing of theory-based typologies. We review recent advances in mixture models as applied to cluster analysis and argue that these tools are particularly important in the social sciences where it is common to claim that high-dimensional objects group together in meaningful clusters. Model-based clustering (MBC) grounds analysis in probability theory, permitting the evaluation of uncertainty and application of informationbased model selection tools. We show that the MBC approach forces analysts to consider dimensionality problems that more traditional clustering tools obscure. We apply MBC to the ""varieties of capitalism,"" a typology receiving significant attention in political science and economic sociology. We find weak and conflicting evidence for the theory's expected grouping. We therefore caution against the current practice of including typology-derived dummy variables in regression and case-comparison research designs."|Model-based Clustering and Typologies in the Social Sciences|http://www.jstor.org/stable/41403740|41403740|2012-01-01|2012|['eng']|['Mathematics - Applied mathematics']|['Political Science', 'Social Sciences']
An individual's health condition can affect the frequency and intensity of episodes that can occur repeatedly and that may be related to an event time of interest. For example, bleeding episodes during pregnancy may indicate problems predictive of preterm delivery. Motivated by this application, we propose a joint model for a multiple episode process and an event time. The frequency of occurrence and severity of the episodes are characterized by a latent variable model, which allows an individual's episode intensity to change dynamically over time. This latent episode intensity is then incorporated as a predictor in a discrete time model for the terminating event. Time-varying coefficients are used to distinguish among effects earlier versus later in gestation. Formulating the model within a Bayesian framework, prior distributions are chosen so that conditional posterior distributions are conjugate after data augmentation. Posterior computation proceeds via an efficient Gibbs sampling algorithm. The methods are illustrated using bleeding episode and gestational length data from a pregnancy study. /// L'état de santé d'un individu peut agir sur la fréquence et l'intensité d'épisodes se produisant de façon répétée. Ces épisodes peuvent eux-mêmes être prédictifs de la survenue d'un événement plus important, qui constitue une variable d'intérêt. Ainsi, des épisodes de saignements pendant la grossesse peuvent révéler des problèmes prédictifs d'accouchement prématuré. C'est du reste cet exemple qui nous a conduits à proposer une modélisation conjointe de l'occurrence des épisodes répétés et du temps de survenue de l'événement final. La fréquence des épisodes et leur sévérité sont caractérisées à l'aide d'un modèle à variables latentes, où l'intensité des épisodes d'un individu peut varier dans le temps, de façon dynamique. Cette intensité latente des épisodes est ensuite introduite, en tant que facteur prédictif, dans un modèle à temps discret de l'événement final. Des coefficients dépendant du temps permettent de distinguer, parmi les effets estimés, ceux qui, au cours de la grossesse, auront une influence précoce ou tardive. Ce modèle est formulé dans un cadre bayésien, et les distributions a priori sont choisies de telle sorte que les distributions conditionnelles a posteriori puissent se conjuguer aisément avec de nouvelles données, au fur et à mesure de leur accumulation (on effectue ces calculs à l'aide d'un échantillonnage de Gibbs). A titre d'illustration, on présente l'application de ces méthodes à des données d'épisodes de saignements et de termes d'accouchements, données extraites d'une étude sur les grossesses.|Bayesian Modeling of Multiple Episode Occurrence and Severity with a Terminating Event|http://www.jstor.org/stable/4541348|4541348|2007-06-01|2007|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Much current analysis of cancer registry data uses the semiparametric proportional hazards Cox model. In this paper, the time-dependent effect of various prognostic indicators on breast cancer survival times from the West Midlands Cancer Intelligence Unit are investigated. Using Bayesian methodology and Markov chain Monte Carlo estimation methods, we develop a parametric dynamic survival model which avoids the proportional hazards assumption. The model has close links to that developed by both Gamerman and Sinha and co-workers: the log-base-line hazard and covariate effects are piecewise constant functions, related between intervals by a simple stochastic evolution process. Here this evolution is assigned a parametric distribution, with a variance that is further included as a hyperparameter. To avoid problems of convergence within the Gibbs sampler, we consider using a reparameterization. It is found that, for some of the prognostic indicators considered, the estimated effects change with increasing follow-up time. In general those prognostic indicators which are thought to be representative of the most hazardous groups (late-staged tumour and oldest age group) have a declining effect.|A Parametric Dynamic Survival Model Applied to Breast Cancer Survival Times|http://www.jstor.org/stable/3592619|3592619|2002-01-01|2002|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
We present a Monte Carlo approach to estimation of the recombination fraction and the profile likelihood for a dichotomous trait and a single marker gene with 2 alleles. The method is an application of a technique known as 'Gibbs sampling', in which random samples of each of the unknowns (here genotypes θ, and nuisance parameters, including the allele frequencies and the penetrances) are drawn from their posterior distributions, given the data and the current values of all the other unknowns. Upon convergence, the resulting samples derive from the marginal distribution of all the unknowns, given only the data, so that the uncertainty in the specification of the nuisance parameters is reflected in the variance of the posterior distribution of θ. Prior knowledge about the distribution of θ and the nuisance parameters can be incorporated using a Bayesian approach, but adoption of a flat prior for θ and point priors for the nuisance parameters would correspond to the standard likelihood approach. The method is easy to program, runs quickly on a microcomputer, and could be generalized to multiple alleles, multipoint linkage, continuous phenotypes and more complex models of disease etiology. The basic approach is illustrated by application to data on cholesterol levels and an a low-density lipoprotein receptor gene in a single large pedigree.|A Gibbs Sampling Approach to Linkage Analysis|http://www.jstor.org/stable/45102504|45102504|1992-01-01|1992|['eng']|['Applied sciences - Engineering', 'Biological sciences - Biology']|['Science & Mathematics', 'Biological Sciences', 'Medicine & Allied Health', 'Health Sciences']
In this paper we support the proposition that the output-unemployment relationship as represented by Okun's law is asymmetric. Okun's coefficients are defined based on a dynamic model that allows for asymmetry in the relationship between cyclical output and unemployment. Using data from the United States for the post-war period, our results show that the short-run effects of positive cyclical output on cyclical unemployment are quantitatively different from those of negative ones, and the data are consistent with the proposition that cyclical unemployment is more sensitive to negative than to positive cyclical output. Several theoretical explanations of asymmetry rationalize the findings.|Asymmetry in Okun's Law|http://www.jstor.org/stable/3696151|3696151|2004-05-01|2004|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Business', 'Political Science', 'Economics', 'Social Sciences']
The task of statistical model selection is to choose a family of distributions among a possible set of families, which is the best approximation of reality manifested in the observed data. In this paper, we survey the model selection criteria discussed in statistical literature. We are mainly concerned with those used in regression analysis and time series.|On Model Selection|http://www.jstor.org/stable/4356163|4356163|2001-01-01|2001|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics']|['Mathematics', 'Science & Mathematics', 'Statistics']
Thin-plate splines have been widely used as spatial smoothers. In this article, we present a Bayesian adaptive thin-plate spline (BATS) suitable for modeling nonstationary spatial data. Fully Bayesian inference can be carried out through efficient Markov chain Monte Carlo simulation. Performance is demonstrated with simulation studies and by an application to a rainfall dataset. A FORTRAN program implementing the method, a proof of the theorem, and the dataset in this article are available online.|Nonstationary Spatial Gaussian Markov Random Fields|http://www.jstor.org/stable/25651302|25651302|2010-03-01|2010|['eng']|['Mathematics - Mathematical objects']|['Science & Mathematics', 'Computer Science', 'Statistics']
ABSTRACT Finite Element (FE) models are widely used in automotive for vehicle design. Even with increasing speed of computers, the simulation of high fidelity FE models is still too time-consuming to perform direct design optimization. As a result, response surface models (RSMs) are commonly used as surrogates of the FE models to reduce the turn-around time. However, RSM may introduce additional sources of uncertainty, such as model bias, and so on. The uncertainty and model bias will affect the trustworthiness of design decisions in design processes. This calls for the development of stochastic model interpolation and extrapolation methods that can address the discrepancy between the RSM and the FE results, and provide prediction intervals of model responses under uncertainty. This paper investigates and compares three stochastic methods for model interpolation and extrapolation, and they are: (1) Bayesian inference-based method, (2) Gaussian process modeling-based method, and (3) Copula-based method, and several validation metrics are proposed to evaluate the prediction results. An analytical case study and a vehicle design problem are used to demonstrate the advantages and disadvantages of these methods.|On Stochastic Model Interpolation and Extrapolation Methods for Vehicle Design|http://www.jstor.org/stable/26268548|26268548|2013-06-01|2013|['eng']|['Applied sciences - Engineering']|
The goal of the paper is to predict the additional amount of antiretroviral treatment that would be required to implement a policy of treating all human immunodeficiency virus (HIV) infected people at the time of detection of infection rather than at the time that their CD4 T-lymphocyte counts are observed to be below a threshold—the current standard of care. We describe a sampling-based inverse prediction method for predicting time from HIV infection to attainment of the CD4 cell threshold and apply it to a set of treatment naive HIV-infected subjects in a village in Botswana who participated in a household survey that collected cross-sectional CD4 cell counts. The inferential target of interest is the population level mean time to reaching the CD4 cell-based treatment threshold in this group of subjects. To address the challenges arising from the fact that these subjects' dates of HIV infection are unknown, we make use of data from an auxiliary cohort study of subjects enrolled shortly after HIV infection in which CD4 cell counts were measured over time. We use a multiple-imputation framework to combine across the different sources of data, and we discuss how the methods compensate for the length-biased sampling that is inherent in cross-sectional screening procedures, such as household surveys. We comment on how the results bear on analyses of costs of implementation of treatment-for-prevention use of antiretroviral drugs in HIV prevention interventions.|Predicting time to threshold for initiating antiretroviral treatment to evaluate cost of treatment as prevention of human immunodeficiency virus|http://www.jstor.org/stable/24771898|24771898|2015-02-01|2015|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
The author shows how geostatistical data that contain measurement errors can be analyzed objectively by a Bayesian approach using Gaussian random fields. He proposes a reference prior and two versions of Jeffreys' prior for the model parameters. He studies the propriety and the existence of moments for the resulting posteriors. He also establishes the existence of the mean and variance of the predictive distributions based on these default priors. His reference prior derives from a representation of the integrated likelihood that is particularly convenient for computation and analysis. He further shows that these default priors are not very sensitive to some aspects of the design and model, and that they have good frequentist properties. Finally, he uses a data set of carbon/nitrogen ratios from a agricultural field to illustrate his approach. /// L'auteur montre comment des données géostatistiques entachées d'erreurs de mesure peuvent être analysées objectivement par une approche bayésienne à l'aide de champs aléatoires gaussiens. Il propose une loi a priori de référence et deux versions de la loi de Jeffreys pour les paramètres du modèle. Il étudie l'intégrabilité et l'existence des moments des lois a posteriori correspondantes. Il démontre aussi l'existence de l'espérance et de la variance des lois prévisionnelles déduites de ces lois a priori objectives. Sa loi a priori de référence découle d'une représentation de la vraisemblance intégrée qui est très commode aux fins de calcul et d'analyse. Il montre par ailleurs que ces lois a priori sont plutôt insensibles à certaines caractéristiques du plan d'expérience et du modèle, en plus de bien se comporter au plan fréquentiste. Enfin, il se sert de données sur le rapport carbone/azote d'une terre agricole pour illustrer son propos.|Objective Bayesian Analysis of Spatial Data with Measurement Error|http://www.jstor.org/stable/20445254|20445254|2007-06-01|2007|['eng']|['Mathematics - Applied mathematics', 'Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
"To evaluate the utility of automated deformable image registration (DIR) algorithms, it is necessary to evaluate both the registration accuracy of the DIR algorithm itself, as well as the registration accuracy of the human readers from whom the ""gold standard"" is obtained. We propose a Bayesian hierarchical model to evaluate the spatial accuracy of human readers and automatic DIR methods based on multiple image registration data generated by human readers and automatic DIR methods. To fully account for the locations of landmarks in all images, we treat the true locations of landmarks as latent variables and impose a hierarchical structure on the magnitude of registration errors observed across image pairs. DIR registration errors are modeled using Gaussian processes with reference prior densities on prior parameters that determine the associated covariance matrices. We develop a Gibbs sampling algorithm to efficiently fit our models to high-dimensional data, and apply the proposed method to analyze an image dataset obtained from a 4D thoracic CT study."|Evaluation of Image Registration Spatial Accuracy Using a Bayesian Hierarchical Model|http://www.jstor.org/stable/24537929|24537929|2014-06-01|2014|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
In microarray experiments, accurate estimation of the gene variance is a key step in the identification of differentially expressed genes. Variance models go from the too stringent homoscedastic assumption to the overparameterized model assuming a specific variance for each gene. Between these two extremes there is some room for intermediate models. We propose a method that identifies clusters of genes with equal variance. We use a mixture model on the gene variance distribution. A test statistic for ranking and detecting differentially expressed genes is proposed. The method is illustrated with publicly available complementary deoxyribonucleic acid microarray experiments, an unpublished data set and further simulation studies.|Mixture Model on the Variance for the Differential Analysis of Gene Expression Data|http://www.jstor.org/stable/3592597|3592597|2005-01-01|2005|['eng']|['Biological sciences - Biology']|['Science & Mathematics', 'Statistics']
Interpersonal communications have long been recognized as an influential source of information for consumers. Internet-based media have facilitated information exchange among firms and consumers, as well as observability and measurement of such exchanges. However, much of the research addressing online communication focuses on ratings collected from online forums. In this paper, we look beyond ratings to a more comprehensive view of online communications. We consider the sales effect of the volume of positive, negative, and neutral online communications captured by Web crawler technology and classified by automated sentiment analysis. Our modeling approach captures two key features of our data, dynamics and endogeneity. In terms of dynamics, we model daily measures of online communications about a firm and its products as contributing to a latent demand-generating stock variable. To account for the endogeneity, we extend the latent instrumental variable technique to account for dynamic endogenous regressors. Our results demonstrate a significant effect of positive, negative, and neutral online communications on daily sales performance. Failure to account for endogeneity results in a severe attenuation of the estimated effects. From a managerial perspective, we demonstrate the importance of accounting for communication valence as well as the impact of shocks to positive, negative, and neutral online communications.|A Dynamic Model of the Effect of Online Communications on Firm Sales|http://www.jstor.org/stable/23012020|23012020|2011-07-01|2011|['eng']|['Information science - Informetrics']|['Business', 'Business & Economics Collection', 'Marketing & Advertising']
This paper studies the multiplicity-correction effect of standard Bayesian variable-selection priors in linear regression. Our first goal is to clarify when, and how, multiplicity correction happens automatically in Bayesian analysis, and to distinguish this correction from the Bayesian Ockham's-razor effect. Our second goal is to contrast empirical-Bayes and fully Bayesian approaches to variable selection through examples, theoretical results and simulations. Considerable differences between the two approaches are found. In particular, we prove a theorem that characterizes a surprising aymptotic discrepancy between fully Bayes and empirical Bayes. This discrepancy arises from a different source than the failure to account for hyperparameter uncertainty in the empirical-Bayes estimate. Indeed, even at the extreme, when the empirical-Bayes estimate converges asymptotically to the true variable-inclusion probability, the potential for a serious difference remains.|BAYES AND EMPIRICAL-BAYES MULTIPLICITY ADJUSTMENT IN THE VARIABLE-SELECTION PROBLEM|http://www.jstor.org/stable/29765241|29765241|2010-10-01|2010|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
Purchasing power parity (PPP) is a proposition equating the nominal exchange rate to the ratio of the domestic to foreign price levels. This article employs Bayesian panel-data methods to test for PPP, in particular, the implied symmetry and proportionality conditions. Using a dataset of all the Organization for Economic Cooperation and Development member countries in the post-Bretton Woods era, I do not find support for symmetry and proportionality in PPP over time and across countries.|Testing Symmetry and Proportionality in PPP: A Panel-Data Approach|http://www.jstor.org/stable/1392398|1392398|1999-10-01|1999|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
We examine the class of extended generalized inverse Gaussian (EGIG) distributions. This class of distributions, which appeared briefly in a monograph by J0rgensen (1982), is more deeply and broadly studied in this paper. We start by deriving its probabilistic properties. Furthermore, we use the EGIG family in two popular and important statistical problems, one from survival analysis and the other from financial econometrics. In the first one, a survival model, we compare our results with those obtained by Achcar and Bolfarine (1986) and found out that the EGIG explains better the data at the cost of one extra parameter. In the second case, we extend the traditional univariate stochastic volatility model by allowing the errors of the squared returns of the BOVESPA index to be driven by a EGIG distribution. We compared our results with those found by Lopes and Migon (2002) and find once again that the EGIG is able to identify and estimate the quantities usually searched by standard univariate stochastic volatility models, such as highly persistent and skewed log-volatilities. Markov chain Monte Carlo are specifically tailored for both applications and a variant of the slice sampler is used to sample from some of the full conditionals.|The extended generalized inverse Gaussian distribution for log-linear and stochastic volatility models|http://www.jstor.org/stable/43601074|43601074|2006-06-01|2006|['eng']|['Mathematics - Mathematical objects', 'Applied sciences - Engineering']|['Science & Mathematics', 'Mathematics', 'Statistics']
We consider a parametric model for time series of counts by constructing a likelihood-based generalization of a model considered by Zeger (1988). We consider a Bayesian approach and propose a class of informative prior distributions for the model parameters that are useful for variable subset selection. The prior specification is motivated from the notion of the existence of data from similar previous studies, called historical data, which is then quantified in a prior distribution for the current study. We derive theoretical and computational properties of the proposed priors and develop novel methods for computing posterior model probabilities. To compute the posterior model probabilities, we show that only posterior samples from the full model are needed to estimate the posterior probabilities for all of the possible subset models. We demonstrate our methodology with a simulated and a real data set.|BAYESIAN VARIABLE SELECTION FOR TIME SERIES COUNT DATA|http://www.jstor.org/stable/24306758|24306758|2000-07-01|2000|['eng']|['Mathematics - Mathematical logic', 'Mathematics - Applied mathematics']|['Mathematics', 'Science and Mathematics', 'Statistics']
Both pravastatin and aspirin are approved by the U.S. Food and Drug Administration (FDA) for secondary prevention of cardiovascular events. This article describes statistical analyses used for a successful submission to the FDA that contends that copackaging pravastatin and aspirin provides a health benefit. From the efficacy perspective this is taken to mean that the combination is more effective than either agent considered alone. We present three Bayesian hierarchical survival models and apply them to the results of five randomized clinical trials. These trials evaluated the benefit of pravastatin in the secondary-prevention setting. Aspirin use was recorded for patients in these trials, but assignment to aspirin was not randomized. We compare the effects of pravastatin and aspirin considered in combination and when given alone. We focus on time to myocardial infarction, although it was just one of several endpoints considered in the presentation to the FDA. Two of the models assume proportional hazards and the third does not. In all three models we adjust for known covariates. Our principal focus is the probability that the combination of pravastatin and aspirin is at least as effective as the agents considered separately. We also find the probability that the combination is synergistic in the sense that the effect of the combination is better than the sum of the effects of the two agents taken alone.|Bayesian Survival Analysis with Nonproportional Hazards: Metanalysis of Combination Pravastatin-Aspirin|http://www.jstor.org/stable/27590351|27590351|2004-03-01|2004|['eng']|['Health sciences - Medical specialties', 'Health sciences - Medical treatment']|['Science & Mathematics', 'Statistics']
The data that are analysed are from a monitoring survey which was carried out in 1994 in the forests of Baden-Württemberg, a federal state in the south-western region of Germany. The survey is part of a large monitoring scheme that has been carried out since the 1980s at different spatial and temporal resolutions to observe the increase in forest damage. One indicator for tree vitality is tree defoliation, which is mainly caused by intrinsic factors, age and stand conditions, but also by biotic (e.g. insects) and abiotic stresses (e.g. industrial emissions). In the survey, needle loss of pine-trees and many potential covariates are recorded at about 580 grid points of a 4 km x 4 km grid. The aim is to identify a set of predictors for needle loss and to investigate the relationships between the needle loss and the predictors. The response variable needle loss is recorded as a percentage in 5% steps estimated by eye using binoculars and categorized into healthy trees (10% or less), intermediate trees (10-25%) and damaged trees (25% or more). We use a Bayesian cumulative threshold model with non-linear functions of continuous variables and a random effect for spatial heterogeneity. For both the non-linear functions and the spatial random effect we use Bayesian versions of P-splines as priors. Our method is novel in that it deals with several non-standard data requirements: the ordinal response variable (the categorized version of needle loss), non-linear effects of covariates, spatial heterogeneity and prediction with missing covariates. The model is a special case of models with a geoadditive or more generally structured additive predictor. Inference can be based on Markov chain Monte Carlo techniques or mixed model technology.|A Spatial Model for the Needle Losses of Pine-Trees in the Forests of Baden-Württemberg: An Application of Bayesian Structured Additive Regression|http://www.jstor.org/stable/4126379|4126379|2007-01-01|2007|['eng']|['Applied sciences - Engineering', 'Biological sciences - Paleontology']|['Science & Mathematics', 'Statistics']
This paper concerns the use of Markov chain Monte Carlo methods for posterior sampling in Bayesian nonparametric mixture models with normalized random measure priors. Making use of some recent posterior characterizations for the class of normalized random measures, we propose novel Markov chain Monte Carlo methods of both marginal type and conditional type. The proposed marginal samplers are generalizations of Neal's well-regarded Algorithm 8 for Dirichlet process mixture models, whereas the conditional sampler is a variation of those recently introduced in the literature. For both the marginal and conditional methods, we consider as a running example a mixture model with an underlying normalized generalized Gamma process prior, and describe comparative simulation results demonstrating the efficacies of the proposed methods.|MCMC for Normalized Random Measure Mixture Models|http://www.jstor.org/stable/43288421|43288421|2013-08-01|2013|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
"Compromises between Bayesian and non-Bayesian significance testing are exemplified by examining distributions of criteria for multinominal equiprobability. They include Pearson's X2, the likelihood-ratio, the Bayes factor F, and a statistic G that previously arose from a Bayesian model by ""Type II Maximum Likelihood."" Its asymptotic distribution, implied by the theory of the ""Type II Likelihood Ratio,"" is remarkably accurate into the extreme tail. F too can be treated as a non-Bayesian criterion and is almost equivalent to G. The relationship between F and its own tail area sheds further light on the relationship between Bayesian and ""Fisherian"" significance."|The Bayes/Non-Bayes Compromise and the Multinomial Distribution|http://www.jstor.org/stable/2286006|2286006|1974-09-01|1974|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
The Lasso estimate for linear regression parameters can be interpreted as a Bayesian posterior mode estimate when the regression parameters have independent Laplace (i.e., double-exponential) priors. Gibbs sampling from this posterior is possible using an expanded hierarchy with conjugate normal priors for the regression parameters and independent exponential priors on their variances. A connection with the inverse-Gaussian distribution provides tractable full conditional distributions. The Bayesian Lasso provides interval estimates (Bayesian credible intervals) that can guide variable selection. Moreover, the structure of the hierarchical model provides both Bayesian and likelihood methods for selecting the Lasso parameter. Slight modifications lead to Bayesian versions of other Lasso-related estimation methods, including bridge regression and a robust variant.|The Bayesian Lasso|http://www.jstor.org/stable/27640090|27640090|2008-06-01|2008|['eng']|['Mathematics - Applied mathematics', 'Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Zellner's g prior remains a popular conventional prior for use in Bayesian variable selection, despite several undesirable consistency issues. In this article we study mixtures of g priors as an alternative to default g priors that resolve many of the problems with the original formulation while maintaining the computational tractability that has made the g prior so popular. We present theoretical properties of the mixture g priors and provide real and simulated examples to compare the mixture formulation with fixed g priors, empirical Bayes approaches, and other default procedures.|Mixtures of g Priors for Bayesian Variable Selection|http://www.jstor.org/stable/27640050|27640050|2008-03-01|2008|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
In the context of state-space modeling, conventional usage of the deviance information criterion (DIC) evaluates the ability of the model to predict an observation at time t given the underlying state at time t. Motivated by the failure of conventional DIC to clearly choose between competing multivariate nonlinear Bayesian state-space models for coho salmon population dynamics, and the computational challenge of alternatives, this work proposes a one-step-ahead DIC, DICp, where prediction is conditional on the state at the previous time point. Simulations revealed that DICp worked well for choosing between state-space models with different process or observation equations. In contrast, conventional DIC could be grossly misleading, with a strong preference for the wrong model. This can be explained by its failure to account for inflated estimates of process error arising from the model mis-specification. DICp is not based on a true conditional likelihood, but is shown to have interpretation as a pseudo-DIC in which the compensatory behavior of the inflated process errors is eliminated. It can be easily calculated using the DIC monitors within popular BUGS software when the process and observation equations are conjugate. The improved performance of DICp is demonstrated by application to the multi-stage modeling of coho salmon abundance in Lobster Creek, Oregon.|A One-Step-Ahead Pseudo-DIC for Comparison of Bayesian State-Space Models|http://www.jstor.org/stable/24538381|24538381|2014-12-01|2014|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
Economical sampling plans to ensure the qualities of Burr type XII distributed lifetimes were established using a truncated life test. The Bayesian inference method was used to address the lot-to-lot variation of products. The sampling plan was characterized by the sample size and the acceptance number to minimize the expected total cost. A simple empirical Bayesian estimation method was provided to estimate the hyperparameters of prior distribution, and simulation studies were conducted to validate the proposed empirical Bayesian estimation method. Lastly, the application of this proposed method was illustrated using two examples.|Economical sampling plans with warranty based on truncated data from Burr type XII distribution|http://www.jstor.org/stable/24505460|24505460|2015-09-01|2015|['eng']|['Applied sciences - Engineering']|['Business & Economics', 'Business']
We address the question of introducing expert knowledge in a hierarchical Bayesian spatiotemporal model. Specifically, we combine avalanche count data and an expert prior judgement about avalanche climatology to cluster French Alpine townships into two coherent groups. A spatial regression defines the a priori probability of belonging to each group and an innovative method is proposed to elicit its parameters by working with an expert on replicates of simulated maps. As a benefit of the extra data knowledge, we show the existence of robust north–south clusters of decreasing–increasing avalanche activity resulting from the interaction of local climate change patterns with altitude distribution. Spatial dependence of avalanche activity has also been inferred and is characterized by an anisotropy, with longer dependence along the north-west–south-east axis.|Adding expert contributions to the spatiotemporal modelling of avalanche activity under different climatic influences|http://www.jstor.org/stable/24772432|24772432|2015-08-01|2015|['eng']|['Physical sciences - Astronomy']|['Science & Mathematics', 'Statistics']
"Massively Parallel Signature Sequencing (MPSS) is a high-throughput, counting-based technology available for gene expression profiling. It produces output that is similar to Serial Analysis of Gene Expression and is ideal for building complex relational databases for gene expression. Our goal is to compare the in vivo global gene expression profiles of tissues infected with different strains of ""Salmonella"" obtained using the MPSS technology. In this article, we develop an exact ANOVA type model for this count data using a zero-inflated Poisson distribution, different from existing methods that assume continuous densities. We adopt two Bayesian hierarchical models—one parametric and the other semiparametric with a Dirichlet process prior that has the ability to ""borrow strength"" across related signatures, where a signature is a specific arrangement of the nucleotides, usually 16–21 base pairs long. We utilize the discreteness of Dirichlet process prior to cluster signatues that exhibit similar differential expression profiles. Tests for differential expression are carried out using nonparametric approaches, while controlling the false discovery rate. We identify several differentially expressed genes that have important biological significance and conclude with a summary of the biological discoveries. This article has supplementary materials online."|"Bayesian Modeling of MPSS Data: Gene Expression Analysis of Bovine ""Salmonella"" Infection"|http://www.jstor.org/stable/27920119|27920119|2010-09-01|2010|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
We introduce a class of Bayesian semiparametric models for regression problems in which the response variable is a count. Our goal is to provide a flexible, easy-to-implement and robust extension of generalised linear models, for datasets of moderate or large size. Our approach is based on modelling the distribution of the response variable using a Dirichlet process, whose mean distribution function is itself random and is given a parametric form, such as a generalised linear model. The effects of the explanatory variables on the response are modelled via both the parameters of the mean distribution function of the Dirichlet process and the total mass parameter. We discuss modelling options and relationships with other approaches. We derive in closed form the marginal posterior distribution of the regression coefficients and discuss its use in inference and computing. We illustrate the benefits of our approach with a prognostic model for early breast cancer patients.|Semiparametric Regression for Count Data|http://www.jstor.org/stable/4140576|4140576|2002-06-01|2002|['eng']|['Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
In many modern experimental settings, observations are obtained in the form of functions and interest focuses on inferences about a collection of such functions. We propose a hierarchical model that allows us simultaneously to estimate multiple curves nonparametrically by using dependent Dirichlet process mixtures of Gaussian distributions to characterize the joint distribution of predictors and outcomes. Function estimates are then induced through the conditional distribution of the outcome given the predictors. The resulting approach allows for flexible estimation and clustering, while borrowing information across curves. We also show that the function estimates we obtain are consistent on the space of integrable functions. As an illustration, we consider an application to the analysis of conductivity and temperature at depth data in the north Atlantic.|Bayesian nonparametric functional data analysis through density estimation|http://www.jstor.org/stable/27798808|27798808|2009-03-01|2009|['eng']|['Applied sciences - Engineering', 'Mathematics - Applied mathematics']|['Science & Mathematics', 'Statistics']
We develop a Bayesian multiple changepoint model to infer spatial phylogenetic variation (SPV) along aligned molecular sequence data. SPV occurs in sequences from organisms that have undergone biological recombination or when evolutionary rates and selective pressures vary along the sequences. This Bayesian approach permits estimation of uncertainty regarding recombination, the crossing-over locations, and all other model parameters. The model assumes that the sites along the data separate into an unknown number of contiguous segments, each with possibly different evolutionary relationships between organisms, evolutionary rates, and transition: transversion ratios. We develop a transition kernel, use reversible-jump Markov chain Monte Carlo to fit our model, and draw inference from both simulated and real data. Through simulation, we examine the minimal length recombinant segment that our model can detect for several levels of evolutionary divergence. We examine the entire genome of a reported human immunodeficiency virus (HIV)- 1 isolate, related to a purported recombinant virus thought to be the causative agent of an epidemic outbreak of HIV-1 infection among intravenous drug users in Russia. We find that regions of the genome differ in their evolutionary history and selective pressures. There is strong evidence for multiple crossovers along the genome and frequent shifts in selective pressure changes throughout the vif through env genes.|Inferring Spatial Phylogenetic Variation along Nucleotide Sequences: A Multiple Changepoint Model|http://www.jstor.org/stable/30045252|30045252|2003-06-01|2003|['eng']|['Biological sciences - Biology']|['Science & Mathematics', 'Statistics']
In objective Bayesian model selection, no single criterion has emerged as dominant in defining objective prior distributions. Indeed, many criteria have been separately proposed and utilized to propose differing prior choices. We first formalize the most general and compelling of the various criteria that have been suggested, together with a new criterion. We then illustrate the potential of these criteria in determining objective model selection priors by considering their application to the problem of variable selection in normal linear models. This results in a new model selection objective prior with a number of compelling properties.|CRITERIA FOR BAYESIAN MODEL CHOICE WITH APPLICATION TO VARIABLE SELECTION|http://www.jstor.org/stable/41713685|41713685|2012-06-01|2012|['eng']|['Mathematics - Mathematical logic']|['Science & Mathematics', 'Statistics']
This article provides a Bayesian method of estimating the marginal posterior distributions for stochastic discount factors associated with observed asset returns. These estimates can be used to provide measures of fit for asset-pricing models and to identify broad features of the characteristics that should be explained. These measures of fit can be used to supplement model-evaluation exercises based on Hansen-Jagannathan bounds.|Bayesian Estimation of Stochastic Discount Factors|http://www.jstor.org/stable/1392249|1392249|1996-10-01|1996|['eng']|['Mathematics - Applied mathematics']|['Business & Economics', 'Science & Mathematics', 'Business', 'Economics', 'Statistics']
"We consider problems involving groups of data where each observation within a group is a draw from a mixture model and where it is desirable to share mixture components between groups. We assume that the number of mixture components is unknown a priori and is to be inferred from the data. In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group. Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, specifically one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process. Such a base measure being discrete, the child Dirichlet processes necessarily share atoms. Thus, as desired, the mixture models in the different groups necessarily share mixture components. We discuss representations of hierarchical Dirichlet processes in terms of a stick-breaking process, and a generalization of the Chinese restaurant process that we refer to as the ""Chinese restaurant franchise."" We present Markov chain Monte Carlo algorithms for posterior inference in hierarchical Dirichlet process mixtures and describe applications to problems in information retrieval and text modeling."|Hierarchical Dirichlet Processes|http://www.jstor.org/stable/27639773|27639773|2006-12-01|2006|['eng']|['Applied sciences - Engineering']|['Science & Mathematics', 'Statistics']
